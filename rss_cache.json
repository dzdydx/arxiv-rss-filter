{
  "https://rss.arxiv.org/rss/cs.CL+cs.CV+cs.MM+cs.LG+cs.SI": {
    "feed": {
      "title": "cs.CL, cs.CV, cs.MM, cs.LG, cs.SI updates on arXiv.org",
      "link": "http://rss.arxiv.org/rss/cs.CL+cs.CV+cs.MM+cs.LG+cs.SI",
      "updated": "Tue, 15 Apr 2025 04:09:58 +0000",
      "published": "Tue, 15 Apr 2025 00:00:00 -0400"
    },
    "entries": [
      {
        "id": "oai:arXiv.org:2504.08772v1",
        "title": "Reward Generation via Large Vision-Language Model in Offline Reinforcement Learning",
        "link": "https://arxiv.org/abs/2504.08772",
        "author": "Younghwan Lee, Tung M. Luu, Donghoon Lee, Chang D. Yoo",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08772v1 Announce Type: new \nAbstract: In offline reinforcement learning (RL), learning from fixed datasets presents a promising solution for domains where real-time interaction with the environment is expensive or risky. However, designing dense reward signals for offline dataset requires significant human effort and domain expertise. Reinforcement learning with human feedback (RLHF) has emerged as an alternative, but it remains costly due to the human-in-the-loop process, prompting interest in automated reward generation models. To address this, we propose Reward Generation via Large Vision-Language Models (RG-VLM), which leverages the reasoning capabilities of LVLMs to generate rewards from offline data without human involvement. RG-VLM improves generalization in long-horizon tasks and can be seamlessly integrated with the sparse reward signals to enhance task performance, demonstrating its potential as an auxiliary reward signal."
      },
      {
        "id": "oai:arXiv.org:2504.08775v1",
        "title": "Layers at Similar Depths Generate Similar Activations Across LLM Architectures",
        "link": "https://arxiv.org/abs/2504.08775",
        "author": "Christopher Wolfram, Aaron Schein",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08775v1 Announce Type: new \nAbstract: How do the latent spaces used by independently-trained LLMs relate to one another? We study the nearest neighbor relationships induced by activations at different layers of 24 open-weight LLMs, and find that they 1) tend to vary from layer to layer within a model, and 2) are approximately shared between corresponding layers of different models. Claim 2 shows that these nearest neighbor relationships are not arbitrary, as they are shared across models, but Claim 1 shows that they are not \"obvious\" either, as there is no single set of nearest neighbor relationships that is universally shared. Together, these suggest that LLMs generate a progression of activation geometries from layer to layer, but that this entire progression is largely shared between models, stretched and squeezed to fit into different architectures."
      },
      {
        "id": "oai:arXiv.org:2504.08776v1",
        "title": "SemCAFE: When Named Entities make the Difference Assessing Web Source Reliability through Entity-level Analytics",
        "link": "https://arxiv.org/abs/2504.08776",
        "author": "Gautam Kishore Shahi, Oshani Seneviratne, Marc Spaniol",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08776v1 Announce Type: new \nAbstract: With the shift from traditional to digital media, the online landscape now hosts not only reliable news articles but also a significant amount of unreliable content. Digital media has faster reachability by significantly influencing public opinion and advancing political agendas. While newspaper readers may be familiar with their preferred outlets political leanings or credibility, determining unreliable news articles is much more challenging. The credibility of many online sources is often opaque, with AI generated content being easily disseminated at minimal cost. Unreliable news articles, particularly those that followed the Russian invasion of Ukraine in 2022, closely mimic the topics and writing styles of credible sources, making them difficult to distinguish. To address this, we introduce SemCAFE, a system designed to detect news reliability by incorporating entity relatedness into its assessment. SemCAFE employs standard Natural Language Processing techniques, such as boilerplate removal and tokenization, alongside entity level semantic analysis using the YAGO knowledge base. By creating a semantic fingerprint for each news article, SemCAFE could assess the credibility of 46,020 reliable and 3,407 unreliable articles on the 2022 Russian invasion of Ukraine. Our approach improved the macro F1 score by 12% over state of the art methods. The sample data and code are available on GitHub"
      },
      {
        "id": "oai:arXiv.org:2504.08778v1",
        "title": "From Tokens to Lattices: Emergent Lattice Structures in Language Models",
        "link": "https://arxiv.org/abs/2504.08778",
        "author": "Bo Xiong, Steffen Staab",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08778v1 Announce Type: new \nAbstract: Pretrained masked language models (MLMs) have demonstrated an impressive capability to comprehend and encode conceptual knowledge, revealing a lattice structure among concepts. This raises a critical question: how does this conceptualization emerge from MLM pretraining? In this paper, we explore this problem from the perspective of Formal Concept Analysis (FCA), a mathematical framework that derives concept lattices from the observations of object-attribute relationships. We show that the MLM's objective implicitly learns a \\emph{formal context} that describes objects, attributes, and their dependencies, which enables the reconstruction of a concept lattice through FCA. We propose a novel framework for concept lattice construction from pretrained MLMs and investigate the origin of the inductive biases of MLMs in lattice structure learning. Our framework differs from previous work because it does not rely on human-defined concepts and allows for discovering \"latent\" concepts that extend beyond human definitions. We create three datasets for evaluation, and the empirical results verify our hypothesis."
      },
      {
        "id": "oai:arXiv.org:2504.08779v1",
        "title": "Can AI Master Construction Management (CM)? Benchmarking State-of-the-Art Large Language Models on CM Certification Exams",
        "link": "https://arxiv.org/abs/2504.08779",
        "author": "Ruoxin Xiong, Yanyu Wang, Suat Gunhan, Yimin Zhu, Charles Berryman",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08779v1 Announce Type: new \nAbstract: The growing complexity of construction management (CM) projects, coupled with challenges such as strict regulatory requirements and labor shortages, requires specialized analytical tools that streamline project workflow and enhance performance. Although large language models (LLMs) have demonstrated exceptional performance in general reasoning tasks, their effectiveness in tackling CM-specific challenges, such as precise quantitative analysis and regulatory interpretation, remains inadequately explored. To bridge this gap, this study introduces CMExamSet, a comprehensive benchmarking dataset comprising 689 authentic multiple-choice questions sourced from four nationally accredited CM certification exams. Our zero-shot evaluation assesses overall accuracy, subject areas (e.g., construction safety), reasoning complexity (single-step and multi-step), and question formats (text-only, figure-referenced, and table-referenced). The results indicate that GPT-4o and Claude 3.7 surpass typical human pass thresholds (70%), with average accuracies of 82% and 83%, respectively. Additionally, both models performed better on single-step tasks, with accuracies of 85.7% (GPT-4o) and 86.7% (Claude 3.7). Multi-step tasks were more challenging, reducing performance to 76.5% and 77.6%, respectively. Furthermore, both LLMs show significant limitations on figure-referenced questions, with accuracies dropping to approximately 40%. Our error pattern analysis further reveals that conceptual misunderstandings are the most common (44.4% and 47.9%), underscoring the need for enhanced domain-specific reasoning models. These findings underscore the potential of LLMs as valuable supplementary analytical tools in CM, while highlighting the need for domain-specific refinements and sustained human oversight in complex decision making."
      },
      {
        "id": "oai:arXiv.org:2504.08781v1",
        "title": "Efficient Evaluation of Large Language Models via Collaborative Filtering",
        "link": "https://arxiv.org/abs/2504.08781",
        "author": "Xu-Xiang Zhong, Chao Yi, Han-Jia Ye",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08781v1 Announce Type: new \nAbstract: With the development of Large Language Models (LLMs), numerous benchmarks have been proposed to measure and compare the capabilities of different LLMs. However, evaluating LLMs is costly due to the large number of test instances and their slow inference speed. In this paper, we aim to explore how to efficiently estimate a model's real performance on a given benchmark based on its evaluation results on a small number of instances sampled from the benchmark. Inspired by Collaborative Filtering (CF) in Recommendation Systems (RS), we treat LLMs as users and test instances as items and propose a two-stage method. In the first stage, we treat instance selection as recommending products to users to choose instances that can easily distinguish model performance. In the second stage, we see performance prediction as rating prediction problem in RS to predict the target LLM's behavior on unselected instances. Experiments on multiple LLMs and datasets imply that our method can accurately estimate the target model's performance while largely reducing its inference overhead."
      },
      {
        "id": "oai:arXiv.org:2504.08782v1",
        "title": "Embedding Hidden Adversarial Capabilities in Pre-Trained Diffusion Models",
        "link": "https://arxiv.org/abs/2504.08782",
        "author": "Lucas Beerens, Desmond J. Higham",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08782v1 Announce Type: new \nAbstract: We introduce a new attack paradigm that embeds hidden adversarial capabilities directly into diffusion models via fine-tuning, without altering their observable behavior or requiring modifications during inference. Unlike prior approaches that target specific images or adjust the generation process to produce adversarial outputs, our method integrates adversarial functionality into the model itself. The resulting tampered model generates high-quality images indistinguishable from those of the original, yet these images cause misclassification in downstream classifiers at a high rate. The misclassification can be targeted to specific output classes. Users can employ this compromised model unaware of its embedded adversarial nature, as it functions identically to a standard diffusion model. We demonstrate the effectiveness and stealthiness of our approach, uncovering a covert attack vector that raises new security concerns. These findings expose a risk arising from the use of externally-supplied models and highlight the urgent need for robust model verification and defense mechanisms against hidden threats in generative models. The code is available at https://github.com/LucasBeerens/CRAFTed-Diffusion ."
      },
      {
        "id": "oai:arXiv.org:2504.08792v1",
        "title": "Enhancing NER Performance in Low-Resource Pakistani Languages using Cross-Lingual Data Augmentation",
        "link": "https://arxiv.org/abs/2504.08792",
        "author": "Toqeer Ehsan, Thamar Solorio",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08792v1 Announce Type: new \nAbstract: Named Entity Recognition (NER), a fundamental task in Natural Language Processing (NLP), has shown significant advancements for high-resource languages. However, due to a lack of annotated datasets and limited representation in Pre-trained Language Models (PLMs), it remains understudied and challenging for low-resource languages. To address these challenges, we propose a data augmentation technique that generates culturally plausible sentences and experiments on four low-resource Pakistani languages; Urdu, Shahmukhi, Sindhi, and Pashto. By fine-tuning multilingual masked Large Language Models (LLMs), our approach demonstrates significant improvements in NER performance for Shahmukhi and Pashto. We further explore the capability of generative LLMs for NER and data augmentation using few-shot learning."
      },
      {
        "id": "oai:arXiv.org:2504.08798v1",
        "title": "Exploring Gradient-Guided Masked Language Model to Detect Textual Adversarial Attacks",
        "link": "https://arxiv.org/abs/2504.08798",
        "author": "Xiaomei Zhang, Zhaoxi Zhang, Yanjun Zhang, Xufei Zheng, Leo Yu Zhang, Shengshan Hu, Shirui Pan",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08798v1 Announce Type: new \nAbstract: Textual adversarial examples pose serious threats to the reliability of natural language processing systems. Recent studies suggest that adversarial examples tend to deviate from the underlying manifold of normal texts, whereas pre-trained masked language models can approximate the manifold of normal data. These findings inspire the exploration of masked language models for detecting textual adversarial attacks. We first introduce Masked Language Model-based Detection (MLMD), leveraging the mask and unmask operations of the masked language modeling (MLM) objective to induce the difference in manifold changes between normal and adversarial texts. Although MLMD achieves competitive detection performance, its exhaustive one-by-one masking strategy introduces significant computational overhead. Our posterior analysis reveals that a significant number of non-keywords in the input are not important for detection but consume resources. Building on this, we introduce Gradient-guided MLMD (GradMLMD), which leverages gradient information to identify and skip non-keywords during detection, significantly reducing resource consumption without compromising detection performance."
      },
      {
        "id": "oai:arXiv.org:2504.08801v1",
        "title": "Learnable Multi-Scale Wavelet Transformer: A Novel Alternative to Self-Attention",
        "link": "https://arxiv.org/abs/2504.08801",
        "author": "Andrew Kiruluta, Priscilla Burity, Samantha Williams",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08801v1 Announce Type: new \nAbstract: Transformer architectures, underpinned by the self-attention mechanism, have achieved state-of-the-art results across numerous natural language processing (NLP) tasks by effectively modeling long-range dependencies. However, the computational complexity of self-attention, scaling quadratically with input sequence length, presents significant challenges for processing very long sequences or operating under resource constraints. This paper introduces the Learnable Multi-Scale Wavelet Transformer (LMWT), a novel architecture that replaces the standard dot-product self-attention with a learnable multi-scale Haar wavelet transform module. Leveraging the intrinsic multi-resolution properties of wavelets, the LMWT efficiently captures both local details and global context. Crucially, the parameters of the wavelet transform, including scale-specific coefficients, are learned end-to-end during training, allowing the model to adapt its decomposition strategy to the data and task. We present the detailed mathematical formulation of the learnable Haar wavelet module and its integration into the transformer framework, supplemented by an architectural diagram. We conduct a comprehensive experimental evaluation on a standard machine translation benchmark (WMT16 En-De), comparing the LMWT against a baseline self-attention transformer using metrics like BLEU score, perplexity, and token accuracy. Furthermore, we analyze the computational complexity, highlighting the linear scaling of our approach, discuss its novelty in the context of related work, and explore the interpretability offered by visualizing the learned Haar coefficients. Our results indicate that the LMWT achieves competitive performance while offering substantial computational advantages, positioning it as a promising and novel alternative for efficient sequence modeling."
      },
      {
        "id": "oai:arXiv.org:2504.08802v1",
        "title": "InfoGain Wavelets: Furthering the Design of Diffusion Wavelets for Graph-Structured Data",
        "link": "https://arxiv.org/abs/2504.08802",
        "author": "David R. Johnson, Smita Krishnaswamy, Michael Perlmutter",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08802v1 Announce Type: new \nAbstract: Diffusion wavelets extract information from graph signals at different scales of resolution by utilizing graph diffusion operators raised to various powers, known as diffusion scales. Traditionally, the diffusion scales are chosen to be dyadic integers, $\\mathbf{2^j}$. Here, we propose a novel, unsupervised method for selecting the diffusion scales based on ideas from information theory. We then show that our method can be incorporated into wavelet-based GNNs via graph classification experiments."
      },
      {
        "id": "oai:arXiv.org:2504.08803v1",
        "title": "A temporal scale transformer framework for precise remaining useful life prediction in fuel cells",
        "link": "https://arxiv.org/abs/2504.08803",
        "author": "Zezhi Tang, Xiaoyu Chen, Xin Jin, Benyuan Zhang, Wenyu Liang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08803v1 Announce Type: new \nAbstract: In exploring Predictive Health Management (PHM) strategies for Proton Exchange Membrane Fuel Cells (PEMFC), the Transformer model, widely used in data-driven approaches, excels in many fields but struggles with time series analysis due to its self-attention mechanism, which yields a complexity of the input sequence squared and low computational efficiency. It also faces challenges in capturing both global long-term dependencies and local details effectively. To tackle this, we propose the Temporal Scale Transformer (TSTransformer), an enhanced version of the inverted Transformer (iTransformer). Unlike traditional Transformers that treat each timestep as an input token, TSTransformer maps sequences of varying lengths into tokens at different stages for inter-sequence modeling, using attention to capture multivariate correlations and feed-forward networks (FFN) to encode sequence representations. By integrating a one-dimensional convolutional layer into the multivariate attention for multi-level scaling of K and V matrices, it improves local feature extraction, captures temporal scale characteristics, and reduces token count and computational costs. Experiments comparing TSTransformer with models like Long Short-Term Memory, iTransformer, and Transformer demonstrate its potential as a powerful tool for advancing PHM in renewable energy, effectively addressing the limitations of pure Transformer models in data-driven time series tasks."
      },
      {
        "id": "oai:arXiv.org:2504.08808v1",
        "title": "Exploring the Effectiveness and Interpretability of Texts in LLM-based Time Series Models",
        "link": "https://arxiv.org/abs/2504.08808",
        "author": "Zhengke Sun, Hangwei Qian, Ivor Tsang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08808v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have been applied to time series forecasting tasks, leveraging pre-trained language models as the backbone and incorporating textual data to purportedly enhance the comprehensive capabilities of LLMs for time series. However, are these texts really helpful for interpretation? This study seeks to investigate the actual efficacy and interpretability of such textual incorporations. Through a series of empirical experiments on textual prompts and textual prototypes, our findings reveal that the misalignment between two modalities exists, and the textual information does not significantly improve time series forecasting performance in many cases. Furthermore, visualization analysis indicates that the textual representations learned by existing frameworks lack sufficient interpretability when applied to time series data. We further propose a novel metric named Semantic Matching Index (SMI) to better evaluate the matching degree between time series and texts during our post hoc interpretability investigation. Our analysis reveals the misalignment and limited interpretability of texts in current time-series LLMs, and we hope this study can raise awareness of the interpretability of texts for time series. The code is available at https://github.com/zachysun/TS-Lang-Exp."
      },
      {
        "id": "oai:arXiv.org:2504.08809v1",
        "title": "Decoupling Contrastive Decoding: Robust Hallucination Mitigation in Multimodal Large Language Models",
        "link": "https://arxiv.org/abs/2504.08809",
        "author": "Wei Chen, Xin Yan, Bin Wen, Fan Yang, Tingting Gao, Di Zhang, Long Chen",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08809v1 Announce Type: new \nAbstract: Although multimodal large language models (MLLMs) exhibit remarkable reasoning capabilities on complex multimodal understanding tasks, they still suffer from the notorious hallucination issue: generating outputs misaligned with obvious visual or factual evidence. Currently, training-based solutions, like direct preference optimization (DPO), leverage paired preference data to suppress hallucinations. However, they risk sacrificing general reasoning capabilities due to the likelihood displacement. Meanwhile, training-free solutions, like contrastive decoding, achieve this goal by subtracting the estimated hallucination pattern from a distorted input. Yet, these handcrafted perturbations (e.g., add noise to images) may poorly capture authentic hallucination patterns. To avoid these weaknesses of existing methods, and realize robust hallucination mitigation (i.e., maintaining general reasoning performance), we propose a novel framework: Decoupling Contrastive Decoding (DCD). Specifically, DCD decouples the learning of positive and negative samples in preference datasets, and trains separate positive and negative image projections within the MLLM. The negative projection implicitly models real hallucination patterns, which enables vision-aware negative images in the contrastive decoding inference stage. Our DCD alleviates likelihood displacement by avoiding pairwise optimization and generalizes robustly without handcrafted degradation. Extensive ablations across hallucination benchmarks and general reasoning tasks demonstrate the effectiveness of DCD, i.e., it matches DPO's hallucination suppression while preserving general capabilities and outperforms the handcrafted contrastive decoding methods."
      },
      {
        "id": "oai:arXiv.org:2504.08810v1",
        "title": "PriM: Principle-Inspired Material Discovery through Multi-Agent Collaboration",
        "link": "https://arxiv.org/abs/2504.08810",
        "author": "Zheyuan Lai, Yingming Pu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08810v1 Announce Type: new \nAbstract: Complex chemical space and limited knowledge scope with biases holds immense challenge for human scientists, yet in automated materials discovery. Existing intelligent methods relies more on numerical computation, leading to inefficient exploration and results with hard-interpretability. To bridge this gap, we introduce a principles-guided material discovery system powered by language inferential multi-agent system (MAS), namely PriM. Our framework integrates automated hypothesis generation with experimental validation in a roundtable system of MAS, enabling systematic exploration while maintaining scientific rigor. Based on our framework, the case study of nano helix demonstrates higher materials exploration rate and property value while providing transparent reasoning pathways. This approach develops an automated-and-transparent paradigm for material discovery, with broad implications for rational design of functional materials. Code is publicly available at our \\href{https://github.com/amair-lab/PriM}{GitHub}."
      },
      {
        "id": "oai:arXiv.org:2504.08811v1",
        "title": "Analogical Learning for Cross-Scenario Generalization: Framework and Application to Intelligent Localization",
        "link": "https://arxiv.org/abs/2504.08811",
        "author": "Zirui Chen, Zhaoyang Zhang, Ziqing Xing, Ridong Li, Zhaohui Yang, Richeng Jin, Chongwen Huang, Yuzhi Yang, M\\'erouane Debbah",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08811v1 Announce Type: new \nAbstract: Existing learning models often exhibit poor generalization when deployed across diverse scenarios. It is mainly due to that the underlying reference frame of the data varies with the deployment environment and settings. However, despite the data of each scenario has its distinct reference frame, its generation generally follows the same underlying physical rule. Based on these findings, this article proposes a brand-new universal deep learning framework named analogical learning (AL), which provides a highly efficient way to implicitly retrieve the reference frame information associated with a scenario and then to make accurate prediction by relative analogy across scenarios. Specifically, an elegant bipartite neural network architecture called Mateformer is designed, the first part of which calculates the relativity within multiple feature spaces between the input data and a small amount of embedded data from the current scenario, while the second part uses these relativity to guide the nonlinear analogy. We apply AL to the typical multi-scenario learning problem of intelligent wireless localization in cellular networks. Extensive experiments show that AL achieves state-of-the-art accuracy, stable transferability and robust adaptation to new scenarios without any tuning, and outperforming conventional methods with a precision improvement of nearly two orders of magnitude. All data and code are available at https://github.com/ziruichen-research/ALLoc."
      },
      {
        "id": "oai:arXiv.org:2504.08812v1",
        "title": "Mechanistic Anomaly Detection for \"Quirky\" Language Models",
        "link": "https://arxiv.org/abs/2504.08812",
        "author": "David O. Johnston, Arkajyoti Chakraborty, Nora Belrose",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08812v1 Announce Type: new \nAbstract: As LLMs grow in capability, the task of supervising LLMs becomes more challenging. Supervision failures can occur if LLMs are sensitive to factors that supervisors are unaware of. We investigate Mechanistic Anomaly Detection (MAD) as a technique to augment supervision of capable models; we use internal model features to identify anomalous training signals so they can be investigated or discarded. We train detectors to flag points from the test environment that differ substantially from the training environment, and experiment with a large variety of detector features and scoring rules to detect anomalies in a set of ``quirky'' language models. We find that detectors can achieve high discrimination on some tasks, but no detector is effective across all models and tasks. MAD techniques may be effective in low-stakes applications, but advances in both detection and evaluation are likely needed if they are to be used in high stakes settings."
      },
      {
        "id": "oai:arXiv.org:2504.08813v1",
        "title": "SafeMLRM: Demystifying Safety in Multi-modal Large Reasoning Models",
        "link": "https://arxiv.org/abs/2504.08813",
        "author": "Junfeng Fang, Yukai Wang, Ruipeng Wang, Zijun Yao, Kun Wang, An Zhang, Xiang Wang, Tat-Seng Chua",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08813v1 Announce Type: new \nAbstract: The rapid advancement of multi-modal large reasoning models (MLRMs) -- enhanced versions of multimodal language models (MLLMs) equipped with reasoning capabilities -- has revolutionized diverse applications. However, their safety implications remain underexplored. While prior work has exposed critical vulnerabilities in unimodal reasoning models, MLRMs introduce distinct risks from cross-modal reasoning pathways. This work presents the first systematic safety analysis of MLRMs through large-scale empirical studies comparing MLRMs with their base MLLMs. Our experiments reveal three critical findings: (1) The Reasoning Tax: Acquiring reasoning capabilities catastrophically degrades inherited safety alignment. MLRMs exhibit 37.44% higher jailbreaking success rates than base MLLMs under adversarial attacks. (2) Safety Blind Spots: While safety degradation is pervasive, certain scenarios (e.g., Illegal Activity) suffer 25 times higher attack rates -- far exceeding the average 3.4 times increase, revealing scenario-specific vulnerabilities with alarming cross-model and datasets consistency. (3) Emergent Self-Correction: Despite tight reasoning-answer safety coupling, MLRMs demonstrate nascent self-correction -- 16.9% of jailbroken reasoning steps are overridden by safe answers, hinting at intrinsic safeguards. These findings underscore the urgency of scenario-aware safety auditing and mechanisms to amplify MLRMs' self-correction potential. To catalyze research, we open-source OpenSafeMLRM, the first toolkit for MLRM safety evaluation, providing unified interface for mainstream models, datasets, and jailbreaking methods. Our work calls for immediate efforts to harden reasoning-augmented AI, ensuring its transformative potential aligns with ethical safeguards."
      },
      {
        "id": "oai:arXiv.org:2504.08816v1",
        "title": "A Graph-Enhanced DeepONet Approach for Real-Time Estimating Hydrogen-Enriched Natural Gas Flow under Variable Operations",
        "link": "https://arxiv.org/abs/2504.08816",
        "author": "Sicheng Liu, Hongchang Huang, Bo Yang, Mingxuan Cai, Xu Yang, Xinping Guan",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08816v1 Announce Type: new \nAbstract: Blending green hydrogen into natural gas presents a promising approach for renewable energy integration and fuel decarbonization. Accurate estimation of hydrogen fraction in hydrogen-enriched natural gas (HENG) pipeline networks is crucial for operational safety and efficiency, yet it remains challenging due to complex dynamics. While existing data-driven approaches adopt end-to-end architectures for HENG flow state estimation, their limited adaptability to varying operational conditions hinders practical applications. To this end, this study proposes a graph-enhanced DeepONet framework for the real-time estimation of HENG flow, especially hydrogen fractions. First, a dual-network architecture, called branch network and trunk network, is employed to characterize operational conditions and sparse sensor measurements to estimate the HENG state at targeted locations and time points. Second, a graph-enhance branch network is proposed to incorporate pipeline topology, improving the estimation accuracy in large-scale pipeline networks. Experimental results demonstrate that the proposed method achieves superior estimation accuracy for HCNG flow under varying operational conditions compared to conventional approaches."
      },
      {
        "id": "oai:arXiv.org:2504.08818v1",
        "title": "From Text to Time? Rethinking the Effectiveness of the Large Language Model for Time Series Forecasting",
        "link": "https://arxiv.org/abs/2504.08818",
        "author": "Xinyu Zhang, Shanshan Feng, Xutao Li",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08818v1 Announce Type: new \nAbstract: Using pre-trained large language models (LLMs) as the backbone for time series prediction has recently gained significant research interest. However, the effectiveness of LLM backbones in this domain remains a topic of debate. Based on thorough empirical analyses, we observe that training and testing LLM-based models on small datasets often leads to the Encoder and Decoder becoming overly adapted to the dataset, thereby obscuring the true predictive capabilities of the LLM backbone. To investigate the genuine potential of LLMs in time series prediction, we introduce three pre-training models with identical architectures but different pre-training strategies. Thereby, large-scale pre-training allows us to create unbiased Encoder and Decoder components tailored to the LLM backbone. Through controlled experiments, we evaluate the zero-shot and few-shot prediction performance of the LLM, offering insights into its capabilities. Extensive experiments reveal that although the LLM backbone demonstrates some promise, its forecasting performance is limited. Our source code is publicly available in the anonymous repository: https://anonymous.4open.science/r/LLM4TS-0B5C."
      },
      {
        "id": "oai:arXiv.org:2504.08820v1",
        "title": "CAReDiO: Cultural Alignment of LLM via Representativeness and Distinctiveness Guided Data Optimization",
        "link": "https://arxiv.org/abs/2504.08820",
        "author": "Jing Yao, Xiaoyuan Yi, Jindong Wang, Zhicheng Dou, Xing Xie",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08820v1 Announce Type: new \nAbstract: As Large Language Models (LLMs) more deeply integrate into human life across various regions, aligning them with pluralistic cultures is crucial for improving user experience and mitigating cultural conflicts. Existing approaches develop culturally aligned LLMs primarily through fine-tuning with massive carefully curated culture-specific corpora. Nevertheless, inspired by culture theories, we identify two key challenges faced by these datasets: (1) Representativeness: These corpora fail to fully capture the target culture's core characteristics with redundancy, causing computation waste; (2) Distinctiveness: They struggle to distinguish the unique nuances of a given culture from shared patterns across other relevant ones, hindering precise cultural modeling. To handle these challenges, we introduce CAReDiO, a novel cultural data construction framework. Specifically, CAReDiO utilizes powerful LLMs to automatically generate cultural conversation data, where both the queries and responses are further optimized by maximizing representativeness and distinctiveness. Using CAReDiO, we construct a small yet effective dataset, covering five cultures, and compare it with several recent cultural corpora. Extensive experiments demonstrate that our method generates more effective data and enables cultural alignment with as few as 100 training samples, enhancing both performance and efficiency."
      },
      {
        "id": "oai:arXiv.org:2504.08821v1",
        "title": "Probabilistic QoS Metric Forecasting in Delay-Tolerant Networks Using Conditional Diffusion Models on Latent Dynamics",
        "link": "https://arxiv.org/abs/2504.08821",
        "author": "Enming Zhang, Zheng Liu, Yu Xiang, Yanwen Qu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08821v1 Announce Type: new \nAbstract: Active QoS metric prediction, commonly employed in the maintenance and operation of DTN, could enhance network performance regarding latency, throughput, energy consumption, and dependability. Naturally formulated as a multivariate time series forecasting problem, it attracts substantial research efforts. Traditional mean regression methods for time series forecasting cannot capture the data complexity adequately, resulting in deteriorated performance in operational tasks in DTNs such as routing. This paper formulates the prediction of QoS metrics in DTN as a probabilistic forecasting problem on multivariate time series, where one could quantify the uncertainty of forecasts by characterizing the distribution of these samples. The proposed approach hires diffusion models and incorporates the latent temporal dynamics of non-stationary and multi-mode data into them. Extensive experiments demonstrate the efficacy of the proposed approach by showing that it outperforms the popular probabilistic time series forecasting methods."
      },
      {
        "id": "oai:arXiv.org:2504.08823v1",
        "title": "FM-LoRA: Factorized Low-Rank Meta-Prompting for Continual Learning",
        "link": "https://arxiv.org/abs/2504.08823",
        "author": "Xiaobing Yu, Jin Yang, Xiao Wu, Peijie Qiu, Xiaofeng Liu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08823v1 Announce Type: new \nAbstract: How to adapt a pre-trained model continuously for sequential tasks with different prediction class labels and domains and finally learn a generalizable model across diverse tasks is a long-lasting challenge. Continual learning (CL) has emerged as a promising approach to leverage pre-trained models (e.g., Transformers) for sequential tasks. While many existing CL methods incrementally store additional learned structures, such as Low-Rank Adaptation (LoRA) adapters or prompts and sometimes even preserve features from previous samples to maintain performance. This leads to unsustainable parameter growth and escalating storage costs as the number of tasks increases. Moreover, current approaches often lack task similarity awareness, which further hinders the models ability to effectively adapt to new tasks without interfering with previously acquired knowledge. To address these challenges, we propose FM-LoRA, a novel and efficient low-rank adaptation method that integrates both a dynamic rank selector (DRS) and dynamic meta-prompting (DMP). This framework allocates model capacity more effectively across tasks by leveraging a shared low-rank subspace critical for preserving knowledge, thereby avoiding continual parameter expansion. Extensive experiments on various CL benchmarks, including ImageNet-R, CIFAR100, and CUB200 for class-incremental learning (CIL), and DomainNet for domain-incremental learning (DIL), with Transformers backbone demonstrate that FM-LoRA effectively mitigates catastrophic forgetting while delivering robust performance across a diverse range of tasks and domains."
      },
      {
        "id": "oai:arXiv.org:2504.08824v1",
        "title": "ColonScopeX: Leveraging Explainable Expert Systems with Multimodal Data for Improved Early Diagnosis of Colorectal Cancer",
        "link": "https://arxiv.org/abs/2504.08824",
        "author": "Natalia Sikora, Robert L. Manschke, Alethea M. Tang, Peter Dunstan, Dean A. Harris, Su Yang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08824v1 Announce Type: new \nAbstract: Colorectal cancer (CRC) ranks as the second leading cause of cancer-related deaths and the third most prevalent malignant tumour worldwide. Early detection of CRC remains problematic due to its non-specific and often embarrassing symptoms, which patients frequently overlook or hesitate to report to clinicians. Crucially, the stage at which CRC is diagnosed significantly impacts survivability, with a survival rate of 80-95\\% for Stage I and a stark decline to 10\\% for Stage IV. Unfortunately, in the UK, only 14.4\\% of cases are diagnosed at the earliest stage (Stage I).\n  In this study, we propose ColonScopeX, a machine learning framework utilizing explainable AI (XAI) methodologies to enhance the early detection of CRC and pre-cancerous lesions. Our approach employs a multimodal model that integrates signals from blood sample measurements, processed using the Savitzky-Golay algorithm for fingerprint smoothing, alongside comprehensive patient metadata, including medication history, comorbidities, age, weight, and BMI. By leveraging XAI techniques, we aim to render the model's decision-making process transparent and interpretable, thereby fostering greater trust and understanding in its predictions. The proposed framework could be utilised as a triage tool or a screening tool of the general population.\n  This research highlights the potential of combining diverse patient data sources and explainable machine learning to tackle critical challenges in medical diagnostics."
      },
      {
        "id": "oai:arXiv.org:2504.08827v1",
        "title": "PatchTrAD: A Patch-Based Transformer focusing on Patch-Wise Reconstruction Error for Time Series Anomaly Detection",
        "link": "https://arxiv.org/abs/2504.08827",
        "author": "Samy-Melwan Vilhes (LITIS), Gilles Gasso (LITIS), Mokhtar Z Alaya (LMAC)",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08827v1 Announce Type: new \nAbstract: Time series anomaly detection (TSAD) focuses on identifying whether observations in streaming data deviate significantly from normal patterns. With the prevalence of connected devices, anomaly detection on time series has become paramount, as it enables real-time monitoring and early detection of irregular behaviors across various application domains. In this work, we introduce PatchTrAD, a Patch-based Transformer model for time series anomaly detection. Our approach leverages a Transformer encoder along with the use of patches under a reconstructionbased framework for anomaly detection. Empirical evaluations on multiple benchmark datasets show that PatchTrAD is on par, in terms of detection performance, with state-of-the-art deep learning models for anomaly detection while being time efficient during inference."
      },
      {
        "id": "oai:arXiv.org:2504.08829v1",
        "title": "Datum-wise Transformer for Synthetic Tabular Data Detection in the Wild",
        "link": "https://arxiv.org/abs/2504.08829",
        "author": "G. Charbel N. Kindji (IRISA, MALT), Elisa Fromont (MALT, IRISA), Lina Maria Rojas-Barahona, Tanguy Urvoy",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08829v1 Announce Type: new \nAbstract: The growing power of generative models raises major concerns about the authenticity of published content. To address this problem, several synthetic content detection methods have been proposed for uniformly structured media such as image or text. However, little work has been done on the detection of synthetic tabular data, despite its importance in industry and government. This form of data is complex to handle due to the diversity of its structures: the number and types of the columns may vary wildly from one table to another. We tackle the tough problem of detecting synthetic tabular data ''in the wild'', i.e. when the model is deployed on table structures it has never seen before. We introduce a novel datum-wise transformer architecture and show that it outperforms existing models. Furthermore, we investigate the application of domain adaptation techniques to enhance the effectiveness of our model, thereby providing a more robust data-forgery detection solution."
      },
      {
        "id": "oai:arXiv.org:2504.08837v1",
        "title": "VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning",
        "link": "https://arxiv.org/abs/2504.08837",
        "author": "Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, Wenhu Chen",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08837v1 Announce Type: new \nAbstract: Recently, slow-thinking systems like GPT-o1 and DeepSeek-R1 have demonstrated great potential in solving challenging problems through explicit reflection. They significantly outperform the best fast-thinking models, such as GPT-4o, on various math and science benchmarks. However, their multimodal reasoning capabilities remain on par with fast-thinking models. For instance, GPT-o1's performance on benchmarks like MathVista, MathVerse, and MathVision is similar to fast-thinking models. In this paper, we aim to enhance the slow-thinking capabilities of vision-language models using reinforcement learning (without relying on distillation) to advance the state of the art. First, we adapt the GRPO algorithm with a novel technique called Selective Sample Replay (SSR) to address the vanishing advantages problem. While this approach yields strong performance, the resulting RL-trained models exhibit limited self-reflection or self-verification. To further encourage slow-thinking, we introduce Forced Rethinking, which appends a textual rethinking trigger to the end of initial rollouts in RL training, explicitly enforcing a self-reflection reasoning step. By combining these two techniques, our model, VL-Rethinker, advances state-of-the-art scores on MathVista, MathVerse, and MathVision to achieve 80.3%, 61.8%, and 43.9% respectively. VL-Rethinker also achieves open-source SoTA on multi-disciplinary benchmarks such as MMMU-Pro, EMMA, and MEGA-Bench, narrowing the gap with GPT-o1."
      },
      {
        "id": "oai:arXiv.org:2504.08838v1",
        "title": "SD$^2$: Self-Distilled Sparse Drafters",
        "link": "https://arxiv.org/abs/2504.08838",
        "author": "Mike Lasby, Nish Sinnadurai, Valavan Manohararajah, Sean Lie, Vithursan Thangarasa",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08838v1 Announce Type: new \nAbstract: Speculative decoding is a powerful technique for reducing the latency of Large Language Models (LLMs), offering a fault-tolerant framework that enables the use of highly compressed draft models. In this work, we introduce Self-Distilled Sparse Drafters (SD$^2$), a novel methodology that leverages self-data distillation and fine-grained weight sparsity to produce highly efficient and well-aligned draft models. SD$^2$ systematically enhances draft token acceptance rates while significantly reducing Multiply-Accumulate operations (MACs), even in the Universal Assisted Generation (UAG) setting, where draft and target models originate from different model families. On a Llama-3.1-70B target model, SD$^2$ provides a $\\times$1.59 higher Mean Accepted Length (MAL) compared to layer-pruned draft models and reduces MACs by over 43.87% with a 8.36% reduction in MAL compared to a dense draft models. Our results highlight the potential of sparsity-aware fine-tuning and compression strategies to improve LLM inference efficiency while maintaining alignment with target models."
      },
      {
        "id": "oai:arXiv.org:2504.08840v1",
        "title": "Adaptive Shrinkage Estimation For Personalized Deep Kernel Regression In Modeling Brain Trajectories",
        "link": "https://arxiv.org/abs/2504.08840",
        "author": "Vasiliki Tassopoulou, Haochang Shou, Christos Davatzikos",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08840v1 Announce Type: new \nAbstract: Longitudinal biomedical studies monitor individuals over time to capture dynamics in brain development, disease progression, and treatment effects. However, estimating trajectories of brain biomarkers is challenging due to biological variability, inconsistencies in measurement protocols (e.g., differences in MRI scanners), scarcity, and irregularity in longitudinal measurements. Herein, we introduce a novel personalized deep kernel regression framework for forecasting brain biomarkers, with application to regional volumetric measurements. Our approach integrates two key components: a population model that captures brain trajectories from a large and diverse cohort, and a subject-specific model that captures individual trajectories. To optimally combine these, we propose Adaptive Shrinkage Estimation, which effectively balances population and subject-specific models. We assess our model's performance through predictive accuracy metrics, uncertainty quantification, and validation against external clinical studies. Benchmarking against state-of-the-art statistical and machine learning models -- including linear mixed effects models, generalized additive models, and deep learning methods -- demonstrates the superior predictive performance of our approach. Additionally, we apply our method to predict trajectories of composite neuroimaging biomarkers, which highlights the versatility of our approach in modeling the progression of longitudinal neuroimaging biomarkers. Furthermore, validation on three external neuroimaging studies confirms the robustness of our method across different clinical contexts. We make the code available at https://github.com/vatass/AdaptiveShrinkageDKGP."
      },
      {
        "id": "oai:arXiv.org:2504.08842v1",
        "title": "Towards Combinatorial Interpretability of Neural Computation",
        "link": "https://arxiv.org/abs/2504.08842",
        "author": "Micah Adler, Dan Alistarh, Nir Shavit",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08842v1 Announce Type: new \nAbstract: We introduce combinatorial interpretability, a methodology for understanding neural computation by analyzing the combinatorial structures in the sign-based categorization of a network's weights and biases. We demonstrate its power through feature channel coding, a theory that explains how neural networks compute Boolean expressions and potentially underlies other categories of neural network computation. According to this theory, features are computed via feature channels: unique cross-neuron encodings shared among the inputs the feature operates on. Because different feature channels share neurons, the neurons are polysemantic and the channels interfere with one another, making the computation appear inscrutable.\n  We show how to decipher these computations by analyzing a network's feature channel coding, offering complete mechanistic interpretations of several small neural networks that were trained with gradient descent. Crucially, this is achieved via static combinatorial analysis of the weight matrices, without examining activations or training new autoencoding networks. Feature channel coding reframes the superposition hypothesis, shifting the focus from neuron activation directionality in high-dimensional space to the combinatorial structure of codes. It also allows us for the first time to exactly quantify and explain the relationship between a network's parameter size and its computational capacity (i.e. the set of features it can compute with low error), a relationship that is implicitly at the core of many modern scaling laws.\n  Though our initial studies of feature channel coding are restricted to Boolean functions, we believe they provide a rich, controlled, and informative research space, and that the path we propose for combinatorial interpretation of neural computation can provide a basis for understanding both artificial and biological neural circuits."
      },
      {
        "id": "oai:arXiv.org:2504.08851v1",
        "title": "Mimic In-Context Learning for Multimodal Tasks",
        "link": "https://arxiv.org/abs/2504.08851",
        "author": "Yuchu Jiang, Jiale Fu, Chenduo Hao, Xinting Hu, Yingzhe Peng, Xin Geng, Xu Yang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08851v1 Announce Type: new \nAbstract: Recently, In-context Learning (ICL) has become a significant inference paradigm in Large Multimodal Models (LMMs), utilizing a few in-context demonstrations (ICDs) to prompt LMMs for new tasks. However, the synergistic effects in multimodal data increase the sensitivity of ICL performance to the configurations of ICDs, stimulating the need for a more stable and general mapping function. Mathematically, in Transformer-based models, ICDs act as ``shift vectors'' added to the hidden states of query tokens. Inspired by this, we introduce Mimic In-Context Learning (MimIC) to learn stable and generalizable shift effects from ICDs. Specifically, compared with some previous shift vector-based methods, MimIC more strictly approximates the shift effects by integrating lightweight learnable modules into LMMs with four key enhancements: 1) inserting shift vectors after attention layers, 2) assigning a shift vector to each attention head, 3) making shift magnitude query-dependent, and 4) employing a layer-wise alignment loss. Extensive experiments on two LMMs (Idefics-9b and Idefics2-8b-base) across three multimodal tasks (VQAv2, OK-VQA, Captioning) demonstrate that MimIC outperforms existing shift vector-based methods. The code is available at https://github.com/Kamichanw/MimIC."
      },
      {
        "id": "oai:arXiv.org:2504.08852v1",
        "title": "ML For Hardware Design Interpretability: Challenges and Opportunities",
        "link": "https://arxiv.org/abs/2504.08852",
        "author": "Raymond Baartmans, Andrew Ensinger, Victor Agostinelli, Lizhong Chen",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08852v1 Announce Type: new \nAbstract: The increasing size and complexity of machine learning (ML) models have driven the growing need for custom hardware accelerators capable of efficiently supporting ML workloads. However, the design of such accelerators remains a time-consuming process, heavily relying on engineers to manually ensure design interpretability through clear documentation and effective communication. Recent advances in large language models (LLMs) offer a promising opportunity to automate these design interpretability tasks, particularly the generation of natural language descriptions for register-transfer level (RTL) code, what we refer to as \"RTL-to-NL tasks.\" In this paper, we examine how design interpretability, particularly in RTL-to-NL tasks, influences the efficiency of the hardware design process. We review existing work adapting LLMs for these tasks, highlight key challenges that remain unaddressed, including those related to data, computation, and model development, and identify opportunities to address them. By doing so, we aim to guide future research in leveraging ML to automate RTL-to-NL tasks and improve hardware design interpretability, thereby accelerating the hardware design process and meeting the increasing demand for custom hardware accelerators in machine learning and beyond."
      },
      {
        "id": "oai:arXiv.org:2504.08866v1",
        "title": "On Transfer-based Universal Attacks in Pure Black-box Setting",
        "link": "https://arxiv.org/abs/2504.08866",
        "author": "Mohammad A. A. K. Jalwana, Naveed Akhtar, Ajmal Mian, Nazanin Rahnavard, Mubarak Shah",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08866v1 Announce Type: new \nAbstract: Despite their impressive performance, deep visual models are susceptible to transferable black-box adversarial attacks. Principally, these attacks craft perturbations in a target model-agnostic manner. However, surprisingly, we find that existing methods in this domain inadvertently take help from various priors that violate the black-box assumption such as the availability of the dataset used to train the target model, and the knowledge of the number of classes in the target model. Consequently, the literature fails to articulate the true potency of transferable black-box attacks. We provide an empirical study of these biases and propose a framework that aids in a prior-free transparent study of this paradigm. Using our framework, we analyze the role of prior knowledge of the target model data and number of classes in attack performance. We also provide several interesting insights based on our analysis, and demonstrate that priors cause overestimation in transferability scores. Finally, we extend our framework to query-based attacks. This extension inspires a novel image-blending technique to prepare data for effective surrogate model training."
      },
      {
        "id": "oai:arXiv.org:2504.08867v1",
        "title": "In almost all shallow analytic neural network optimization landscapes, efficient minimizers have strongly convex neighborhoods",
        "link": "https://arxiv.org/abs/2504.08867",
        "author": "Felix Benning, Steffen Dereich",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08867v1 Announce Type: new \nAbstract: Whether or not a local minimum of a cost function has a strongly convex neighborhood greatly influences the asymptotic convergence rate of optimizers. In this article, we rigorously analyze the prevalence of this property for the mean squared error induced by shallow, 1-hidden layer neural networks with analytic activation functions when applied to regression problems. The parameter space is divided into two domains: the 'efficient domain' (all parameters for which the respective realization function cannot be generated by a network having a smaller number of neurons) and the 'redundant domain' (the remaining parameters). In almost all regression problems on the efficient domain the optimization landscape only features local minima that are strongly convex. Formally, we will show that for certain randomly picked regression problems the optimization landscape is almost surely a Morse function on the efficient domain. The redundant domain has significantly smaller dimension than the efficient domain and on this domain, potential local minima are never isolated."
      },
      {
        "id": "oai:arXiv.org:2504.08872v1",
        "title": "Personalizing Federated Learning for Hierarchical Edge Networks with Non-IID Data",
        "link": "https://arxiv.org/abs/2504.08872",
        "author": "Seunghyun Lee, Omid Tavallaie, Shuaijun Chen, Kanchana Thilakarathna, Suranga Seneviratne, Adel Nadjaran Toosi, Albert Y. Zomaya",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08872v1 Announce Type: new \nAbstract: Accommodating edge networks between IoT devices and the cloud server in Hierarchical Federated Learning (HFL) enhances communication efficiency without compromising data privacy. However, devices connected to the same edge often share geographic or contextual similarities, leading to varying edge-level data heterogeneity with different subsets of labels per edge, on top of device-level heterogeneity. This hierarchical non-Independent and Identically Distributed (non-IID) nature, which implies that each edge has its own optimization goal, has been overlooked in HFL research. Therefore, existing edge-accommodated HFL demonstrates inconsistent performance across edges in various hierarchical non-IID scenarios. To ensure robust performance with diverse edge-level non-IID data, we propose a Personalized Hierarchical Edge-enabled Federated Learning (PHE-FL), which personalizes each edge model to perform well on the unique class distributions specific to each edge. We evaluated PHE-FL across 4 scenarios with varying levels of edge-level non-IIDness, with extreme IoT device level non-IIDness. To accurately assess the effectiveness of our personalization approach, we deployed test sets on each edge server instead of the cloud server, and used both balanced and imbalanced test sets. Extensive experiments show that PHE-FL achieves up to 83 percent higher accuracy compared to existing federated learning approaches that incorporate edge networks, given the same number of training rounds. Moreover, PHE-FL exhibits improved stability, as evidenced by reduced accuracy fluctuations relative to the state-of-the-art FedAvg with two-level (edge and cloud) aggregation."
      },
      {
        "id": "oai:arXiv.org:2504.08874v1",
        "title": "Distilling and exploiting quantitative insights from Large Language Models for enhanced Bayesian optimization of chemical reactions",
        "link": "https://arxiv.org/abs/2504.08874",
        "author": "Roshan Patel, Saeed Moayedpour, Louis De Lescure, Lorenzo Kogler-Anele, Alan Cherney, Sven Jager, Yasser Jangjou",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08874v1 Announce Type: new \nAbstract: Machine learning and Bayesian optimization (BO) algorithms can significantly accelerate the optimization of chemical reactions. Transfer learning can bolster the effectiveness of BO algorithms in low-data regimes by leveraging pre-existing chemical information or data outside the direct optimization task (i.e., source data). Large language models (LLMs) have demonstrated that chemical information present in foundation training data can give them utility for processing chemical data. Furthermore, they can be augmented with and help synthesize potentially multiple modalities of source chemical data germane to the optimization task. In this work, we examine how chemical information from LLMs can be elicited and used for transfer learning to accelerate the BO of reaction conditions to maximize yield. Specifically, we show that a survey-like prompting scheme and preference learning can be used to infer a utility function which models prior chemical information embedded in LLMs over a chemical parameter space; we find that the utility function shows modest correlation to true experimental measurements (yield) over the parameter space despite operating in a zero-shot setting. Furthermore, we show that the utility function can be leveraged to focus BO efforts in promising regions of the parameter space, improving the yield of the initial BO query and enhancing optimization in 4 of the 6 datasets studied. Overall, we view this work as a step towards bridging the gap between the chemistry knowledge embedded in LLMs and the capabilities of principled BO methods to accelerate reaction optimization."
      },
      {
        "id": "oai:arXiv.org:2504.08877v1",
        "title": "The SERENADE project: Sensor-Based Explainable Detection of Cognitive Decline",
        "link": "https://arxiv.org/abs/2504.08877",
        "author": "Gabriele Civitarese, Michele Fiori, Andrea Arighi, Daniela Galimberti, Graziana Florio, Claudio Bettini",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08877v1 Announce Type: new \nAbstract: Mild Cognitive Impairment (MCI) affects 12-18% of individuals over 60. MCI patients exhibit cognitive dysfunctions without significant daily functional loss. While MCI may progress to dementia, predicting this transition remains a clinical challenge due to limited and unreliable indicators. Behavioral changes, like in the execution of Activities of Daily Living (ADLs), can signal such progression. Sensorized smart homes and wearable devices offer an innovative solution for continuous, non-intrusive monitoring ADLs for MCI patients. However, current machine learning models for detecting behavioral changes lack transparency, hindering clinicians' trust. This paper introduces the SERENADE project, a European Union-funded initiative that aims to detect and explain behavioral changes associated with cognitive decline using explainable AI methods. SERENADE aims at collecting one year of data from 30 MCI patients living alone, leveraging AI to support clinical decision-making and offering a new approach to early dementia detection."
      },
      {
        "id": "oai:arXiv.org:2504.08893v1",
        "title": "Knowledge Graph-extended Retrieval Augmented Generation for Question Answering",
        "link": "https://arxiv.org/abs/2504.08893",
        "author": "Jasper Linders, Jakub M. Tomczak",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08893v1 Announce Type: new \nAbstract: Large Language Models (LLMs) and Knowledge Graphs (KGs) offer a promising approach to robust and explainable Question Answering (QA). While LLMs excel at natural language understanding, they suffer from knowledge gaps and hallucinations. KGs provide structured knowledge but lack natural language interaction. Ideally, an AI system should be both robust to missing facts as well as easy to communicate with. This paper proposes such a system that integrates LLMs and KGs without requiring training, ensuring adaptability across different KGs with minimal human effort. The resulting approach can be classified as a specific form of a Retrieval Augmented Generation (RAG) with a KG, thus, it is dubbed Knowledge Graph-extended Retrieval Augmented Generation (KG-RAG). It includes a question decomposition module to enhance multi-hop information retrieval and answer explainability. Using In-Context Learning (ICL) and Chain-of-Thought (CoT) prompting, it generates explicit reasoning chains processed separately to improve truthfulness. Experiments on the MetaQA benchmark show increased accuracy for multi-hop questions, though with a slight trade-off in single-hop performance compared to LLM with KG baselines. These findings demonstrate KG-RAG's potential to improve transparency in QA by bridging unstructured language understanding with structured knowledge retrieval."
      },
      {
        "id": "oai:arXiv.org:2504.08896v1",
        "title": "Position: Beyond Euclidean -- Foundation Models Should Embrace Non-Euclidean Geometries",
        "link": "https://arxiv.org/abs/2504.08896",
        "author": "Neil He, Jiahong Liu, Buze Zhang, Ngoc Bui, Ali Maatouk, Menglin Yang, Irwin King, Melanie Weber, Rex Ying",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08896v1 Announce Type: new \nAbstract: In the era of foundation models and Large Language Models (LLMs), Euclidean space has been the de facto geometric setting for machine learning architectures. However, recent literature has demonstrated that this choice comes with fundamental limitations. At a large scale, real-world data often exhibit inherently non-Euclidean structures, such as multi-way relationships, hierarchies, symmetries, and non-isotropic scaling, in a variety of domains, such as languages, vision, and the natural sciences. It is challenging to effectively capture these structures within the constraints of Euclidean spaces. This position paper argues that moving beyond Euclidean geometry is not merely an optional enhancement but a necessity to maintain the scaling law for the next-generation of foundation models. By adopting these geometries, foundation models could more efficiently leverage the aforementioned structures. Task-aware adaptability that dynamically reconfigures embeddings to match the geometry of downstream applications could further enhance efficiency and expressivity. Our position is supported by a series of theoretical and empirical investigations of prevalent foundation models.Finally, we outline a roadmap for integrating non-Euclidean geometries into foundation models, including strategies for building geometric foundation models via fine-tuning, training from scratch, and hybrid approaches."
      },
      {
        "id": "oai:arXiv.org:2504.08897v1",
        "title": "Toward Spiking Neural Network Local Learning Modules Resistant to Adversarial Attacks",
        "link": "https://arxiv.org/abs/2504.08897",
        "author": "Jiaqi Lin, Abhronil Sengupta",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08897v1 Announce Type: new \nAbstract: Recent research has shown the vulnerability of Spiking Neural Networks (SNNs) under adversarial examples that are nearly indistinguishable from clean data in the context of frame-based and event-based information. The majority of these studies are constrained in generating adversarial examples using Backpropagation Through Time (BPTT), a gradient-based method which lacks biological plausibility. In contrast, local learning methods, which relax many of BPTT's constraints, remain under-explored in the context of adversarial attacks. To address this problem, we examine adversarial robustness in SNNs through the framework of four types of training algorithms. We provide an in-depth analysis of the ineffectiveness of gradient-based adversarial attacks to generate adversarial instances in this scenario. To overcome these limitations, we introduce a hybrid adversarial attack paradigm that leverages the transferability of adversarial instances. The proposed hybrid approach demonstrates superior performance, outperforming existing adversarial attack methods. Furthermore, the generalizability of the method is assessed under multi-step adversarial attacks, adversarial attacks in black-box FGSM scenarios, and within the non-spiking domain."
      },
      {
        "id": "oai:arXiv.org:2504.08901v1",
        "title": "HAL-NeRF: High Accuracy Localization Leveraging Neural Radiance Fields",
        "link": "https://arxiv.org/abs/2504.08901",
        "author": "Asterios Reppas, Grigorios-Aris Cheimariotis, Panos K. Papadopoulos, Panagiotis Frasiolas, Dimitrios Zarpalas",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08901v1 Announce Type: new \nAbstract: Precise camera localization is a critical task in XR applications and robotics. Using only the camera captures as input to a system is an inexpensive option that enables localization in large indoor and outdoor environments, but it presents challenges in achieving high accuracy. Specifically, camera relocalization methods, such as Absolute Pose Regression (APR), can localize cameras with a median translation error of more than $0.5m$ in outdoor scenes. This paper presents HAL-NeRF, a high-accuracy localization method that combines a CNN pose regressor with a refinement module based on a Monte Carlo particle filter. The Nerfacto model, an implementation of Neural Radiance Fields (NeRFs), is used to augment the data for training the pose regressor and to measure photometric loss in the particle filter refinement module. HAL-NeRF leverages Nerfacto's ability to synthesize high-quality novel views, significantly improving the performance of the localization pipeline. HAL-NeRF achieves state-of-the-art results that are conventionally measured as the average of the median per scene errors. The translation error was $0.025m$ and the rotation error was $0.59$ degrees and 0.04m and 0.58 degrees on the 7-Scenes dataset and Cambridge Landmarks datasets respectively, with the trade-off of increased computational time. This work highlights the potential of combining APR with NeRF-based refinement techniques to advance monocular camera relocalization accuracy."
      },
      {
        "id": "oai:arXiv.org:2504.08902v1",
        "title": "LookingGlass: Generative Anamorphoses via Laplacian Pyramid Warping",
        "link": "https://arxiv.org/abs/2504.08902",
        "author": "Pascal Chang, Sergio Sancho, Jingwei Tang, Markus Gross, Vinicius C. Azevedo",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08902v1 Announce Type: new \nAbstract: Anamorphosis refers to a category of images that are intentionally distorted, making them unrecognizable when viewed directly. Their true form only reveals itself when seen from a specific viewpoint, which can be through some catadioptric device like a mirror or a lens. While the construction of these mathematical devices can be traced back to as early as the 17th century, they are only interpretable when viewed from a specific vantage point and tend to lose meaning when seen normally. In this paper, we revisit these famous optical illusions with a generative twist. With the help of latent rectified flow models, we propose a method to create anamorphic images that still retain a valid interpretation when viewed directly. To this end, we introduce Laplacian Pyramid Warping, a frequency-aware image warping technique key to generating high-quality visuals. Our work extends Visual Anagrams (arXiv:2311.17919) to latent space models and to a wider range of spatial transforms, enabling the creation of novel generative perceptual illusions."
      },
      {
        "id": "oai:arXiv.org:2504.08905v1",
        "title": "Forecasting Communication Derailments Through Conversation Generation",
        "link": "https://arxiv.org/abs/2504.08905",
        "author": "Yunfan Zhang, Kathleen McKeown, Smaranda Muresan",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08905v1 Announce Type: new \nAbstract: Forecasting communication derailment can be useful in real-world settings such as online content moderation, conflict resolution, and business negotiations. However, despite language models' success at identifying offensive speech present in conversations, they struggle to forecast future communication derailments. In contrast to prior work that predicts conversation outcomes solely based on the past conversation history, our approach samples multiple future conversation trajectories conditioned on existing conversation history using a fine-tuned LLM. It predicts the communication outcome based on the consensus of these trajectories. We also experimented with leveraging socio-linguistic attributes, which reflect turn-level conversation dynamics, as guidance when generating future conversations. Our method of future conversation trajectories surpasses state-of-the-art results on English communication derailment prediction benchmarks and demonstrates significant accuracy gains in ablation studies."
      },
      {
        "id": "oai:arXiv.org:2504.08906v1",
        "title": "Robust SAM: On the Adversarial Robustness of Vision Foundation Models",
        "link": "https://arxiv.org/abs/2504.08906",
        "author": "Jiahuan Long, Zhengqin Xu, Tingsong Jiang, Wen Yao, Shuai Jia, Chao Ma, Xiaoqian Chen",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08906v1 Announce Type: new \nAbstract: The Segment Anything Model (SAM) is a widely used vision foundation model with diverse applications, including image segmentation, detection, and tracking. Given SAM's wide applications, understanding its robustness against adversarial attacks is crucial for real-world deployment. However, research on SAM's robustness is still in its early stages. Existing attacks often overlook the role of prompts in evaluating SAM's robustness, and there has been insufficient exploration of defense methods to balance the robustness and accuracy. To address these gaps, this paper proposes an adversarial robustness framework designed to evaluate and enhance the robustness of SAM. Specifically, we introduce a cross-prompt attack method to enhance the attack transferability across different prompt types. Besides attacking, we propose a few-parameter adaptation strategy to defend SAM against various adversarial attacks. To balance robustness and accuracy, we use the singular value decomposition (SVD) to constrain the space of trainable parameters, where only singular values are adaptable. Experiments demonstrate that our cross-prompt attack method outperforms previous approaches in terms of attack success rate on both SAM and SAM 2. By adapting only 512 parameters, we achieve at least a 15\\% improvement in mean intersection over union (mIoU) against various adversarial attacks. Compared to previous defense methods, our approach enhances the robustness of SAM while maximally maintaining its original performance."
      },
      {
        "id": "oai:arXiv.org:2504.08912v1",
        "title": "HyperCore: The Core Framework for Building Hyperbolic Foundation Models with Comprehensive Modules",
        "link": "https://arxiv.org/abs/2504.08912",
        "author": "Neil He, Menglin Yang, Rex Ying",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08912v1 Announce Type: new \nAbstract: Hyperbolic neural networks have emerged as a powerful tool for modeling hierarchical data across diverse modalities. Recent studies show that token distributions in foundation models exhibit scale-free properties, suggesting that hyperbolic space is a more suitable ambient space than Euclidean space for many pre-training and downstream tasks. However, existing tools lack essential components for building hyperbolic foundation models, making it difficult to leverage recent advancements. We introduce HyperCore, a comprehensive open-source framework that provides core modules for constructing hyperbolic foundation models across multiple modalities. HyperCore's modules can be effortlessly combined to develop novel hyperbolic foundation models, eliminating the need to extensively modify Euclidean modules from scratch and possible redundant research efforts. To demonstrate its versatility, we build and test the first fully hyperbolic vision transformers (LViT) with a fine-tuning pipeline, the first fully hyperbolic multimodal CLIP model (L-CLIP), and a hybrid Graph RAG with a hyperbolic graph encoder. Our experiments demonstrate that LViT outperforms its Euclidean counterpart. Additionally, we benchmark and reproduce experiments across hyperbolic GNNs, CNNs, Transformers, and vision Transformers to highlight HyperCore's advantages."
      },
      {
        "id": "oai:arXiv.org:2504.08915v1",
        "title": "Parameter-Free Fine-tuning via Redundancy Elimination for Vision Foundation Models",
        "link": "https://arxiv.org/abs/2504.08915",
        "author": "Jiahuan Long, Tingsong Jiang, Wen Yao, Yizhe Xiong, Zhengqin Xu, Shuai Jia, Chao Ma",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08915v1 Announce Type: new \nAbstract: Vision foundation models (VFMs) are large pre-trained models that form the backbone of various vision tasks. Fine-tuning VFMs can further unlock their potential for downstream tasks or scenarios. However, VFMs often contain significant feature redundancy, which may limit their adaptability to new tasks. In this paper, we investigate the redundancies in the segment anything model (SAM) and then propose a parameter-free fine-tuning method to address this issue. Unlike traditional fine-tuning methods that adjust parameters, our method emphasizes selecting, reusing, and enhancing pre-trained features, offering a new perspective on model fine-tuning. Specifically, we introduce a channel selection algorithm based on the model's output difference to identify redundant and effective channels. By selectively replacing the redundant channels with more effective ones, we filter out less useful features and reuse the more relevant features to downstream tasks, thereby enhancing the task-specific feature representation. Experiments on both out-of-domain and in-domain datasets demonstrate the efficiency and effectiveness of our method. Notably, our approach can seamlessly integrate with existing fine-tuning strategies (e.g., LoRA, Adapter), further boosting the performance of already fine-tuned models. Moreover, since our channel selection involves only model inference, our method significantly reduces computational and GPU memory overhead."
      },
      {
        "id": "oai:arXiv.org:2504.08919v1",
        "title": "Are We Merely Justifying Results ex Post Facto? Quantifying Explanatory Inversion in Post-Hoc Model Explanations",
        "link": "https://arxiv.org/abs/2504.08919",
        "author": "Zhen Tan, Song Wang, Yifan Li, Yu Kong, Jundong Li, Tianlong Chen, Huan Liu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08919v1 Announce Type: new \nAbstract: Post-hoc explanation methods provide interpretation by attributing predictions to input features. Natural explanations are expected to interpret how the inputs lead to the predictions. Thus, a fundamental question arises: Do these explanations unintentionally reverse the natural relationship between inputs and outputs? Specifically, are the explanations rationalizing predictions from the output rather than reflecting the true decision process? To investigate such explanatory inversion, we propose Inversion Quantification (IQ), a framework that quantifies the degree to which explanations rely on outputs and deviate from faithful input-output relationships. Using the framework, we demonstrate on synthetic datasets that widely used methods such as LIME and SHAP are prone to such inversion, particularly in the presence of spurious correlations, across tabular, image, and text domains. Finally, we propose Reproduce-by-Poking (RBP), a simple and model-agnostic enhancement to post-hoc explanation methods that integrates forward perturbation checks. We further show that under the IQ framework, RBP theoretically guarantees the mitigation of explanatory inversion. Empirically, for example, on the synthesized data, RBP can reduce the inversion by 1.8% on average across iconic post-hoc explanation approaches and domains."
      },
      {
        "id": "oai:arXiv.org:2504.08930v1",
        "title": "An Adaptive Vector Index Partitioning Scheme for Low-Latency RAG Pipeline",
        "link": "https://arxiv.org/abs/2504.08930",
        "author": "Junkyum Kim, Divya Mahajan",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08930v1 Announce Type: new \nAbstract: Retrieval Augmented Generation (RAG) systems enhance response quality by integrating Large Language Models (LLMs) with vector databases, enabling external knowledge retrieval to support language model reasoning. While RAG enables efficient question answering with smaller LLMs, existing optimizations for vector search and LLM serving have largely been developed in isolation. As a result, their integration often leads to suboptimal end-to-end performance. ... This paper introduces VectorLiteRAG, an optimized vector index partitioning mechanism designed for RAG systems that enhances the responsiveness of the system by jointly optimizing vector search and LLM serving across CPU and GPU system. A key challenge is to determine which indices and how much of the vector index should reside on the GPU and adjusting LLM batch sizes to balance the pipeline for lower Time-To-First-Token (TTFT) and meeting user-defined Service-Level Objectives (SLOs). To address this, we leverage the insight that cluster access in vector databases exhibits access skew, where a subset of clusters are queried significantly more frequently than others. VectorLiteRAG exploits this property through an optimized memory distribution strategy, dynamically allocating the minimum number of vector indices corresponding to frequently accessed clusters onto the GPU HBM to ensure a balanced pipeline with the LLM for high responsiveness. This adaptive partitioning scheme is guided by a statistical model that informs memory allocation and workload distribution. Our evaluation demonstrates that VectorLiteRAG improves vector search responsiveness by 2x, significantly reduces end-to-end TTFT in RAG systems by intelligently balancing memory resources between vector search and LLM execution."
      },
      {
        "id": "oai:arXiv.org:2504.08934v1",
        "title": "Long Context In-Context Compression by Getting to the Gist of Gisting",
        "link": "https://arxiv.org/abs/2504.08934",
        "author": "Aleksandar Petrov, Mark Sandler, Andrey Zhmoginov, Nolan Miller, Max Vladymyrov",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08934v1 Announce Type: new \nAbstract: Long context processing is critical for the adoption of LLMs, but existing methods often introduce architectural complexity that hinders their practical adoption. Gisting, an in-context compression method with no architectural modification to the decoder transformer, is a promising approach due to its simplicity and compatibility with existing frameworks. While effective for short instructions, we demonstrate that gisting struggles with longer contexts, with significant performance drops even at minimal compression rates. Surprisingly, a simple average pooling baseline consistently outperforms gisting. We analyze the limitations of gisting, including information flow interruptions, capacity limitations and the inability to restrict its attention to subsets of the context. Motivated by theoretical insights into the performance gap between gisting and average pooling, and supported by extensive experimentation, we propose GistPool, a new in-context compression method. GistPool preserves the simplicity of gisting, while significantly boosting its performance on long context compression tasks."
      },
      {
        "id": "oai:arXiv.org:2504.08940v1",
        "title": "Combining Forecasts using Meta-Learning: A Comparative Study for Complex Seasonality",
        "link": "https://arxiv.org/abs/2504.08940",
        "author": "Grzegorz Dudek",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08940v1 Announce Type: new \nAbstract: In this paper, we investigate meta-learning for combining forecasts generated by models of different types. While typical approaches for combining forecasts involve simple averaging, machine learning techniques enable more sophisticated methods of combining through meta-learning, leading to improved forecasting accuracy. We use linear regression, $k$-nearest neighbors, multilayer perceptron, random forest, and long short-term memory as meta-learners. We define global and local meta-learning variants for time series with complex seasonality and compare meta-learners on multiple forecasting problems, demonstrating their superior performance compared to simple averaging."
      },
      {
        "id": "oai:arXiv.org:2504.08942v1",
        "title": "AgentRewardBench: Evaluating Automatic Evaluations of Web Agent Trajectories",
        "link": "https://arxiv.org/abs/2504.08942",
        "author": "Xing Han L\\`u, Amirhossein Kazemnejad, Nicholas Meade, Arkil Patel, Dongchan Shin, Alejandra Zambrano, Karolina Sta\\'nczak, Peter Shaw, Christopher J. Pal, Siva Reddy",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08942v1 Announce Type: new \nAbstract: Web agents enable users to perform tasks on web browsers through natural language interaction. Evaluating web agents trajectories is an important problem, since it helps us determine whether the agent successfully completed the tasks. Rule-based methods are widely used for this purpose, but they are challenging to extend to new tasks and may not always recognize successful trajectories. We may achieve higher accuracy through human evaluation, but the process would be substantially slower and more expensive. Automatic evaluations with LLMs may avoid the challenges of designing new rules and manually annotating trajectories, enabling faster and cost-effective evaluation. However, it is unclear how effective they are at evaluating web agents. To this end, we propose AgentRewardBench, the first benchmark to assess the effectiveness of LLM judges for evaluating web agents. AgentRewardBench contains 1302 trajectories across 5 benchmarks and 4 LLMs. Each trajectory in AgentRewardBench is reviewed by an expert, who answers questions pertaining to the success, side effects, and repetitiveness of the agent. Using our benchmark, we evaluate 12 LLM judges and find that no single LLM excels across all benchmarks. We also find that the rule-based evaluation used by common benchmarks tends to underreport the success rate of web agents, highlighting a key weakness of rule-based evaluation and the need to develop more flexible automatic evaluations. We release the benchmark at: https://agent-reward-bench.github.io"
      },
      {
        "id": "oai:arXiv.org:2504.08943v1",
        "title": "Investigating the Treacherous Turn in Deep Reinforcement Learning",
        "link": "https://arxiv.org/abs/2504.08943",
        "author": "Chace Ashcraft, Kiran Karra, Josh Carney, Nathan Drenkow",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08943v1 Announce Type: new \nAbstract: The Treacherous Turn refers to the scenario where an artificial intelligence (AI) agent subtly, and perhaps covertly, learns to perform a behavior that benefits itself but is deemed undesirable and potentially harmful to a human supervisor. During training, the agent learns to behave as expected by the human supervisor, but when deployed to perform its task, it performs an alternate behavior without the supervisor there to prevent it. Initial experiments applying DRL to an implementation of the A Link to the Past example do not produce the treacherous turn effect naturally, despite various modifications to the environment intended to produce it. However, in this work, we find the treacherous behavior to be reproducible in a DRL agent when using other trojan injection strategies. This approach deviates from the prototypical treacherous turn behavior since the behavior is explicitly trained into the agent, rather than occurring as an emergent consequence of environmental complexity or poor objective specification. Nonetheless, these experiments provide new insights into the challenges of producing agents capable of true treacherous turn behavior."
      },
      {
        "id": "oai:arXiv.org:2504.08947v1",
        "title": "Forecasting Cryptocurrency Prices using Contextual ES-adRNN with Exogenous Variables",
        "link": "https://arxiv.org/abs/2504.08947",
        "author": "Slawek Smyl, Grzegorz Dudek, Pawe{\\l} Pe{\\l}ka",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08947v1 Announce Type: new \nAbstract: In this paper, we introduce a new approach to multivariate forecasting cryptocurrency prices using a hybrid contextual model combining exponential smoothing (ES) and recurrent neural network (RNN). The model consists of two tracks: the context track and the main track. The context track provides additional information to the main track, extracted from representative series. This information as well as information extracted from exogenous variables is dynamically adjusted to the individual series forecasted by the main track. The RNN stacked architecture with hierarchical dilations, incorporating recently developed attentive dilated recurrent cells, allows the model to capture short and long-term dependencies across time series and dynamically weight input information. The model generates both point daily forecasts and predictive intervals for one-day, one-week and four-week horizons. We apply our model to forecast prices of 15 cryptocurrencies based on 17 input variables and compare its performance with that of comparative models, including both statistical and ML ones."
      },
      {
        "id": "oai:arXiv.org:2504.08958v1",
        "title": "Generating Planning Feedback for Open-Ended Programming Exercises with LLMs",
        "link": "https://arxiv.org/abs/2504.08958",
        "author": "Mehmet Arif Demirta\\c{s}, Claire Zheng, Max Fowler, Kathryn Cunningham",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08958v1 Announce Type: new \nAbstract: To complete an open-ended programming exercise, students need to both plan a high-level solution and implement it using the appropriate syntax. However, these problems are often autograded on the correctness of the final submission through test cases, and students cannot get feedback on their planning process. Large language models (LLM) may be able to generate this feedback by detecting the overall code structure even for submissions with syntax errors. To this end, we propose an approach that detects which high-level goals and patterns (i.e. programming plans) exist in a student program with LLMs. We show that both the full GPT-4o model and a small variant (GPT-4o-mini) can detect these plans with remarkable accuracy, outperforming baselines inspired by conventional approaches to code analysis. We further show that the smaller, cost-effective variant (GPT-4o-mini) achieves results on par with state-of-the-art (GPT-4o) after fine-tuning, creating promising implications for smaller models for real-time grading. These smaller models can be incorporated into autograders for open-ended code-writing exercises to provide feedback for students' implicit planning skills, even when their program is syntactically incorrect. Furthermore, LLMs may be useful in providing feedback for problems in other domains where students start with a set of high-level solution steps and iteratively compute the output, such as math and physics problems."
      },
      {
        "id": "oai:arXiv.org:2504.08959v1",
        "title": "MotionDreamer: One-to-Many Motion Synthesis with Localized Generative Masked Transformer",
        "link": "https://arxiv.org/abs/2504.08959",
        "author": "Yilin Wang, Chuan Guo, Yuxuan Mu, Muhammad Gohar Javed, Xinxin Zuo, Juwei Lu, Hai Jiang, Li Cheng",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08959v1 Announce Type: new \nAbstract: Generative masked transformers have demonstrated remarkable success across various content generation tasks, primarily due to their ability to effectively model large-scale dataset distributions with high consistency. However, in the animation domain, large datasets are not always available. Applying generative masked modeling to generate diverse instances from a single MoCap reference may lead to overfitting, a challenge that remains unexplored. In this work, we present MotionDreamer, a localized masked modeling paradigm designed to learn internal motion patterns from a given motion with arbitrary topology and duration. By embedding the given motion into quantized tokens with a novel distribution regularization method, MotionDreamer constructs a robust and informative codebook for local motion patterns. Moreover, a sliding window local attention is introduced in our masked transformer, enabling the generation of natural yet diverse animations that closely resemble the reference motion patterns. As demonstrated through comprehensive experiments, MotionDreamer outperforms the state-of-the-art methods that are typically GAN or Diffusion-based in both faithfulness and diversity. Thanks to the consistency and robustness of the quantization-based approach, MotionDreamer can also effectively perform downstream tasks such as temporal motion editing, \\textcolor{update}{crowd animation}, and beat-aligned dance generation, all using a single reference motion. Visit our project page: https://motiondreamer.github.io/"
      },
      {
        "id": "oai:arXiv.org:2504.08960v1",
        "title": "Quantifying the Spread of Online Incivility in Brazilian Politics",
        "link": "https://arxiv.org/abs/2504.08960",
        "author": "Yuan Zhang, Michael Amsler, Laia Castro Herrero, Frank Esser, Alexandre Bovet",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08960v1 Announce Type: new \nAbstract: Incivility refers to behaviors that violate collective norms and disrupt cooperation within the political process. Although large-scale online data and automated techniques have enabled the quantitative analysis of uncivil discourse, prior research has predominantly focused on impoliteness or toxicity, often overlooking other behaviors that undermine democratic values. To address this gap, we propose a multidimensional conceptual framework encompassing Impoliteness, Physical Harm and Violent Political Rhetoric, Hate Speech and Stereotyping, and Threats to Democratic Institutions and Values. Using this framework, we measure the spread of online political incivility in Brazil using approximately 5 million tweets posted by 2,307 political influencers during the 2022 Brazilian general election. Through statistical modeling and network analysis, we examine the dynamics of uncivil posts at different election stages, identify key disseminators and audiences, and explore the mechanisms driving the spread of uncivil information online. Our findings indicate that impoliteness is more likely to surge during election campaigns. In contrast, the other dimensions of incivility are often triggered by specific violent events. Moreover, we find that left-aligned individual influencers are the primary disseminators of online incivility in the Brazilian Twitter/X sphere and that they disseminate not only direct incivility but also indirect incivility when discussing or opposing incivility expressed by others. They relay those content from politicians, media agents, and individuals to reach broader audiences, revealing a diffusion pattern mixing the direct and two-step flows of communication theory. This study offers new insights into the multidimensional nature of incivility in Brazilian politics and provides a conceptual framework that can be extended to other political contexts."
      },
      {
        "id": "oai:arXiv.org:2504.08961v1",
        "title": "A Fully Automated Pipeline for Conversational Discourse Annotation: Tree Scheme Generation and Labeling with Large Language Models",
        "link": "https://arxiv.org/abs/2504.08961",
        "author": "Kseniia Petukhova, Ekaterina Kochmar",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08961v1 Announce Type: new \nAbstract: Recent advances in Large Language Models (LLMs) have shown promise in automating discourse annotation for conversations. While manually designing tree annotation schemes significantly improves annotation quality for humans and models, their creation remains time-consuming and requires expert knowledge. We propose a fully automated pipeline that uses LLMs to construct such schemes and perform annotation. We evaluate our approach on speech functions (SFs) and the Switchboard-DAMSL (SWBD-DAMSL) taxonomies. Our experiments compare various design choices, and we show that frequency-guided decision trees, paired with an advanced LLM for annotation, can outperform previously manually designed trees and even match or surpass human annotators while significantly reducing the time required for annotation. We release all code and resultant schemes and annotations to facilitate future research on discourse annotation."
      },
      {
        "id": "oai:arXiv.org:2504.08964v1",
        "title": "Bidirectional Linear Recurrent Models for Sequence-Level Multisource Fusion",
        "link": "https://arxiv.org/abs/2504.08964",
        "author": "Qisai Liu, Zhanhong Jiang, Joshua R. Waite, Chao Liu, Aditya Balu, Soumik Sarkar",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08964v1 Announce Type: new \nAbstract: Sequence modeling is a critical yet challenging task with wide-ranging applications, especially in time series forecasting for domains like weather prediction, temperature monitoring, and energy load forecasting. Transformers, with their attention mechanism, have emerged as state-of-the-art due to their efficient parallel training, but they suffer from quadratic time complexity, limiting their scalability for long sequences. In contrast, recurrent neural networks (RNNs) offer linear time complexity, spurring renewed interest in linear RNNs for more computationally efficient sequence modeling. In this work, we introduce BLUR (Bidirectional Linear Unit for Recurrent network), which uses forward and backward linear recurrent units (LRUs) to capture both past and future dependencies with high computational efficiency. BLUR maintains the linear time complexity of traditional RNNs, while enabling fast parallel training through LRUs. Furthermore, it offers provably stable training and strong approximation capabilities, making it highly effective for modeling long-term dependencies. Extensive experiments on sequential image and time series datasets reveal that BLUR not only surpasses transformers and traditional RNNs in accuracy but also significantly reduces computational costs, making it particularly suitable for real-world forecasting tasks. Our code is available here."
      },
      {
        "id": "oai:arXiv.org:2504.08966v1",
        "title": "PACT: Pruning and Clustering-Based Token Reduction for Faster Visual Language Models",
        "link": "https://arxiv.org/abs/2504.08966",
        "author": "Mohamed Dhouib, Davide Buscaldi, Sonia Vanier, Aymen Shabou",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08966v1 Announce Type: new \nAbstract: Visual Language Models require substantial computational resources for inference due to the additional input tokens needed to represent visual information. However, these visual tokens often contain redundant and unimportant information, resulting in an unnecessarily high number of tokens. To address this, we introduce PACT, a method that reduces inference time and memory usage by pruning irrelevant tokens and merging visually redundant ones at an early layer of the language model. Our approach uses a novel importance metric to identify unimportant tokens without relying on attention scores, making it compatible with FlashAttention. We also propose a novel clustering algorithm, called Distance Bounded Density Peak Clustering, which efficiently clusters visual tokens while constraining the distances between elements within a cluster by a predefined threshold. We demonstrate the effectiveness of PACT through extensive experiments."
      },
      {
        "id": "oai:arXiv.org:2504.08970v1",
        "title": "On Large-scale Evaluation of Embedding Models for Knowledge Graph Completion",
        "link": "https://arxiv.org/abs/2504.08970",
        "author": "Nasim Shirvani-Mahdavi, Farahnaz Akrami, Chengkai Li",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08970v1 Announce Type: new \nAbstract: Knowledge graph embedding (KGE) models are extensively studied for knowledge graph completion, yet their evaluation remains constrained by unrealistic benchmarks. Commonly used datasets are either faulty or too small to reflect real-world data. Few studies examine the role of mediator nodes, which are essential for modeling n-ary relationships, or investigate model performance variation across domains. Standard evaluation metrics rely on the closed-world assumption, which penalizes models for correctly predicting missing triples, contradicting the fundamental goals of link prediction. These metrics often compress accuracy assessment into a single value, obscuring models' specific strengths and weaknesses. The prevailing evaluation protocol operates under the unrealistic assumption that an entity's properties, for which values are to be predicted, are known in advance. While alternative protocols such as property prediction, entity-pair ranking and triple classification address some of these limitations, they remain underutilized. This paper conducts a comprehensive evaluation of four representative KGE models on large-scale datasets FB-CVT-REV and FB+CVT-REV. Our analysis reveals critical insights, including substantial performance variations between small and large datasets, both in relative rankings and absolute metrics, systematic overestimation of model capabilities when n-ary relations are binarized, and fundamental limitations in current evaluation protocols and metrics."
      },
      {
        "id": "oai:arXiv.org:2504.08981v1",
        "title": "AGENT: An Aerial Vehicle Generation and Design Tool Using Large Language Models",
        "link": "https://arxiv.org/abs/2504.08981",
        "author": "Colin Samplawski, Adam D. Cobb, Susmit Jha",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08981v1 Announce Type: new \nAbstract: Computer-aided design (CAD) is a promising application area for emerging artificial intelligence methods. Traditional workflows for cyberphysical systems create detailed digital models which can be evaluated by physics simulators in order to narrow the search space before creating physical prototypes. A major bottleneck of this approach is that the simulators are often computationally expensive and slow. Recent advancements in AI methods offer the possibility to accelerate these pipelines. We use the recently released AircraftVerse dataset, which is especially suited for developing and evaluating large language models for designs. AircraftVerse contains a diverse set of UAV designs represented via textual design trees together with detailed physics simulation results. Following the recent success of large language models (LLMs), we propose AGENT (Aircraft GENeraTor). AGENT is a comprehensive design tool built on the CodeT5+ LLM which learns powerful representations of aircraft textual designs directly from JSON files. We develop a curriculum of training tasks which imbues a single model with a suite of useful features. AGENT is able to generate designs conditioned on properties of flight dynamics (hover time, maximum speed, etc.). Additionally, AGENT can issue evaluations of designs allowing it to act as a surrogate model of the physics simulation that underlies the AircraftVerse dataset. We present a series of experiments which demonstrate our system's abilities. We are able to achieve strong performance using the smallest member of the CodeT5+ family (220M parameters). This allows for a flexible and powerful system which can be executed on a single GPU enabling a clear path toward future deployment."
      },
      {
        "id": "oai:arXiv.org:2504.08982v1",
        "title": "Adaptive Additive Parameter Updates of Vision Transformers for Few-Shot Continual Learning",
        "link": "https://arxiv.org/abs/2504.08982",
        "author": "Kyle Stein, Andrew Arash Mahyari, Guillermo Francia III, Eman El-Sheikh",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08982v1 Announce Type: new \nAbstract: Integrating new class information without losing previously acquired knowledge remains a central challenge in artificial intelligence, often referred to as catastrophic forgetting. Few-shot class incremental learning (FSCIL) addresses this by first training a model on a robust dataset of base classes and then incrementally adapting it in successive sessions using only a few labeled examples per novel class. However, this approach is prone to overfitting on the limited new data, which can compromise overall performance and exacerbate forgetting. In this work, we propose a simple yet effective novel FSCIL framework that leverages a frozen Vision Transformer (ViT) backbone augmented with parameter-efficient additive updates. Our approach freezes the pre-trained ViT parameters and selectively injects trainable weights into the self-attention modules via an additive update mechanism. This design updates only a small subset of parameters to accommodate new classes without sacrificing the representations learned during the base session. By fine-tuning a limited number of parameters, our method preserves the generalizable features in the frozen ViT while reducing the risk of overfitting. Furthermore, as most parameters remain fixed, the model avoids overwriting previously learned knowledge when small novel data batches are introduced. Extensive experiments on benchmark datasets demonstrate that our approach yields state-of-the-art performance compared to baseline FSCIL methods."
      },
      {
        "id": "oai:arXiv.org:2504.08994v1",
        "title": "ReCA: A Parametric ReLU Composite Activation Function",
        "link": "https://arxiv.org/abs/2504.08994",
        "author": "John Chidiac, Danielle Azar",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08994v1 Announce Type: new \nAbstract: Activation functions have been shown to affect the performance of deep neural networks significantly. While the Rectified Linear Unit (ReLU) remains the dominant choice in practice, the optimal activation function for deep neural networks remains an open research question. In this paper, we propose a novel parametric activation function, ReCA, based on ReLU, which has been shown to outperform all baselines on state-of-the-art datasets using different complex neural network architectures."
      },
      {
        "id": "oai:arXiv.org:2504.09021v1",
        "title": "A Champion-level Vision-based Reinforcement Learning Agent for Competitive Racing in Gran Turismo 7",
        "link": "https://arxiv.org/abs/2504.09021",
        "author": "Hojoon Lee, Takuma Seno, Jun Jet Tai, Kaushik Subramanian, Kenta Kawamoto, Peter Stone, Peter R. Wurman",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09021v1 Announce Type: new \nAbstract: Deep reinforcement learning has achieved superhuman racing performance in high-fidelity simulators like Gran Turismo 7 (GT7). It typically utilizes global features that require instrumentation external to a car, such as precise localization of agents and opponents, limiting real-world applicability. To address this limitation, we introduce a vision-based autonomous racing agent that relies solely on ego-centric camera views and onboard sensor data, eliminating the need for precise localization during inference. This agent employs an asymmetric actor-critic framework: the actor uses a recurrent neural network with the sensor data local to the car to retain track layouts and opponent positions, while the critic accesses the global features during training. Evaluated in GT7, our agent consistently outperforms GT7's built-drivers. To our knowledge, this work presents the first vision-based autonomous racing agent to demonstrate champion-level performance in competitive racing scenarios."
      },
      {
        "id": "oai:arXiv.org:2504.09026v1",
        "title": "Detecting Instruction Fine-tuning Attack on Language Models with Influence Function",
        "link": "https://arxiv.org/abs/2504.09026",
        "author": "Jiawei Li",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09026v1 Announce Type: new \nAbstract: Instruction fine-tuning attacks pose a significant threat to large language models (LLMs) by subtly embedding poisoned data in fine-tuning datasets, which can trigger harmful or unintended responses across a range of tasks. This undermines model alignment and poses security risks in real-world deployment. In this work, we present a simple and effective approach to detect and mitigate such attacks using influence functions, a classical statistical tool adapted for machine learning interpretation. Traditionally, the high computational costs of influence functions have limited their application to large models and datasets. The recent Eigenvalue-Corrected Kronecker-Factored Approximate Curvature (EK-FAC) approximation method enables efficient influence score computation, making it feasible for large-scale analysis.\n  We are the first to apply influence functions for detecting language model instruction fine-tuning attacks on large-scale datasets, as both the instruction fine-tuning attack on language models and the influence calculation approximation technique are relatively new. Our large-scale empirical evaluation of influence functions on 50,000 fine-tuning examples and 32 tasks reveals a strong association between influence scores and sentiment. Building on this, we introduce a novel sentiment transformation combined with influence functions to detect and remove critical poisons -- poisoned data points that skew model predictions. Removing these poisons (only 1% of total data) recovers model performance to near-clean levels, demonstrating the effectiveness and efficiency of our approach. Artifact is available at https://github.com/lijiawei20161002/Poison-Detection.\n  WARNING: This paper contains offensive data examples."
      },
      {
        "id": "oai:arXiv.org:2504.09027v1",
        "title": "Associating transportation planning-related measures with Mild Cognitive Impairment",
        "link": "https://arxiv.org/abs/2504.09027",
        "author": "Souradeep Chattopadhyay, Guillermo Basulto-Elias, Jun Ha Chang, Matthew Rizzo, Shauna Hallmark, Anuj Sharma, Soumik Sarkar",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09027v1 Announce Type: new \nAbstract: Understanding the relationship between mild cognitive impairment and driving behavior is essential to improve road safety, especially among older adults. In this study, we computed certain variables that reflect daily driving habits, such as trips to specific locations (e.g., home, work, medical, social, and errands) of older drivers in Nebraska using geohashing. The computed variables were then analyzed using a two-fold approach involving data visualization and machine learning models (C5.0, Random Forest, Support Vector Machines) to investigate the efficiency of the computed variables in predicting whether a driver is cognitively impaired or unimpaired. The C5.0 model demonstrated robust and stable performance with a median recall of 74\\%, indicating that our methodology was able to identify cognitive impairment in drivers 74\\% of the time correctly. This highlights our model's effectiveness in minimizing false negatives which is an important consideration given the cost of missing impaired drivers could be potentially high. Our findings highlight the potential of life space variables in understanding and predicting cognitive decline, offering avenues for early intervention and tailored support for affected individuals."
      },
      {
        "id": "oai:arXiv.org:2504.09028v1",
        "title": "Towards On-Device Learning and Reconfigurable Hardware Implementation for Encoded Single-Photon Signal Processing",
        "link": "https://arxiv.org/abs/2504.09028",
        "author": "Zhenya Zang, Xingda Li, David Day Uei Li",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09028v1 Announce Type: new \nAbstract: Deep neural networks (DNNs) enhance the accuracy and efficiency of reconstructing key parameters from time-resolved photon arrival signals recorded by single-photon detectors. However, the performance of conventional backpropagation-based DNNs is highly dependent on various parameters of the optical setup and biological samples under examination, necessitating frequent network retraining, either through transfer learning or from scratch. Newly collected data must also be stored and transferred to a high-performance GPU server for retraining, introducing latency and storage overhead. To address these challenges, we propose an online training algorithm based on a One-Sided Jacobi rotation-based Online Sequential Extreme Learning Machine (OSOS-ELM). We fully exploit parallelism in executing OSOS-ELM on a heterogeneous FPGA with integrated ARM cores. Extensive evaluations of OSOS-ELM and OSELM demonstrate that both achieve comparable accuracy across different network dimensions (i.e., input, hidden, and output layers), while OSOS-ELM proves to be more hardware-efficient. By leveraging the parallelism of OSOS-ELM, we implement a holistic computing prototype on a Xilinx ZCU104 FPGA, which integrates a multi-core CPU and programmable logic fabric. We validate our approach through three case studies involving single-photon signal analysis: sensing through fog using commercial single-photon LiDAR, fluorescence lifetime estimation in FLIM, and blood flow index reconstruction in DCS, all utilizing one-dimensional data encoded from photonic signals. From a hardware perspective, we optimize the OSOS-ELM workload by employing multi-tasked processing on ARM CPU cores and pipelined execution on the FPGA's logic fabric. We also implement our OSOS-ELM on the NVIDIA Jetson Xavier NX GPU to comprehensively investigate its computing performance on another type of heterogeneous computing platform."
      },
      {
        "id": "oai:arXiv.org:2504.09033v1",
        "title": "Chest X-ray Classification using Deep Convolution Models on Low-resolution images with Uncertain Labels",
        "link": "https://arxiv.org/abs/2504.09033",
        "author": "Snigdha Agarwal, Neelam Sinha",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09033v1 Announce Type: new \nAbstract: Deep Convolutional Neural Networks have consistently proven to achieve state-of-the-art results on a lot of imaging tasks over the past years' majority of which comprise of high-quality data. However, it is important to work on low-resolution images since it could be a cheaper alternative for remote healthcare access where the primary need of automated pathology identification models occurs. Medical diagnosis using low-resolution images is challenging since critical details may not be easily identifiable. In this paper, we report classification results by experimenting on different input image sizes of Chest X-rays to deep CNN models and discuss the feasibility of classification on varying image sizes. We also leverage the noisy labels in the dataset by proposing a Randomized Flipping of labels techniques. We use an ensemble of multi-label classification models on frontal and lateral studies. Our models are trained on 5 out of the 14 chest pathologies of the publicly available CheXpert dataset. We incorporate techniques such as augmentation, regularization for model improvement and use class activation maps to visualize the neural network's decision making. Comparison with classification results on data from 200 subjects, obtained on the corresponding high-resolution images, reported in the original CheXpert paper, has been presented. For pathologies Cardiomegaly, Consolidation and Edema, we obtain 3% higher accuracy with our model architecture."
      },
      {
        "id": "oai:arXiv.org:2504.09039v1",
        "title": "Sculpting Memory: Multi-Concept Forgetting in Diffusion Models via Dynamic Mask and Concept-Aware Optimization",
        "link": "https://arxiv.org/abs/2504.09039",
        "author": "Gen Li, Yang Xiao, Jie Ji, Kaiyuan Deng, Bo Hui, Linke Guo, Xiaolong Ma",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09039v1 Announce Type: new \nAbstract: Text-to-image (T2I) diffusion models have achieved remarkable success in generating high-quality images from textual prompts. However, their ability to store vast amounts of knowledge raises concerns in scenarios where selective forgetting is necessary, such as removing copyrighted content, reducing biases, or eliminating harmful concepts. While existing unlearning methods can remove certain concepts, they struggle with multi-concept forgetting due to instability, residual knowledge persistence, and generation quality degradation. To address these challenges, we propose \\textbf{Dynamic Mask coupled with Concept-Aware Loss}, a novel unlearning framework designed for multi-concept forgetting in diffusion models. Our \\textbf{Dynamic Mask} mechanism adaptively updates gradient masks based on current optimization states, allowing selective weight modifications that prevent interference with unrelated knowledge. Additionally, our \\textbf{Concept-Aware Loss} explicitly guides the unlearning process by enforcing semantic consistency through superclass alignment, while a regularization loss based on knowledge distillation ensures that previously unlearned concepts remain forgotten during sequential unlearning. We conduct extensive experiments to evaluate our approach. Results demonstrate that our method outperforms existing unlearning techniques in forgetting effectiveness, output fidelity, and semantic coherence, particularly in multi-concept scenarios. Our work provides a principled and flexible framework for stable and high-fidelity unlearning in generative models. The code will be released publicly."
      },
      {
        "id": "oai:arXiv.org:2504.09048v1",
        "title": "BlockGaussian: Efficient Large-Scale Scene NovelView Synthesis via Adaptive Block-Based Gaussian Splatting",
        "link": "https://arxiv.org/abs/2504.09048",
        "author": "Yongchang Wu, Zipeng Qi, Zhenwei Shi, Zhengxia Zou",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09048v1 Announce Type: new \nAbstract: The recent advancements in 3D Gaussian Splatting (3DGS) have demonstrated remarkable potential in novel view synthesis tasks. The divide-and-conquer paradigm has enabled large-scale scene reconstruction, but significant challenges remain in scene partitioning, optimization, and merging processes. This paper introduces BlockGaussian, a novel framework incorporating a content-aware scene partition strategy and visibility-aware block optimization to achieve efficient and high-quality large-scale scene reconstruction. Specifically, our approach considers the content-complexity variation across different regions and balances computational load during scene partitioning, enabling efficient scene reconstruction. To tackle the supervision mismatch issue during independent block optimization, we introduce auxiliary points during individual block optimization to align the ground-truth supervision, which enhances the reconstruction quality. Furthermore, we propose a pseudo-view geometry constraint that effectively mitigates rendering degradation caused by airspace floaters during block merging. Extensive experiments on large-scale scenes demonstrate that our approach achieves state-of-the-art performance in both reconstruction efficiency and rendering quality, with a 5x speedup in optimization and an average PSNR improvement of 1.21 dB on multiple benchmarks. Notably, BlockGaussian significantly reduces computational requirements, enabling large-scale scene reconstruction on a single 24GB VRAM device. The project page is available at https://github.com/SunshineWYC/BlockGaussian"
      },
      {
        "id": "oai:arXiv.org:2504.09049v1",
        "title": "From Punchlines to Predictions: A Metric to Assess LLM Performance in Identifying Humor in Stand-Up Comedy",
        "link": "https://arxiv.org/abs/2504.09049",
        "author": "Adrianna Romanowski, Pedro H. V. Valois, Kazuhiro Fukui",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09049v1 Announce Type: new \nAbstract: Comedy serves as a profound reflection of the times we live in and is a staple element of human interactions. In light of the widespread adoption of Large Language Models (LLMs), the intersection of humor and AI has become no laughing matter. Advancements in the naturalness of human-computer interaction correlates with improvements in AI systems' abilities to understand humor. In this study, we assess the ability of models in accurately identifying humorous quotes from a stand-up comedy transcript. Stand-up comedy's unique comedic narratives make it an ideal dataset to improve the overall naturalness of comedic understanding. We propose a novel humor detection metric designed to evaluate LLMs amongst various prompts on their capability to extract humorous punchlines. The metric has a modular structure that offers three different scoring methods - fuzzy string matching, sentence embedding, and subspace similarity - to provide an overarching assessment of a model's performance. The model's results are compared against those of human evaluators on the same task. Our metric reveals that regardless of prompt engineering, leading models, ChatGPT, Claude, and DeepSeek, achieve scores of at most 51% in humor detection. Notably, this performance surpasses that of humans who achieve a score of 41%. The analysis of human evaluators and LLMs reveals variability in agreement, highlighting the subjectivity inherent in humor and the complexities involved in extracting humorous quotes from live performance transcripts. Code available at https://github.com/swaggirl9000/humor."
      },
      {
        "id": "oai:arXiv.org:2504.09060v1",
        "title": "Multimodal 3D Genome Pre-training",
        "link": "https://arxiv.org/abs/2504.09060",
        "author": "Minghao Yang, Pengteng Li, Yan Liang, Qianyi Cai, Zhihang Zheng, Shichen Zhang, Pengfei Zhang, Zhi-An Huang, Hui Xiong",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09060v1 Announce Type: new \nAbstract: Deep learning techniques have driven significant progress in various analytical tasks within 3D genomics in computational biology. However, a holistic understanding of 3D genomics knowledge remains underexplored. Here, we propose MIX-HIC, the first multimodal foundation model of 3D genome that integrates both 3D genome structure and epigenomic tracks, which obtains unified and comprehensive semantics. For accurate heterogeneous semantic fusion, we design the cross-modal interaction and mapping blocks for robust unified representation, yielding the accurate aggregation of 3D genome knowledge. Besides, we introduce the first large-scale dataset comprising over 1 million pairwise samples of Hi-C contact maps and epigenomic tracks for high-quality pre-training, enabling the exploration of functional implications in 3D genomics. Extensive experiments show that MIX-HIC can significantly surpass existing state-of-the-art methods in diverse downstream tasks. This work provides a valuable resource for advancing 3D genomics research."
      },
      {
        "id": "oai:arXiv.org:2504.09062v1",
        "title": "You Need a Transition Plane: Bridging Continuous Panoramic 3D Reconstruction with Perspective Gaussian Splatting",
        "link": "https://arxiv.org/abs/2504.09062",
        "author": "Zhijie Shen, Chunyu Lin, Shujuan Huang, Lang Nie, Kang Liao, Yao Zhao",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09062v1 Announce Type: new \nAbstract: Recently, reconstructing scenes from a single panoramic image using advanced 3D Gaussian Splatting (3DGS) techniques has attracted growing interest. Panoramic images offer a 360$\\times$ 180 field of view (FoV), capturing the entire scene in a single shot. However, panoramic images introduce severe distortion, making it challenging to render 3D Gaussians into 2D distorted equirectangular space directly. Converting equirectangular images to cubemap projections partially alleviates this problem but introduces new challenges, such as projection distortion and discontinuities across cube-face boundaries. To address these limitations, we present a novel framework, named TPGS, to bridge continuous panoramic 3D scene reconstruction with perspective Gaussian splatting. Firstly, we introduce a Transition Plane between adjacent cube faces to enable smoother transitions in splatting directions and mitigate optimization ambiguity in the boundary region. Moreover, an intra-to-inter face optimization strategy is proposed to enhance local details and restore visual consistency across cube-face boundaries. Specifically, we optimize 3D Gaussians within individual cube faces and then fine-tune them in the stitched panoramic space. Additionally, we introduce a spherical sampling technique to eliminate visible stitching seams. Extensive experiments on indoor and outdoor, egocentric, and roaming benchmark datasets demonstrate that our approach outperforms existing state-of-the-art methods. Code and models will be available at https://github.com/zhijieshen-bjtu/TPGS."
      },
      {
        "id": "oai:arXiv.org:2504.09063v1",
        "title": "A Practical Approach to using Supervised Machine Learning Models to Classify Aviation Safety Occurrences",
        "link": "https://arxiv.org/abs/2504.09063",
        "author": "Bryan Y. Siow",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09063v1 Announce Type: new \nAbstract: This paper describes a practical approach of using supervised machine learning (ML) models to assist safety investigators to classify aviation occurrences into either incident or serious incident categories. Our implementation currently deployed as a ML web application is trained on a labelled dataset derived from publicly available aviation investigation reports. A selection of five supervised learning models (Support Vector Machine, Logistic Regression, Random Forest Classifier, XGBoost and K-Nearest Neighbors) were evaluated. This paper showed the best performing ML algorithm was the Random Forest Classifier with accuracy = 0.77, F1 Score = 0.78 and MCC = 0.51 (average of 100 sample runs). The study had also explored the effect of applying Synthetic Minority Over-sampling Technique (SMOTE) to the imbalanced dataset, and the overall observation ranged from no significant effect to substantial degradation in performance for some of the models after the SMOTE adjustment."
      },
      {
        "id": "oai:arXiv.org:2504.09064v1",
        "title": "PQS (Prune, Quantize, and Sort): Low-Bitwidth Accumulation of Dot Products in Neural Network Computations",
        "link": "https://arxiv.org/abs/2504.09064",
        "author": "Vikas Natesh, H. T. Kung",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09064v1 Announce Type: new \nAbstract: We present PQS, which uses three techniques together - Prune, Quantize, and Sort - to achieve low-bitwidth accumulation of dot products in neural network computations. In conventional quantized (e.g., 8-bit) dot products, partial results are accumulated into wide (e.g., 32-bit) accumulators to avoid overflows when accumulating intermediate partial sums. However, such wide accumulators increase memory bandwidth usage and reduce energy efficiency. We show that iterative N:M pruning in floating point followed by quantization to 8 (or fewer) bits, and accumulation of partial products in a sorted order (\"small to large\") allows for accurate, compressed models with short dot product lengths that do not require wide accumulators. We design, analyze, and implement the PQS algorithm to eliminate accumulation overflows at inference time for several neural networks. Our method offers a 2.5x reduction in accumulator bitwidth while achieving model accuracy on par with floating-point baselines for multiple image classification tasks."
      },
      {
        "id": "oai:arXiv.org:2504.09066v1",
        "title": "Hyperlocal disaster damage assessment using bi-temporal street-view imagery and pre-trained vision models",
        "link": "https://arxiv.org/abs/2504.09066",
        "author": "Yifan Yang, Lei Zou, Bing Zhou, Daoyang Li, Binbin Lin, Joynal Abedin, Mingzheng Yang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09066v1 Announce Type: new \nAbstract: Street-view images offer unique advantages for disaster damage estimation as they capture impacts from a visual perspective and provide detailed, on-the-ground insights. Despite several investigations attempting to analyze street-view images for damage estimation, they mainly focus on post-disaster images. The potential of time-series street-view images remains underexplored. Pre-disaster images provide valuable benchmarks for accurate damage estimations at building and street levels. These images could aid annotators in objectively labeling post-disaster impacts, improving the reliability of labeled data sets for model training, and potentially enhancing the model performance in damage evaluation. The goal of this study is to estimate hyperlocal, on-the-ground disaster damages using bi-temporal street-view images and advanced pre-trained vision models. Street-view images before and after 2024 Hurricane Milton in Horseshoe Beach, Florida, were collected for experiments. The objectives are: (1) to assess the performance gains of incorporating pre-disaster street-view images as a no-damage category in fine-tuning pre-trained models, including Swin Transformer and ConvNeXt, for damage level classification; (2) to design and evaluate a dual-channel algorithm that reads pair-wise pre- and post-disaster street-view images for hyperlocal damage assessment. The results indicate that incorporating pre-disaster street-view images and employing a dual-channel processing framework can significantly enhance damage assessment accuracy. The accuracy improves from 66.14% with the Swin Transformer baseline to 77.11% with the dual-channel Feature-Fusion ConvNeXt model. This research enables rapid, operational damage assessments at hyperlocal spatial resolutions, providing valuable insights to support effective decision-making in disaster management and resilience planning."
      },
      {
        "id": "oai:arXiv.org:2504.09069v1",
        "title": "UniFlowRestore: A General Video Restoration Framework via Flow Matching and Prompt Guidance",
        "link": "https://arxiv.org/abs/2504.09069",
        "author": "Shuning Sun, Yu Zhang, Chen Wu, Dianjie Lu, Dianjie Lu, Guijuan Zhan, Yang Weng, Zhuoran Zheng",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09069v1 Announce Type: new \nAbstract: Video imaging is often affected by complex degradations such as blur, noise, and compression artifacts. Traditional restoration methods follow a \"single-task single-model\" paradigm, resulting in poor generalization and high computational cost, limiting their applicability in real-world scenarios with diverse degradation types. We propose UniFlowRestore, a general video restoration framework that models restoration as a time-continuous evolution under a prompt-guided and physics-informed vector field. A physics-aware backbone PhysicsUNet encodes degradation priors as potential energy, while PromptGenerator produces task-relevant prompts as momentum. These components define a Hamiltonian system whose vector field integrates inertial dynamics, decaying physical gradients, and prompt-based guidance. The system is optimized via a fixed-step ODE solver to achieve efficient and unified restoration across tasks. Experiments show that UniFlowRestore delivers stateof-the-art performance with strong generalization and efficiency. Quantitative results demonstrate that UniFlowRestore achieves state-of-the-art performance, attaining the highest PSNR (33.89 dB) and SSIM (0.97) on the video denoising task, while maintaining top or second-best scores across all evaluated tasks."
      },
      {
        "id": "oai:arXiv.org:2504.09071v1",
        "title": "Exploration of Plan-Guided Summarization for Narrative Texts: the Case of Small Language Models",
        "link": "https://arxiv.org/abs/2504.09071",
        "author": "Matt Grenander, Siddharth Varia, Paula Czarnowska, Yogarshi Vyas, Kishaloy Halder, Bonan Min",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09071v1 Announce Type: new \nAbstract: Plan-guided summarization attempts to reduce hallucinations in small language models (SLMs) by grounding generated summaries to the source text, typically by targeting fine-grained details such as dates or named entities. In this work, we investigate whether plan-based approaches in SLMs improve summarization in long document, narrative tasks. Narrative texts' length and complexity often mean they are difficult to summarize faithfully. We analyze existing plan-guided solutions targeting fine-grained details, and also propose our own higher-level, narrative-based plan formulation. Our results show that neither approach significantly improves on a baseline without planning in either summary quality or faithfulness. Human evaluation reveals that while plan-guided approaches are often well grounded to their plan, plans are equally likely to contain hallucinations compared to summaries. As a result, the plan-guided summaries are just as unfaithful as those from models without planning. Our work serves as a cautionary tale to plan-guided approaches to summarization, especially for long, complex domains such as narrative texts."
      },
      {
        "id": "oai:arXiv.org:2504.09073v1",
        "title": "A Multi-view Discourse Framework for Integrating Semantic and Syntactic Features in Dialog Agents",
        "link": "https://arxiv.org/abs/2504.09073",
        "author": "Akanksha Mehndiratta, Krishna Asawa",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09073v1 Announce Type: new \nAbstract: Multiturn dialogue models aim to generate human-like responses by leveraging conversational context, consisting of utterances from previous exchanges. Existing methods often neglect the interactions between these utterances or treat all of them as equally significant. This paper introduces a discourse-aware framework for response selection in retrieval-based dialogue systems. The proposed model first encodes each utterance and response with contextual, positional, and syntactic features using Multi-view Canonical Correlation Analysis (MCCA). It then learns discourse tokens that capture relationships between an utterance and its surrounding turns in a shared subspace via Canonical Correlation Analysis (CCA). This two-step approach effectively integrates semantic and syntactic features to build discourse-level understanding. Experiments on the Ubuntu Dialogue Corpus demonstrate that our model achieves significant improvements in automatic evaluation metrics, highlighting its effectiveness in response selection."
      },
      {
        "id": "oai:arXiv.org:2504.09076v1",
        "title": "Exploring Synergistic Ensemble Learning: Uniting CNNs, MLP-Mixers, and Vision Transformers to Enhance Image Classification",
        "link": "https://arxiv.org/abs/2504.09076",
        "author": "Mk Bashar, Ocean Monjur, Samia Islam, Mohammad Galib Shams, Niamul Quader",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09076v1 Announce Type: new \nAbstract: In recent years, Convolutional Neural Networks (CNNs), MLP-mixers, and Vision Transformers have risen to prominence as leading neural architectures in image classification. Prior research has underscored the distinct advantages of each architecture, and there is growing evidence that combining modules from different architectures can boost performance. In this study, we build upon and improve previous work exploring the complementarity between different architectures. Instead of heuristically merging modules from various architectures through trial and error, we preserve the integrity of each architecture and combine them using ensemble techniques. By maintaining the distinctiveness of each architecture, we aim to explore their inherent complementarity more deeply and with implicit isolation. This approach provides a more systematic understanding of their individual strengths.\n  In addition to uncovering insights into architectural complementarity, we showcase the effectiveness of even basic ensemble methods that combine models from diverse architectures. These methods outperform ensembles comprised of similar architectures. Our straightforward ensemble framework serves as a foundational strategy for blending complementary architectures, offering a solid starting point for further investigations into the unique strengths and synergies among different architectures and their ensembles in image classification. A direct outcome of this work is the creation of an ensemble of classification networks that surpasses the accuracy of the previous state-of-the-art single classification network on ImageNet, setting a new benchmark, all while requiring less overall latency."
      },
      {
        "id": "oai:arXiv.org:2504.09077v1",
        "title": "A Visual Self-attention Mechanism Facial Expression Recognition Network beyond Convnext",
        "link": "https://arxiv.org/abs/2504.09077",
        "author": "Bingyu Nan, Feng Liu, Xuezhong Qian, Wei Song",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09077v1 Announce Type: new \nAbstract: Facial expression recognition is an important research direction in the field of artificial intelligence. Although new breakthroughs have been made in recent years, the uneven distribution of datasets and the similarity between different categories of facial expressions, as well as the differences within the same category among different subjects, remain challenges. This paper proposes a visual facial expression signal feature processing network based on truncated ConvNeXt approach(Conv-cut), to improve the accuracy of FER under challenging conditions. The network uses a truncated ConvNeXt-Base as the feature extractor, and then we designed a Detail Extraction Block to extract detailed features, and introduced a Self-Attention mechanism to enable the network to learn the extracted features more effectively. To evaluate the proposed Conv-cut approach, we conducted experiments on the RAF-DB and FERPlus datasets, and the results show that our model has achieved state-of-the-art performance. Our code could be accessed at Github."
      },
      {
        "id": "oai:arXiv.org:2504.09083v1",
        "title": "Using Vision Language Models for Safety Hazard Identification in Construction",
        "link": "https://arxiv.org/abs/2504.09083",
        "author": "Muhammad Adil, Gaang Lee, Vicente A. Gonzalez, Qipei Mei",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09083v1 Announce Type: new \nAbstract: Safety hazard identification and prevention are the key elements of proactive safety management. Previous research has extensively explored the applications of computer vision to automatically identify hazards from image clips collected from construction sites. However, these methods struggle to identify context-specific hazards, as they focus on detecting predefined individual entities without understanding their spatial relationships and interactions. Furthermore, their limited adaptability to varying construction site guidelines and conditions hinders their generalization across different projects. These limitations reduce their ability to assess hazards in complex construction environments and adaptability to unseen risks, leading to potential safety gaps. To address these challenges, we proposed and experimentally validated a Vision Language Model (VLM)-based framework for the identification of construction hazards. The framework incorporates a prompt engineering module that structures safety guidelines into contextual queries, allowing VLM to process visual information and generate hazard assessments aligned with the regulation guide. Within this framework, we evaluated state-of-the-art VLMs, including GPT-4o, Gemini, Llama 3.2, and InternVL2, using a custom dataset of 1100 construction site images. Experimental results show that GPT-4o and Gemini 1.5 Pro outperformed alternatives and displayed promising BERTScore of 0.906 and 0.888 respectively, highlighting their ability to identify both general and context-specific hazards. However, processing times remain a significant challenge, impacting real-time feasibility. These findings offer insights into the practical deployment of VLMs for construction site hazard detection, thereby contributing to the enhancement of proactive safety management."
      },
      {
        "id": "oai:arXiv.org:2504.09085v1",
        "title": "crowd-hpo: Realistic Hyperparameter Optimization and Benchmarking for Learning from Crowds with Noisy Labels",
        "link": "https://arxiv.org/abs/2504.09085",
        "author": "Marek Herde, Lukas L\\\"uhrs, Denis Huseljic, Bernhard Sick",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09085v1 Announce Type: new \nAbstract: Crowdworking is a cost-efficient solution to acquire class labels. Since these labels are subject to noise, various approaches to learning from crowds have been proposed. Typically, these approaches are evaluated with default hyperparameters, resulting in suboptimal performance, or with hyperparameters tuned using a validation set with ground truth class labels, representing an often unrealistic scenario. Moreover, both experimental setups can produce different rankings of approaches, complicating comparisons between studies. Therefore, we introduce crowd-hpo as a realistic benchmark and experimentation protocol including hyperparameter optimization under noisy crowd-labeled data. At its core, crowd-hpo investigates model selection criteria to identify well-performing hyperparameter configurations only with access to noisy crowd-labeled validation data. Extensive experimental evaluations with neural networks show that these criteria are effective for optimizing hyperparameters in learning from crowds approaches. Accordingly, incorporating such criteria into experimentation protocols is essential for enabling more realistic and fair benchmarking."
      },
      {
        "id": "oai:arXiv.org:2504.09086v1",
        "title": "RICCARDO: Radar Hit Prediction and Convolution for Camera-Radar 3D Object Detection",
        "link": "https://arxiv.org/abs/2504.09086",
        "author": "Yunfei Long, Abhinav Kumar, Xiaoming Liu, Daniel Morris",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09086v1 Announce Type: new \nAbstract: Radar hits reflect from points on both the boundary and internal to object outlines. This results in a complex distribution of radar hits that depends on factors including object category, size, and orientation. Current radar-camera fusion methods implicitly account for this with a black-box neural network. In this paper, we explicitly utilize a radar hit distribution model to assist fusion. First, we build a model to predict radar hit distributions conditioned on object properties obtained from a monocular detector. Second, we use the predicted distribution as a kernel to match actual measured radar points in the neighborhood of the monocular detections, generating matching scores at nearby positions. Finally, a fusion stage combines context with the kernel detector to refine the matching scores. Our method achieves the state-of-the-art radar-camera detection performance on nuScenes. Our source code is available at https://github.com/longyunf/riccardo."
      },
      {
        "id": "oai:arXiv.org:2504.09094v1",
        "title": "Enhancing Dialogue Systems with Discourse-Level Understanding Using Deep Canonical Correlation Analysis",
        "link": "https://arxiv.org/abs/2504.09094",
        "author": "Akanksha Mehndiratta, Krishna Asawa",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09094v1 Announce Type: new \nAbstract: The evolution of conversational agents has been driven by the need for more contextually aware systems that can effectively manage dialogue over extended interactions. To address the limitations of existing models in capturing and utilizing long-term conversational history, we propose a novel framework that integrates Deep Canonical Correlation Analysis (DCCA) for discourse-level understanding. This framework learns discourse tokens to capture relationships between utterances and their surrounding context, enabling a better understanding of long-term dependencies. Experiments on the Ubuntu Dialogue Corpus demonstrate significant enhancement in response selection, based on the improved automatic evaluation metric scores. The results highlight the potential of DCCA in improving dialogue systems by allowing them to filter out irrelevant context and retain critical discourse information for more accurate response retrieval."
      },
      {
        "id": "oai:arXiv.org:2504.09096v1",
        "title": "High dimensional online calibration in polynomial time",
        "link": "https://arxiv.org/abs/2504.09096",
        "author": "Binghui Peng",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09096v1 Announce Type: new \nAbstract: In online (sequential) calibration, a forecaster predicts probability distributions over a finite outcome space $[d]$ over a sequence of $T$ days, with the goal of being calibrated. While asymptotically calibrated strategies are known to exist, they suffer from the curse of dimensionality: the best known algorithms require $\\exp(d)$ days to achieve non-trivial calibration.\n  In this work, we present the first asymptotically calibrated strategy that guarantees non-trivial calibration after a polynomial number of rounds. Specifically, for any desired accuracy $\\epsilon > 0$, our forecaster becomes $\\epsilon$-calibrated after $T = d^{O(1/\\epsilon^2)}$ days. We complement this result with a lower bound, proving that at least $T = d^{\\Omega(\\log(1/\\epsilon))}$ rounds are necessary to achieve $\\epsilon$-calibration. Our results resolve the open questions posed by [Abernethy-Mannor'11, Hazan-Kakade'12].\n  Our algorithm is inspired by recent breakthroughs in swap regret minimization [Peng-Rubinstein'24, Dagan et al.'24]. Despite its strong theoretical guarantees, the approach is remarkably simple and intuitive: it randomly selects among a set of sub-forecasters, each of which predicts the empirical outcome frequency over recent time windows."
      },
      {
        "id": "oai:arXiv.org:2504.09097v1",
        "title": "BIGS: Bimanual Category-agnostic Interaction Reconstruction from Monocular Videos via 3D Gaussian Splatting",
        "link": "https://arxiv.org/abs/2504.09097",
        "author": "Jeongwan On, Kyeonghwan Gwak, Gunyoung Kang, Junuk Cha, Soohyun Hwang, Hyein Hwang, Seungryul Baek",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09097v1 Announce Type: new \nAbstract: Reconstructing 3Ds of hand-object interaction (HOI) is a fundamental problem that can find numerous applications. Despite recent advances, there is no comprehensive pipeline yet for bimanual class-agnostic interaction reconstruction from a monocular RGB video, where two hands and an unknown object are interacting with each other. Previous works tackled the limited hand-object interaction case, where object templates are pre-known or only one hand is involved in the interaction. The bimanual interaction reconstruction exhibits severe occlusions introduced by complex interactions between two hands and an object. To solve this, we first introduce BIGS (Bimanual Interaction 3D Gaussian Splatting), a method that reconstructs 3D Gaussians of hands and an unknown object from a monocular video. To robustly obtain object Gaussians avoiding severe occlusions, we leverage prior knowledge of pre-trained diffusion model with score distillation sampling (SDS) loss, to reconstruct unseen object parts. For hand Gaussians, we exploit the 3D priors of hand model (i.e., MANO) and share a single Gaussian for two hands to effectively accumulate hand 3D information, given limited views. To further consider the 3D alignment between hands and objects, we include the interacting-subjects optimization step during Gaussian optimization. Our method achieves the state-of-the-art accuracy on two challenging datasets, in terms of 3D hand pose estimation (MPJPE), 3D object reconstruction (CDh, CDo, F10), and rendering quality (PSNR, SSIM, LPIPS), respectively."
      },
      {
        "id": "oai:arXiv.org:2504.09101v1",
        "title": "Synthetic Aircraft Trajectory Generation Using Time-Based VQ-VAE",
        "link": "https://arxiv.org/abs/2504.09101",
        "author": "Abdulmajid Murad, Massimiliano Ruocco",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09101v1 Announce Type: new \nAbstract: In modern air traffic management, generating synthetic flight trajectories has emerged as a promising solution for addressing data scarcity, protecting sensitive information, and supporting large-scale analyses. In this paper, we propose a novel method for trajectory synthesis by adapting the Time-Based Vector Quantized Variational Autoencoder (TimeVQVAE). Our approach leverages time-frequency domain processing, vector quantization, and transformer-based priors to capture both global and local dynamics in flight data. By discretizing the latent space and integrating transformer priors, the model learns long-range spatiotemporal dependencies and preserves coherence across entire flight paths. We evaluate the adapted TimeVQVAE using an extensive suite of quality, statistical, and distributional metrics, as well as a flyability assessment conducted in an open-source air traffic simulator. Results indicate that TimeVQVAE outperforms a temporal convolutional VAE baseline, generating synthetic trajectories that mirror real flight data in terms of spatial accuracy, temporal consistency, and statistical properties. Furthermore, the simulator-based assessment shows that most generated trajectories maintain operational feasibility, although occasional outliers underscore the potential need for additional domain-specific constraints. Overall, our findings underscore the importance of multi-scale representation learning for capturing complex flight behaviors and demonstrate the promise of TimeVQVAE in producing representative synthetic trajectories for downstream tasks such as model training, airspace design, and air traffic forecasting."
      },
      {
        "id": "oai:arXiv.org:2504.09106v1",
        "title": "Multi-modal and Multi-view Fundus Image Fusion for Retinopathy Diagnosis via Multi-scale Cross-attention and Shifted Window Self-attention",
        "link": "https://arxiv.org/abs/2504.09106",
        "author": "Yonghao Huang, Leiting Chen, Chuan Zhou",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09106v1 Announce Type: new \nAbstract: The joint interpretation of multi-modal and multi-view fundus images is critical for retinopathy prevention, as different views can show the complete 3D eyeball field and different modalities can provide complementary lesion areas. Compared with single images, the sequence relationships in multi-modal and multi-view fundus images contain long-range dependencies in lesion features. By modeling the long-range dependencies in these sequences, lesion areas can be more comprehensively mined, and modality-specific lesions can be detected. To learn the long-range dependency relationship and fuse complementary multi-scale lesion features between different fundus modalities, we design a multi-modal fundus image fusion method based on multi-scale cross-attention, which solves the static receptive field problem in previous multi-modal medical fusion methods based on attention. To capture multi-view relative positional relationships between different views and fuse comprehensive lesion features between different views, we design a multi-view fundus image fusion method based on shifted window self-attention, which also solves the computational complexity of the multi-view fundus fusion method based on self-attention is quadratic to the size and number of multi-view fundus images. Finally, we design a multi-task retinopathy diagnosis framework to help ophthalmologists reduce workload and improve diagnostic accuracy by combining the proposed two fusion methods. The experimental results of retinopathy classification and report generation tasks indicate our method's potential to improve the efficiency and reliability of retinopathy diagnosis in clinical practice, achieving a classification accuracy of 82.53\\% and a report generation BlEU-1 of 0.543."
      },
      {
        "id": "oai:arXiv.org:2504.09107v1",
        "title": "Shrinkage Initialization for Smooth Learning of Neural Networks",
        "link": "https://arxiv.org/abs/2504.09107",
        "author": "Miao Cheng, Feiyan Zhou, Hongwei Zou, Limin Wang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09107v1 Announce Type: new \nAbstract: The successes of intelligent systems have quite relied on the artificial learning of information, which lead to the broad applications of neural learning solutions. As a common sense, the training of neural networks can be largely improved by specifically defined initialization, neuron layers as well as the activation functions. Though there are sequential layer based initialization available, the generalized solution to initial stages is still desired. In this work, an improved approach to initialization of neural learning is presented, which adopts the shrinkage approach to initialize the transformation of each layer of networks. It can be universally adapted for the structures of any networks with random layers, while stable performance can be attained. Furthermore, the smooth learning of networks is adopted in this work, due to the diverse influence on neural learning. Experimental results on several artificial data sets demonstrate that, the proposed method is able to present robust results with the shrinkage initialization, and competent for smooth learning of neural networks."
      },
      {
        "id": "oai:arXiv.org:2504.09109v1",
        "title": "Probability Distribution Alignment and Low-Rank Weight Decomposition for Source-Free Domain Adaptive Brain Decoding",
        "link": "https://arxiv.org/abs/2504.09109",
        "author": "Ganxi Xu, Jinyi Long, Hanrui Wu, Jia Zhang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09109v1 Announce Type: new \nAbstract: Brain decoding currently faces significant challenges in individual differences, modality alignment, and high-dimensional embeddings. To address individual differences, researchers often use source subject data, which leads to issues such as privacy leakage and heavy data storage burdens. In modality alignment, current works focus on aligning the softmax probability distribution but neglect the alignment of marginal probability distributions, resulting in modality misalignment. Additionally, images and text are aligned separately with fMRI without considering the complex interplay between images and text, leading to poor image reconstruction. Finally, the enormous dimensionality of CLIP embeddings causes significant computational costs. Although the dimensionality of CLIP embeddings can be reduced by ignoring the number of patches obtained from images and the number of tokens acquired from text, this comes at the cost of a significant drop in model performance, creating a dilemma. To overcome these limitations, we propose a source-free domain adaptation-based brain decoding framework"
      },
      {
        "id": "oai:arXiv.org:2504.09114v1",
        "title": "Deploying Large AI Models on Resource-Limited Devices with Split Federated Learning",
        "link": "https://arxiv.org/abs/2504.09114",
        "author": "Xianke Qiang, Hongda Liu, Xinran Zhang, Zheng Chang, Ying-Chang Liang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09114v1 Announce Type: new \nAbstract: Large Artificial Intelligence Models (LAMs) powered by massive datasets, extensive parameter scales, and extensive computational resources, leading to significant transformations across various industries. Yet, their practical deployment on resource-limited mobile edge devices is hindered by critical challenges such as data privacy, constrained resources, and high overhead costs. Addressing this gap, this paper proposes a novel framework, named Quantized Split Federated Fine-Tuning Large AI Model (SFLAM). By partitioning the training load between edge devices and servers using a split learning paradigm, SFLAM can facilitate the operation of large models on devices and significantly lowers the memory requirements on edge devices. Additionally, SFLAM incorporates quantization management, power control, and bandwidth allocation strategies to enhance training efficiency while concurrently reducing energy consumption and communication latency. A theoretical analysis exploring the latency-energy trade-off is presented, and the framework's efficacy is validated via comprehensive simulations. The findings indicate that SFLAM achieves superior performance in terms of learning efficiency and scalability compared to conventional methods, thereby providing a valuable approach for enabling advanced AI services in resource-constrained scenarios."
      },
      {
        "id": "oai:arXiv.org:2504.09118v1",
        "title": "Optimizing FDTD Solvers for Electromagnetics: A Compiler-Guided Approach with High-Level Tensor Abstractions",
        "link": "https://arxiv.org/abs/2504.09118",
        "author": "Yifei He, M{\\aa}ns I. Andersson, Stefano Markidis",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09118v1 Announce Type: new \nAbstract: The Finite Difference Time Domain (FDTD) method is a widely used numerical technique for solving Maxwell's equations, particularly in computational electromagnetics and photonics. It enables accurate modeling of wave propagation in complex media and structures but comes with significant computational challenges. Traditional FDTD implementations rely on handwritten, platform-specific code that optimizes certain kernels while underperforming in others. The lack of portability increases development overhead and creates performance bottlenecks, limiting scalability across modern hardware architectures. To address these challenges, we introduce an end-to-end domain-specific compiler based on the MLIR/LLVM infrastructure for FDTD simulations. Our approach generates efficient and portable code optimized for diverse hardware platforms.We implement the three-dimensional FDTD kernel as operations on a 3D tensor abstraction with explicit computational semantics. High-level optimizations such as loop tiling, fusion, and vectorization are automatically applied by the compiler. We evaluate our customized code generation pipeline on Intel, AMD, and ARM platforms, achieving up to $10\\times$ speedup over baseline Python implementation using NumPy."
      },
      {
        "id": "oai:arXiv.org:2504.09129v1",
        "title": "A Constrained Optimization Approach for Gaussian Splatting from Coarsely-posed Images and Noisy Lidar Point Clouds",
        "link": "https://arxiv.org/abs/2504.09129",
        "author": "Jizong Peng, Tze Ho Elden Tse, Kai Xu, Wenchao Gao, Angela Yao",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09129v1 Announce Type: new \nAbstract: 3D Gaussian Splatting (3DGS) is a powerful reconstruction technique, but it needs to be initialized from accurate camera poses and high-fidelity point clouds. Typically, the initialization is taken from Structure-from-Motion (SfM) algorithms; however, SfM is time-consuming and restricts the application of 3DGS in real-world scenarios and large-scale scene reconstruction. We introduce a constrained optimization method for simultaneous camera pose estimation and 3D reconstruction that does not require SfM support. Core to our approach is decomposing a camera pose into a sequence of camera-to-(device-)center and (device-)center-to-world optimizations. To facilitate, we propose two optimization constraints conditioned to the sensitivity of each parameter group and restricts each parameter's search space. In addition, as we learn the scene geometry directly from the noisy point clouds, we propose geometric constraints to improve the reconstruction quality. Experiments demonstrate that the proposed method significantly outperforms the existing (multi-modal) 3DGS baseline and methods supplemented by COLMAP on both our collected dataset and two public benchmarks."
      },
      {
        "id": "oai:arXiv.org:2504.09130v1",
        "title": "VisuoThink: Empowering LVLM Reasoning with Multimodal Tree Search",
        "link": "https://arxiv.org/abs/2504.09130",
        "author": "Yikun Wang, Siyin Wang, Qinyuan Cheng, Zhaoye Fei, Liang Ding, Qipeng Guo, Dacheng Tao, Xipeng Qiu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09130v1 Announce Type: new \nAbstract: Recent advancements in Large Vision-Language Models have showcased remarkable capabilities. However, they often falter when confronted with complex reasoning tasks that humans typically address through visual aids and deliberate, step-by-step thinking. While existing methods have explored text-based slow thinking or rudimentary visual assistance, they fall short of capturing the intricate, interleaved nature of human visual-verbal reasoning processes. To overcome these limitations and inspired by the mechanisms of slow thinking in human cognition, we introduce VisuoThink, a novel framework that seamlessly integrates visuospatial and linguistic domains. VisuoThink facilitates multimodal slow thinking by enabling progressive visual-textual reasoning and incorporates test-time scaling through look-ahead tree search. Extensive experiments demonstrate that VisuoThink significantly enhances reasoning capabilities via inference-time scaling, even without fine-tuning, achieving state-of-the-art performance in tasks involving geometry and spatial reasoning."
      },
      {
        "id": "oai:arXiv.org:2504.09132v1",
        "title": "Self-Supervised Autoencoder Network for Robust Heart Rate Extraction from Noisy Photoplethysmogram: Applying Blind Source Separation to Biosignal Analysis",
        "link": "https://arxiv.org/abs/2504.09132",
        "author": "Matthew B. Webster, Dongheon Lee, Joonnyong Lee",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09132v1 Announce Type: new \nAbstract: Biosignals can be viewed as mixtures measuring particular physiological events, and blind source separation (BSS) aims to extract underlying source signals from mixtures. This paper proposes a self-supervised multi-encoder autoencoder (MEAE) to separate heartbeat-related source signals from photoplethysmogram (PPG), enhancing heart rate (HR) detection in noisy PPG data. The MEAE is trained on PPG signals from a large open polysomnography database without any pre-processing or data selection. The trained network is then applied to a noisy PPG dataset collected during the daily activities of nine subjects. The extracted heartbeat-related source signal significantly improves HR detection as compared to the original PPG. The absence of pre-processing and the self-supervised nature of the proposed method, combined with its strong performance, highlight the potential of BSS in biosignal analysis."
      },
      {
        "id": "oai:arXiv.org:2504.09135v1",
        "title": "Efficient and Asymptotically Unbiased Constrained Decoding for Large Language Models",
        "link": "https://arxiv.org/abs/2504.09135",
        "author": "Haotian Ye, Himanshu Jain, Chong You, Ananda Theertha Suresh, Haowei Lin, James Zou, Felix Yu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09135v1 Announce Type: new \nAbstract: In real-world applications of large language models, outputs are often required to be confined: selecting items from predefined product or document sets, generating phrases that comply with safety standards, or conforming to specialized formatting styles. To control the generation, constrained decoding has been widely adopted. However, existing prefix-tree-based constrained decoding is inefficient under GPU-based model inference paradigms, and it introduces unintended biases into the output distribution. This paper introduces Dynamic Importance Sampling for Constrained Decoding (DISC) with GPU-based Parallel Prefix-Verification (PPV), a novel algorithm that leverages dynamic importance sampling to achieve theoretically guaranteed asymptotic unbiasedness and overcomes the inefficiency of prefix-tree. Extensive experiments demonstrate the superiority of our method over existing methods in both efficiency and output quality. These results highlight the potential of our methods to improve constrained generation in applications where adherence to specific constraints is essential."
      },
      {
        "id": "oai:arXiv.org:2504.09147v1",
        "title": "Kernel-Based Enhanced Oversampling Method for Imbalanced Classification",
        "link": "https://arxiv.org/abs/2504.09147",
        "author": "Wenjie Li, Sibo Zhu, Zhijian Li, Hanlin Wang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09147v1 Announce Type: new \nAbstract: This paper introduces a novel oversampling technique designed to improve classification performance on imbalanced datasets. The proposed method enhances the traditional SMOTE algorithm by incorporating convex combination and kernel-based weighting to generate synthetic samples that better represent the minority class. Through experiments on multiple real-world datasets, we demonstrate that the new technique outperforms existing methods in terms of F1-score, G-mean, and AUC, providing a robust solution for handling imbalanced datasets in classification tasks."
      },
      {
        "id": "oai:arXiv.org:2504.09149v1",
        "title": "MASH: Masked Anchored SpHerical Distances for 3D Shape Representation and Generation",
        "link": "https://arxiv.org/abs/2504.09149",
        "author": "Changhao Li, Yu Xin, Xiaowei Zhou, Ariel Shamir, Hao Zhang, Ligang Liu, Ruizhen Hu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09149v1 Announce Type: new \nAbstract: We introduce Masked Anchored SpHerical Distances (MASH), a novel multi-view and parametrized representation of 3D shapes. Inspired by multi-view geometry and motivated by the importance of perceptual shape understanding for learning 3D shapes, MASH represents a 3D shape as a collection of observable local surface patches, each defined by a spherical distance function emanating from an anchor point. We further leverage the compactness of spherical harmonics to encode the MASH functions, combined with a generalized view cone with a parameterized base that masks the spatial extent of the spherical function to attain locality. We develop a differentiable optimization algorithm capable of converting any point cloud into a MASH representation accurately approximating ground-truth surfaces with arbitrary geometry and topology. Extensive experiments demonstrate that MASH is versatile for multiple applications including surface reconstruction, shape generation, completion, and blending, achieving superior performance thanks to its unique representation encompassing both implicit and explicit features."
      },
      {
        "id": "oai:arXiv.org:2504.09152v1",
        "title": "MatWheel: Addressing Data Scarcity in Materials Science Through Synthetic Data",
        "link": "https://arxiv.org/abs/2504.09152",
        "author": "Wentao Li, Yizhe Chen, Jiangjie Qiu, Xiaonan Wang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09152v1 Announce Type: new \nAbstract: Data scarcity and the high cost of annotation have long been persistent challenges in the field of materials science. Inspired by its potential in other fields like computer vision, we propose the MatWheel framework, which train the material property prediction model using the synthetic data generated by the conditional generative model. We explore two scenarios: fully-supervised and semi-supervised learning. Using CGCNN for property prediction and Con-CDVAE as the conditional generative model, experiments on two data-scarce material property datasets from Matminer database are conducted. Results show that synthetic data has potential in extreme data-scarce scenarios, achieving performance close to or exceeding that of real samples in all two tasks. We also find that pseudo-labels have little impact on generated data quality. Future work will integrate advanced models and optimize generation conditions to boost the effectiveness of the materials data flywheel."
      },
      {
        "id": "oai:arXiv.org:2504.09154v1",
        "title": "Exploring Modality Disruption in Multimodal Fake News Detection",
        "link": "https://arxiv.org/abs/2504.09154",
        "author": "Moyang Liu, Kaiying Yan, Yukun Liu, Ruibo Fu, Zhengqi Wen, Xuefei Liu, Chenxing Li",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09154v1 Announce Type: new \nAbstract: The rapid growth of social media has led to the widespread dissemination of fake news across multiple content forms, including text, images, audio, and video. Compared to unimodal fake news detection, multimodal fake news detection benefits from the increased availability of information across multiple modalities. However, in the context of social media, certain modalities in multimodal fake news detection tasks may contain disruptive or over-expressive information. These elements often include exaggerated or embellished content. We define this phenomenon as modality disruption and explore its impact on detection models through experiments. To address the issue of modality disruption in a targeted manner, we propose a multimodal fake news detection framework, FND-MoE. Additionally, we design a two-pass feature selection mechanism to further mitigate the impact of modality disruption. Extensive experiments on the FakeSV and FVC-2018 datasets demonstrate that FND-MoE significantly outperforms state-of-the-art methods, with accuracy improvements of 3.45% and 3.71% on the respective datasets compared to baseline models."
      },
      {
        "id": "oai:arXiv.org:2504.09155v1",
        "title": "Evolved Hierarchical Masking for Self-Supervised Learning",
        "link": "https://arxiv.org/abs/2504.09155",
        "author": "Zhanzhou Feng, Shiliang Zhang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09155v1 Announce Type: new \nAbstract: Existing Masked Image Modeling methods apply fixed mask patterns to guide the self-supervised training. As those mask patterns resort to different criteria to depict image contents, sticking to a fixed pattern leads to a limited vision cues modeling capability.This paper introduces an evolved hierarchical masking method to pursue general visual cues modeling in self-supervised learning. The proposed method leverages the vision model being trained to parse the input visual cues into a hierarchy structure, which is hence adopted to generate masks accordingly. The accuracy of hierarchy is on par with the capability of the model being trained, leading to evolved mask patterns at different training stages. Initially, generated masks focus on low-level visual cues to grasp basic textures, then gradually evolve to depict higher-level cues to reinforce the learning of more complicated object semantics and contexts. Our method does not require extra pre-trained models or annotations and ensures training efficiency by evolving the training difficulty. We conduct extensive experiments on seven downstream tasks including partial-duplicate image retrieval relying on low-level details, as well as image classification and semantic segmentation that require semantic parsing capability. Experimental results demonstrate that it substantially boosts performance across these tasks. For instance, it surpasses the recent MAE by 1.1\\% in imageNet-1K classification and 1.4\\% in ADE20K segmentation with the same training epochs. We also align the proposed method with the current research focus on LLMs. The proposed approach bridges the gap with large-scale pre-training on semantic demanding tasks and enhances intricate detail perception in tasks requiring low-level feature recognition."
      },
      {
        "id": "oai:arXiv.org:2504.09156v1",
        "title": "LEREL: Lipschitz Continuity-Constrained Emotion Recognition Ensemble Learning For Electroencephalography",
        "link": "https://arxiv.org/abs/2504.09156",
        "author": "Shengyu Gong, Yueyang Li, Zijian Kang, Weiming Zeng, Hongjie Yan, Wai Ting Siok, Nizhuan Wang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09156v1 Announce Type: new \nAbstract: Accurate and efficient perception of emotional states in oneself and others is crucial, as emotion-related disorders are associated with severe psychosocial impairments. While electroencephalography (EEG) offers a powerful tool for emotion detection, current EEG-based emotion recognition (EER) methods face key limitations: insufficient model stability, limited accuracy in processing high-dimensional nonlinear EEG signals, and poor robustness against intra-subject variability and signal noise. To address these challenges, we propose LEREL (Lipschitz continuity-constrained Emotion Recognition Ensemble Learning), a novel framework that significantly enhances both the accuracy and robustness of emotion recognition performance. The LEREL framework employs Lipschitz continuity constraints to enhance model stability and generalization in EEG emotion recognition, reducing signal variability and noise susceptibility while maintaining strong performance on small-sample datasets. The ensemble learning strategy reduces single-model bias and variance through multi-classifier decision fusion, further optimizing overall performance. Experimental results on three public benchmark datasets (EAV, FACED and SEED) demonstrate LEREL's effectiveness, achieving average recognition accuracies of 76.43%, 83.00% and 89.22%, respectively."
      },
      {
        "id": "oai:arXiv.org:2504.09160v1",
        "title": "SCFlow2: Plug-and-Play Object Pose Refiner with Shape-Constraint Scene Flow",
        "link": "https://arxiv.org/abs/2504.09160",
        "author": "Qingyuan Wang, Rui Song, Jiaojiao Li, Kerui Cheng, David Ferstl, Yinlin Hu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09160v1 Announce Type: new \nAbstract: We introduce SCFlow2, a plug-and-play refinement framework for 6D object pose estimation. Most recent 6D object pose methods rely on refinement to get accurate results. However, most existing refinement methods either suffer from noises in establishing correspondences, or rely on retraining for novel objects. SCFlow2 is based on the SCFlow model designed for refinement with shape constraint, but formulates the additional depth as a regularization in the iteration via 3D scene flow for RGBD frames. The key design of SCFlow2 is an introduction of geometry constraints into the training of recurrent matching network, by combining the rigid-motion embeddings in 3D scene flow and 3D shape prior of the target. We train SCFlow2 on a combination of dataset Objaverse, GSO and ShapeNet, and evaluate on BOP datasets with novel objects. After using our method as a post-processing, most state-of-the-art methods produce significantly better results, without any retraining or fine-tuning. The source code is available at https://scflow2.github.io."
      },
      {
        "id": "oai:arXiv.org:2504.09163v1",
        "title": "Deconfounded Reasoning for Multimodal Fake News Detection via Causal Intervention",
        "link": "https://arxiv.org/abs/2504.09163",
        "author": "Moyang Liu, Kaiying Yan, Yukun Liu, Ruibo Fu, Zhengqi Wen, Xuefei Liu, Chenxing Li",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09163v1 Announce Type: new \nAbstract: The rapid growth of social media has led to the widespread dissemination of fake news across multiple content forms, including text, images, audio, and video. Traditional unimodal detection methods fall short in addressing complex cross-modal manipulations; as a result, multimodal fake news detection has emerged as a more effective solution. However, existing multimodal approaches, especially in the context of fake news detection on social media, often overlook the confounders hidden within complex cross-modal interactions, leading models to rely on spurious statistical correlations rather than genuine causal mechanisms. In this paper, we propose the Causal Intervention-based Multimodal Deconfounded Detection (CIMDD) framework, which systematically models three types of confounders via a unified Structural Causal Model (SCM): (1) Lexical Semantic Confounder (LSC); (2) Latent Visual Confounder (LVC); (3) Dynamic Cross-Modal Coupling Confounder (DCCC). To mitigate the influence of these confounders, we specifically design three causal modules based on backdoor adjustment, frontdoor adjustment, and cross-modal joint intervention to block spurious correlations from different perspectives and achieve causal disentanglement of representations for deconfounded reasoning. Experimental results on the FakeSV and FVC datasets demonstrate that CIMDD significantly improves detection accuracy, outperforming state-of-the-art methods by 4.27% and 4.80%, respectively. Furthermore, extensive experimental results indicate that CIMDD exhibits strong generalization and robustness across diverse multimodal scenarios."
      },
      {
        "id": "oai:arXiv.org:2504.09164v1",
        "title": "Can postgraduate translation students identify machine-generated text?",
        "link": "https://arxiv.org/abs/2504.09164",
        "author": "Michael Farrell",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09164v1 Announce Type: new \nAbstract: Given the growing use of generative artificial intelligence as a tool for creating multilingual content and bypassing both machine and traditional translation methods, this study explores the ability of linguistically trained individuals to discern machine-generated output from human-written text (HT). After brief training sessions on the textual anomalies typically found in synthetic text (ST), twenty-three postgraduate translation students analysed excerpts of Italian prose and assigned likelihood scores to indicate whether they believed they were human-written or AI-generated (ChatGPT-4o). The results show that, on average, the students struggled to distinguish between HT and ST, with only two participants achieving notable accuracy. Closer analysis revealed that the students often identified the same textual anomalies in both HT and ST, although features such as low burstiness and self-contradiction were more frequently associated with ST. These findings suggest the need for improvements in the preparatory training. Moreover, the study raises questions about the necessity of editing synthetic text to make it sound more human-like and recommends further research to determine whether AI-generated text is already sufficiently natural-sounding not to require further refinement."
      },
      {
        "id": "oai:arXiv.org:2504.09170v1",
        "title": "Langformers: Unified NLP Pipelines for Language Models",
        "link": "https://arxiv.org/abs/2504.09170",
        "author": "Rabindra Lamsal, Maria Rodriguez Read, Shanika Karunasekera",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09170v1 Announce Type: new \nAbstract: Transformer-based language models have revolutionized the field of natural language processing (NLP). However, using these models often involves navigating multiple frameworks and tools, as well as writing repetitive boilerplate code. This complexity can discourage non-programmers and beginners, and even slow down prototyping for experienced developers. To address these challenges, we introduce Langformers, an open-source Python library designed to streamline NLP pipelines through a unified, factory-based interface for large language model (LLM) and masked language model (MLM) tasks. Langformers integrates conversational AI, MLM pretraining, text classification, sentence embedding/reranking, data labelling, semantic search, and knowledge distillation into a cohesive API, supporting popular platforms such as Hugging Face and Ollama. Key innovations include: (1) task-specific factories that abstract training, inference, and deployment complexities; (2) built-in memory and streaming for conversational agents; and (3) lightweight, modular design that prioritizes ease of use. Documentation: https://langformers.com"
      },
      {
        "id": "oai:arXiv.org:2504.09179v1",
        "title": "A Confounding Factors-Inhibition Adversarial Learning Framework for Multi-site fMRI Mental Disorder Identification",
        "link": "https://arxiv.org/abs/2504.09179",
        "author": "Xin Wen, Shijie Guo, Wenbo Ning, Rui Cao, Yan Niu, Bin Wan, Peng Wei, Xiaobo Liu, Jie Xiang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09179v1 Announce Type: new \nAbstract: In open data sets of functional magnetic resonance imaging (fMRI), the heterogeneity of the data is typically attributed to a combination of factors, including differences in scanning procedures, the presence of confounding effects, and population diversities between multiple sites. These factors contribute to the diminished effectiveness of representation learning, which in turn affects the overall efficacy of subsequent classification procedures. To address these limitations, we propose a novel multi-site adversarial learning network (MSalNET) for fMRI-based mental disorder detection. Firstly, a representation learning module is introduced with a node information assembly (NIA) mechanism to better extract features from functional connectivity (FC). This mechanism aggregates edge information from both horizontal and vertical directions, effectively assembling node information. Secondly, to generalize the feature across sites, we proposed a site-level feature extraction module that can learn from individual FC data, which circumvents additional prior information. Lastly, an adversarial learning network is proposed as a means of balancing the trade-off between individual classification and site regression tasks, with the introduction of a novel loss function. The proposed method was evaluated on two multi-site fMRI datasets, i.e., Autism Brain Imaging Data Exchange (ABIDE) and ADHD-200. The results indicate that the proposed method achieves a better performance than other related algorithms with the accuracy of 75.56 and 68.92 in ABIDE and ADHD-200 datasets, respectively. Furthermore, the result of the site regression indicates that the proposed method reduces site variability from a data-driven perspective. The most discriminative brain regions revealed by NIA are consistent with statistical findings, uncovering the \"black box\" of deep learning to a certain extent."
      },
      {
        "id": "oai:arXiv.org:2504.09184v1",
        "title": "Parameterized Synthetic Text Generation with SimpleStories",
        "link": "https://arxiv.org/abs/2504.09184",
        "author": "Lennart Finke, Thomas Dooms, Mat Allen, Juan Diego Rodriguez, Noa Nabeshima, Dan Braun",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09184v1 Announce Type: new \nAbstract: We present SimpleStories, a large synthetic story dataset in simple language, consisting of 2 million stories each in English and Japanese. Our method employs parametrization of prompts with features at multiple levels of abstraction, allowing for systematic control over story characteristics to ensure broad syntactic and semantic diversity. Building on and addressing limitations in the TinyStories dataset, our approach demonstrates that simplicity and variety can be achieved simultaneously in synthetic text generation at scale."
      },
      {
        "id": "oai:arXiv.org:2504.09185v1",
        "title": "Repetitive Contrastive Learning Enhances Mamba's Selectivity in Time Series Prediction",
        "link": "https://arxiv.org/abs/2504.09185",
        "author": "Wenbo Yan, Hanzhong Cao, Ying Tan",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09185v1 Announce Type: new \nAbstract: Long sequence prediction is a key challenge in time series forecasting. While Mamba-based models have shown strong performance due to their sequence selection capabilities, they still struggle with insufficient focus on critical time steps and incomplete noise suppression, caused by limited selective abilities. To address this, we introduce Repetitive Contrastive Learning (RCL), a token-level contrastive pretraining framework aimed at enhancing Mamba's selective capabilities. RCL pretrains a single Mamba block to strengthen its selective abilities and then transfers these pretrained parameters to initialize Mamba blocks in various backbone models, improving their temporal prediction performance. RCL uses sequence augmentation with Gaussian noise and applies inter-sequence and intra-sequence contrastive learning to help the Mamba module prioritize information-rich time steps while ignoring noisy ones. Extensive experiments show that RCL consistently boosts the performance of backbone models, surpassing existing methods and achieving state-of-the-art results. Additionally, we propose two metrics to quantify Mamba's selective capabilities, providing theoretical, qualitative, and quantitative evidence for the improvements brought by RCL."
      },
      {
        "id": "oai:arXiv.org:2504.09191v1",
        "title": "Feature-Aware Malicious Output Detection and Mitigation",
        "link": "https://arxiv.org/abs/2504.09191",
        "author": "Weilong Dong, Peiguang Li, Yu Tian, Xinyi Zeng, Fengdi Li, Sirui Wang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09191v1 Announce Type: new \nAbstract: The rapid advancement of large language models (LLMs) has brought significant benefits to various domains while introducing substantial risks. Despite being fine-tuned through reinforcement learning, LLMs lack the capability to discern malicious content, limiting their defense against jailbreak. To address these safety concerns, we propose a feature-aware method for harmful response rejection (FMM), which detects the presence of malicious features within the model's feature space and adaptively adjusts the model's rejection mechanism. By employing a simple discriminator, we detect potential malicious traits during the decoding phase. Upon detecting features indicative of toxic tokens, FMM regenerates the current token. By employing activation patching, an additional rejection vector is incorporated during the subsequent token generation, steering the model towards a refusal response. Experimental results demonstrate the effectiveness of our approach across multiple language models and diverse attack techniques, while crucially maintaining the models' standard generation capabilities."
      },
      {
        "id": "oai:arXiv.org:2504.09192v1",
        "title": "Towards More Efficient, Robust, Instance-adaptive, and Generalizable Online Learning",
        "link": "https://arxiv.org/abs/2504.09192",
        "author": "Zhiyong Wang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09192v1 Announce Type: new \nAbstract: The primary goal of my Ph.D. study is to develop provably efficient and practical algorithms for data-driven online sequential decision-making under uncertainty. My work focuses on reinforcement learning (RL), multi-armed bandits, and their applications, including recommendation systems, computer networks, video analytics, and large language models (LLMs). Online learning methods, such as bandits and RL, have demonstrated remarkable success - ranging from outperforming human players in complex games like Atari and Go to advancing robotics, recommendation systems, and fine-tuning LLMs. Despite these successes, many established algorithms rely on idealized models that can fail under model misspecifications or adversarial perturbations, particularly in settings where accurate prior knowledge of the underlying model class is unavailable or where malicious users operate within dynamic systems. These challenges are pervasive in real-world applications, where robust and adaptive solutions are critical. Furthermore, while worst-case guarantees provide theoretical reliability, they often fail to capture instance-dependent performance, which can lead to more efficient and practical solutions. Another key challenge lies in generalizing to new, unseen environments, a crucial requirement for deploying these methods in dynamic and unpredictable settings. To address these limitations, my research aims to develop more efficient, robust, instance-adaptive, and generalizable online learning algorithms for both reinforcement learning and bandits. Towards this end, I focus on developing more efficient, robust, instance-adaptive, and generalizable for both general reinforcement learning (RL) and bandits."
      },
      {
        "id": "oai:arXiv.org:2504.09195v1",
        "title": "ReferGPT: Towards Zero-Shot Referring Multi-Object Tracking",
        "link": "https://arxiv.org/abs/2504.09195",
        "author": "Tzoulio Chamiti, Leandro Di Bella, Adrian Munteanu, Nikos Deligiannis",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09195v1 Announce Type: new \nAbstract: Tracking multiple objects based on textual queries is a challenging task that requires linking language understanding with object association across frames. Previous works typically train the whole process end-to-end or integrate an additional referring text module into a multi-object tracker, but they both require supervised training and potentially struggle with generalization to open-set queries. In this work, we introduce ReferGPT, a novel zero-shot referring multi-object tracking framework. We provide a multi-modal large language model (MLLM) with spatial knowledge enabling it to generate 3D-aware captions. This enhances its descriptive capabilities and supports a more flexible referring vocabulary without training. We also propose a robust query-matching strategy, leveraging CLIP-based semantic encoding and fuzzy matching to associate MLLM generated captions with user queries. Extensive experiments on Refer-KITTI, Refer-KITTIv2 and Refer-KITTI+ demonstrate that ReferGPT achieves competitive performance against trained methods, showcasing its robustness and zero-shot capabilities in autonomous driving. The codes are available on https://github.com/Tzoulio/ReferGPT"
      },
      {
        "id": "oai:arXiv.org:2504.09196v1",
        "title": "RT-DATR:Real-time Unsupervised Domain Adaptive Detection Transformer with Adversarial Feature Learning",
        "link": "https://arxiv.org/abs/2504.09196",
        "author": "Feng Lv, Chunlong Xia, Shuo Wang, Huo Cao",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09196v1 Announce Type: new \nAbstract: Despite domain-adaptive object detectors based on CNN and transformers have made significant progress in cross-domain detection tasks, it is regrettable that domain adaptation for real-time transformer-based detectors has not yet been explored. Directly applying existing domain adaptation algorithms has proven to be suboptimal. In this paper, we propose RT-DATR, a simple and efficient real-time domain adaptive detection transformer. Building on RT-DETR as our base detector, we first introduce a local object-level feature alignment module to significantly enhance the feature representation of domain invariance during object transfer. Additionally, we introduce a scene semantic feature alignment module designed to boost cross-domain detection performance by aligning scene semantic features. Finally, we introduced a domain query and decoupled it from the object query to further align the instance feature distribution within the decoder layer, reduce the domain gap, and maintain discriminative ability. Experimental results on various benchmarks demonstrate that our method outperforms current state-of-the-art approaches. Our code will be released soon."
      },
      {
        "id": "oai:arXiv.org:2504.09202v1",
        "title": "From Visual Explanations to Counterfactual Explanations with Latent Diffusion",
        "link": "https://arxiv.org/abs/2504.09202",
        "author": "Tung Luu, Nam Le, Duc Le, Bac Le",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09202v1 Announce Type: new \nAbstract: Visual counterfactual explanations are ideal hypothetical images that change the decision-making of the classifier with high confidence toward the desired class while remaining visually plausible and close to the initial image. In this paper, we propose a new approach to tackle two key challenges in recent prominent works: i) determining which specific counterfactual features are crucial for distinguishing the \"concept\" of the target class from the original class, and ii) supplying valuable explanations for the non-robust classifier without relying on the support of an adversarially robust model. Our method identifies the essential region for modification through algorithms that provide visual explanations, and then our framework generates realistic counterfactual explanations by combining adversarial attacks based on pruning the adversarial gradient of the target classifier and the latent diffusion model. The proposed method outperforms previous state-of-the-art results on various evaluation criteria on ImageNet and CelebA-HQ datasets. In general, our method can be applied to arbitrary classifiers, highlight the strong association between visual and counterfactual explanations, make semantically meaningful changes from the target classifier, and provide observers with subtle counterfactual images."
      },
      {
        "id": "oai:arXiv.org:2504.09203v1",
        "title": "AerOSeg: Harnessing SAM for Open-Vocabulary Segmentation in Remote Sensing Images",
        "link": "https://arxiv.org/abs/2504.09203",
        "author": "Saikat Dutta, Akhil Vasim, Siddhant Gole, Hamid Rezatofighi, Biplab Banerjee",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09203v1 Announce Type: new \nAbstract: Image segmentation beyond predefined categories is a key challenge in remote sensing, where novel and unseen classes often emerge during inference. Open-vocabulary image Segmentation addresses these generalization issues in traditional supervised segmentation models while reducing reliance on extensive per-pixel annotations, which are both expensive and labor-intensive to obtain. Most Open-Vocabulary Segmentation (OVS) methods are designed for natural images but struggle with remote sensing data due to scale variations, orientation changes, and complex scene compositions. This necessitates the development of OVS approaches specifically tailored for remote sensing. In this context, we propose AerOSeg, a novel OVS approach for remote sensing data. First, we compute robust image-text correlation features using multiple rotated versions of the input image and domain-specific prompts. These features are then refined through spatial and class refinement blocks. Inspired by the success of the Segment Anything Model (SAM) in diverse domains, we leverage SAM features to guide the spatial refinement of correlation features. Additionally, we introduce a semantic back-projection module and loss to ensure the seamless propagation of SAM's semantic information throughout the segmentation pipeline. Finally, we enhance the refined correlation features using a multi-scale attention-aware decoder to produce the final segmentation map. We validate our SAM-guided Open-Vocabulary Remote Sensing Segmentation model on three benchmark remote sensing datasets: iSAID, DLRSD, and OpenEarthMap. Our model outperforms state-of-the-art open-vocabulary segmentation methods, achieving an average improvement of 2.54 h-mIoU."
      },
      {
        "id": "oai:arXiv.org:2504.09205v1",
        "title": "Query-based Knowledge Transfer for Heterogeneous Learning Environments",
        "link": "https://arxiv.org/abs/2504.09205",
        "author": "Norah Alballa, Wenxuan Zhang, Ziquan Liu, Ahmed M. Abdelmoniem, Mohamed Elhoseiny, Marco Canini",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09205v1 Announce Type: new \nAbstract: Decentralized collaborative learning under data heterogeneity and privacy constraints has rapidly advanced. However, existing solutions like federated learning, ensembles, and transfer learning, often fail to adequately serve the unique needs of clients, especially when local data representation is limited. To address this issue, we propose a novel framework called Query-based Knowledge Transfer (QKT) that enables tailored knowledge acquisition to fulfill specific client needs without direct data exchange. QKT employs a data-free masking strategy to facilitate communication-efficient query-focused knowledge transfer while refining task-specific parameters to mitigate knowledge interference and forgetting. Our experiments, conducted on both standard and clinical benchmarks, show that QKT significantly outperforms existing collaborative learning methods by an average of 20.91\\% points in single-class query settings and an average of 14.32\\% points in multi-class query scenarios. Further analysis and ablation studies reveal that QKT effectively balances the learning of new and existing knowledge, showing strong potential for its application in decentralized learning."
      },
      {
        "id": "oai:arXiv.org:2504.09206v1",
        "title": "Rethinking Remaining Useful Life Prediction with Scarce Time Series Data: Regression under Indirect Supervision",
        "link": "https://arxiv.org/abs/2504.09206",
        "author": "Jiaxiang Cheng, Yipeng Pang, Guoqiang Hu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09206v1 Announce Type: new \nAbstract: Supervised time series prediction relies on directly measured target variables, but real-world use cases such as predicting remaining useful life (RUL) involve indirect supervision, where the target variable is labeled as a function of another dependent variable. Trending temporal regression techniques rely on sequential time series inputs to capture temporal patterns, requiring interpolation when dealing with sparsely and irregularly sampled covariates along the timeline. However, interpolation can introduce significant biases, particularly with highly scarce data. In this paper, we address the RUL prediction problem with data scarcity as time series regression under indirect supervision. We introduce a unified framework called parameterized static regression, which takes single data points as inputs for regression of target values, inherently handling data scarcity without requiring interpolation. The time dependency under indirect supervision is captured via a parametrical rectification (PR) process, approximating a parametric function during inference with historical posteriori estimates, following the same underlying distribution used for labeling during training. Additionally, we propose a novel batch training technique for tasks in indirect supervision to prevent overfitting and enhance efficiency. We evaluate our model on public benchmarks for RUL prediction with simulated data scarcity. Our method demonstrates competitive performance in prediction accuracy when dealing with highly scarce time series data."
      },
      {
        "id": "oai:arXiv.org:2504.09210v1",
        "title": "FairACE: Achieving Degree Fairness in Graph Neural Networks via Contrastive and Adversarial Group-Balanced Training",
        "link": "https://arxiv.org/abs/2504.09210",
        "author": "Jiaxin Liu, Xiaoqian Jiang, Cangqi Zhou, Jing Zhang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09210v1 Announce Type: new \nAbstract: Fairness has been a significant challenge in graph neural networks (GNNs) since degree biases often result in un-equal prediction performance among nodes with varying degrees. Existing GNN models focus on prediction accuracy, frequently overlooking fairness across different degree groups. To addressthis issue, we propose a novel GNN framework, namely Fairness- Aware Asymmetric Contrastive Ensemble (FairACE), which inte-grates asymmetric contrastive learning with adversarial training to improve degree fairness. FairACE captures one-hop local neighborhood information and two-hop monophily similarity to create fairer node representations and employs a degree fairness regulator to balance performance between high-degree and low-degree nodes. During model training, a novel group-balanced fairness loss is proposed to minimize classification disparities across degree groups. In addition, we also propose a novel fairness metric, the Accuracy Distribution Gap (ADG), which can quantitatively assess and ensure equitable performance across different degree-based node groups. Experimental results on both synthetic and real-world datasets demonstrate that FairACE significantly improves degree fairness metrics while maintaining competitive accuracy in comparison to the state-of-the-art GNN models."
      },
      {
        "id": "oai:arXiv.org:2504.09211v1",
        "title": "Accurate Diagnosis of Respiratory Viruses Using an Explainable Machine Learning with Mid-Infrared Biomolecular Fingerprinting of Nasopharyngeal Secretions",
        "link": "https://arxiv.org/abs/2504.09211",
        "author": "Wenwen Zhang, Zhouzhuo Tang, Yingmei Feng, Xia Yu, Qi Jie Wang, Zhiping Lin",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09211v1 Announce Type: new \nAbstract: Accurate identification of respiratory viruses (RVs) is critical for outbreak control and public health. This study presents a diagnostic system that combines Attenuated Total Reflectance Fourier Transform Infrared Spectroscopy (ATR-FTIR) from nasopharyngeal secretions with an explainable Rotary Position Embedding-Sparse Attention Transformer (RoPE-SAT) model to accurately identify multiple RVs within 10 minutes. Spectral data (4000-00 cm-1) were collected, and the bio-fingerprint region (1800-900 cm-1) was employed for analysis. Standard normal variate (SNV) normalization and second-order derivation were applied to reduce scattering and baseline drift. Gradient-weighted class activation mapping (Grad-CAM) was employed to generate saliency maps, highlighting spectral regions most relevant to classification and enhancing the interpretability of model outputs. Two independent cohorts from Beijing Youan Hospital, processed with different viral transport media (VTMs) and drying methods, were evaluated, with one including influenza B, SARS-CoV-2, and healthy controls, and the other including mycoplasma, SARS-CoV-2, and healthy controls. The model achieved sensitivity and specificity above 94.40% across both cohorts. By correlating model-selected infrared regions with known biomolecular signatures, we verified that the system effectively recognizes virus-specific spectral fingerprints, including lipids, Amide I, Amide II, Amide III, nucleic acids, and carbohydrates, and leverages their weighted contributions for accurate classification."
      },
      {
        "id": "oai:arXiv.org:2504.09215v1",
        "title": "Multi-scale Activation, Refinement, and Aggregation: Exploring Diverse Cues for Fine-Grained Bird Recognition",
        "link": "https://arxiv.org/abs/2504.09215",
        "author": "Zhicheng Zhang, Hao Tang, Jinhui Tang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09215v1 Announce Type: new \nAbstract: Given the critical role of birds in ecosystems, Fine-Grained Bird Recognition (FGBR) has gained increasing attention, particularly in distinguishing birds within similar subcategories. Although Vision Transformer (ViT)-based methods often outperform Convolutional Neural Network (CNN)-based methods in FGBR, recent studies reveal that the limited receptive field of plain ViT model hinders representational richness and makes them vulnerable to scale variance. Thus, enhancing the multi-scale capabilities of existing ViT-based models to overcome this bottleneck in FGBR is a worthwhile pursuit. In this paper, we propose a novel framework for FGBR, namely Multi-scale Diverse Cues Modeling (MDCM), which explores diverse cues at different scales across various stages of a multi-scale Vision Transformer (MS-ViT) in an \"Activation-Selection-Aggregation\" paradigm. Specifically, we first propose a multi-scale cue activation module to ensure the discriminative cues learned at different stage are mutually different. Subsequently, a multi-scale token selection mechanism is proposed to remove redundant noise and highlight discriminative, scale-specific cues at each stage. Finally, the selected tokens from each stage are independently utilized for bird recognition, and the recognition results from multiple stages are adaptively fused through a multi-scale dynamic aggregation mechanism for final model decisions. Both qualitative and quantitative results demonstrate the effectiveness of our proposed MDCM, which outperforms CNN- and ViT-based models on several widely-used FGBR benchmarks."
      },
      {
        "id": "oai:arXiv.org:2504.09223v1",
        "title": "DL-QAT: Weight-Decomposed Low-Rank Quantization-Aware Training for Large Language Models",
        "link": "https://arxiv.org/abs/2504.09223",
        "author": "Wenjin Ke, Zhe Li, Dong Li, Lu Tian, Emad Barsoum",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09223v1 Announce Type: new \nAbstract: Improving the efficiency of inference in Large Language Models (LLMs) is a critical area of research. Post-training Quantization (PTQ) is a popular technique, but it often faces challenges at low-bit levels, particularly in downstream tasks. Quantization-aware Training (QAT) can alleviate this problem, but it requires significantly more computational resources. To tackle this, we introduced Weight-Decomposed Low-Rank Quantization-Aware Training (DL-QAT), which merges the advantages of QAT while training only less than 1% of the total parameters. Specifically, we introduce a group-specific quantization magnitude to adjust the overall scale of each quantization group. Within each quantization group, we use LoRA matrices to update the weight size and direction in the quantization space. We validated the effectiveness of our method on the LLaMA and LLaMA2 model families. The results show significant improvements over our baseline method across different quantization granularities. For instance, for LLaMA-7B, our approach outperforms the previous state-of-the-art method by 4.2% in MMLU on 3-bit LLaMA-7B model. Additionally, our quantization results on pre-trained models also surpass previous QAT methods, demonstrating the superior performance and efficiency of our approach."
      },
      {
        "id": "oai:arXiv.org:2504.09228v1",
        "title": "Learning Occlusion-Robust Vision Transformers for Real-Time UAV Tracking",
        "link": "https://arxiv.org/abs/2504.09228",
        "author": "You Wu, Xucheng Wang, Xiangyang Yang, Mengyuan Liu, Dan Zeng, Hengzhou Ye, Shuiwang Li",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09228v1 Announce Type: new \nAbstract: Single-stream architectures using Vision Transformer (ViT) backbones show great potential for real-time UAV tracking recently. However, frequent occlusions from obstacles like buildings and trees expose a major drawback: these models often lack strategies to handle occlusions effectively. New methods are needed to enhance the occlusion resilience of single-stream ViT models in aerial tracking. In this work, we propose to learn Occlusion-Robust Representations (ORR) based on ViTs for UAV tracking by enforcing an invariance of the feature representation of a target with respect to random masking operations modeled by a spatial Cox process. Hopefully, this random masking approximately simulates target occlusions, thereby enabling us to learn ViTs that are robust to target occlusion for UAV tracking. This framework is termed ORTrack. Additionally, to facilitate real-time applications, we propose an Adaptive Feature-Based Knowledge Distillation (AFKD) method to create a more compact tracker, which adaptively mimics the behavior of the teacher model ORTrack according to the task's difficulty. This student model, dubbed ORTrack-D, retains much of ORTrack's performance while offering higher efficiency. Extensive experiments on multiple benchmarks validate the effectiveness of our method, demonstrating its state-of-the-art performance. Codes is available at https://github.com/wuyou3474/ORTrack."
      },
      {
        "id": "oai:arXiv.org:2504.09246v1",
        "title": "Type-Constrained Code Generation with Language Models",
        "link": "https://arxiv.org/abs/2504.09246",
        "author": "Niels M\\\"undler, Jingxuan He, Hao Wang, Koushik Sen, Dawn Song, Martin Vechev",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09246v1 Announce Type: new \nAbstract: Large language models (LLMs) have achieved notable success in code generation. However, they still frequently produce uncompilable output because their next-token inference procedure does not model formal aspects of code. Although constrained decoding is a promising approach to alleviate this issue, it has only been applied to handle either domain-specific languages or syntactic language features. This leaves typing errors, which are beyond the domain of syntax and generally hard to adequately constrain. To address this challenge, we introduce a type-constrained decoding approach that leverages type systems to guide code generation. We develop novel prefix automata for this purpose and introduce a sound approach to enforce well-typedness based on type inference and a search over inhabitable types. We formalize our approach on a simply-typed language and extend it to TypeScript to demonstrate practicality. Our evaluation on HumanEval shows that our approach reduces compilation errors by more than half and increases functional correctness in code synthesis, translation, and repair tasks across LLMs of various sizes and model families, including SOTA open-weight models with more than 30B parameters."
      },
      {
        "id": "oai:arXiv.org:2504.09249v1",
        "title": "NoTeS-Bank: Benchmarking Neural Transcription and Search for Scientific Notes Understanding",
        "link": "https://arxiv.org/abs/2504.09249",
        "author": "Aniket Pal, Sanket Biswas, Alloy Das, Ayush Lodh, Priyanka Banerjee, Soumitri Chattopadhyay, Dimosthenis Karatzas, Josep Llados, C. V. Jawahar",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09249v1 Announce Type: new \nAbstract: Understanding and reasoning over academic handwritten notes remains a challenge in document AI, particularly for mathematical equations, diagrams, and scientific notations. Existing visual question answering (VQA) benchmarks focus on printed or structured handwritten text, limiting generalization to real-world note-taking. To address this, we introduce NoTeS-Bank, an evaluation benchmark for Neural Transcription and Search in note-based question answering. NoTeS-Bank comprises complex notes across multiple domains, requiring models to process unstructured and multimodal content. The benchmark defines two tasks: (1) Evidence-Based VQA, where models retrieve localized answers with bounding-box evidence, and (2) Open-Domain VQA, where models classify the domain before retrieving relevant documents and answers. Unlike classical Document VQA datasets relying on optical character recognition (OCR) and structured data, NoTeS-BANK demands vision-language fusion, retrieval, and multimodal reasoning. We benchmark state-of-the-art Vision-Language Models (VLMs) and retrieval frameworks, exposing structured transcription and reasoning limitations. NoTeS-Bank provides a rigorous evaluation with NDCG@5, MRR, Recall@K, IoU, and ANLS, establishing a new standard for visual document understanding and reasoning."
      },
      {
        "id": "oai:arXiv.org:2504.09255v1",
        "title": "FVQ: A Large-Scale Dataset and A LMM-based Method for Face Video Quality Assessment",
        "link": "https://arxiv.org/abs/2504.09255",
        "author": "Sijing Wu, Yunhao Li, Ziwen Xu, Yixuan Gao, Huiyu Duan, Wei Sun, Guangtao Zhai",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09255v1 Announce Type: new \nAbstract: Face video quality assessment (FVQA) deserves to be explored in addition to general video quality assessment (VQA), as face videos are the primary content on social media platforms and human visual system (HVS) is particularly sensitive to human faces. However, FVQA is rarely explored due to the lack of large-scale FVQA datasets. To fill this gap, we present the first large-scale in-the-wild FVQA dataset, FVQ-20K, which contains 20,000 in-the-wild face videos together with corresponding mean opinion score (MOS) annotations. Along with the FVQ-20K dataset, we further propose a specialized FVQA method named FVQ-Rater to achieve human-like rating and scoring for face video, which is the first attempt to explore the potential of large multimodal models (LMMs) for the FVQA task. Concretely, we elaborately extract multi-dimensional features including spatial features, temporal features, and face-specific features (i.e., portrait features and face embeddings) to provide comprehensive visual information, and take advantage of the LoRA-based instruction tuning technique to achieve quality-specific fine-tuning, which shows superior performance on both FVQ-20K and CFVQA datasets. Extensive experiments and comprehensive analysis demonstrate the significant potential of the FVQ-20K dataset and FVQ-Rater method in promoting the development of FVQA."
      },
      {
        "id": "oai:arXiv.org:2504.09257v1",
        "title": "MiMIC: Multi-Modal Indian Earnings Calls Dataset to Predict Stock Prices",
        "link": "https://arxiv.org/abs/2504.09257",
        "author": "Sohom Ghosh, Arnab Maji, Sudip Kumar Naskar",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09257v1 Announce Type: new \nAbstract: Predicting stock market prices following corporate earnings calls remains a significant challenge for investors and researchers alike, requiring innovative approaches that can process diverse information sources. This study investigates the impact of corporate earnings calls on stock prices by introducing a multi-modal predictive model. We leverage textual data from earnings call transcripts, along with images and tables from accompanying presentations, to forecast stock price movements on the trading day immediately following these calls. To facilitate this research, we developed the MiMIC (Multi-Modal Indian Earnings Calls) dataset, encompassing companies representing the Nifty 50, Nifty MidCap 50, and Nifty Small 50 indices. The dataset includes earnings call transcripts, presentations, fundamentals, technical indicators, and subsequent stock prices. We present a multimodal analytical framework that integrates quantitative variables with predictive signals derived from textual and visual modalities, thereby enabling a holistic approach to feature representation and analysis. This multi-modal approach demonstrates the potential for integrating diverse information sources to enhance financial forecasting accuracy. To promote further research in computational economics, we have made the MiMIC dataset publicly available under the CC-NC-SA-4.0 licence. Our work contributes to the growing body of literature on market reactions to corporate communications and highlights the efficacy of multi-modal machine learning techniques in financial analysis."
      },
      {
        "id": "oai:arXiv.org:2504.09258v1",
        "title": "PathVLM-R1: A Reinforcement Learning-Driven Reasoning Model for Pathology Visual-Language Tasks",
        "link": "https://arxiv.org/abs/2504.09258",
        "author": "Jianyu Wu, Hao Yang, Xinhua Zeng, Guibing He, Zhiyu Chen, Zihui Li, Xiaochuan Zhang, Yangyang Ma, Run Fang, Yang Liu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09258v1 Announce Type: new \nAbstract: The diagnosis of pathological images is often limited by expert availability and regional disparities, highlighting the importance of automated diagnosis using Vision-Language Models (VLMs). Traditional multimodal models typically emphasize outcomes over the reasoning process, compromising the reliability of clinical decisions. To address the weak reasoning abilities and lack of supervised processes in pathological VLMs, we have innovatively proposed PathVLM-R1, a visual language model designed specifically for pathological images. We have based our model on Qwen2.5-VL-7B-Instruct and enhanced its performance for pathological tasks through meticulously designed post-training strategies. Firstly, we conduct supervised fine-tuning guided by pathological data to imbue the model with foundational pathological knowledge, forming a new pathological base model. Subsequently, we introduce Group Relative Policy Optimization (GRPO) and propose a dual reward-driven reinforcement learning optimization, ensuring strict constraint on logical supervision of the reasoning process and accuracy of results via cross-modal process reward and outcome accuracy reward. In the pathological image question-answering tasks, the testing results of PathVLM-R1 demonstrate a 14% improvement in accuracy compared to baseline methods, and it demonstrated superior performance compared to the Qwen2.5-VL-32B version despite having a significantly smaller parameter size. Furthermore, in out-domain data evaluation involving four medical imaging modalities: Computed Tomography (CT), dermoscopy, fundus photography, and Optical Coherence Tomography (OCT) images: PathVLM-R1's transfer performance improved by an average of 17.3% compared to traditional SFT methods. These results clearly indicate that PathVLM-R1 not only enhances accuracy but also possesses broad applicability and expansion potential."
      },
      {
        "id": "oai:arXiv.org:2504.09261v1",
        "title": "Head-Aware KV Cache Compression for Efficient Visual Autoregressive Modeling",
        "link": "https://arxiv.org/abs/2504.09261",
        "author": "Ziran Qin, Youru Lv, Mingbao Lin, Zeren Zhang, Danping Zou, Weiyao Lin",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09261v1 Announce Type: new \nAbstract: Visual Autoregressive (VAR) models have emerged as a powerful approach for multi-modal content creation, offering high efficiency and quality across diverse multimedia applications. However, they face significant memory bottlenecks due to extensive KV cache accumulation during inference. Existing KV cache compression techniques for large language models are suboptimal for VAR models due to, as we identify in this paper, two distinct categories of attention heads in VAR models: Structural Heads, which preserve spatial coherence through diagonal attention patterns, and Contextual Heads, which maintain semantic consistency through vertical attention patterns. These differences render single-strategy KV compression techniques ineffective for VAR models. To address this, we propose HACK, a training-free Head-Aware Compression method for KV cache. HACK allocates asymmetric cache budgets and employs pattern-specific compression strategies tailored to the essential characteristics of each head category. Experiments on Infinity-2B, Infinity-8B, and VAR-d30 demonstrate its effectiveness in text-to-image and class-conditional generation tasks. HACK can hack down up to 50\\% and 70\\% of cache with minimal performance degradation for VAR-d30 and Infinity-8B, respectively. Even with 70\\% and 90\\% KV cache compression in VAR-d30 and Infinity-8B, HACK still maintains high-quality generation while reducing memory usage by 44.2\\% and 58.9\\%, respectively."
      },
      {
        "id": "oai:arXiv.org:2504.09265v1",
        "title": "Mixture of Group Experts for Learning Invariant Representations",
        "link": "https://arxiv.org/abs/2504.09265",
        "author": "Lei Kang, Jia Li, Mi Tian, Hua Huang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09265v1 Announce Type: new \nAbstract: Sparsely activated Mixture-of-Experts (MoE) models effectively increase the number of parameters while maintaining consistent computational costs per token. However, vanilla MoE models often suffer from limited diversity and specialization among experts, constraining their performance and scalability, especially as the number of experts increases. In this paper, we present a novel perspective on vanilla MoE with top-$k$ routing inspired by sparse representation. This allows us to bridge established theoretical insights from sparse representation into MoE models. Building on this foundation, we propose a group sparse regularization approach for the input of top-$k$ routing, termed Mixture of Group Experts (MoGE). MoGE indirectly regularizes experts by imposing structural constraints on the routing inputs, while preserving the original MoE architecture. Furthermore, we organize the routing input into a 2D topographic map, spatially grouping neighboring elements. This structure enables MoGE to capture representations invariant to minor transformations, thereby significantly enhancing expert diversity and specialization. Comprehensive evaluations across various Transformer models for image classification and language modeling tasks demonstrate that MoGE substantially outperforms its MoE counterpart, with minimal additional memory and computation overhead. Our approach provides a simple yet effective solution to scale the number of experts and reduce redundancy among them. The source code is included in the supplementary material and will be publicly released."
      },
      {
        "id": "oai:arXiv.org:2504.09282v1",
        "title": "VideoAds for Fast-Paced Video Understanding: Where Opensource Foundation Models Beat GPT-4o & Gemini-1.5 Pro",
        "link": "https://arxiv.org/abs/2504.09282",
        "author": "Zheyuan Zhang, Monica Dou, Linkai Peng, Hongyi Pan, Ulas Bagci, Boqing Gong",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09282v1 Announce Type: new \nAbstract: Advertisement videos serve as a rich and valuable source of purpose-driven information, encompassing high-quality visual, textual, and contextual cues designed to engage viewers. They are often more complex than general videos of similar duration due to their structured narratives and rapid scene transitions, posing significant challenges to multi-modal large language models (MLLMs). In this work, we introduce VideoAds, the first dataset tailored for benchmarking the performance of MLLMs on advertisement videos. VideoAds comprises well-curated advertisement videos with complex temporal structures, accompanied by \\textbf{manually} annotated diverse questions across three core tasks: visual finding, video summary, and visual reasoning. We propose a quantitative measure to compare VideoAds against existing benchmarks in terms of video complexity. Through extensive experiments, we find that Qwen2.5-VL-72B, an opensource MLLM, achieves 73.35\\% accuracy on VideoAds, outperforming GPT-4o (66.82\\%) and Gemini-1.5 Pro (69.66\\%); the two proprietary models especially fall behind the opensource model in video summarization and reasoning, but perform the best in visual finding. Notably, human experts easily achieve a remarkable accuracy of 94.27\\%. These results underscore the necessity of advancing MLLMs' temporal modeling capabilities and highlight VideoAds as a potentially pivotal benchmark for future research in understanding video that requires high FPS sampling. The dataset and evaluation code will be publicly available at https://videoadsbenchmark.netlify.app."
      },
      {
        "id": "oai:arXiv.org:2504.09289v1",
        "title": "Sparse Hybrid Linear-Morphological Networks",
        "link": "https://arxiv.org/abs/2504.09289",
        "author": "Konstantinos Fotopoulos, Christos Garoufis, Petros Maragos",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09289v1 Announce Type: new \nAbstract: We investigate hybrid linear-morphological networks. Recent studies highlight the inherent affinity of morphological layers to pruning, but also their difficulty in training. We propose a hybrid network structure, wherein morphological layers are inserted between the linear layers of the network, in place of activation functions. We experiment with the following morphological layers: 1) maxout pooling layers (as a special case of a morphological layer), 2) fully connected dense morphological layers, and 3) a novel, sparsely initialized variant of (2). We conduct experiments on the Magna-Tag-A-Tune (music auto-tagging) and CIFAR-10 (image classification) datasets, replacing the linear classification heads of state-of-the-art convolutional network architectures with our proposed network structure for the various morphological layers. We demonstrate that these networks induce sparsity to their linear layers, making them more prunable under L1 unstructured pruning. We also show that on MTAT our proposed sparsely initialized layer achieves slightly better performance than ReLU, maxout, and densely initialized max-plus layers, and exhibits faster initial convergence."
      },
      {
        "id": "oai:arXiv.org:2504.09291v1",
        "title": "Towards Explainable Partial-AIGC Image Quality Assessment",
        "link": "https://arxiv.org/abs/2504.09291",
        "author": "Jiaying Qian, Ziheng Jia, Zicheng Zhang, Zeyu Zhang, Guangtao Zhai, Xiongkuo Min",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09291v1 Announce Type: new \nAbstract: The rapid advancement of AI-driven visual generation technologies has catalyzed significant breakthroughs in image manipulation, particularly in achieving photorealistic localized editing effects on natural scene images (NSIs). Despite extensive research on image quality assessment (IQA) for AI-generated images (AGIs), most studies focus on fully AI-generated outputs (e.g., text-to-image generation), leaving the quality assessment of partial-AIGC images (PAIs)-images with localized AI-driven edits an almost unprecedented field. Motivated by this gap, we construct the first large-scale PAI dataset towards explainable partial-AIGC image quality assessment (EPAIQA), the EPAIQA-15K, which includes 15K images with localized AI manipulation in different regions and over 300K multi-dimensional human ratings. Based on this, we leverage large multi-modal models (LMMs) and propose a three-stage model training paradigm. This paradigm progressively trains the LMM for editing region grounding, quantitative quality scoring, and quality explanation. Finally, we develop the EPAIQA series models, which possess explainable quality feedback capabilities. Our work represents a pioneering effort in the perceptual IQA field for comprehensive PAI quality assessment."
      },
      {
        "id": "oai:arXiv.org:2504.09297v1",
        "title": "Cycle Training with Semi-Supervised Domain Adaptation: Bridging Accuracy and Efficiency for Real-Time Mobile Scene Detection",
        "link": "https://arxiv.org/abs/2504.09297",
        "author": "Huu-Phong Phan-Nguyen, Anh Dao, Tien-Huy Nguyen, Tuan Quang, Huu-Loc Tran, Tinh-Anh Nguyen-Nhu, Huy-Thach Pham, Quan Nguyen, Hoang M. Le, Quang-Vinh Dinh",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09297v1 Announce Type: new \nAbstract: Nowadays, smartphones are ubiquitous, and almost everyone owns one. At the same time, the rapid development of AI has spurred extensive research on applying deep learning techniques to image classification. However, due to the limited resources available on mobile devices, significant challenges remain in balancing accuracy with computational efficiency. In this paper, we propose a novel training framework called Cycle Training, which adopts a three-stage training process that alternates between exploration and stabilization phases to optimize model performance. Additionally, we incorporate Semi-Supervised Domain Adaptation (SSDA) to leverage the power of large models and unlabeled data, thereby effectively expanding the training dataset. Comprehensive experiments on the CamSSD dataset for mobile scene detection demonstrate that our framework not only significantly improves classification accuracy but also ensures real-time inference efficiency. Specifically, our method achieves a 94.00% in Top-1 accuracy and a 99.17% in Top-3 accuracy and runs inference in just 1.61ms using CPU, demonstrating its suitability for real-world mobile deployment."
      },
      {
        "id": "oai:arXiv.org:2504.09298v1",
        "title": "A Lightweight Moment Retrieval System with Global Re-Ranking and Robust Adaptive Bidirectional Temporal Search",
        "link": "https://arxiv.org/abs/2504.09298",
        "author": "Tinh-Anh Nguyen-Nhu, Huu-Loc Tran, Nguyen-Khang Le, Minh-Nhat Nguyen, Tien-Huy Nguyen, Hoang-Long Nguyen-Huu, Huu-Phong Phan-Nguyen, Huy-Thach Pham, Quan Nguyen, Hoang M. Le, Quang-Vinh Dinh",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09298v1 Announce Type: new \nAbstract: The exponential growth of digital video content has posed critical challenges in moment-level video retrieval, where existing methodologies struggle to efficiently localize specific segments within an expansive video corpus. Current retrieval systems are constrained by computational inefficiencies, temporal context limitations, and the intrinsic complexity of navigating video content. In this paper, we address these limitations through a novel Interactive Video Corpus Moment Retrieval framework that integrates a SuperGlobal Reranking mechanism and Adaptive Bidirectional Temporal Search (ABTS), strategically optimizing query similarity, temporal stability, and computational resources. By preprocessing a large corpus of videos using a keyframe extraction model and deduplication technique through image hashing, our approach provides a scalable solution that significantly reduces storage requirements while maintaining high localization precision across diverse video repositories."
      },
      {
        "id": "oai:arXiv.org:2504.09299v1",
        "title": "Beyond Glucose-Only Assessment: Advancing Nocturnal Hypoglycemia Prediction in Children with Type 1 Diabetes",
        "link": "https://arxiv.org/abs/2504.09299",
        "author": "Marco Voegeli, Sonia Laguna, Heike Leutheuser, Marc Pfister, Marie-Anne Burckhardt, Julia E Vogt",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09299v1 Announce Type: new \nAbstract: The dead-in-bed syndrome describes the sudden and unexplained death of young individuals with Type 1 Diabetes (T1D) without prior long-term complications. One leading hypothesis attributes this phenomenon to nocturnal hypoglycemia (NH), a dangerous drop in blood glucose during sleep. This study aims to improve NH prediction in children with T1D by leveraging physiological data and machine learning (ML) techniques. We analyze an in-house dataset collected from 16 children with T1D, integrating physiological metrics from wearable sensors. We explore predictive performance through feature engineering, model selection, architectures, and oversampling. To address data limitations, we apply transfer learning from a publicly available adult dataset. Our results achieve an AUROC of 0.75 +- 0.21 on the in-house dataset, further improving to 0.78 +- 0.05 with transfer learning. This research moves beyond glucose-only predictions by incorporating physiological parameters, showcasing the potential of ML to enhance NH detection and improve clinical decision-making for pediatric diabetes management."
      },
      {
        "id": "oai:arXiv.org:2504.09305v1",
        "title": "Enhancing Contrastive Demonstration Selection with Semantic Diversity for Robust In-Context Machine Translation",
        "link": "https://arxiv.org/abs/2504.09305",
        "author": "Owen Patterson, Chee Ng",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09305v1 Announce Type: new \nAbstract: In-Context Learning (ICL) empowers large language models to perform tasks by conditioning on a few input-output examples. However, the performance of ICL is highly sensitive to the selection of these demonstrations. While existing methods focus on similarity or contrastive selection, they often overlook the importance of diversity among the chosen examples. In this paper, we propose DiverseConE (Diversity-Enhanced Contrastive Example Selection), a novel approach for demonstration selection in in-context learning for machine translation. Our method builds upon contrastive selection by incorporating a diversity enhancement step based on embedding space dissimilarity. We conduct extensive experiments on the Llama2-7b model across four language pairs (English-Chinese, Chinese-English, Russian-German, German-Russian) in 1-shot and 3-shot settings, using COMET20 and COMET22 for evaluation. Our results demonstrate that DiverseConE consistently outperforms strong baseline methods, including random selection, BM25, TopK, and a state-of-the-art contrastive selection method. Further analysis, including diversity metrics and human evaluation, validates the effectiveness of our approach and highlights the benefits of considering demonstration diversity for improved translation quality."
      },
      {
        "id": "oai:arXiv.org:2504.09309v1",
        "title": "Improving the Accuracy and Efficiency of Legal Document Tagging with Large Language Models and Instruction Prompts",
        "link": "https://arxiv.org/abs/2504.09309",
        "author": "Emily Johnson, Xavier Holt, Noah Wilson",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09309v1 Announce Type: new \nAbstract: Legal multi-label classification is a critical task for organizing and accessing the vast amount of legal documentation. Despite its importance, it faces challenges such as the complexity of legal language, intricate label dependencies, and significant label imbalance. In this paper, we propose Legal-LLM, a novel approach that leverages the instruction-following capabilities of Large Language Models (LLMs) through fine-tuning. We reframe the multi-label classification task as a structured generation problem, instructing the LLM to directly output the relevant legal categories for a given document. We evaluate our method on two benchmark datasets, POSTURE50K and EURLEX57K, using micro-F1 and macro-F1 scores. Our experimental results demonstrate that Legal-LLM outperforms a range of strong baseline models, including traditional methods and other Transformer-based approaches. Furthermore, ablation studies and human evaluations validate the effectiveness of our approach, particularly in handling label imbalance and generating relevant and accurate legal labels."
      },
      {
        "id": "oai:arXiv.org:2504.09322v1",
        "title": "MedIL: Implicit Latent Spaces for Generating Heterogeneous Medical Images at Arbitrary Resolutions",
        "link": "https://arxiv.org/abs/2504.09322",
        "author": "Tyler Spears, Shen Zhu, Yinzhu Jin, Aman Shrivastava, P. Thomas Fletcher",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09322v1 Announce Type: new \nAbstract: In this work, we introduce MedIL, a first-of-its-kind autoencoder built for encoding medical images with heterogeneous sizes and resolutions for image generation. Medical images are often large and heterogeneous, where fine details are of vital clinical importance. Image properties change drastically when considering acquisition equipment, patient demographics, and pathology, making realistic medical image generation challenging. Recent work in latent diffusion models (LDMs) has shown success in generating images resampled to a fixed-size. However, this is a narrow subset of the resolutions native to image acquisition, and resampling discards fine anatomical details. MedIL utilizes implicit neural representations to treat images as continuous signals, where encoding and decoding can be performed at arbitrary resolutions without prior resampling. We quantitatively and qualitatively show how MedIL compresses and preserves clinically-relevant features over large multi-site, multi-resolution datasets of both T1w brain MRIs and lung CTs. We further demonstrate how MedIL can influence the quality of images generated with a diffusion model, and discuss how MedIL can enhance generative models to resemble raw clinical acquisitions."
      },
      {
        "id": "oai:arXiv.org:2504.09326v1",
        "title": "Infused Suppression Of Magnification Artefacts For Micro-AU Detection",
        "link": "https://arxiv.org/abs/2504.09326",
        "author": "Huai-Qian Khor, Yante Li, Xingxun Jiang, Guoying Zhao",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09326v1 Announce Type: new \nAbstract: Facial micro-expressions are spontaneous, brief and subtle facial motions that unveil the underlying, suppressed emotions. Detecting Action Units (AUs) in micro-expressions is crucial because it yields a finer representation of facial motions than categorical emotions, effectively resolving the ambiguity among different expressions. One of the difficulties in micro-expression analysis is that facial motions are subtle and brief, thereby increasing the difficulty in correlating facial motion features to AU occurrence. To bridge the subtlety issue, flow-related features and motion magnification are a few common approaches as they can yield descriptive motion changes and increased motion amplitude respectively. While motion magnification can amplify the motion changes, it also accounts for illumination changes and projection errors during the amplification process, thereby creating motion artefacts that confuse the model to learn inauthentic magnified motion features. The problem is further aggravated in the context of a more complicated task where more AU classes are analyzed in cross-database settings. To address this issue, we propose InfuseNet, a layer-wise unitary feature infusion framework that leverages motion context to constrain the Action Unit (AU) learning within an informative facial movement region, thereby alleviating the influence of magnification artefacts. On top of that, we propose leveraging magnified latent features instead of reconstructing magnified samples to limit the distortion and artefacts caused by the projection inaccuracy in the motion reconstruction process. Via alleviating the magnification artefacts, InfuseNet has surpassed the state-of-the-art results in the CD6ME protocol. Further quantitative studies have also demonstrated the efficacy of motion artefacts alleviation."
      },
      {
        "id": "oai:arXiv.org:2504.09328v1",
        "title": "Text To 3D Object Generation For Scalable Room Assembly",
        "link": "https://arxiv.org/abs/2504.09328",
        "author": "Sonia Laguna, Alberto Garcia-Garcia, Marie-Julie Rakotosaona, Stylianos Moschoglou, Leonhard Helminger, Sergio Orts-Escolano",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09328v1 Announce Type: new \nAbstract: Modern machine learning models for scene understanding, such as depth estimation and object tracking, rely on large, high-quality datasets that mimic real-world deployment scenarios. To address data scarcity, we propose an end-to-end system for synthetic data generation for scalable, high-quality, and customizable 3D indoor scenes. By integrating and adapting text-to-image and multi-view diffusion models with Neural Radiance Field-based meshing, this system generates highfidelity 3D object assets from text prompts and incorporates them into pre-defined floor plans using a rendering tool. By introducing novel loss functions and training strategies into existing methods, the system supports on-demand scene generation, aiming to alleviate the scarcity of current available data, generally manually crafted by artists. This system advances the role of synthetic data in addressing machine learning training limitations, enabling more robust and generalizable models for real-world applications."
      },
      {
        "id": "oai:arXiv.org:2504.09330v1",
        "title": "Regretful Decisions under Label Noise",
        "link": "https://arxiv.org/abs/2504.09330",
        "author": "Sujay Nagaraj, Yang Liu, Flavio P. Calmon, Berk Ustun",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09330v1 Announce Type: new \nAbstract: Machine learning models are routinely used to support decisions that affect individuals -- be it to screen a patient for a serious illness or to gauge their response to treatment. In these tasks, we are limited to learning models from datasets with noisy labels. In this paper, we study the instance-level impact of learning under label noise. We introduce a notion of regret for this regime which measures the number of unforeseen mistakes due to noisy labels. We show that standard approaches to learning under label noise can return models that perform well at a population level while subjecting individuals to a lottery of mistakes. We present a versatile approach to estimate the likelihood of mistakes at the individual level from a noisy dataset by training models over plausible realizations of datasets without label noise. This is supported by a comprehensive empirical study of label noise in clinical prediction tasks. Our results reveal how failure to anticipate mistakes can compromise model reliability and adoption, and demonstrate how we can address these challenges by anticipating and avoiding regretful decisions."
      },
      {
        "id": "oai:arXiv.org:2504.09335v1",
        "title": "Efficient Implementation of Reinforcement Learning over Homomorphic Encryption",
        "link": "https://arxiv.org/abs/2504.09335",
        "author": "Jihoon Suh, Takashi Tanaka",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09335v1 Announce Type: new \nAbstract: We investigate encrypted control policy synthesis over the cloud. While encrypted control implementations have been studied previously, we focus on the less explored paradigm of privacy-preserving control synthesis, which can involve heavier computations ideal for cloud outsourcing. We classify control policy synthesis into model-based, simulator-driven, and data-driven approaches and examine their implementation over fully homomorphic encryption (FHE) for privacy enhancements. A key challenge arises from comparison operations (min or max) in standard reinforcement learning algorithms, which are difficult to execute over encrypted data. This observation motivates our focus on Relative-Entropy-regularized reinforcement learning (RL) problems, which simplifies encrypted evaluation of synthesis algorithms due to their comparison-free structures. We demonstrate how linearly solvable value iteration, path integral control, and Z-learning can be readily implemented over FHE. We conduct a case study of our approach through numerical simulations of encrypted Z-learning in a grid world environment using the CKKS encryption scheme, showing convergence with acceptable approximation error. Our work suggests the potential for secure and efficient cloud-based reinforcement learning."
      },
      {
        "id": "oai:arXiv.org:2504.09339v1",
        "title": "Towards Optimal Differentially Private Regret Bounds in Linear MDPs",
        "link": "https://arxiv.org/abs/2504.09339",
        "author": "Sharan Sahu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09339v1 Announce Type: new \nAbstract: We study regret minimization under privacy constraints in episodic inhomogeneous linear Markov Decision Processes (MDPs), motivated by the growing use of reinforcement learning (RL) in personalized decision-making systems that rely on sensitive user data. In this setting, both transition probabilities and reward functions are assumed to be linear in a feature mapping $\\phi(s, a)$, and we aim to ensure privacy through joint differential privacy (JDP), a relaxation of differential privacy suited to online learning. Prior work has established suboptimal regret bounds by privatizing the LSVI-UCB algorithm, which achieves $\\widetilde{O}(\\sqrt{d^3 H^4 K})$ regret in the non-private setting. Building on recent advances that improve this to minimax optimal regret $\\widetilde{O}(HD\\sqrt{K})$ via LSVI-UCB++ with Bernstein-style bonuses, we design a new differentially private algorithm by privatizing LSVI-UCB++ and adapting techniques for variance-aware analysis from offline RL. Our algorithm achieves a regret bound of $\\widetilde{O}(d \\sqrt{H^3 K} + H^{4.5} d^{7/6} K^{1/2} / \\epsilon)$, improving over previous private methods. Empirical results show that our algorithm retains near-optimal utility compared to non-private baselines, indicating that privacy can be achieved with minimal performance degradation in this setting."
      },
      {
        "id": "oai:arXiv.org:2504.09341v1",
        "title": "Minority Reports: Balancing Cost and Quality in Ground Truth Data Annotation",
        "link": "https://arxiv.org/abs/2504.09341",
        "author": "Hsuan Wei Liao, Christopher Klugmann, Daniel Kondermann, Rafid Mahmood",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09341v1 Announce Type: new \nAbstract: High-quality data annotation is an essential but laborious and costly aspect of developing machine learning-based software. We explore the inherent tradeoff between annotation accuracy and cost by detecting and removing minority reports -- instances where annotators provide incorrect responses -- that indicate unnecessary redundancy in task assignments. We propose an approach to prune potentially redundant annotation task assignments before they are executed by estimating the likelihood of an annotator disagreeing with the majority vote for a given task. Our approach is informed by an empirical analysis over computer vision datasets annotated by a professional data annotation platform, which reveals that the likelihood of a minority report event is dependent primarily on image ambiguity, worker variability, and worker fatigue. Simulations over these datasets show that we can reduce the number of annotations required by over 60% with a small compromise in label quality, saving approximately 6.6 days-equivalent of labor. Our approach provides annotation service platforms with a method to balance cost and dataset quality. Machine learning practitioners can tailor annotation accuracy levels according to specific application needs, thereby optimizing budget allocation while maintaining the data quality necessary for critical settings like autonomous driving technology."
      },
      {
        "id": "oai:arXiv.org:2504.09344v1",
        "title": "Context-Aware Adaptive Sampling for Intelligent Data Acquisition Systems Using DQN",
        "link": "https://arxiv.org/abs/2504.09344",
        "author": "Weiqiang Huang, Juecen Zhan, Yumeng Sun, Xu Han, Tai An, Nan Jiang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09344v1 Announce Type: new \nAbstract: Multi-sensor systems are widely used in the Internet of Things, environmental monitoring, and intelligent manufacturing. However, traditional fixed-frequency sampling strategies often lead to severe data redundancy, high energy consumption, and limited adaptability, failing to meet the dynamic sensing needs of complex environments. To address these issues, this paper proposes a DQN-based multi-sensor adaptive sampling optimization method. By leveraging a reinforcement learning framework to learn the optimal sampling strategy, the method balances data quality, energy consumption, and redundancy. We first model the multi-sensor sampling task as a Markov Decision Process (MDP), then employ a Deep Q-Network to optimize the sampling policy. Experiments on the Intel Lab Data dataset confirm that, compared with fixed-frequency sampling, threshold-triggered sampling, and other reinforcement learning approaches, DQN significantly improves data quality while lowering average energy consumption and redundancy rates. Moreover, in heterogeneous multi-sensor environments, DQN-based adaptive sampling shows enhanced robustness, maintaining superior data collection performance even in the presence of interference factors. These findings demonstrate that DQN-based adaptive sampling can enhance overall data acquisition efficiency in multi-sensor systems, providing a new solution for efficient and intelligent sensing."
      },
      {
        "id": "oai:arXiv.org:2504.09354v1",
        "title": "REMEMBER: Retrieval-based Explainable Multimodal Evidence-guided Modeling for Brain Evaluation and Reasoning in Zero- and Few-shot Neurodegenerative Diagnosis",
        "link": "https://arxiv.org/abs/2504.09354",
        "author": "Duy-Cat Can, Quang-Huy Tang, Huong Ha, Binh T. Nguyen, Oliver Y. Ch\\'en",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09354v1 Announce Type: new \nAbstract: Timely and accurate diagnosis of neurodegenerative disorders, such as Alzheimer's disease, is central to disease management. Existing deep learning models require large-scale annotated datasets and often function as \"black boxes\". Additionally, datasets in clinical practice are frequently small or unlabeled, restricting the full potential of deep learning methods. Here, we introduce REMEMBER -- Retrieval-based Explainable Multimodal Evidence-guided Modeling for Brain Evaluation and Reasoning -- a new machine learning framework that facilitates zero- and few-shot Alzheimer's diagnosis using brain MRI scans through a reference-based reasoning process. Specifically, REMEMBER first trains a contrastively aligned vision-text model using expert-annotated reference data and extends pseudo-text modalities that encode abnormality types, diagnosis labels, and composite clinical descriptions. Then, at inference time, REMEMBER retrieves similar, human-validated cases from a curated dataset and integrates their contextual information through a dedicated evidence encoding module and attention-based inference head. Such an evidence-guided design enables REMEMBER to imitate real-world clinical decision-making process by grounding predictions in retrieved imaging and textual context. Specifically, REMEMBER outputs diagnostic predictions alongside an interpretable report, including reference images and explanations aligned with clinical workflows. Experimental results demonstrate that REMEMBER achieves robust zero- and few-shot performance and offers a powerful and explainable framework to neuroimaging-based diagnosis in the real world, especially under limited data."
      },
      {
        "id": "oai:arXiv.org:2504.09361v1",
        "title": "PapMOT: Exploring Adversarial Patch Attack against Multiple Object Tracking",
        "link": "https://arxiv.org/abs/2504.09361",
        "author": "Jiahuan Long, Tingsong Jiang, Wen Yao, Shuai Jia, Weijia Zhang, Weien Zhou, Chao Ma, Xiaoqian Chen",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09361v1 Announce Type: new \nAbstract: Tracking multiple objects in a continuous video stream is crucial for many computer vision tasks. It involves detecting and associating objects with their respective identities across successive frames. Despite significant progress made in multiple object tracking (MOT), recent studies have revealed the vulnerability of existing MOT methods to adversarial attacks. Nevertheless, all of these attacks belong to digital attacks that inject pixel-level noise into input images, and are therefore ineffective in physical scenarios. To fill this gap, we propose PapMOT, which can generate physical adversarial patches against MOT for both digital and physical scenarios. Besides attacking the detection mechanism, PapMOT also optimizes a printable patch that can be detected as new targets to mislead the identity association process. Moreover, we introduce a patch enhancement strategy to further degrade the temporal consistency of tracking results across video frames, resulting in more aggressive attacks. We further develop new evaluation metrics to assess the robustness of MOT against such attacks. Extensive evaluations on multiple datasets demonstrate that our PapMOT can successfully attack various architectures of MOT trackers in digital scenarios. We also validate the effectiveness of PapMOT for physical attacks by deploying printed adversarial patches in the real world."
      },
      {
        "id": "oai:arXiv.org:2504.09373v1",
        "title": "QUDsim: Quantifying Discourse Similarities in LLM-Generated Text",
        "link": "https://arxiv.org/abs/2504.09373",
        "author": "Ramya Namuduri, Yating Wu, Anshun Asher Zheng, Manya Wadhwa, Greg Durrett, Junyi Jessy Li",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09373v1 Announce Type: new \nAbstract: As large language models become increasingly capable at various writing tasks, their weakness at generating unique and creative content becomes a major liability. Although LLMs have the ability to generate text covering diverse topics, there is an overall sense of repetitiveness across texts that we aim to formalize and quantify via a similarity metric. The familiarity between documents arises from the persistence of underlying discourse structures. However, existing similarity metrics dependent on lexical overlap and syntactic patterns largely capture $\\textit{content}$ overlap, thus making them unsuitable for detecting $\\textit{structural}$ similarities. We introduce an abstraction based on linguistic theories in Questions Under Discussion (QUD) and question semantics to help quantify differences in discourse progression. We then use this framework to build $\\textbf{QUDsim}$, a similarity metric that can detect discursive parallels between documents. Using QUDsim, we find that LLMs often reuse discourse structures (more so than humans) across samples, even when content differs. Furthermore, LLMs are not only repetitive and structurally uniform, but are also divergent from human authors in the types of structures they use."
      },
      {
        "id": "oai:arXiv.org:2504.09376v1",
        "title": "Cross-Partisan Interactions on Twitter",
        "link": "https://arxiv.org/abs/2504.09376",
        "author": "Yusuf M\\\"ucahit \\c{C}etinkaya, Vahid Ghafouri, Guillermo Suarez-Tangil, Jose Such, Tu\\u{g}rulcan Elmas",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09376v1 Announce Type: new \nAbstract: Many social media studies argue that social media creates echo chambers where some users only interact with peers of the same political orientation. However, recent studies suggest that a substantial amount of Cross-Partisan Interactions (CPIs) do exist - even within echo chambers, but they may be toxic. There is no consensus about how such interactions occur and when they lead to healthy or toxic dialogue. In this paper, we study a comprehensive Twitter dataset that consists of 3 million tweets from 2020 related to the U.S. context to understand the dynamics behind CPIs. We investigate factors that are more associated with such interactions, including how users engage in CPIs, which topics are more contentious, and what are the stances associated with healthy interactions. We find that CPIs are significantly influenced by the nature of the topics being discussed, with politically charged events acting as strong catalysts. The political discourse and pre-established political views sway how users participate in CPIs, but the direction in which users go is nuanced. While Democrats engage in cross-partisan interactions slightly more frequently, these interactions often involve more negative and nonconstructive stances compared to their intra-party interactions. In contrast, Republicans tend to maintain a more consistent tone across interactions. Although users are more likely to engage in CPIs with popular accounts in general, this is less common among Republicans who often engage in CPIs with accounts with a low number of followers for personal matters. Our study has implications beyond Twitter as identifying topics with low toxicity and high CPI can help highlight potential opportunities for reducing polarization while topics with high toxicity and low CPI may action targeted interventions when moderating harm."
      },
      {
        "id": "oai:arXiv.org:2504.09377v1",
        "title": "Beyond Degradation Conditions: All-in-One Image Restoration via HOG Transformers",
        "link": "https://arxiv.org/abs/2504.09377",
        "author": "Jiawei Wu, Zhifei Yang, Zhe Wang, Zhi Jin",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09377v1 Announce Type: new \nAbstract: All-in-one image restoration, which aims to address diverse degradations within a unified framework, is critical for practical applications. However, existing methods rely on predicting and integrating degradation conditions, which can misactivate degradation-specific features in complex scenarios, limiting their restoration performance. To address this issue, we propose a novel all-in-one image restoration framework guided by Histograms of Oriented Gradients (HOG), named HOGformer. By leveraging the degradation-discriminative capability of HOG descriptors, HOGformer employs a dynamic self-attention mechanism that adaptively attends to long-range spatial dependencies based on degradation-aware HOG cues. To enhance the degradation sensitivity of attention inputs, we design a HOG-guided local dynamic-range convolution module that captures long-range degradation similarities while maintaining awareness of global structural information. Furthermore, we propose a dynamic interaction feed-forward module, efficiently increasing the model capacity to adapt to different degradations through channel-spatial interactions. Extensive experiments across diverse benchmarks, including adverse weather and natural degradations, demonstrate that HOGformer achieves state-of-the-art performance and generalizes effectively to complex real-world degradations. Code is available at https://github.com/Fire-friend/HOGformer."
      },
      {
        "id": "oai:arXiv.org:2504.09378v1",
        "title": "Can you map it to English? The Role of Cross-Lingual Alignment in Multilingual Performance of LLMs",
        "link": "https://arxiv.org/abs/2504.09378",
        "author": "Kartik Ravisankar, Hyojung Han, Marine Carpuat",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09378v1 Announce Type: new \nAbstract: Large language models (LLMs) pre-trained predominantly on English text exhibit surprising multilingual capabilities, yet the mechanisms driving cross-lingual generalization remain poorly understood. This work investigates how the alignment of representations for text written in different languages correlates with LLM performance on natural language understanding tasks and translation tasks, both at the language and the instance level. For this purpose, we introduce cross-lingual alignment metrics such as the Discriminative Alignment Index (DALI) to quantify the alignment at an instance level for discriminative tasks. Through experiments on three natural language understanding tasks (Belebele, XStoryCloze, XCOPA), and machine translation, we find that while cross-lingual alignment metrics strongly correlate with task accuracy at the language level, the sample-level alignment often fails to distinguish correct from incorrect predictions, exposing alignment as a necessary but insufficient condition for success."
      },
      {
        "id": "oai:arXiv.org:2504.09379v1",
        "title": "Low-Light Image Enhancement using Event-Based Illumination Estimation",
        "link": "https://arxiv.org/abs/2504.09379",
        "author": "Lei Sun, Yuhan Bao, Jiajun Zhai, Jingyun Liang, Yulun Zhang, Kaiwei Wang, Danda Pani Paudel, Luc Van Gool",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09379v1 Announce Type: new \nAbstract: Low-light image enhancement (LLIE) aims to improve the visibility of images captured in poorly lit environments. Prevalent event-based solutions primarily utilize events triggered by motion, i.e., ''motion events'' to strengthen only the edge texture, while leaving the high dynamic range and excellent low-light responsiveness of event cameras largely unexplored. This paper instead opens a new avenue from the perspective of estimating the illumination using ''temporal-mapping'' events, i.e., by converting the timestamps of events triggered by a transmittance modulation into brightness values. The resulting fine-grained illumination cues facilitate a more effective decomposition and enhancement of the reflectance component in low-light images through the proposed Illumination-aided Reflectance Enhancement module. Furthermore, the degradation model of temporal-mapping events under low-light condition is investigated for realistic training data synthesizing. To address the lack of datasets under this regime, we construct a beam-splitter setup and collect EvLowLight dataset that includes images, temporal-mapping events, and motion events. Extensive experiments across 5 synthetic datasets and our real-world EvLowLight dataset substantiate that the devised pipeline, dubbed RetinEV, excels in producing well-illuminated, high dynamic range images, outperforming previous state-of-the-art event-based methods by up to 6.62 dB, while maintaining an efficient inference speed of 35.6 frame-per-second on a 640X480 image."
      },
      {
        "id": "oai:arXiv.org:2504.09384v1",
        "title": "Contour Flow Constraint: Preserving Global Shape Similarity for Deep Learning based Image Segmentation",
        "link": "https://arxiv.org/abs/2504.09384",
        "author": "Shengzhe Chen, Zhaoxuan Dong, Jun Liu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09384v1 Announce Type: new \nAbstract: For effective image segmentation, it is crucial to employ constraints informed by prior knowledge about the characteristics of the areas to be segmented to yield favorable segmentation outcomes. However, the existing methods have primarily focused on priors of specific properties or shapes, lacking consideration of the general global shape similarity from a Contour Flow (CF) perspective. Furthermore, naturally integrating this contour flow prior image segmentation model into the activation functions of deep convolutional networks through mathematical methods is currently unexplored. In this paper, we establish a concept of global shape similarity based on the premise that two shapes exhibit comparable contours. Furthermore, we mathematically derive a contour flow constraint that ensures the preservation of global shape similarity. We propose two implementations to integrate the constraint with deep neural networks. Firstly, the constraint is converted to a shape loss, which can be seamlessly incorporated into the training phase for any learning-based segmentation framework. Secondly, we add the constraint into a variational segmentation model and derive its iterative schemes for solution. The scheme is then unrolled to get the architecture of the proposed CFSSnet. Validation experiments on diverse datasets are conducted on classic benchmark deep network segmentation models. The results indicate a great improvement in segmentation accuracy and shape similarity for the proposed shape loss, showcasing the general adaptability of the proposed loss term regardless of specific network architectures. CFSSnet shows robustness in segmenting noise-contaminated images, and inherent capability to preserve global shape similarity."
      },
      {
        "id": "oai:arXiv.org:2504.09385v1",
        "title": "Expressivity of Quadratic Neural ODEs",
        "link": "https://arxiv.org/abs/2504.09385",
        "author": "Joshua Hanson, Maxim Raginsky",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09385v1 Announce Type: new \nAbstract: This work focuses on deriving quantitative approximation error bounds for neural ordinary differential equations having at most quadratic nonlinearities in the dynamics. The simple dynamics of this model form demonstrates how expressivity can be derived primarily from iteratively composing many basic elementary operations, versus from the complexity of those elementary operations themselves. Like the analog differential analyzer and universal polynomial DAEs, the expressivity is derived instead primarily from the \"depth\" of the model. These results contribute to our understanding of what depth specifically imparts to the capabilities of deep learning architectures."
      },
      {
        "id": "oai:arXiv.org:2504.09387v1",
        "title": "On Language Models' Sensitivity to Suspicious Coincidences",
        "link": "https://arxiv.org/abs/2504.09387",
        "author": "Sriram Padmanabhan, Kanishka Misra, Kyle Mahowald, Eunsol Choi",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09387v1 Announce Type: new \nAbstract: Humans are sensitive to suspicious coincidences when generalizing inductively over data, as they make assumptions as to how the data was sampled. This results in smaller, more specific hypotheses being favored over more general ones. For instance, when provided the set {Austin, Dallas, Houston}, one is more likely to think that this is sampled from \"Texas Cities\" over \"US Cities\" even though both are compatible. Suspicious coincidence is strongly connected to pragmatic reasoning, and can serve as a testbed to analyze systems on their sensitivity towards the communicative goals of the task (i.e., figuring out the true category underlying the data). In this paper, we analyze whether suspicious coincidence effects are reflected in language models' (LMs) behavior. We do so in the context of two domains: 1) the number game, where humans made judgments of whether a number (e.g., 4) fits a list of given numbers (e.g., 16, 32, 2); and 2) by extending the number game setup to prominent cities. For both domains, the data is compatible with multiple hypotheses and we study which hypothesis is most consistent with the models' behavior. On analyzing five models, we do not find strong evidence for suspicious coincidences in LMs' zero-shot behavior. However, when provided access to the hypotheses space via chain-of-thought or explicit prompting, LMs start to show an effect resembling suspicious coincidences, sometimes even showing effects consistent with humans. Our study suggests that inductive reasoning behavior in LMs can be enhanced with explicit access to the hypothesis landscape."
      },
      {
        "id": "oai:arXiv.org:2504.09389v1",
        "title": "Beyond Memorization: Mapping the Originality-Quality Frontier of Language Models",
        "link": "https://arxiv.org/abs/2504.09389",
        "author": "Vishakh Padmakumar, Chen Yueh-Han, Jane Pan, Valerie Chen, He He",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09389v1 Announce Type: new \nAbstract: As large language models (LLMs) are increasingly used for ideation and scientific discovery, it is important to evaluate their ability to generate novel output. Prior work evaluates novelty as the originality with respect to training data, but original outputs can be low quality. In contrast, non-expert judges may favor high-quality but memorized outputs, limiting the reliability of human preference as a metric. We propose a new novelty metric for LLM generations that balances originality and quality -- the harmonic mean of the fraction of \\ngrams unseen during training and a task-specific quality score. We evaluate the novelty of generations from two families of open-data models (OLMo and Pythia) on three creative tasks: story completion, poetry writing, and creative tool use. We find that LLM generated text is less novel than human written text. To elicit more novel outputs, we experiment with various inference-time methods, which reveals a trade-off between originality and quality. While these methods can boost novelty, they do so by increasing originality at the expense of quality. In contrast, increasing model size or applying post-training reliably shifts the Pareto frontier, highlighting that starting with a stronger base model is a more effective way to improve novelty."
      },
      {
        "id": "oai:arXiv.org:2504.09393v1",
        "title": "Vision Transformers Exhibit Human-Like Biases: Evidence of Orientation and Color Selectivity, Categorical Perception, and Phase Transitions",
        "link": "https://arxiv.org/abs/2504.09393",
        "author": "Nooshin Bahador",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09393v1 Announce Type: new \nAbstract: This study explored whether Vision Transformers (ViTs) developed orientation and color biases similar to those observed in the human brain. Using synthetic datasets with controlled variations in noise levels, angles, lengths, widths, and colors, we analyzed the behavior of ViTs fine-tuned with LoRA. Our findings revealed four key insights: First, ViTs exhibited an oblique effect showing the lowest angle prediction errors at 180 deg (horizontal) across all conditions. Second, angle prediction errors varied by color. Errors were highest for bluish hues and lowest for yellowish ones. Additionally, clustering analysis of angle prediction errors showed that ViTs grouped colors in a way that aligned with human perceptual categories. In addition to orientation and color biases, we observed phase transition phenomena. While two phase transitions occurred consistently across all conditions, the training loss curves exhibited delayed transitions when color was incorporated as an additional data attribute. Finally, we observed that attention heads in certain layers inherently develop specialized capabilities, functioning as task-agnostic feature extractors regardless of the downstream task. These observations suggest that biases and properties arise primarily from pre-training on the original dataset which shapes the model's foundational representations and the inherent architectural constraints of the vision transformer, rather than being solely determined by downstream data statistics."
      },
      {
        "id": "oai:arXiv.org:2504.09394v1",
        "title": "Evaluation Under Imperfect Benchmarks and Ratings: A Case Study in Text Simplification",
        "link": "https://arxiv.org/abs/2504.09394",
        "author": "Joseph Liu, Yoonsoo Nam, Xinyue Cui, Swabha Swayamdipta",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09394v1 Announce Type: new \nAbstract: Despite the successes of language models, their evaluation remains a daunting challenge for new and existing tasks. We consider the task of text simplification, commonly used to improve information accessibility, where evaluation faces two major challenges. First, the data in existing benchmarks might not reflect the capabilities of current language models on the task, often containing disfluent, incoherent, or simplistic examples. Second, existing human ratings associated with the benchmarks often contain a high degree of disagreement, resulting in inconsistent ratings; nevertheless, existing metrics still have to show higher correlations with these imperfect ratings. As a result, evaluation for the task is not reliable and does not reflect expected trends (e.g., more powerful models being assigned higher scores). We address these challenges for the task of text simplification through three contributions. First, we introduce SynthSimpliEval, a synthetic benchmark for text simplification featuring simplified sentences generated by models of varying sizes. Through a pilot study, we show that human ratings on our benchmark exhibit high inter-annotator agreement and reflect the expected trend: larger models produce higher-quality simplifications. Second, we show that auto-evaluation with a panel of LLM judges (LLMs-as-a-jury) often suffices to obtain consistent ratings for the evaluation of text simplification. Third, we demonstrate that existing learnable metrics for text simplification benefit from training on our LLMs-as-a-jury-rated synthetic data, closing the gap with pure LLMs-as-a-jury for evaluation. Overall, through our case study on text simplification, we show that a reliable evaluation requires higher quality test data, which could be obtained through synthetic data and LLMs-as-a-jury ratings."
      },
      {
        "id": "oai:arXiv.org:2504.09396v1",
        "title": "Adaptive Insurance Reserving with CVaR-Constrained Reinforcement Learning under Macroeconomic Regimes",
        "link": "https://arxiv.org/abs/2504.09396",
        "author": "Stella C. Dong, James R. Finlay",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09396v1 Announce Type: new \nAbstract: This paper proposes a reinforcement learning (RL) framework for insurance reserving that integrates tail-risk sensitivity, macroeconomic regime modeling, and regulatory compliance. The reserving problem is formulated as a finite-horizon Markov Decision Process (MDP), in which reserve adjustments are optimized using Proximal Policy Optimization (PPO) subject to Conditional Value-at-Risk (CVaR) constraints. To enhance policy robustness across varying economic conditions, the agent is trained using a regime-aware curriculum that progressively increases volatility exposure.\n  The reward structure penalizes reserve shortfall, capital inefficiency, and solvency floor violations, with design elements informed by Solvency II and Own Risk and Solvency Assessment (ORSA) frameworks. Empirical evaluations on two industry datasets--Workers' Compensation, and Other Liability--demonstrate that the RL-CVaR agent achieves superior performance relative to classical reserving methods across multiple criteria, including tail-risk control (CVaR$_{0.95}$), capital efficiency, and regulatory violation rate. The framework also accommodates fixed-shock stress testing and regime-stratified analysis, providing a principled and extensible approach to reserving under uncertainty."
      },
      {
        "id": "oai:arXiv.org:2504.09398v1",
        "title": "Composable NLP Workflows for BERT-based Ranking and QA System",
        "link": "https://arxiv.org/abs/2504.09398",
        "author": "Gaurav Kumar, Murali Mohana Krishna Dandu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09398v1 Announce Type: new \nAbstract: There has been a lot of progress towards building NLP models that scale to multiple tasks. However, real-world systems contain multiple components and it is tedious to handle cross-task interaction with varying levels of text granularity. In this work, we built an end-to-end Ranking and Question-Answering (QA) system using Forte, a toolkit that makes composable NLP pipelines. We utilized state-of-the-art deep learning models such as BERT, RoBERTa in our pipeline, evaluated the performance on MS-MARCO and Covid-19 datasets using metrics such as BLUE, MRR, F1 and compared the results of ranking and QA systems with their corresponding benchmark results. The modular nature of our pipeline and low latency of reranker makes it easy to build complex NLP applications easily."
      },
      {
        "id": "oai:arXiv.org:2504.09402v1",
        "title": "Question Tokens Deserve More Attention: Enhancing Large Language Models without Training through Step-by-Step Reading and Question Attention Recalibration",
        "link": "https://arxiv.org/abs/2504.09402",
        "author": "Feijiang Han, Licheng Guo, Hengtao Cui, Zhiyuan Lyu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09402v1 Announce Type: new \nAbstract: Large Language Models (LLMs) often struggle with tasks that require a deep understanding of complex questions, especially when faced with long-range dependencies or multi-step reasoning. This work investigates the limitations of current LLMs in question comprehension and identifies three insights: (1) repeating question tokens improves comprehension by increasing attention to question regions, (2) increased backward dependencies negatively affect performance due to unidirectional attentional constraints, and (3) recalibrating attentional mechanisms to prioritize question-relevant regions improves performance.\n  Based on these findings, we first propose a family of prompt-based strategies - Step-by-Step Reading (SSR), SSR+, and SSR++ - that guide LLMs to incrementally process question tokens and align their reasoning with the input structure. These methods significantly improve performance, with SSR++ achieving state-of-the-art results on several benchmarks: 96.66% on GSM8K, 94.61% on ASDiv, and 76.28% on AQuA. Second, we introduce a training-free attention recalibration mechanism that dynamically adjusts attention distributions during inference to emphasize question-relevant regions. This method improves the accuracy of LLaMA 3.1-8B on AQuA by 5.17% without changing model parameters or input prompts.\n  Taken together, our results highlight the importance of structured prompt design and attention optimization in improving LLM comprehension, providing lightweight yet effective tools for improving performance in various NLP tasks."
      },
      {
        "id": "oai:arXiv.org:2504.09405v1",
        "title": "Tin-Tin: Towards Tiny Learning on Tiny Devices with Integer-based Neural Network Training",
        "link": "https://arxiv.org/abs/2504.09405",
        "author": "Yi Hu, Jinhang Zuo, Eddie Zhang, Bob Iannucci, Carlee Joe-Wong",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09405v1 Announce Type: new \nAbstract: Recent advancements in machine learning (ML) have enabled its deployment on resource-constrained edge devices, fostering innovative applications such as intelligent environmental sensing. However, these devices, particularly microcontrollers (MCUs), face substantial challenges due to limited memory, computing capabilities, and the absence of dedicated floating-point units (FPUs). These constraints hinder the deployment of complex ML models, especially those requiring lifelong learning capabilities. To address these challenges, we propose Tin-Tin, an integer-based on-device training framework designed specifically for low-power MCUs. Tin-Tin introduces novel integer rescaling techniques to efficiently manage dynamic ranges and facilitate efficient weight updates using integer data types. Unlike existing methods optimized for devices with FPUs, GPUs, or FPGAs, Tin-Tin addresses the unique demands of tiny MCUs, prioritizing energy efficiency and optimized memory utilization. We validate the effectiveness of Tin-Tin through end-to-end application examples on real-world tiny devices, demonstrating its potential to support energy-efficient and sustainable ML applications on edge platforms."
      },
      {
        "id": "oai:arXiv.org:2504.09407v1",
        "title": "UXAgent: A System for Simulating Usability Testing of Web Design with LLM Agents",
        "link": "https://arxiv.org/abs/2504.09407",
        "author": "Yuxuan Lu, Bingsheng Yao, Hansu Gu, Jing Huang, Jessie Wang, Yang Li, Jiri Gesi, Qi He, Toby Jia-Jun Li, Dakuo Wang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09407v1 Announce Type: new \nAbstract: Usability testing is a fundamental research method that user experience (UX) researchers use to evaluate and iterate a web design, but\\textbf{ how to evaluate and iterate the usability testing study design } itself? Recent advances in Large Language Model-simulated Agent (\\textbf{LLM Agent}) research inspired us to design \\textbf{UXAgent} to support UX researchers in evaluating and reiterating their usability testing study design before they conduct the real human-subject study. Our system features a Persona Generator module, an LLM Agent module, and a Universal Browser Connector module to automatically generate thousands of simulated users to interactively test the target website. The system also provides an Agent Interview Interface and a Video Replay Interface so that the UX researchers can easily review and analyze the generated qualitative and quantitative log data. Through a heuristic evaluation, five UX researcher participants praised the innovation of our system but also expressed concerns about the future of LLM Agent usage in UX studies."
      },
      {
        "id": "oai:arXiv.org:2504.09415v1",
        "title": "Nash Equilibrium Between Consumer Electronic Devices and DoS Attacker for Distributed IoT-enabled RSE Systems",
        "link": "https://arxiv.org/abs/2504.09415",
        "author": "Gengcan Chen, Donghong Cai, Zahid Khan, Jawad Ahmad, Wadii Boulila",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09415v1 Announce Type: new \nAbstract: In electronic consumer Internet of Things (IoT), consumer electronic devices as edge devices require less computational overhead and the remote state estimation (RSE) of consumer electronic devices is always at risk of denial-of-service (DoS) attacks. Therefore, the adversarial strategy between consumer electronic devices and DoS attackers is critical. This paper focuses on the adversarial strategy between consumer electronic devices and DoS attackers in IoT-enabled RSE Systems. We first propose a remote joint estimation model for distributed measurements to effectively reduce consumer electronic device workload and minimize data leakage risks. The Kalman filter is deployed on the remote estimator, and the DoS attacks with open-loop as well as closed-loop are considered. We further introduce advanced reinforcement learning techniques, including centralized and distributed Minimax-DQN, to address high-dimensional decision-making challenges in both open-loop and closed-loop scenarios. Especially, the Q-network instead of the Q-table is used in the proposed approaches, which effectively solves the challenge of Q-learning. Moreover, the proposed distributed Minimax-DQN reduces the action space to expedite the search for Nash Equilibrium (NE). The experimental results validate that the proposed model can expeditiously restore the RSE error covariance to a stable state in the presence of DoS attacks, exhibiting notable attack robustness. The proposed centralized and distributed Minimax-DQN effectively resolves the NE in both open and closed-loop case, showcasing remarkable performance in terms of convergence. It reveals that substantial advantages in both efficiency and stability are achieved compared with the state-of-the-art methods."
      },
      {
        "id": "oai:arXiv.org:2504.09416v1",
        "title": "Spatially Directional Dual-Attention GAT for Spatial Fluoride Health Risk Modeling",
        "link": "https://arxiv.org/abs/2504.09416",
        "author": "Da Yuan",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09416v1 Announce Type: new \nAbstract: Environmental exposure to fluoride is a major public health concern, particularly in regions with naturally elevated fluoride concentrations. Accurate modeling of fluoride-related health risks, such as dental fluorosis, requires spatially aware learning frameworks capable of capturing both geographic and semantic heterogeneity. In this work, we propose Spatially Directional Dual-Attention Graph Attention Network (SDD-GAT), a novel spatial graph neural network designed for fine-grained health risk prediction. SDD-GAT introduces a dual-graph architecture that disentangles geographic proximity and attribute similarity, and incorporates a directional attention mechanism that explicitly encodes spatial orientation and distance into the message passing process. To further enhance spatial coherence, we introduce a spatial smoothness regularization term that enforces consistency in predictions across neighboring locations. We evaluate SDD-GAT on a large-scale dataset covering over 50,000 fluoride monitoring samples and fluorosis records across Guizhou Province, China. Results show that SDD-GAT significantly outperforms traditional models and state-of-the-art GNNs in both regression and classification tasks, while also exhibiting improved spatial autocorrelation as measured by Moran's I. Our framework provides a generalizable foundation for spatial health risk modeling and geospatial learning under complex environmental settings."
      },
      {
        "id": "oai:arXiv.org:2504.09420v1",
        "title": "SaRO: Enhancing LLM Safety through Reasoning-based Alignment",
        "link": "https://arxiv.org/abs/2504.09420",
        "author": "Yutao Mou, Yuxiao Luo, Shikun Zhang, Wei Ye",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09420v1 Announce Type: new \nAbstract: Current safety alignment techniques for large language models (LLMs) face two key challenges: (1) under-generalization, which leaves models vulnerable to novel jailbreak attacks, and (2) over-alignment, which leads to the excessive refusal of benign instructions. Our preliminary investigation reveals semantic overlap between jailbreak/harmful queries and normal prompts in embedding space, suggesting that more effective safety alignment requires a deeper semantic understanding. This motivates us to incorporate safety-policy-driven reasoning into the alignment process. To this end, we propose the Safety-oriented Reasoning Optimization Framework (SaRO), which consists of two stages: (1) Reasoning-style Warmup (RW) that enables LLMs to internalize long-chain reasoning through supervised fine-tuning, and (2) Safety-oriented Reasoning Process Optimization (SRPO) that promotes safety reflection via direct preference optimization (DPO). Extensive experiments demonstrate the superiority of SaRO over traditional alignment methods."
      },
      {
        "id": "oai:arXiv.org:2504.09421v1",
        "title": "ClinicalGPT-R1: Pushing reasoning capability of generalist disease diagnosis with large language model",
        "link": "https://arxiv.org/abs/2504.09421",
        "author": "Wuyang Lan, Wenzheng Wang, Changwei Ji, Guoxing Yang, Yongbo Zhang, Xiaohong Liu, Song Wu, Guangyu Wang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09421v1 Announce Type: new \nAbstract: Recent advances in reasoning with large language models (LLMs)has shown remarkable reasoning capabilities in domains such as mathematics and coding, yet their application to clinical diagnosis remains underexplored. Here, we introduce ClinicalGPT-R1, a reasoning enhanced generalist large language model for disease diagnosis. Trained on a dataset of 20,000 real-world clinical records, ClinicalGPT-R1 leverages diverse training strategies to enhance diagnostic reasoning. To benchmark performance, we curated MedBench-Hard, a challenging dataset spanning seven major medical specialties and representative diseases. Experimental results demonstrate that ClinicalGPT-R1 outperforms GPT-4o in Chinese diagnostic tasks and achieves comparable performance to GPT-4 in English settings. This comparative study effectively validates the superior performance of ClinicalGPT-R1 in disease diagnosis tasks. Resources are available at https://github.com/medfound/medfound."
      },
      {
        "id": "oai:arXiv.org:2504.09424v1",
        "title": "Comparing Performance of Preprocessing Techniques for Traffic Sign Recognition Using a HOG-SVM",
        "link": "https://arxiv.org/abs/2504.09424",
        "author": "Luis Vieira",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09424v1 Announce Type: new \nAbstract: This study compares the performance of various preprocessing techniques for Traffic Sign Recognition (TSR) using Histogram of Oriented Gradients (HOG) and Support Vector Machine (SVM) on the German Traffic Sign Recognition Benchmark (GTSRB) dataset. Techniques such as CLAHE, HUE, and YUV were evaluated for their impact on classification accuracy. Results indicate that YUV in particular significantly enhance the performance of the HOG-SVM classifier (improving accuracy from 89.65% to 91.25%), providing insights into improvements for preprocessing pipeline of TSR applications."
      },
      {
        "id": "oai:arXiv.org:2504.09426v1",
        "title": "BabyVLM: Data-Efficient Pretraining of VLMs Inspired by Infant Learning",
        "link": "https://arxiv.org/abs/2504.09426",
        "author": "Shengao Wang, Arjun Chandra, Aoming Liu, Venkatesh Saligrama, Boqing Gong",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09426v1 Announce Type: new \nAbstract: Human infants rapidly develop visual reasoning skills from minimal input, suggesting that developmentally inspired pretraining could significantly enhance the efficiency of vision-language models (VLMs). Although recent efforts have leveraged infant-inspired datasets like SAYCam, existing evaluation benchmarks remain misaligned--they are either too simplistic, narrowly scoped, or tailored for large-scale pretrained models. Additionally, training exclusively on infant data overlooks the broader, diverse input from which infants naturally learn. To address these limitations, we propose BabyVLM, a novel framework comprising comprehensive in-domain evaluation benchmarks and a synthetic training dataset created via child-directed transformations of existing datasets. We demonstrate that VLMs trained with our synthetic dataset achieve superior performance on BabyVLM tasks compared to models trained solely on SAYCam or general-purpose data of the SAYCam size. BabyVLM thus provides a robust, developmentally aligned evaluation tool and illustrates how compact models trained on carefully curated data can generalize effectively, opening pathways toward data-efficient vision-language learning paradigms."
      },
      {
        "id": "oai:arXiv.org:2504.09427v1",
        "title": "Ensemble-Enhanced Graph Autoencoder with GAT and Transformer-Based Encoders for Robust Fault Diagnosis",
        "link": "https://arxiv.org/abs/2504.09427",
        "author": "Moirangthem Tiken Singh",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09427v1 Announce Type: new \nAbstract: Fault classification in industrial machinery is vital for enhancing reliability and reducing downtime, yet it remains challenging due to the variability of vibration patterns across diverse operating conditions. This study introduces a novel graph-based framework for fault classification, converting time-series vibration data from machinery operating at varying horsepower levels into a graph representation. We utilize Shannon's entropy to determine the optimal window size for data segmentation, ensuring each segment captures significant temporal patterns, and employ Dynamic Time Warping (DTW) to define graph edges based on segment similarity. A Graph Auto Encoder (GAE) with a deep graph transformer encoder, decoder, and ensemble classifier is developed to learn latent graph representations and classify faults across various categories. The GAE's performance is evaluated on the Case Western Reserve University (CWRU) dataset, with cross-dataset generalization assessed on the HUST dataset. Results show that GAE achieves a mean F1-score of 0.99 on the CWRU dataset, significantly outperforming baseline models-CNN, LSTM, RNN, GRU, and Bi-LSTM (F1-scores: 0.94-0.97, p < 0.05, Wilcoxon signed-rank test for Bi-LSTM: p < 0.05) -- particularly in challenging classes (e.g., Class 8: 0.99 vs. 0.71 for Bi-LSTM). Visualization of dataset characteristics reveals that datasets with amplified vibration patterns and diverse fault dynamics enhance generalization. This framework provides a robust solution for fault diagnosis under varying conditions, offering insights into dataset impacts on model performance."
      },
      {
        "id": "oai:arXiv.org:2504.09428v1",
        "title": "FROG: Effective Friend Recommendation in Online Games via Modality-aware User Preferences",
        "link": "https://arxiv.org/abs/2504.09428",
        "author": "Qiwei Wang, Dandan Lin, Wenqing Lin, Ziming Wu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09428v1 Announce Type: new \nAbstract: Due to the convenience of mobile devices, the online games have become an important part for user entertainments in reality, creating a demand for friend recommendation in online games. However, none of existing approaches can effectively incorporate the multi-modal user features (\\emph{e.g.}, images and texts) with the structural information in the friendship graph, due to the following limitations: (1) some of them ignore the high-order structural proximity between users, (2) some fail to learn the pairwise relevance between users at modality-specific level, and (3) some cannot capture both the local and global user preferences on different modalities. By addressing these issues, in this paper, we propose an end-to-end model \\textsc{FROG} that better models the user preferences on potential friends. Comprehensive experiments on both offline evaluation and online deployment at \\kw{Tencent} have demonstrated the superiority of \\textsc{FROG} over existing approaches."
      },
      {
        "id": "oai:arXiv.org:2504.09434v1",
        "title": "Constants of motion network revisited",
        "link": "https://arxiv.org/abs/2504.09434",
        "author": "Wenqi Fang, Chao Chen, Yongkui Yang, Zheng Wang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09434v1 Announce Type: new \nAbstract: Discovering constants of motion is meaningful in helping understand the dynamical systems, but inevitably needs proficient mathematical skills and keen analytical capabilities. With the prevalence of deep learning, methods employing neural networks, such as Constant Of Motion nETwork (COMET), are promising in handling this scientific problem. Although the COMET method can produce better predictions on dynamics by exploiting the discovered constants of motion, there is still plenty of room to sharpen it. In this paper, we propose a novel neural network architecture, built using the singular-value-decomposition (SVD) technique, and a two-phase training algorithm to improve the performance of COMET. Extensive experiments show that our approach not only retains the advantages of COMET, such as applying to non-Hamiltonian systems and indicating the number of constants of motion, but also can be more lightweight and noise-robust than COMET."
      },
      {
        "id": "oai:arXiv.org:2504.09439v1",
        "title": "Identity-Aware Vision-Language Model for Explainable Face Forgery Detection",
        "link": "https://arxiv.org/abs/2504.09439",
        "author": "Junhao Xu, Jingjing Chen, Yang Jiao, Jiacheng Zhang, Zhiyu Tan, Hao Li, Yu-Gang Jiang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09439v1 Announce Type: new \nAbstract: Recent advances in generative artificial intelligence have enabled the creation of highly realistic image forgeries, raising significant concerns about digital media authenticity. While existing detection methods demonstrate promising results on benchmark datasets, they face critical limitations in real-world applications. First, existing detectors typically fail to detect semantic inconsistencies with the person's identity, such as implausible behaviors or incompatible environmental contexts in given images. Second, these methods rely heavily on low-level visual cues, making them effective for known forgeries but less reliable against new or unseen manipulation techniques. To address these challenges, we present a novel personalized vision-language model (VLM) that integrates low-level visual artifact analysis and high-level semantic inconsistency detection. Unlike previous VLM-based methods, our approach avoids resource-intensive supervised fine-tuning that often struggles to preserve distinct identity characteristics. Instead, we employ a lightweight method that dynamically encodes identity-specific information into specialized identifier tokens. This design enables the model to learn distinct identity characteristics while maintaining robust generalization capabilities. We further enhance detection capabilities through a lightweight detection adapter that extracts fine-grained information from shallow features of the vision encoder, preserving critical low-level evidence. Comprehensive experiments demonstrate that our approach achieves 94.25% accuracy and 94.08% F1 score, outperforming both traditional forgery detectors and general VLMs while requiring only 10 extra tokens."
      },
      {
        "id": "oai:arXiv.org:2504.09441v1",
        "title": "Structure-Accurate Medical Image Translation based on Dynamic Frequency Balance and Knowledge Guidance",
        "link": "https://arxiv.org/abs/2504.09441",
        "author": "Jiahua Xu, Dawei Zhou, Lei Hu, Zaiyi Liu, Nannan Wang, Xinbo Gao",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09441v1 Announce Type: new \nAbstract: Multimodal medical images play a crucial role in the precise and comprehensive clinical diagnosis. Diffusion model is a powerful strategy to synthesize the required medical images. However, existing approaches still suffer from the problem of anatomical structure distortion due to the overfitting of high-frequency information and the weakening of low-frequency information. Thus, we propose a novel method based on dynamic frequency balance and knowledge guidance. Specifically, we first extract the low-frequency and high-frequency components by decomposing the critical features of the model using wavelet transform. Then, a dynamic frequency balance module is designed to adaptively adjust frequency for enhancing global low-frequency features and effective high-frequency details as well as suppressing high-frequency noise. To further overcome the challenges posed by the large differences between different medical modalities, we construct a knowledge-guided mechanism that fuses the prior clinical knowledge from a visual language model with visual features, to facilitate the generation of accurate anatomical structures. Experimental evaluations on multiple datasets show the proposed method achieves significant improvements in qualitative and quantitative assessments, verifying its effectiveness and superiority."
      },
      {
        "id": "oai:arXiv.org:2504.09446v1",
        "title": "Sparse Deformable Mamba for Hyperspectral Image Classification",
        "link": "https://arxiv.org/abs/2504.09446",
        "author": "Lincoln Linlin Xu, Yimin Zhu, Zack Dewis, Zhengsen Xu, Motasem Alkayid, Mabel Heffring, Saeid Taleghanidoozdoozan",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09446v1 Announce Type: new \nAbstract: Although the recent Mamba models significantly improve hyperspectral image (HSI) classification, one critical challenge is caused by the difficulty to build the Mamba sequence efficiently and effectively. This paper presents a Sparse Deformable Mamba (SDMamba) approach for enhanced HSI classification, with the following contributions. First, to enhance Mamba sequence, an efficient Sparse Deformable Sequencing (SDS) approach is designed to adaptively learn the \"optimal\" sequence, leading to sparse and deformable Mamba sequence with increased detail preservation and decreased computations. Second, to boost spatial-spectral feature learning, based on SDS, a Sparse Deformable Spatial Mamba Module (SDSpaM) and a Sparse Deformable Spectral Mamba Module (SDSpeM) are designed for tailored modeling of the spatial information spectral information. Last, to improve the fusion of SDSpaM and SDSpeM, an attention based feature fusion approach is designed to integrate the outputs of the SDSpaM and SDSpeM. The proposed method is tested on several benchmark datasets with many state-of-the-art approaches, demonstrating that the proposed approach can achieve higher accuracy, faster speed, and better detail small-class preservation capability."
      },
      {
        "id": "oai:arXiv.org:2504.09448v1",
        "title": "InfoBound: A Provable Information-Bounds Inspired Framework for Both OoD Generalization and OoD Detection",
        "link": "https://arxiv.org/abs/2504.09448",
        "author": "Lin Zhu, Yifeng Yang, Zichao Nie, Yuan Gao, Jiarui Li, Qinying Gu, Xinbing Wang, Chenghu Zhou, Nanyang Ye",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09448v1 Announce Type: new \nAbstract: In real-world scenarios, distribution shifts give rise to the importance of two problems: out-of-distribution (OoD) generalization, which focuses on models' generalization ability against covariate shifts (i.e., the changes of environments), and OoD detection, which aims to be aware of semantic shifts (i.e., test-time unseen classes). Real-world testing environments often involve a combination of both covariate and semantic shifts. While numerous methods have been proposed to address these critical issues, only a few works tackled them simultaneously. Moreover, prior works often improve one problem but sacrifice the other. To overcome these limitations, we delve into boosting OoD detection and OoD generalization from the perspective of information theory, which can be easily applied to existing models and different tasks. Building upon the theoretical bounds for mutual information and conditional entropy, we provide a unified approach, composed of Mutual Information Minimization (MI-Min) and Conditional Entropy Maximizing (CE-Max). Extensive experiments and comprehensive evaluations on multi-label image classification and object detection have demonstrated the superiority of our method. It successfully mitigates trade-offs between the two challenges compared to competitive baselines."
      },
      {
        "id": "oai:arXiv.org:2504.09449v1",
        "title": "aweSOM: a CPU/GPU-accelerated Self-organizing Map and Statistically Combined Ensemble Framework for Machine-learning Clustering Analysis",
        "link": "https://arxiv.org/abs/2504.09449",
        "author": "Trung Ha, Joonas N\\\"attil\\\"a, Jordy Davelaar",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09449v1 Announce Type: new \nAbstract: We introduce aweSOM, an open-source Python package for machine learning (ML) clustering and classification, using a Self-organizing Maps (SOM) algorithm that incorporates CPU/GPU acceleration to accommodate large ($N > 10^6$, where $N$ is the number of data points), multidimensional datasets. aweSOM consists of two main modules, one that handles the initialization and training of the SOM, and another that stacks the results of multiple SOM realizations to obtain more statistically robust clusters. Existing Python-based SOM implementations (e.g., POPSOM, Yuan (2018); MiniSom, Vettigli (2018); sklearn-som) primarily serve as proof-of-concept demonstrations, optimized for smaller datasets, but lacking scalability for large, multidimensional data. aweSOM provides a solution for this gap in capability, with good performance scaling up to $\\sim 10^8$ individual points, and capable of utilizing multiple features per point. We compare the code performance against the legacy implementations it is based on, and find a 10-100x speed up, as well as significantly improved memory efficiency, due to several built-in optimizations."
      },
      {
        "id": "oai:arXiv.org:2504.09451v1",
        "title": "FractalForensics: Proactive Deepfake Detection and Localization via Fractal Watermarks",
        "link": "https://arxiv.org/abs/2504.09451",
        "author": "Tianyi Wang, Harry Cheng, Ming-Hui Liu, Mohan Kankanhalli",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09451v1 Announce Type: new \nAbstract: Proactive Deepfake detection via robust watermarks has been raised ever since passive Deepfake detectors encountered challenges in identifying high-quality synthetic images. However, while demonstrating reasonable detection performance, they lack localization functionality and explainability in detection results. Additionally, the unstable robustness of watermarks can significantly affect the detection performance accordingly. In this study, we propose novel fractal watermarks for proactive Deepfake detection and localization, namely FractalForensics. Benefiting from the characteristics of fractals, we devise a parameter-driven watermark generation pipeline that derives fractal-based watermarks and conducts one-way encryption regarding the parameters selected. Subsequently, we propose a semi-fragile watermarking framework for watermark embedding and recovery, trained to be robust against benign image processing operations and fragile when facing Deepfake manipulations in a black-box setting. Meanwhile, we introduce an entry-to-patch strategy that implicitly embeds the watermark matrix entries into image patches at corresponding positions, achieving localization of Deepfake manipulations. Extensive experiments demonstrate satisfactory robustness and fragility of our approach against common image processing operations and Deepfake manipulations, outperforming state-of-the-art semi-fragile watermarking algorithms and passive detectors for Deepfake detection. Furthermore, by highlighting the areas manipulated, our method provides explainability for the proactive Deepfake detection results."
      },
      {
        "id": "oai:arXiv.org:2504.09454v1",
        "title": "D$^2$iT: Dynamic Diffusion Transformer for Accurate Image Generation",
        "link": "https://arxiv.org/abs/2504.09454",
        "author": "Weinan Jia, Mengqi Huang, Nan Chen, Lei Zhang, Zhendong Mao",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09454v1 Announce Type: new \nAbstract: Diffusion models are widely recognized for their ability to generate high-fidelity images. Despite the excellent performance and scalability of the Diffusion Transformer (DiT) architecture, it applies fixed compression across different image regions during the diffusion process, disregarding the naturally varying information densities present in these regions. However, large compression leads to limited local realism, while small compression increases computational complexity and compromises global consistency, ultimately impacting the quality of generated images. To address these limitations, we propose dynamically compressing different image regions by recognizing the importance of different regions, and introduce a novel two-stage framework designed to enhance the effectiveness and efficiency of image generation: (1) Dynamic VAE (DVAE) at first stage employs a hierarchical encoder to encode different image regions at different downsampling rates, tailored to their specific information densities, thereby providing more accurate and natural latent codes for the diffusion process. (2) Dynamic Diffusion Transformer (D$^2$iT) at second stage generates images by predicting multi-grained noise, consisting of coarse-grained (less latent code in smooth regions) and fine-grained (more latent codes in detailed regions), through an novel combination of the Dynamic Grain Transformer and the Dynamic Content Transformer. The strategy of combining rough prediction of noise with detailed regions correction achieves a unification of global consistency and local realism. Comprehensive experiments on various generation tasks validate the effectiveness of our approach. Code will be released at https://github.com/jiawn-creator/Dynamic-DiT."
      },
      {
        "id": "oai:arXiv.org:2504.09455v1",
        "title": "Enhancing Wide-Angle Image Using Narrow-Angle View of the Same Scene",
        "link": "https://arxiv.org/abs/2504.09455",
        "author": "Hussain Md. Safwan, Mahbub Islam Mahim, Fawwaz Mohammed Amin",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09455v1 Announce Type: new \nAbstract: A common dilemma while photographing a scene is whether to capture it in wider angle, allowing more of the scene to be covered but in lesser details or to click in narrow angle that captures better details but leaves out portions of the scene. We propose a novel method in this paper that infuses wider shots with finer quality details that is usually associated with an image captured by the primary lens by capturing the same scene using both narrow and wide field of view (FoV) lenses. We do so by training a GAN-based model to learn to extract the visual quality parameters from a narrow angle shot and to transfer these to the corresponding wide-angle image of the scene. We have mentioned in details the proposed technique to isolate the visual essence of an image and to transfer it into another image. We have also elaborately discussed our implementation details and have presented the results of evaluation over several benchmark datasets and comparisons with contemporary advancements in the field."
      },
      {
        "id": "oai:arXiv.org:2504.09459v1",
        "title": "Measuring Leakage in Concept-Based Methods: An Information Theoretic Approach",
        "link": "https://arxiv.org/abs/2504.09459",
        "author": "Mikael Makonnen, Moritz Vandenhirtz, Sonia Laguna, Julia E Vogt",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09459v1 Announce Type: new \nAbstract: Concept Bottleneck Models (CBMs) aim to enhance interpretability by structuring predictions around human-understandable concepts. However, unintended information leakage, where predictive signals bypass the concept bottleneck, compromises their transparency. This paper introduces an information-theoretic measure to quantify leakage in CBMs, capturing the extent to which concept embeddings encode additional, unintended information beyond the specified concepts. We validate the measure through controlled synthetic experiments, demonstrating its effectiveness in detecting leakage trends across various configurations. Our findings highlight that feature and concept dimensionality significantly influence leakage, and that classifier choice impacts measurement stability, with XGBoost emerging as the most reliable estimator. Additionally, preliminary investigations indicate that the measure exhibits the anticipated behavior when applied to soft joint CBMs, suggesting its reliability in leakage quantification beyond fully synthetic settings. While this study rigorously evaluates the measure in controlled synthetic experiments, future work can extend its application to real-world datasets."
      },
      {
        "id": "oai:arXiv.org:2504.09463v1",
        "title": "Comorbidity-Informed Transfer Learning for Neuro-developmental Disorder Diagnosis",
        "link": "https://arxiv.org/abs/2504.09463",
        "author": "Xin Wen, Shijie Guo, Wenbo Ning, Rui Cao, Jie Xiang, Xiaobo Liu, Jintai Chen",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09463v1 Announce Type: new \nAbstract: Neuro-developmental disorders are manifested as dysfunctions in cognition, communication, behaviour and adaptability, and deep learning-based computer-aided diagnosis (CAD) can alleviate the increasingly strained healthcare resources on neuroimaging. However, neuroimaging such as fMRI contains complex spatio-temporal features, which makes the corresponding representations susceptible to a variety of distractions, thus leading to less effective in CAD. For the first time, we present a Comorbidity-Informed Transfer Learning(CITL) framework for diagnosing neuro-developmental disorders using fMRI. In CITL, a new reinforced representation generation network is proposed, which first combines transfer learning with pseudo-labelling to remove interfering patterns from the temporal domain of fMRI and generates new representations using encoder-decoder architecture. The new representations are then trained in an architecturally simple classification network to obtain CAD model. In particular, the framework fully considers the comorbidity mechanisms of neuro-developmental disorders and effectively integrates them with semi-supervised learning and transfer learning, providing new perspectives on interdisciplinary. Experimental results demonstrate that CITL achieves competitive accuracies of 76.32% and 73.15% for detecting autism spectrum disorder and attention deficit hyperactivity disorder, respectively, which outperforms existing related transfer learning work for 7.2% and 0.5% respectively."
      },
      {
        "id": "oai:arXiv.org:2504.09472v1",
        "title": "CamMimic: Zero-Shot Image To Camera Motion Personalized Video Generation Using Diffusion Models",
        "link": "https://arxiv.org/abs/2504.09472",
        "author": "Pooja Guhan, Divya Kothandaraman, Tsung-Wei Huang, Guan-Ming Su, Dinesh Manocha",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09472v1 Announce Type: new \nAbstract: We introduce CamMimic, an innovative algorithm tailored for dynamic video editing needs. It is designed to seamlessly transfer the camera motion observed in a given reference video onto any scene of the user's choice in a zero-shot manner without requiring any additional data. Our algorithm achieves this using a two-phase strategy by leveraging a text-to-video diffusion model. In the first phase, we develop a multi-concept learning method using a combination of LoRA layers and an orthogonality loss to capture and understand the underlying spatial-temporal characteristics of the reference video as well as the spatial features of the user's desired scene. The second phase proposes a unique homography-based refinement strategy to enhance the temporal and spatial alignment of the generated video. We demonstrate the efficacy of our method through experiments conducted on a dataset containing combinations of diverse scenes and reference videos containing a variety of camera motions. In the absence of an established metric for assessing camera motion transfer between unrelated scenes, we propose CameraScore, a novel metric that utilizes homography representations to measure camera motion similarity between the reference and generated videos. Extensive quantitative and qualitative evaluations demonstrate that our approach generates high-quality, motion-enhanced videos. Additionally, a user study reveals that 70.31% of participants preferred our method for scene preservation, while 90.45% favored it for motion transfer. We hope this work lays the foundation for future advancements in camera motion transfer across different scenes."
      },
      {
        "id": "oai:arXiv.org:2504.09480v1",
        "title": "Vision-Language Model for Object Detection and Segmentation: A Review and Evaluation",
        "link": "https://arxiv.org/abs/2504.09480",
        "author": "Yongchao Feng, Yajie Liu, Shuai Yang, Wenrui Cai, Jinqing Zhang, Qiqi Zhan, Ziyue Huang, Hongxi Yan, Qiao Wan, Chenguang Liu, Junzhe Wang, Jiahui Lv, Ziqi Liu, Tengyuan Shi, Qingjie Liu, Yunhong Wang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09480v1 Announce Type: new \nAbstract: Vision-Language Model (VLM) have gained widespread adoption in Open-Vocabulary (OV) object detection and segmentation tasks. Despite they have shown promise on OV-related tasks, their effectiveness in conventional vision tasks has thus far been unevaluated. In this work, we present the systematic review of VLM-based detection and segmentation, view VLM as the foundational model and conduct comprehensive evaluations across multiple downstream tasks for the first time: 1) The evaluation spans eight detection scenarios (closed-set detection, domain adaptation, crowded objects, etc.) and eight segmentation scenarios (few-shot, open-world, small object, etc.), revealing distinct performance advantages and limitations of various VLM architectures across tasks. 2) As for detection tasks, we evaluate VLMs under three finetuning granularities: \\textit{zero prediction}, \\textit{visual fine-tuning}, and \\textit{text prompt}, and further analyze how different finetuning strategies impact performance under varied task. 3) Based on empirical findings, we provide in-depth analysis of the correlations between task characteristics, model architectures, and training methodologies, offering insights for future VLM design. 4) We believe that this work shall be valuable to the pattern recognition experts working in the fields of computer vision, multimodal learning, and vision foundation models by introducing them to the problem, and familiarizing them with the current status of the progress while providing promising directions for future research. A project associated with this review and evaluation has been created at https://github.com/better-chao/perceptual_abilities_evaluation."
      },
      {
        "id": "oai:arXiv.org:2504.09481v1",
        "title": "Rethinking the generalization of drug target affinity prediction algorithms via similarity aware evaluation",
        "link": "https://arxiv.org/abs/2504.09481",
        "author": "Chenbin Zhang, Zhiqiang Hu, Chuchu Jiang, Wen Chen, Jie Xu, Shaoting Zhang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09481v1 Announce Type: new \nAbstract: Drug-target binding affinity prediction is a fundamental task for drug discovery. It has been extensively explored in literature and promising results are reported. However, in this paper, we demonstrate that the results may be misleading and cannot be well generalized to real practice. The core observation is that the canonical randomized split of a test set in conventional evaluation leaves the test set dominated by samples with high similarity to the training set. The performance of models is severely degraded on samples with lower similarity to the training set but the drawback is highly overlooked in current evaluation. As a result, the performance can hardly be trusted when the model meets low-similarity samples in real practice. To address this problem, we propose a framework of similarity aware evaluation in which a novel split methodology is proposed to adapt to any desired distribution. This is achieved by a formulation of optimization problems which are approximately and efficiently solved by gradient descent. We perform extensive experiments across five representative methods in four datasets for two typical target evaluations and compare them with various counterpart methods. Results demonstrate that the proposed split methodology can significantly better fit desired distributions and guide the development of models. Code is released at https://github.com/Amshoreline/SAE/tree/main."
      },
      {
        "id": "oai:arXiv.org:2504.09482v1",
        "title": "HalluShift: Measuring Distribution Shifts towards Hallucination Detection in LLMs",
        "link": "https://arxiv.org/abs/2504.09482",
        "author": "Sharanya Dasgupta, Sujoy Nath, Arkaprabha Basu, Pourya Shamsolmoali, Swagatam Das",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09482v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have recently garnered widespread attention due to their adeptness at generating innovative responses to the given prompts across a multitude of domains. However, LLMs often suffer from the inherent limitation of hallucinations and generate incorrect information while maintaining well-structured and coherent responses. In this work, we hypothesize that hallucinations stem from the internal dynamics of LLMs. Our observations indicate that, during passage generation, LLMs tend to deviate from factual accuracy in subtle parts of responses, eventually shifting toward misinformation. This phenomenon bears a resemblance to human cognition, where individuals may hallucinate while maintaining logical coherence, embedding uncertainty within minor segments of their speech. To investigate this further, we introduce an innovative approach, HalluShift, designed to analyze the distribution shifts in the internal state space and token probabilities of the LLM-generated responses. Our method attains superior performance compared to existing baselines across various benchmark datasets. Our codebase is available at https://github.com/sharanya-dasgupta001/hallushift."
      },
      {
        "id": "oai:arXiv.org:2504.09484v1",
        "title": "An overview of condensation phenomenon in deep learning",
        "link": "https://arxiv.org/abs/2504.09484",
        "author": "Zhi-Qin John Xu, Yaoyu Zhang, Zhangchen Zhou",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09484v1 Announce Type: new \nAbstract: In this paper, we provide an overview of a common phenomenon, condensation, observed during the nonlinear training of neural networks: During the nonlinear training of neural networks, neurons in the same layer tend to condense into groups with similar outputs. Empirical observations suggest that the number of condensed clusters of neurons in the same layer typically increases monotonically as training progresses. Neural networks with small weight initializations or Dropout optimization can facilitate this condensation process. We also examine the underlying mechanisms of condensation from the perspectives of training dynamics and the structure of the loss landscape. The condensation phenomenon offers valuable insights into the generalization abilities of neural networks and correlates to stronger reasoning abilities in transformer-based language models."
      },
      {
        "id": "oai:arXiv.org:2504.09485v1",
        "title": "GenEDA: Unleashing Generative Reasoning on Netlist via Multimodal Encoder-Decoder Aligned Foundation Model",
        "link": "https://arxiv.org/abs/2504.09485",
        "author": "Wenji Fang, Jing Wang, Yao Lu, Shang Liu, Zhiyao Xie",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09485v1 Announce Type: new \nAbstract: The success of foundation AI has motivated the research of circuit foundation models, which are customized to assist the integrated circuit (IC) design process. However, existing pre-trained circuit models are typically limited to standalone encoders for predictive tasks or decoders for generative tasks. These two model types are developed independently, operate on different circuit modalities, and reside in separate latent spaces, which restricts their ability to complement each other for more advanced applications. In this work, we present GenEDA, the first framework that aligns circuit encoders with decoders within a shared latent space. GenEDA bridges the gap between graph-based circuit representations and text-based large language models (LLMs), enabling communication between their respective latent spaces. To achieve the alignment, we propose two paradigms that support both open-source trainable LLMs and commercial frozen LLMs. Built on this aligned architecture, GenEDA enables three unprecedented generative reasoning tasks over netlists, where the model reversely generates the high-level functionality from low-level netlists in different granularities. These tasks extend traditional gate-type prediction to direct generation of full-circuit functionality. Experiments demonstrate that GenEDA significantly boosts advanced LLMs' (e.g., GPT-4o and DeepSeek-V3) performance in all tasks."
      },
      {
        "id": "oai:arXiv.org:2504.09486v1",
        "title": "Adaptive Cluster-Based Synthetic Minority Oversampling Technique for Traffic Mode Choice Prediction with Imbalanced Dataset",
        "link": "https://arxiv.org/abs/2504.09486",
        "author": "Guang An Ooi, Shehab Ahmed",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09486v1 Announce Type: new \nAbstract: Urban datasets such as citizen transportation modes often contain disproportionately distributed classes, posing significant challenges to the classification of under-represented samples using data-driven models. In the literature, various resampling methods have been developed to create synthetic data for minority classes (oversampling) or remove samples from majority classes (undersampling) to alleviate class imbalance. However, oversampling approaches tend to overgeneralize minor classes that are closely clustered and neglect sparse regions which may contain crucial information. Conversely, undersampling methods potentially remove useful information on certain subgroups. Hence, a resampling approach that takes the inherent distribution of data into consideration is required to ensure appropriate synthetic data creation. This study proposes an adaptive cluster-based synthetic minority oversampling technique. Density-based spatial clustering is applied on minority classes to identify subgroups based on their input features. The classes in each of these subgroups are then oversampled according to the ratio of data points of their local cluster to the largest majority class. When used in conjunction with machine learning models such as random forest and extreme gradient boosting, this oversampling method results in significantly higher F1 scores for the minority classes compared to other resampling techniques. These improved models provide accurate classification of transportation modes."
      },
      {
        "id": "oai:arXiv.org:2504.09488v1",
        "title": "Kongzi: A Historical Large Language Model with Fact Enhancement",
        "link": "https://arxiv.org/abs/2504.09488",
        "author": "Jiashu Yang, Ningning Wang, Yian Zhao, Chaoran Feng, Junjia Du, Hao Pang, Zhirui Fang, Xuxin Cheng",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09488v1 Announce Type: new \nAbstract: The capabilities of the latest large language models (LLMs) have been extended from pure natural language understanding to complex reasoning tasks. However, current reasoning models often exhibit factual inaccuracies in longer reasoning chains, which poses challenges for historical reasoning and limits the potential of LLMs in complex, knowledge-intensive tasks. Historical studies require not only the accurate presentation of factual information but also the ability to establish cross-temporal correlations and derive coherent conclusions from fragmentary and often ambiguous sources. To address these challenges, we propose Kongzi, a large language model specifically designed for historical analysis. Through the integration of curated, high-quality historical data and a novel fact-reinforcement learning strategy, Kongzi demonstrates strong factual alignment and sophisticated reasoning depth. Extensive experiments on tasks such as historical question answering and narrative generation demonstrate that Kongzi outperforms existing models in both factual accuracy and reasoning depth. By effectively addressing the unique challenges inherent in historical texts, Kongzi sets a new standard for the development of accurate and reliable LLMs in professional domains."
      },
      {
        "id": "oai:arXiv.org:2504.09491v1",
        "title": "DropoutGS: Dropping Out Gaussians for Better Sparse-view Rendering",
        "link": "https://arxiv.org/abs/2504.09491",
        "author": "Yexing Xu, Longguang Wang, Minglin Chen, Sheng Ao, Li Li, Yulan Guo",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09491v1 Announce Type: new \nAbstract: Although 3D Gaussian Splatting (3DGS) has demonstrated promising results in novel view synthesis, its performance degrades dramatically with sparse inputs and generates undesirable artifacts. As the number of training views decreases, the novel view synthesis task degrades to a highly under-determined problem such that existing methods suffer from the notorious overfitting issue. Interestingly, we observe that models with fewer Gaussian primitives exhibit less overfitting under sparse inputs. Inspired by this observation, we propose a Random Dropout Regularization (RDR) to exploit the advantages of low-complexity models to alleviate overfitting. In addition, to remedy the lack of high-frequency details for these models, an Edge-guided Splitting Strategy (ESS) is developed. With these two techniques, our method (termed DropoutGS) provides a simple yet effective plug-in approach to improve the generalization performance of existing 3DGS methods. Extensive experiments show that our DropoutGS produces state-of-the-art performance under sparse views on benchmark datasets including Blender, LLFF, and DTU. The project page is at: https://xuyx55.github.io/DropoutGS/."
      },
      {
        "id": "oai:arXiv.org:2504.09493v1",
        "title": "Federated Prototype Graph Learning",
        "link": "https://arxiv.org/abs/2504.09493",
        "author": "Zhengyu Wu, Xunkai Li, Yinlin Zhu, Rong-Hua Li, Guoren Wang, Chenghu Zhou",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09493v1 Announce Type: new \nAbstract: In recent years, Federated Graph Learning (FGL) has gained significant attention for its distributed training capabilities in graph-based machine intelligence applications, mitigating data silos while offering a new perspective for privacy-preserve large-scale graph learning. However, multi-level FGL heterogeneity presents various client-server collaboration challenges: (1) Model-level: The variation in clients for expected performance and scalability necessitates the deployment of heterogeneous models. Unfortunately, most FGL methods rigidly demand identical client models due to the direct model weight aggregation on the server. (2) Data-level: The intricate nature of graphs, marked by the entanglement of node profiles and topology, poses an optimization dilemma. This implies that models obtained by federated training struggle to achieve superior performance. (3) Communication-level: Some FGL methods attempt to increase message sharing among clients or between clients and the server to improve training, which inevitably leads to high communication costs. In this paper, we propose FedPG as a general prototype-guided optimization method for the above multi-level FGL heterogeneity. Specifically, on the client side, we integrate multi-level topology-aware prototypes to capture local graph semantics. Subsequently, on the server side, leveraging the uploaded prototypes, we employ topology-guided contrastive learning and personalized technology to tailor global prototypes for each client, broadcasting them to improve local training. Experiments demonstrate that FedPG outperforms SOTA baselines by an average of 3.57\\% in accuracy while reducing communication costs by 168x."
      },
      {
        "id": "oai:arXiv.org:2504.09498v1",
        "title": "EasyREG: Easy Depth-Based Markerless Registration and Tracking using Augmented Reality Device for Surgical Guidance",
        "link": "https://arxiv.org/abs/2504.09498",
        "author": "Yue Yang, Christoph Leuze, Brian Hargreaves, Bruce Daniel, Fred Baik",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09498v1 Announce Type: new \nAbstract: The use of Augmented Reality (AR) devices for surgical guidance has gained increasing traction in the medical field. Traditional registration methods often rely on external fiducial markers to achieve high accuracy and real-time performance. However, these markers introduce cumbersome calibration procedures and can be challenging to deploy in clinical settings. While commercial solutions have attempted real-time markerless tracking using the native RGB cameras of AR devices, their accuracy remains questionable for medical guidance, primarily due to occlusions and significant outliers between the live sensor data and the preoperative target anatomy point cloud derived from MRI or CT scans. In this work, we present a markerless framework that relies only on the depth sensor of AR devices and consists of two modules: a registration module for high-precision, outlier-robust target anatomy localization, and a tracking module for real-time pose estimation. The registration module integrates depth sensor error correction, a human-in-the-loop region filtering technique, and a robust global alignment with curvature-aware feature sampling, followed by local ICP refinement, for markerless alignment of preoperative models with patient anatomy. The tracking module employs a fast and robust registration algorithm that uses the initial pose from the registration module to estimate the target pose in real-time. We comprehensively evaluated the performance of both modules through simulation and real-world measurements. The results indicate that our markerless system achieves superior performance for registration and comparable performance for tracking to industrial solutions. The two-module design makes our system a one-stop solution for surgical procedures where the target anatomy moves or stays static during surgery."
      },
      {
        "id": "oai:arXiv.org:2504.09499v1",
        "title": "Decoding the mechanisms of the Hattrick football manager game using Bayesian network structure learning for optimal decision-making",
        "link": "https://arxiv.org/abs/2504.09499",
        "author": "Anthony C. Constantinou, Nicholas Higgins, Neville K. Kitson",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09499v1 Announce Type: new \nAbstract: Hattrick is a free web-based probabilistic football manager game with over 200,000 users competing for titles at national and international levels. Launched in Sweden in 1997 as part of an MSc project, the game's slow-paced design has fostered a loyal community, with many users remaining active for decades. Hattrick's game-engine mechanics are partially hidden, and users have attempted to decode them with incremental success over the years. Rule-based, statistical and machine learning models have been developed to aid this effort and are widely used by the community. However, these models or tools have not been formally described or evaluated in the scientific literature. This study is the first to explore Hattrick using structure learning techniques and Bayesian networks, integrating both data and domain knowledge to develop models capable of explaining and simulating the game engine. We present a comprehensive analysis assessing the effectiveness of structure learning algorithms in relation to knowledge-based structures, and show that while structure learning may achieve a higher overall network fit, it does not result in more accurate predictions for selected variables of interest, when compared to knowledge-based networks that produce a lower overall network fit. Additionally, we introduce and publicly share a fully specified Bayesian network model that matches the performance of top models used by the Hattrick community. We further demonstrate how analysis extends beyond prediction by providing a visual representation of conditional dependencies, and using the best performing Bayesian network model for in-game decision-making. To support future research, we make all data, graphical structures, and models publicly available online."
      },
      {
        "id": "oai:arXiv.org:2504.09502v1",
        "title": "PCM-SAR: Physics-Driven Contrastive Mutual Learning for SAR Classification",
        "link": "https://arxiv.org/abs/2504.09502",
        "author": "Pengfei Wang (School of Computer Science,Engineering, Central South University, Changsha, China), Hao Zheng (School of Computer Science,Engineering, Central South University, Changsha, China), Zhigang Hu (School of Computer Science,Engineering, Central South University, Changsha, China), Aikun Xu (School of Computer Science,Engineering, Central South University, Changsha, China), Meiguang Zheng (School of Computer Science,Engineering, Central South University, Changsha, China), Liu Yang (School of Computer Science,Engineering, Central South University, Changsha, China)",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09502v1 Announce Type: new \nAbstract: Existing SAR image classification methods based on Contrastive Learning often rely on sample generation strategies designed for optical images, failing to capture the distinct semantic and physical characteristics of SAR data. To address this, we propose Physics-Driven Contrastive Mutual Learning for SAR Classification (PCM-SAR), which incorporates domain-specific physical insights to improve sample generation and feature extraction. PCM-SAR utilizes the gray-level co-occurrence matrix (GLCM) to simulate realistic noise patterns and applies semantic detection for unsupervised local sampling, ensuring generated samples accurately reflect SAR imaging properties. Additionally, a multi-level feature fusion mechanism based on mutual learning enables collaborative refinement of feature representations. Notably, PCM-SAR significantly enhances smaller models by refining SAR feature representations, compensating for their limited capacity. Experimental results show that PCM-SAR consistently outperforms SOTA methods across diverse datasets and SAR classification tasks."
      },
      {
        "id": "oai:arXiv.org:2504.09504v1",
        "title": "MADLLM: Multivariate Anomaly Detection via Pre-trained LLMs",
        "link": "https://arxiv.org/abs/2504.09504",
        "author": "Wei Tao, Xiaoyang Qu, Kai Lu, Jiguang Wan, Guokuan Li, Jianzong Wang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09504v1 Announce Type: new \nAbstract: When applying pre-trained large language models (LLMs) to address anomaly detection tasks, the multivariate time series (MTS) modality of anomaly detection does not align with the text modality of LLMs. Existing methods simply transform the MTS data into multiple univariate time series sequences, which can cause many problems. This paper introduces MADLLM, a novel multivariate anomaly detection method via pre-trained LLMs. We design a new triple encoding technique to align the MTS modality with the text modality of LLMs. Specifically, this technique integrates the traditional patch embedding method with two novel embedding approaches: Skip Embedding, which alters the order of patch processing in traditional methods to help LLMs retain knowledge of previous features, and Feature Embedding, which leverages contrastive learning to allow the model to better understand the correlations between different features. Experimental results demonstrate that our method outperforms state-of-the-art methods in various public anomaly detection datasets."
      },
      {
        "id": "oai:arXiv.org:2504.09506v1",
        "title": "Pillar-Voxel Fusion Network for 3D Object Detection in Airborne Hyperspectral Point Clouds",
        "link": "https://arxiv.org/abs/2504.09506",
        "author": "Yanze Jiang, Yanfeng Gu, Xian Li",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09506v1 Announce Type: new \nAbstract: Hyperspectral point clouds (HPCs) can simultaneously characterize 3D spatial and spectral information of ground objects, offering excellent 3D perception and target recognition capabilities. Current approaches for generating HPCs often involve fusion techniques with hyperspectral images and LiDAR point clouds, which inevitably lead to geometric-spectral distortions due to fusion errors and obstacle occlusions. These adverse effects limit their performance in downstream fine-grained tasks across multiple scenarios, particularly in airborne applications. To address these issues, we propose PiV-AHPC, a 3D object detection network for airborne HPCs. To the best of our knowledge, this is the first attempt at this HPCs task. Specifically, we first develop a pillar-voxel dual-branch encoder, where the former captures spectral and vertical structural features from HPCs to overcome spectral distortion, while the latter emphasizes extracting accurate 3D spatial features from point clouds. A multi-level feature fusion mechanism is devised to enhance information interaction between the two branches, achieving neighborhood feature alignment and channel-adaptive selection, thereby organically integrating heterogeneous features and mitigating geometric distortion. Extensive experiments on two airborne HPCs datasets demonstrate that PiV-AHPC possesses state-of-the-art detection performance and high generalization capability."
      },
      {
        "id": "oai:arXiv.org:2504.09507v1",
        "title": "FVOS for MOSE Track of 4th PVUW Challenge: 3rd Place Solution",
        "link": "https://arxiv.org/abs/2504.09507",
        "author": "Mengjiao Wang, Junpei Zhang, Xu Liu, Yuting Yang, Mengru Ma",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09507v1 Announce Type: new \nAbstract: Video Object Segmentation (VOS) is one of the most fundamental and challenging tasks in computer vision and has a wide range of applications. Most existing methods rely on spatiotemporal memory networks to extract frame-level features and have achieved promising results on commonly used datasets. However, these methods often struggle in more complex real-world scenarios. This paper addresses this issue, aiming to achieve accurate segmentation of video objects in challenging scenes. We propose fine-tuning VOS (FVOS), optimizing existing methods for specific datasets through tailored training. Additionally, we introduce a morphological post-processing strategy to address the issue of excessively large gaps between adjacent objects in single-model predictions. Finally, we apply a voting-based fusion method on multi-scale segmentation results to generate the final output. Our approach achieves J&amp;F scores of 76.81% and 83.92% during the validation and testing stages, respectively, securing third place overall in the MOSE Track of the 4th PVUW challenge 2025."
      },
      {
        "id": "oai:arXiv.org:2504.09513v1",
        "title": "DiffuMural: Restoring Dunhuang Murals with Multi-scale Diffusion",
        "link": "https://arxiv.org/abs/2504.09513",
        "author": "Puyu Han, Jiaju Kang, Yuhang Pan, Erting Pan, Zeyu Zhang, Qunchao Jin, Juntao Jiang, Zhichen Liu, Luqi Gong",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09513v1 Announce Type: new \nAbstract: Large-scale pre-trained diffusion models have produced excellent results in the field of conditional image generation. However, restoration of ancient murals, as an important downstream task in this field, poses significant challenges to diffusion model-based restoration methods due to its large defective area and scarce training samples. Conditional restoration tasks are more concerned with whether the restored part meets the aesthetic standards of mural restoration in terms of overall style and seam detail, and such metrics for evaluating heuristic image complements are lacking in current research. We therefore propose DiffuMural, a combined Multi-scale convergence and Collaborative Diffusion mechanism with ControlNet and cyclic consistency loss to optimise the matching between the generated images and the conditional control. DiffuMural demonstrates outstanding capabilities in mural restoration, leveraging training data from 23 large-scale Dunhuang murals that exhibit consistent visual aesthetics. The model excels in restoring intricate details, achieving a coherent overall appearance, and addressing the unique challenges posed by incomplete murals lacking factual grounding. Our evaluation framework incorporates four key metrics to quantitatively assess incomplete murals: factual accuracy, textural detail, contextual semantics, and holistic visual coherence. Furthermore, we integrate humanistic value assessments to ensure the restored murals retain their cultural and artistic significance. Extensive experiments validate that our method outperforms state-of-the-art (SOTA) approaches in both qualitative and quantitative metrics."
      },
      {
        "id": "oai:arXiv.org:2504.09514v1",
        "title": "Capturing Longitudinal Changes in Brain Morphology Using Temporally Parameterized Neural Displacement Fields",
        "link": "https://arxiv.org/abs/2504.09514",
        "author": "Aisha L. Shuaibu, Kieran A. Gibb, Peter A. Wijeratne, Ivor J. A. Simpson",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09514v1 Announce Type: new \nAbstract: Longitudinal image registration enables studying temporal changes in brain morphology which is useful in applications where monitoring the growth or atrophy of specific structures is important. However this task is challenging due to; noise/artifacts in the data and quantifying small anatomical changes between sequential scans. We propose a novel longitudinal registration method that models structural changes using temporally parameterized neural displacement fields. Specifically, we implement an implicit neural representation (INR) using a multi-layer perceptron that serves as a continuous coordinate-based approximation of the deformation field at any time point. In effect, for any N scans of a particular subject, our model takes as input a 3D spatial coordinate location x, y, z and a corresponding temporal representation t and learns to describe the continuous morphology of structures for both observed and unobserved points in time. Furthermore, we leverage the analytic derivatives of the INR to derive a new regularization function that enforces monotonic rate of change in the trajectory of the voxels, which is shown to provide more biologically plausible patterns. We demonstrate the effectiveness of our method on 4D brain MR registration."
      },
      {
        "id": "oai:arXiv.org:2504.09518v1",
        "title": "3D CoCa: Contrastive Learners are 3D Captioners",
        "link": "https://arxiv.org/abs/2504.09518",
        "author": "Ting Huang, Zeyu Zhang, Yemin Wang, Hao Tang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09518v1 Announce Type: new \nAbstract: 3D captioning, which aims to describe the content of 3D scenes in natural language, remains highly challenging due to the inherent sparsity of point clouds and weak cross-modal alignment in existing methods. To address these challenges, we propose 3D CoCa, a novel unified framework that seamlessly combines contrastive vision-language learning with 3D caption generation in a single architecture. Our approach leverages a frozen CLIP vision-language backbone to provide rich semantic priors, a spatially-aware 3D scene encoder to capture geometric context, and a multi-modal decoder to generate descriptive captions. Unlike prior two-stage methods that rely on explicit object proposals, 3D CoCa jointly optimizes contrastive and captioning objectives in a shared feature space, eliminating the need for external detectors or handcrafted proposals. This joint training paradigm yields stronger spatial reasoning and richer semantic grounding by aligning 3D and textual representations. Extensive experiments on the ScanRefer and Nr3D benchmarks demonstrate that 3D CoCa significantly outperforms current state-of-the-arts by 10.2% and 5.76% in CIDEr at 0.5IoU, respectively. Code will be available at https://github.com/AIGeeksGroup/3DCoCa."
      },
      {
        "id": "oai:arXiv.org:2504.09522v1",
        "title": "How new data permeates LLM knowledge and how to dilute it",
        "link": "https://arxiv.org/abs/2504.09522",
        "author": "Chen Sun, Renat Aksitov, Andrey Zhmoginov, Nolan Andrew Miller, Max Vladymyrov, Ulrich Rueckert, Been Kim, Mark Sandler",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09522v1 Announce Type: new \nAbstract: Large language models learn and continually learn through the accumulation of gradient-based updates, but how individual pieces of new information affect existing knowledge, leading to both beneficial generalization and problematic hallucination, remains poorly understood. We demonstrate that when learning new information, LLMs exhibit a \"priming\" effect: learning a new fact can cause the model to inappropriately apply that knowledge in unrelated contexts. To systematically study this phenomenon, we introduce \"Outlandish,\" a carefully curated dataset of 1320 diverse text samples designed to probe how new knowledge permeates through an LLM's existing knowledge base. Using this dataset, we show that the degree of priming after learning new information can be predicted by measuring the token probability of key words before learning. This relationship holds robustly across different model architectures (PALM-2, Gemma, Llama), sizes, and training stages. Finally, we develop two novel techniques to modulate how new knowledge affects existing model behavior: (1) a ``stepping-stone'' text augmentation strategy and (2) an ``ignore-k'' update pruning method. These approaches reduce undesirable priming effects by 50-95\\% while preserving the model's ability to learn new information. Our findings provide both empirical insights into how LLMs learn and practical tools for improving the specificity of knowledge insertion in language models. Further materials: https://sunchipsster1.github.io/projects/outlandish/"
      },
      {
        "id": "oai:arXiv.org:2504.09525v1",
        "title": "SimLabel: Similarity-Weighted Semi-supervision for Multi-annotator Learning with Missing Labels",
        "link": "https://arxiv.org/abs/2504.09525",
        "author": "Liyun Zhang, Zheng Lian, Hong Liu, Takanori Takebe, Yuta Nakashima",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09525v1 Announce Type: new \nAbstract: Multi-annotator learning has emerged as an important research direction for capturing diverse perspectives in subjective annotation tasks. Typically, due to the large scale of datasets, each annotator can only label a subset of samples, resulting in incomplete (or missing) annotations per annotator. Traditional methods generally skip model updates for missing parts, leading to inefficient data utilization. In this paper, we propose a novel similarity-weighted semi-supervised learning framework (SimLabel) that leverages inter-annotator similarities to generate weighted soft labels. This approach enables the prediction, utilization, and updating of missing parts rather than discarding them. Meanwhile, we introduce a confidence assessment mechanism combining maximum probability with entropy-based uncertainty metrics to prioritize high-confidence predictions to impute missing labels during training as an iterative refinement pipeline, which continuously improves both inter-annotator similarity estimates and individual model performance. For the comprehensive evaluation, we contribute a new video emotion recognition dataset, AMER2, which exhibits higher missing rates than its predecessor, AMER. Experimental results demonstrate our method's superior performance compared to existing approaches under missing-label conditions."
      },
      {
        "id": "oai:arXiv.org:2504.09528v1",
        "title": "AeroLite: Tag-Guided Lightweight Generation of Aerial Image Captions",
        "link": "https://arxiv.org/abs/2504.09528",
        "author": "Xing Zi, Tengjun Ni, Xianjing Fan, Xian Tao, Jun Li, Ali Braytee, Mukesh Prasad",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09528v1 Announce Type: new \nAbstract: Accurate and automated captioning of aerial imagery is crucial for applications like environmental monitoring, urban planning, and disaster management. However, this task remains challenging due to complex spatial semantics and domain variability. To address these issues, we introduce \\textbf{AeroLite}, a lightweight, tag-guided captioning framework designed to equip small-scale language models (1--3B parameters) with robust and interpretable captioning capabilities specifically for remote sensing images. \\textbf{AeroLite} leverages GPT-4o to generate a large-scale, semantically rich pseudo-caption dataset by integrating multiple remote sensing benchmarks, including DLRSD, iSAID, LoveDA, WHU, and RSSCN7. To explicitly capture key semantic elements such as orientation and land-use types, AeroLite employs natural language processing techniques to extract relevant semantic tags. These tags are then learned by a dedicated multi-label CLIP encoder, ensuring precise semantic predictions. To effectively fuse visual and semantic information, we propose a novel bridging multilayer perceptron (MLP) architecture, aligning semantic tags with visual embeddings while maintaining minimal computational overhead. AeroLite's flexible design also enables seamless integration with various pretrained large language models. We adopt a two-stage LoRA-based training approach: the initial stage leverages our pseudo-caption dataset to capture broad remote sensing semantics, followed by fine-tuning on smaller, curated datasets like UCM and Sydney Captions to refine domain-specific alignment. Experimental evaluations demonstrate that AeroLite surpasses significantly larger models (e.g., 13B parameters) in standard captioning metrics, including BLEU and METEOR, while maintaining substantially lower computational costs."
      },
      {
        "id": "oai:arXiv.org:2504.09530v1",
        "title": "Trajectory-guided Motion Perception for Facial Expression Quality Assessment in Neurological Disorders",
        "link": "https://arxiv.org/abs/2504.09530",
        "author": "Shuchao Duan, Amirhossein Dadashzadeh, Alan Whone, Majid Mirmehdi",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09530v1 Announce Type: new \nAbstract: Automated facial expression quality assessment (FEQA) in neurological disorders is critical for enhancing diagnostic accuracy and improving patient care, yet effectively capturing the subtle motions and nuances of facial muscle movements remains a challenge. We propose to analyse facial landmark trajectories, a compact yet informative representation, that encodes these subtle motions from a high-level structural perspective. Hence, we introduce Trajectory-guided Motion Perception Transformer (TraMP-Former), a novel FEQA framework that fuses landmark trajectory features for fine-grained motion capture with visual semantic cues from RGB frames, ultimately regressing the combined features into a quality score. Extensive experiments demonstrate that TraMP-Former achieves new state-of-the-art performance on benchmark datasets with neurological disorders, including PFED5 (up by 6.51%) and an augmented Toronto NeuroFace (up by 7.62%). Our ablation studies further validate the efficiency and effectiveness of landmark trajectories in FEQA. Our code is available at https://github.com/shuchaoduan/TraMP-Former."
      },
      {
        "id": "oai:arXiv.org:2504.09535v1",
        "title": "FastRSR: Efficient and Accurate Road Surface Reconstruction from Bird's Eye View",
        "link": "https://arxiv.org/abs/2504.09535",
        "author": "Yuting Zhao, Yuheng Ji, Xiaoshuai Hao, Shuxiao Li",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09535v1 Announce Type: new \nAbstract: Road Surface Reconstruction (RSR) is crucial for autonomous driving, enabling the understanding of road surface conditions. Recently, RSR from the Bird's Eye View (BEV) has gained attention for its potential to enhance performance. However, existing methods for transforming perspective views to BEV face challenges such as information loss and representation sparsity. Moreover, stereo matching in BEV is limited by the need to balance accuracy with inference speed. To address these challenges, we propose two efficient and accurate BEV-based RSR models: FastRSR-mono and FastRSR-stereo. Specifically, we first introduce Depth-Aware Projection (DAP), an efficient view transformation strategy designed to mitigate information loss and sparsity by querying depth and image features to aggregate BEV data within specific road surface regions using a pre-computed look-up table. To optimize accuracy and speed in stereo matching, we design the Spatial Attention Enhancement (SAE) and Confidence Attention Generation (CAG) modules. SAE adaptively highlights important regions, while CAG focuses on high-confidence predictions and filters out irrelevant information. FastRSR achieves state-of-the-art performance, exceeding monocular competitors by over 6.0% in elevation absolute error and providing at least a 3.0x speedup by stereo methods on the RSRD dataset. The source code will be released."
      },
      {
        "id": "oai:arXiv.org:2504.09540v1",
        "title": "EmbodiedOcc++: Boosting Embodied 3D Occupancy Prediction with Plane Regularization and Uncertainty Sampler",
        "link": "https://arxiv.org/abs/2504.09540",
        "author": "Hao Wang, Xiaobao Wei, Xiaoan Zhang, Jianing Li, Chengyu Bai, Ying Li, Ming Lu, Wenzhao Zheng, Shanghang Zhang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09540v1 Announce Type: new \nAbstract: Online 3D occupancy prediction provides a comprehensive spatial understanding of embodied environments. While the innovative EmbodiedOcc framework utilizes 3D semantic Gaussians for progressive indoor occupancy prediction, it overlooks the geometric characteristics of indoor environments, which are primarily characterized by planar structures. This paper introduces EmbodiedOcc++, enhancing the original framework with two key innovations: a Geometry-guided Refinement Module (GRM) that constrains Gaussian updates through plane regularization, along with a Semantic-aware Uncertainty Sampler (SUS) that enables more effective updates in overlapping regions between consecutive frames. GRM regularizes the position update to align with surface normals. It determines the adaptive regularization weight using curvature-based and depth-based constraints, allowing semantic Gaussians to align accurately with planar surfaces while adapting in complex regions. To effectively improve geometric consistency from different views, SUS adaptively selects proper Gaussians to update. Comprehensive experiments on the EmbodiedOcc-ScanNet benchmark demonstrate that EmbodiedOcc++ achieves state-of-the-art performance across different settings. Our method demonstrates improved edge accuracy and retains more geometric details while ensuring computational efficiency, which is essential for online embodied perception. The code will be released at: https://github.com/PKUHaoWang/EmbodiedOcc2."
      },
      {
        "id": "oai:arXiv.org:2504.09544v1",
        "title": "Causal integration of chemical structures improves representations of microscopy images for morphological profiling",
        "link": "https://arxiv.org/abs/2504.09544",
        "author": "Yemin Yu, Neil Tenenholtz, Lester Mackey, Ying Wei, David Alvarez-Melis, Ava P. Amini, Alex X. Lu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09544v1 Announce Type: new \nAbstract: Recent advances in self-supervised deep learning have improved our ability to quantify cellular morphological changes in high-throughput microscopy screens, a process known as morphological profiling. However, most current methods only learn from images, despite many screens being inherently multimodal, as they involve both a chemical or genetic perturbation as well as an image-based readout. We hypothesized that incorporating chemical compound structure during self-supervised pre-training could improve learned representations of images in high-throughput microscopy screens. We introduce a representation learning framework, MICON (Molecular-Image Contrastive Learning), that models chemical compounds as treatments that induce counterfactual transformations of cell phenotypes. MICON significantly outperforms classical hand-crafted features such as CellProfiler and existing deep-learning-based representation learning methods in challenging evaluation settings where models must identify reproducible effects of drugs across independent replicates and data-generating centers. We demonstrate that incorporating chemical compound information into the learning process provides consistent improvements in our evaluation setting and that modeling compounds specifically as treatments in a causal framework outperforms approaches that directly align images and compounds in a single representation space. Our findings point to a new direction for representation learning in morphological profiling, suggesting that methods should explicitly account for the multimodal nature of microscopy screening data."
      },
      {
        "id": "oai:arXiv.org:2504.09549v1",
        "title": "SD-ReID: View-aware Stable Diffusion for Aerial-Ground Person Re-Identification",
        "link": "https://arxiv.org/abs/2504.09549",
        "author": "Xiang Hu, Pingping Zhang, Yuhao Wang, Bin Yan, Huchuan Lu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09549v1 Announce Type: new \nAbstract: Aerial-Ground Person Re-IDentification (AG-ReID) aims to retrieve specific persons across cameras with different viewpoints. Previous works focus on designing discriminative ReID models to maintain identity consistency despite drastic changes in camera viewpoints. The core idea behind these methods is quite natural, but designing a view-robust network is a very challenging task. Moreover, they overlook the contribution of view-specific features in enhancing the model's capability to represent persons. To address these issues, we propose a novel two-stage feature learning framework named SD-ReID for AG-ReID, which takes advantage of the powerful understanding capacity of generative models, e.g., Stable Diffusion (SD), to generate view-specific features between different viewpoints. In the first stage, we train a simple ViT-based model to extract coarse-grained representations and controllable conditions. Then, in the second stage, we fine-tune the SD model to learn complementary representations guided by the controllable conditions. Furthermore, we propose the View-Refine Decoder (VRD) to obtain additional controllable conditions to generate missing cross-view features. Finally, we use the coarse-grained representations and all-view features generated by SD to retrieve target persons. Extensive experiments on the AG-ReID benchmarks demonstrate the effectiveness of our proposed SD-ReID. The source code will be available upon acceptance."
      },
      {
        "id": "oai:arXiv.org:2504.09555v1",
        "title": "Mitigating Long-tail Distribution in Oracle Bone Inscriptions: Dataset, Model, and Benchmark",
        "link": "https://arxiv.org/abs/2504.09555",
        "author": "Jinhao Li, Zijian Chen, Runze Dong, Tingzhu Chen, Changbo Wang, Guangtao Zhai",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09555v1 Announce Type: new \nAbstract: The oracle bone inscription (OBI) recognition plays a significant role in understanding the history and culture of ancient China. However, the existing OBI datasets suffer from a long-tail distribution problem, leading to biased performance of OBI recognition models across majority and minority classes. With recent advancements in generative models, OBI synthesis-based data augmentation has become a promising avenue to expand the sample size of minority classes. Unfortunately, current OBI datasets lack large-scale structure-aligned image pairs for generative model training. To address these problems, we first present the Oracle-P15K, a structure-aligned OBI dataset for OBI generation and denoising, consisting of 14,542 images infused with domain knowledge from OBI experts. Second, we propose a diffusion model-based pseudo OBI generator, called OBIDiff, to achieve realistic and controllable OBI generation. Given a clean glyph image and a target rubbing-style image, it can effectively transfer the noise style of the original rubbing to the glyph image. Extensive experiments on OBI downstream tasks and user preference studies show the effectiveness of the proposed Oracle-P15K dataset and demonstrate that OBIDiff can accurately preserve inherent glyph structures while transferring authentic rubbing styles effectively."
      },
      {
        "id": "oai:arXiv.org:2504.09566v1",
        "title": "Syzygy of Thoughts: Improving LLM CoT with the Minimal Free Resolution",
        "link": "https://arxiv.org/abs/2504.09566",
        "author": "Chenghao Li, Chaoning Zhang, Yi Lu, Jiaquan Zhang, Qigan Sun, Xudong Wang, Jiwei Wei, Guoqing Wang, Yang Yang, Heng Tao Shen",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09566v1 Announce Type: new \nAbstract: Chain-of-Thought (CoT) prompting enhances the reasoning of large language models (LLMs) by decomposing problems into sequential steps, mimicking human logic and reducing errors. However, complex tasks with vast solution spaces and vague constraints often exceed the capacity of a single reasoning chain. Inspired by Minimal Free Resolution (MFR) in commutative algebra and algebraic geometry, we propose Syzygy of Thoughts (SoT)-a novel framework that extends CoT by introducing auxiliary, interrelated reasoning paths. SoT captures deeper logical dependencies, enabling more robust and structured problem-solving. MFR decomposes a module into a sequence of free modules with minimal rank, providing a structured analytical approach to complex systems. This method introduces the concepts of \"Module\", \"Betti numbers\",\"Freeness\", \"Mapping\", \"Exactness\" and \"Minimality\", enabling the systematic decomposition of the original complex problem into logically complete minimal subproblems while preserving key problem features and reducing reasoning length. We tested SoT across diverse datasets (e.g., GSM8K, MATH) and models (e.g., GPT-4o-mini, Qwen2.5), achieving inference accuracy that matches or surpasses mainstream CoTs standards. Additionally, by aligning the sampling process with algebraic constraints, our approach enhances the scalability of inference time in LLMs, ensuring both transparent reasoning and high performance. Our code will be publicly available at https://github.com/dlMARiA/Syzygy-of-thoughts."
      },
      {
        "id": "oai:arXiv.org:2504.09570v1",
        "title": "LLMs Can Achieve High-quality Simultaneous Machine Translation as Efficiently as Offline",
        "link": "https://arxiv.org/abs/2504.09570",
        "author": "Biao Fu, Minpeng Liao, Kai Fan, Chengxi Li, Liang Zhang, Yidong Chen, Xiaodong Shi",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09570v1 Announce Type: new \nAbstract: When the complete source sentence is provided, Large Language Models (LLMs) perform excellently in offline machine translation even with a simple prompt \"Translate the following sentence from [src lang] into [tgt lang]:\". However, in many real scenarios, the source tokens arrive in a streaming manner and simultaneous machine translation (SiMT) is required, then the efficiency and performance of decoder-only LLMs are significantly limited by their auto-regressive nature. To enable LLMs to achieve high-quality SiMT as efficiently as offline translation, we propose a novel paradigm that includes constructing supervised fine-tuning (SFT) data for SiMT, along with new training and inference strategies. To replicate the token input/output stream in SiMT, the source and target tokens are rearranged into an interleaved sequence, separated by special tokens according to varying latency requirements. This enables powerful LLMs to learn read and write operations adaptively, based on varying latency prompts, while still maintaining efficient auto-regressive decoding. Experimental results show that, even with limited SFT data, our approach achieves state-of-the-art performance across various SiMT benchmarks, and preserves the original abilities of offline translation. Moreover, our approach generalizes well to document-level SiMT setting without requiring specific fine-tuning, even beyond the offline translation model."
      },
      {
        "id": "oai:arXiv.org:2504.09586v1",
        "title": "Short-Path Prompting in LLMs: Analyzing Reasoning Instability and Solutions for Robust Performance",
        "link": "https://arxiv.org/abs/2504.09586",
        "author": "Zuoli Tang, Junjie Ou, Kaiqin Hu, Chunwei Wu, Zhaoxin Huan, Chilin Fu, Xiaolu Zhang, Jun Zhou, Chenliang Li",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09586v1 Announce Type: new \nAbstract: Recent years have witnessed significant progress in large language models' (LLMs) reasoning, which is largely due to the chain-of-thought (CoT) approaches, allowing models to generate intermediate reasoning steps before reaching the final answer. Building on these advances, state-of-the-art LLMs are instruction-tuned to provide long and detailed CoT pathways when responding to reasoning-related questions. However, human beings are naturally cognitive misers and will prompt language models to give rather short responses, thus raising a significant conflict with CoT reasoning. In this paper, we delve into how LLMs' reasoning performance changes when users provide short-path prompts. The results and analysis reveal that language models can reason effectively and robustly without explicit CoT prompts, while under short-path prompting, LLMs' reasoning ability drops significantly and becomes unstable, even on grade-school problems. To address this issue, we propose two approaches: an instruction-guided approach and a fine-tuning approach, both designed to effectively manage the conflict. Experimental results show that both methods achieve high accuracy, providing insights into the trade-off between instruction adherence and reasoning accuracy in current models."
      },
      {
        "id": "oai:arXiv.org:2504.09588v1",
        "title": "TextSplat: Text-Guided Semantic Fusion for Generalizable Gaussian Splatting",
        "link": "https://arxiv.org/abs/2504.09588",
        "author": "Zhicong Wu, Hongbin Xu, Gang Xu, Ping Nie, Zhixin Yan, Jinkai Zheng, Liangqiong Qu, Ming Li, Liqiang Nie",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09588v1 Announce Type: new \nAbstract: Recent advancements in Generalizable Gaussian Splatting have enabled robust 3D reconstruction from sparse input views by utilizing feed-forward Gaussian Splatting models, achieving superior cross-scene generalization. However, while many methods focus on geometric consistency, they often neglect the potential of text-driven guidance to enhance semantic understanding, which is crucial for accurately reconstructing fine-grained details in complex scenes. To address this limitation, we propose TextSplat--the first text-driven Generalizable Gaussian Splatting framework. By employing a text-guided fusion of diverse semantic cues, our framework learns robust cross-modal feature representations that improve the alignment of geometric and semantic information, producing high-fidelity 3D reconstructions. Specifically, our framework employs three parallel modules to obtain complementary representations: the Diffusion Prior Depth Estimator for accurate depth information, the Semantic Aware Segmentation Network for detailed semantic information, and the Multi-View Interaction Network for refined cross-view features. Then, in the Text-Guided Semantic Fusion Module, these representations are integrated via the text-guided and attention-based feature aggregation mechanism, resulting in enhanced 3D Gaussian parameters enriched with detailed semantic cues. Experimental results on various benchmark datasets demonstrate improved performance compared to existing methods across multiple evaluation metrics, validating the effectiveness of our framework. The code will be publicly available."
      },
      {
        "id": "oai:arXiv.org:2504.09598v1",
        "title": "DualPrompt-MedCap: A Dual-Prompt Enhanced Approach for Medical Image Captioning",
        "link": "https://arxiv.org/abs/2504.09598",
        "author": "Yining Zhao, Ali Braytee, Mukesh Prasad",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09598v1 Announce Type: new \nAbstract: Medical image captioning via vision-language models has shown promising potential for clinical diagnosis assistance. However, generating contextually relevant descriptions with accurate modality recognition remains challenging. We present DualPrompt-MedCap, a novel dual-prompt enhancement framework that augments Large Vision-Language Models (LVLMs) through two specialized components: (1) a modality-aware prompt derived from a semi-supervised classification model pretrained on medical question-answer pairs, and (2) a question-guided prompt leveraging biomedical language model embeddings. To address the lack of captioning ground truth, we also propose an evaluation framework that jointly considers spatial-semantic relevance and medical narrative quality. Experiments on multiple medical datasets demonstrate that DualPrompt-MedCap outperforms the baseline BLIP-3 by achieving a 22% improvement in modality recognition accuracy while generating more comprehensive and question-aligned descriptions. Our method enables the generation of clinically accurate reports that can serve as medical experts' prior knowledge and automatic annotations for downstream vision-language tasks."
      },
      {
        "id": "oai:arXiv.org:2504.09601v1",
        "title": "Mixture-of-Shape-Experts (MoSE): End-to-End Shape Dictionary Framework to Prompt SAM for Generalizable Medical Segmentation",
        "link": "https://arxiv.org/abs/2504.09601",
        "author": "Jia Wei, Xiaoqi Zhao, Jonghye Woo, Jinsong Ouyang, Georges El Fakhri, Qingyu Chen, Xiaofeng Liu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09601v1 Announce Type: new \nAbstract: Single domain generalization (SDG) has recently attracted growing attention in medical image segmentation. One promising strategy for SDG is to leverage consistent semantic shape priors across different imaging protocols, scanner vendors, and clinical sites. However, existing dictionary learning methods that encode shape priors often suffer from limited representational power with a small set of offline computed shape elements, or overfitting when the dictionary size grows. Moreover, they are not readily compatible with large foundation models such as the Segment Anything Model (SAM). In this paper, we propose a novel Mixture-of-Shape-Experts (MoSE) framework that seamlessly integrates the idea of mixture-of-experts (MoE) training into dictionary learning to efficiently capture diverse and robust shape priors. Our method conceptualizes each dictionary atom as a shape expert, which specializes in encoding distinct semantic shape information. A gating network dynamically fuses these shape experts into a robust shape map, with sparse activation guided by SAM encoding to prevent overfitting. We further provide this shape map as a prompt to SAM, utilizing the powerful generalization capability of SAM through bidirectional integration. All modules, including the shape dictionary, are trained in an end-to-end manner. Extensive experiments on multiple public datasets demonstrate its effectiveness."
      },
      {
        "id": "oai:arXiv.org:2504.09604v1",
        "title": "Mitigating Many-Shot Jailbreaking",
        "link": "https://arxiv.org/abs/2504.09604",
        "author": "Christopher M. Ackerman, Nina Panickssery",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09604v1 Announce Type: new \nAbstract: Many-shot jailbreaking (MSJ) is an adversarial technique that exploits the long context windows of modern LLMs to circumvent model safety training by including in the prompt many examples of a ``fake'' assistant responding inappropriately before the final request. With enough examples, the model's in-context learning abilities override its safety training, and it responds as if it were the ``fake'' assistant. In this work, we probe the effectiveness of different fine tuning and input sanitization approaches on mitigating MSJ attacks, alone and in combination. We find incremental mitigation effectiveness for each, and we show that the combined techniques significantly reduce the effectiveness of MSJ attacks, while retaining model performance in benign in-context learning and conversational tasks. We suggest that our approach could meaningfully ameliorate this vulnerability if incorporated into model safety post-training."
      },
      {
        "id": "oai:arXiv.org:2504.09606v1",
        "title": "Early-Bird Diffusion: Investigating and Leveraging Timestep-Aware Early-Bird Tickets in Diffusion Models for Efficient Training",
        "link": "https://arxiv.org/abs/2504.09606",
        "author": "Lexington Whalen (Celine), Zhenbang Du (Celine), Haoran You (Celine), Chaojian Li (Celine), Sixu Li (Celine),  Yingyan (Celine),  Lin",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09606v1 Announce Type: new \nAbstract: Training diffusion models (DMs) requires substantial computational resources due to multiple forward and backward passes across numerous timesteps, motivating research into efficient training techniques. In this paper, we propose EB-Diff-Train, a new efficient DM training approach that is orthogonal to other methods of accelerating DM training, by investigating and leveraging Early-Bird (EB) tickets -- sparse subnetworks that manifest early in the training process and maintain high generation quality.\n  We first investigate the existence of traditional EB tickets in DMs, enabling competitive generation quality without fully training a dense model.\n  Then, we delve into the concept of diffusion-dedicated EB tickets, drawing on insights from varying importance of different timestep regions. These tickets adapt their sparsity levels according to the importance of corresponding timestep regions, allowing for aggressive sparsity during non-critical regions while conserving computational resources for crucial timestep regions.\n  Building on this, we develop an efficient DM training technique that derives timestep-aware EB tickets, trains them in parallel, and combines them during inference for image generation. Extensive experiments validate the existence of both traditional and timestep-aware EB tickets, as well as the effectiveness of our proposed EB-Diff-Train method. This approach can significantly reduce training time both spatially and temporally -- achieving 2.9$\\times$ to 5.8$\\times$ speedups over training unpruned dense models, and up to 10.3$\\times$ faster training compared to standard train-prune-finetune pipelines -- without compromising generative quality.\n  Our code is available at https://github.com/GATECH-EIC/Early-Bird-Diffusion."
      },
      {
        "id": "oai:arXiv.org:2504.09608v1",
        "title": "ERL-MPP: Evolutionary Reinforcement Learning with Multi-head Puzzle Perception for Solving Large-scale Jigsaw Puzzles of Eroded Gaps",
        "link": "https://arxiv.org/abs/2504.09608",
        "author": "Xingke Song, Xiaoying Yang, Chenglin Yao, Jianfeng Ren, Ruibin Bai, Xin Chen, Xudong Jiang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09608v1 Announce Type: new \nAbstract: Solving jigsaw puzzles has been extensively studied. While most existing models focus on solving either small-scale puzzles or puzzles with no gap between fragments, solving large-scale puzzles with gaps presents distinctive challenges in both image understanding and combinatorial optimization. To tackle these challenges, we propose a framework of Evolutionary Reinforcement Learning with Multi-head Puzzle Perception (ERL-MPP) to derive a better set of swapping actions for solving the puzzles. Specifically, to tackle the challenges of perceiving the puzzle with gaps, a Multi-head Puzzle Perception Network (MPPN) with a shared encoder is designed, where multiple puzzlet heads comprehensively perceive the local assembly status, and a discriminator head provides a global assessment of the puzzle. To explore the large swapping action space efficiently, an Evolutionary Reinforcement Learning (EvoRL) agent is designed, where an actor recommends a set of suitable swapping actions from a large action space based on the perceived puzzle status, a critic updates the actor using the estimated rewards and the puzzle status, and an evaluator coupled with evolutionary strategies evolves the actions aligning with the historical assembly experience. The proposed ERL-MPP is comprehensively evaluated on the JPLEG-5 dataset with large gaps and the MIT dataset with large-scale puzzles. It significantly outperforms all state-of-the-art models on both datasets."
      },
      {
        "id": "oai:arXiv.org:2504.09620v1",
        "title": "Metropolis-Hastings Captioning Game: Knowledge Fusion of Vision Language Models via Decentralized Bayesian Inference",
        "link": "https://arxiv.org/abs/2504.09620",
        "author": "Yuta Matsui, Ryosuke Yamaki, Ryo Ueda, Seitaro Shinagawa, Tadahiro Taniguchi",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09620v1 Announce Type: new \nAbstract: We propose the Metropolis-Hastings Captioning Game (MHCG), a method to fuse knowledge of multiple vision-language models (VLMs) by learning from each other. Although existing methods that combine multiple models suffer from inference costs and architectural constraints, MHCG avoids these problems by performing decentralized Bayesian inference through a process resembling a language game. The knowledge fusion process establishes communication between two VLM agents alternately captioning images and learning from each other. We conduct two image-captioning experiments with two VLMs, each pre-trained on a different dataset. The first experiment demonstrates that MHCG achieves consistent improvement in reference-free evaluation metrics. The second experiment investigates how MHCG contributes to sharing VLMs' category-level vocabulary by observing the occurrence of the vocabulary in the generated captions."
      },
      {
        "id": "oai:arXiv.org:2504.09621v1",
        "title": "Tokenize Image Patches: Global Context Fusion for Effective Haze Removal in Large Images",
        "link": "https://arxiv.org/abs/2504.09621",
        "author": "Jiuchen Chen, Xinyu Yan, Qizhi Xu, Kaiqi Li",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09621v1 Announce Type: new \nAbstract: Global contextual information and local detail features are essential for haze removal tasks. Deep learning models perform well on small, low-resolution images, but they encounter difficulties with large, high-resolution ones due to GPU memory limitations. As a compromise, they often resort to image slicing or downsampling. The former diminishes global information, while the latter discards high-frequency details. To address these challenges, we propose DehazeXL, a haze removal method that effectively balances global context and local feature extraction, enabling end-to-end modeling of large images on mainstream GPU hardware. Additionally, to evaluate the efficiency of global context utilization in haze removal performance, we design a visual attribution method tailored to the characteristics of haze removal tasks. Finally, recognizing the lack of benchmark datasets for haze removal in large images, we have developed an ultra-high-resolution haze removal dataset (8KDehaze) to support model training and testing. It includes 10000 pairs of clear and hazy remote sensing images, each sized at 8192 $\\times$ 8192 pixels. Extensive experiments demonstrate that DehazeXL can infer images up to 10240 $\\times$ 10240 pixels with only 21 GB of memory, achieving state-of-the-art results among all evaluated methods. The source code and experimental dataset are available at https://github.com/CastleChen339/DehazeXL."
      },
      {
        "id": "oai:arXiv.org:2504.09623v1",
        "title": "Ges3ViG: Incorporating Pointing Gestures into Language-Based 3D Visual Grounding for Embodied Reference Understanding",
        "link": "https://arxiv.org/abs/2504.09623",
        "author": "Atharv Mahesh Mane, Dulanga Weerakoon, Vigneshwaran Subbaraju, Sougata Sen, Sanjay E. Sarma, Archan Misra",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09623v1 Announce Type: new \nAbstract: 3-Dimensional Embodied Reference Understanding (3D-ERU) combines a language description and an accompanying pointing gesture to identify the most relevant target object in a 3D scene. Although prior work has explored pure language-based 3D grounding, there has been limited exploration of 3D-ERU, which also incorporates human pointing gestures. To address this gap, we introduce a data augmentation framework-Imputer, and use it to curate a new benchmark dataset-ImputeRefer for 3D-ERU, by incorporating human pointing gestures into existing 3D scene datasets that only contain language instructions. We also propose Ges3ViG, a novel model for 3D-ERU that achieves ~30% improvement in accuracy as compared to other 3D-ERU models and ~9% compared to other purely language-based 3D grounding models. Our code and dataset are available at https://github.com/AtharvMane/Ges3ViG."
      },
      {
        "id": "oai:arXiv.org:2504.09629v1",
        "title": "Quantization Error Propagation: Revisiting Layer-Wise Post-Training Quantization",
        "link": "https://arxiv.org/abs/2504.09629",
        "author": "Yamato Arai, Yuma Ichikawa",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09629v1 Announce Type: new \nAbstract: Layer-wise post-training quantization has emerged as a widely used technique for compressing large language models (LLMs) without retraining. However, recent progress in this line of research is saturating, underscoring the need to revisit its core limitation and explore further improvements. This study identifies a critical bottleneck in existing layer-wise PTQ methods: the accumulation of quantization errors across layers significantly degrades performance, particularly in low-bit regimes. To address this, we propose Quantization Error Propagation (QEP), a lightweight and general framework that enhances layer-wise PTQ by explicitly propagating the quantization error which enable compensating for accumulated quantization errors. Additionally, we introduce a tunable propagation mechanism that allows for control over both propagation strength and computational overhead, making the framework adaptable to various architectures and resource constraints. Empirical evaluation on LLaMA2 models (7B, 13B, 70B) demonstrate that incorporating QEP into standard layer-wise PTQ pipelines outperforms standard PTQ methods. Notably, QEP yields substantial performance improvements under extreme low-bit quantization settings."
      },
      {
        "id": "oai:arXiv.org:2504.09639v1",
        "title": "Leveraging Reasoning Model Answers to Enhance Non-Reasoning Model Capability",
        "link": "https://arxiv.org/abs/2504.09639",
        "author": "Haotian Wang, Han Zhao, Shuaiting Chen, Xiaoyu Tian, Sitong Zhao, Yunjie Ji, Yiping Peng, Xiangang Li",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09639v1 Announce Type: new \nAbstract: Recent advancements in large language models (LLMs), such as DeepSeek-R1 and OpenAI-o1, have demonstrated the significant effectiveness of test-time scaling, achieving substantial performance gains across various benchmarks. These advanced models utilize deliberate \"thinking\" steps to systematically enhance answer quality. In this paper, we propose leveraging these high-quality outputs generated by reasoning-intensive models to improve less computationally demanding, non-reasoning models. We explore and compare methodologies for utilizing the answers produced by reasoning models to train and improve non-reasoning models. Through straightforward Supervised Fine-Tuning (SFT) experiments on established benchmarks, we demonstrate consistent improvements across various benchmarks, underscoring the potential of this approach for advancing the ability of models to answer questions directly."
      },
      {
        "id": "oai:arXiv.org:2504.09641v1",
        "title": "TinyLLaVA-Video-R1: Towards Smaller LMMs for Video Reasoning",
        "link": "https://arxiv.org/abs/2504.09641",
        "author": "Xingjian Zhang, Siwei Wen, Wenjun Wu, Lei Huang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09641v1 Announce Type: new \nAbstract: Recently, improving the reasoning ability of large multimodal models (LMMs) through reinforcement learning has made great progress. However, most existing works are based on highly reasoning-intensive datasets such as mathematics and code, and researchers generally choose large-scale models as the foundation. We argue that exploring small-scale models' reasoning capabilities remains valuable for researchers with limited computational resources. Moreover, enabling models to explain their reasoning processes on general question-answering datasets is equally meaningful. Therefore, we present the small-scale video reasoning model TinyLLaVA-Video-R1. Based on TinyLLaVA-Video, a traceably trained video understanding model with no more than 4B parameters, it not only demonstrates significantly improved reasoning and thinking capabilities after using reinforcement learning on general Video-QA datasets, but also exhibits the emergent characteristic of \"aha moments\". Furthermore, we share a series of experimental findings, aiming to provide practical insights for future exploration of video reasoning (thinking) abilities in small-scale models. It is available at https://github.com/ZhangXJ199/TinyLLaVA-Video-R1."
      },
      {
        "id": "oai:arXiv.org:2504.09643v1",
        "title": "Iterative Self-Training for Code Generation via Reinforced Re-Ranking",
        "link": "https://arxiv.org/abs/2504.09643",
        "author": "Nikita Sorokin, Ivan Sedykh, Valentin Malykh",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09643v1 Announce Type: new \nAbstract: Generating high-quality code that solves complex programming tasks is challenging, especially with current decoder-based models that produce highly stochastic outputs. In code generation, even minor errors can easily break the entire solution. Leveraging multiple sampled solutions can significantly improve the overall output quality.\n  One effective way to enhance code generation is by pairing a code generation model with a reranker model, which selects the best solution from the generated samples. We propose a novel iterative self-training approach for self-training reranker models using Proximal Policy Optimization (PPO), aimed at improving both reranking accuracy and the overall code generation process. Unlike traditional PPO approaches, where the focus is on optimizing a generative model with a reward model, our approach emphasizes the development of a robust reward/reranking model. This model improves the quality of generated code through reranking and addresses problems and errors that the reward model might overlook during PPO alignment with the reranker. Our method iteratively refines the training dataset by re-evaluating outputs, identifying high-scoring negative examples, and incorporating them into the training loop, that boosting model performance.\n  Our evaluation on the MultiPL-E dataset demonstrates that our 13.4B parameter model outperforms a 33B model in code generation quality while being three times faster. Moreover, it achieves performance comparable to GPT-4 and surpasses it in one programming language."
      },
      {
        "id": "oai:arXiv.org:2504.09644v1",
        "title": "SegEarth-R1: Geospatial Pixel Reasoning via Large Language Model",
        "link": "https://arxiv.org/abs/2504.09644",
        "author": "Kaiyu Li, Zepeng Xin, Li Pang, Chao Pang, Yupeng Deng, Jing Yao, Guisong Xia, Deyu Meng, Zhi Wang, Xiangyong Cao",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09644v1 Announce Type: new \nAbstract: Remote sensing has become critical for understanding environmental dynamics, urban planning, and disaster management. However, traditional remote sensing workflows often rely on explicit segmentation or detection methods, which struggle to handle complex, implicit queries that require reasoning over spatial context, domain knowledge, and implicit user intent. Motivated by this, we introduce a new task, \\ie, geospatial pixel reasoning, which allows implicit querying and reasoning and generates the mask of the target region. To advance this task, we construct and release the first large-scale benchmark dataset called EarthReason, which comprises 5,434 manually annotated image masks with over 30,000 implicit question-answer pairs. Moreover, we propose SegEarth-R1, a simple yet effective language-guided segmentation baseline that integrates a hierarchical visual encoder, a large language model (LLM) for instruction parsing, and a tailored mask generator for spatial correlation. The design of SegEarth-R1 incorporates domain-specific adaptations, including aggressive visual token compression to handle ultra-high-resolution remote sensing images, a description projection module to fuse language and multi-scale features, and a streamlined mask prediction pipeline that directly queries description embeddings. Extensive experiments demonstrate that SegEarth-R1 achieves state-of-the-art performance on both reasoning and referring segmentation tasks, significantly outperforming traditional and LLM-based segmentation methods. Our data and code will be released at https://github.com/earth-insights/SegEarth-R1."
      },
      {
        "id": "oai:arXiv.org:2504.09645v1",
        "title": "Myanmar XNLI: Building a Dataset and Exploring Low-resource Approaches to Natural Language Inference with Myanmar",
        "link": "https://arxiv.org/abs/2504.09645",
        "author": "Aung Kyaw Htet, Mark Dras",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09645v1 Announce Type: new \nAbstract: Despite dramatic recent progress in NLP, it is still a major challenge to apply Large Language Models (LLM) to low-resource languages. This is made visible in benchmarks such as Cross-Lingual Natural Language Inference (XNLI), a key task that demonstrates cross-lingual capabilities of NLP systems across a set of 15 languages. In this paper, we extend the XNLI task for one additional low-resource language, Myanmar, as a proxy challenge for broader low-resource languages, and make three core contributions. First, we build a dataset called Myanmar XNLI (myXNLI) using community crowd-sourced methods, as an extension to the existing XNLI corpus. This involves a two-stage process of community-based construction followed by expert verification; through an analysis, we demonstrate and quantify the value of the expert verification stage in the context of community-based construction for low-resource languages. We make the myXNLI dataset available to the community for future research. Second, we carry out evaluations of recent multilingual language models on the myXNLI benchmark, as well as explore data-augmentation methods to improve model performance. Our data-augmentation methods improve model accuracy by up to 2 percentage points for Myanmar, while uplifting other languages at the same time. Third, we investigate how well these data-augmentation methods generalise to other low-resource languages in the XNLI dataset."
      },
      {
        "id": "oai:arXiv.org:2504.09648v1",
        "title": "RANSAC Revisited: An Improved Algorithm for Robust Subspace Recovery under Adversarial and Noisy Corruptions",
        "link": "https://arxiv.org/abs/2504.09648",
        "author": "Guixian Chen, Jianhao Ma, Salar Fattahi",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09648v1 Announce Type: new \nAbstract: In this paper, we study the problem of robust subspace recovery (RSR) in the presence of both strong adversarial corruptions and Gaussian noise. Specifically, given a limited number of noisy samples -- some of which are tampered by an adaptive and strong adversary -- we aim to recover a low-dimensional subspace that approximately contains a significant fraction of the uncorrupted samples, up to an error that scales with the Gaussian noise. Existing approaches to this problem often suffer from high computational costs or rely on restrictive distributional assumptions, limiting their applicability in truly adversarial settings. To address these challenges, we revisit the classical random sample consensus (RANSAC) algorithm, which offers strong robustness to adversarial outliers, but sacrifices efficiency and robustness against Gaussian noise and model misspecification in the process. We propose a two-stage algorithm, RANSAC+, that precisely pinpoints and remedies the failure modes of standard RANSAC. Our method is provably robust to both Gaussian and adversarial corruptions, achieves near-optimal sample complexity without requiring prior knowledge of the subspace dimension, and is more efficient than existing RANSAC-type methods."
      },
      {
        "id": "oai:arXiv.org:2504.09656v1",
        "title": "KeyVID: Keyframe-Aware Video Diffusion for Audio-Synchronized Visual Animation",
        "link": "https://arxiv.org/abs/2504.09656",
        "author": "Xingrui Wang, Jiang Liu, Ze Wang, Xiaodong Yu, Jialian Wu, Ximeng Sun, Yusheng Su, Alan Yuille, Zicheng Liu, Emad Barsoum",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09656v1 Announce Type: new \nAbstract: Generating video from various conditions, such as text, image, and audio, enables both spatial and temporal control, leading to high-quality generation results. Videos with dramatic motions often require a higher frame rate to ensure smooth motion. Currently, most audio-to-visual animation models use uniformly sampled frames from video clips. However, these uniformly sampled frames fail to capture significant key moments in dramatic motions at low frame rates and require significantly more memory when increasing the number of frames directly. In this paper, we propose KeyVID, a keyframe-aware audio-to-visual animation framework that significantly improves the generation quality for key moments in audio signals while maintaining computation efficiency. Given an image and an audio input, we first localize keyframe time steps from the audio. Then, we use a keyframe generator to generate the corresponding visual keyframes. Finally, we generate all intermediate frames using the motion interpolator. Through extensive experiments, we demonstrate that KeyVID significantly improves audio-video synchronization and video quality across multiple datasets, particularly for highly dynamic motions. The code is released in https://github.com/XingruiWang/KeyVID."
      },
      {
        "id": "oai:arXiv.org:2504.09663v1",
        "title": "Ordinary Least Squares as an Attention Mechanism",
        "link": "https://arxiv.org/abs/2504.09663",
        "author": "Philippe Goulet Coulombe",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09663v1 Announce Type: new \nAbstract: I show that ordinary least squares (OLS) predictions can be rewritten as the output of a restricted attention module, akin to those forming the backbone of large language models. This connection offers an alternative perspective on attention beyond the conventional information retrieval framework, making it more accessible to researchers and analysts with a background in traditional statistics. It falls into place when OLS is framed as a similarity-based method in a transformed regressor space, distinct from the standard view based on partial correlations. In fact, the OLS solution can be recast as the outcome of an alternative problem: minimizing squared prediction errors by optimizing the embedding space in which training and test vectors are compared via inner products. Rather than estimating coefficients directly, we equivalently learn optimal encoding and decoding operations for predictors. From this vantage point, OLS maps naturally onto the query-key-value structure of attention mechanisms. Building on this foundation, I discuss key elements of Transformer-style attention and draw connections to classic ideas from time series econometrics."
      },
      {
        "id": "oai:arXiv.org:2504.09664v1",
        "title": "Adapting to the Unknown: Robust Meta-Learning for Zero-Shot Financial Time Series Forecasting",
        "link": "https://arxiv.org/abs/2504.09664",
        "author": "Anxian Liu, Junying Ma, Guang Zhang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09664v1 Announce Type: new \nAbstract: Financial time series forecasting in the zero-shot setting is essential for risk management and investment decision-making, particularly during abrupt market regime shifts or in emerging markets with limited historical data. While Model-Agnostic Meta-Learning (MAML)-based approaches have shown promise in this domain, existing meta task construction strategies often lead to suboptimal performance, especially when dealing with highly turbulent financial time series. To address this challenge, we propose a novel task construction method that leverages learned embeddings for more effective meta-learning in the zero-shot setting. Specifically, we construct two complementary types of meta-tasks based on the learned embeddings: intra-cluster tasks and inter-cluster tasks. To capture diverse fine-grained patterns, we apply stochastic projection matrices to the learned embeddings and use clustering algorithm to form the tasks. Additionally, to improve generalization capabilities, we employ hard task mining strategies and leverage inter-cluster tasks to identify invariant patterns across different time series. Extensive experiments on the real world financial dataset demonstrate that our method significantly outperforms existing approaches, showing better generalization ability in the zero-shot scenario."
      },
      {
        "id": "oai:arXiv.org:2504.09665v1",
        "title": "CLEAR-KGQA: Clarification-Enhanced Ambiguity Resolution for Knowledge Graph Question Answering",
        "link": "https://arxiv.org/abs/2504.09665",
        "author": "Liqiang Wen, Guanming Xiong, Tong Mo, Bing Li, Weiping Li, Wen Zhao",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09665v1 Announce Type: new \nAbstract: This study addresses the challenge of ambiguity in knowledge graph question answering (KGQA). While recent KGQA systems have made significant progress, particularly with the integration of large language models (LLMs), they typically assume user queries are unambiguous, which is an assumption that rarely holds in real-world applications. To address these limitations, we propose a novel framework that dynamically handles both entity ambiguity (e.g., distinguishing between entities with similar names) and intent ambiguity (e.g., clarifying different interpretations of user queries) through interactive clarification. Our approach employs a Bayesian inference mechanism to quantify query ambiguity and guide LLMs in determining when and how to request clarification from users within a multi-turn dialogue framework. We further develop a two-agent interaction framework where an LLM-based user simulator enables iterative refinement of logical forms through simulated user feedback. Experimental results on the WebQSP and CWQ dataset demonstrate that our method significantly improves performance by effectively resolving semantic ambiguities. Additionally, we contribute a refined dataset of disambiguated queries, derived from interaction histories, to facilitate future research in this direction."
      },
      {
        "id": "oai:arXiv.org:2504.09666v1",
        "title": "Uncertainty Guided Refinement for Fine-Grained Salient Object Detection",
        "link": "https://arxiv.org/abs/2504.09666",
        "author": "Yao Yuan, Pan Gao, Qun Dai, Jie Qin, Wei Xiang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09666v1 Announce Type: new \nAbstract: Recently, salient object detection (SOD) methods have achieved impressive performance. However, salient regions predicted by existing methods usually contain unsaturated regions and shadows, which limits the model for reliable fine-grained predictions. To address this, we introduce the uncertainty guidance learning approach to SOD, intended to enhance the model's perception of uncertain regions. Specifically, we design a novel Uncertainty Guided Refinement Attention Network (UGRAN), which incorporates three important components, i.e., the Multilevel Interaction Attention (MIA) module, the Scale Spatial-Consistent Attention (SSCA) module, and the Uncertainty Refinement Attention (URA) module. Unlike conventional methods dedicated to enhancing features, the proposed MIA facilitates the interaction and perception of multilevel features, leveraging the complementary characteristics among multilevel features. Then, through the proposed SSCA, the salient information across diverse scales within the aggregated features can be integrated more comprehensively and integrally. In the subsequent steps, we utilize the uncertainty map generated from the saliency prediction map to enhance the model's perception capability of uncertain regions, generating a highly-saturated fine-grained saliency prediction map. Additionally, we devise an adaptive dynamic partition (ADP) mechanism to minimize the computational overhead of the URA module and improve the utilization of uncertainty guidance. Experiments on seven benchmark datasets demonstrate the superiority of the proposed UGRAN over the state-of-the-art methodologies. Codes will be released at https://github.com/I2-Multimedia-Lab/UGRAN."
      },
      {
        "id": "oai:arXiv.org:2504.09671v1",
        "title": "LightHeadEd: Relightable & Editable Head Avatars from a Smartphone",
        "link": "https://arxiv.org/abs/2504.09671",
        "author": "Pranav Manu, Astitva Srivastava, Amit Raj, Varun Jampani, Avinash Sharma, P. J. Narayanan",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09671v1 Announce Type: new \nAbstract: Creating photorealistic, animatable, and relightable 3D head avatars traditionally requires expensive Lightstage with multiple calibrated cameras, making it inaccessible for widespread adoption. To bridge this gap, we present a novel, cost-effective approach for creating high-quality relightable head avatars using only a smartphone equipped with polaroid filters. Our approach involves simultaneously capturing cross-polarized and parallel-polarized video streams in a dark room with a single point-light source, separating the skin's diffuse and specular components during dynamic facial performances. We introduce a hybrid representation that embeds 2D Gaussians in the UV space of a parametric head model, facilitating efficient real-time rendering while preserving high-fidelity geometric details. Our learning-based neural analysis-by-synthesis pipeline decouples pose and expression-dependent geometrical offsets from appearance, decomposing the surface into albedo, normal, and specular UV texture maps, along with the environment maps. We collect a unique dataset of various subjects performing diverse facial expressions and head movements."
      },
      {
        "id": "oai:arXiv.org:2504.09680v1",
        "title": "SPOT: Spatio-Temporal Pattern Mining and Optimization for Load Consolidation in Freight Transportation Networks",
        "link": "https://arxiv.org/abs/2504.09680",
        "author": "Sikai Cheng, Amira Hijazi, Jeren Konak, Alan Erera, Pascal Van Hentenryck",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09680v1 Announce Type: new \nAbstract: Freight consolidation has significant potential to reduce transportation costs and mitigate congestion and pollution. An effective load consolidation plan relies on carefully chosen consolidation points to ensure alignment with existing transportation management processes, such as driver scheduling, personnel planning, and terminal operations. This complexity represents a significant challenge when searching for optimal consolidation strategies. Traditional optimization-based methods provide exact solutions, but their computational complexity makes them impractical for large-scale instances and they fail to leverage historical data. Machine learning-based approaches address these issues but often ignore operational constraints, leading to infeasible consolidation plans.\n  This work proposes SPOT, an end-to-end approach that integrates the benefits of machine learning (ML) and optimization for load consolidation. The ML component plays a key role in the planning phase by identifying the consolidation points through spatio-temporal clustering and constrained frequent itemset mining, while the optimization selects the most cost-effective feasible consolidation routes for a given operational day. Extensive experiments conducted on industrial load data demonstrate that SPOT significantly reduces travel distance and transportation costs (by about 50% on large terminals) compared to the existing industry-standard load planning strategy and a neighborhood-based heuristic. Moreover, the ML component provides valuable tactical-level insights by identifying frequently recurring consolidation opportunities that guide proactive planning. In addition, SPOT is computationally efficient and can be easily scaled to accommodate large transportation networks."
      },
      {
        "id": "oai:arXiv.org:2504.09685v1",
        "title": "Can LLMs Revolutionize the Design of Explainable and Efficient TinyML Models?",
        "link": "https://arxiv.org/abs/2504.09685",
        "author": "Christophe El Zeinaty, Wassim Hamidouche, Glenn Herrou, Daniel Menard, Merouane Debbah",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09685v1 Announce Type: new \nAbstract: This paper introduces a novel framework for designing efficient neural network architectures specifically tailored to tiny machine learning (TinyML) platforms. By leveraging large language models (LLMs) for neural architecture search (NAS), a vision transformer (ViT)-based knowledge distillation (KD) strategy, and an explainability module, the approach strikes an optimal balance between accuracy, computational efficiency, and memory usage. The LLM-guided search explores a hierarchical search space, refining candidate architectures through Pareto optimization based on accuracy, multiply-accumulate operations (MACs), and memory metrics. The best-performing architectures are further fine-tuned using logits-based KD with a pre-trained ViT-B/16 model, which enhances generalization without increasing model size. Evaluated on the CIFAR-100 dataset and deployed on an STM32H7 microcontroller (MCU), the three proposed models, LMaNet-Elite, LMaNet-Core, and QwNet-Core, achieve accuracy scores of 74.50%, 74.20% and 73.00%, respectively. All three models surpass current state-of-the-art (SOTA) models, such as MCUNet-in3/in4 (69.62% / 72.86%) and XiNet (72.27%), while maintaining a low computational cost of less than 100 million MACs and adhering to the stringent 320 KB static random-access memory (SRAM) constraint. These results demonstrate the efficiency and performance of the proposed framework for TinyML platforms, underscoring the potential of combining LLM-driven search, Pareto optimization, KD, and explainability to develop accurate, efficient, and interpretable models. This approach opens new possibilities in NAS, enabling the design of efficient architectures specifically suited for TinyML."
      },
      {
        "id": "oai:arXiv.org:2504.09687v1",
        "title": "Domain-Adaptive Continued Pre-Training of Small Language Models",
        "link": "https://arxiv.org/abs/2504.09687",
        "author": "Salman Faroz",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09687v1 Announce Type: new \nAbstract: Continued pre-training of small language models offers a promising path for domain adaptation with limited computational resources. I've investigated this approach within educational domains, evaluating it as a resource-efficient alternative to training models from scratch. Using a 125M parameter model, I demonstrate significant performance improvements through incremental training on 400 million tokens, followed by further training to reach 1 billion tokens. My approach includes comprehensive data preprocessing, memory-optimized training configurations, and benchmark-based evaluation. Results show notable gains in knowledge-intensive tasks (MMLU +8.1%) and contextual understanding (HellaSwag +7.6%), while revealing educational domain specialization trade-offs. I analyze token efficiency, catastrophic forgetting mitigation strategies, and scaling patterns. My findings suggest that thoughtful preprocessing and training methodologies enable meaningful improvements in language model capabilities even with constrained computational resources, opening pathways for domain-specific adaptation of smaller language models."
      },
      {
        "id": "oai:arXiv.org:2504.09694v1",
        "title": "Computer-Aided Layout Generation for Building Design: A Review",
        "link": "https://arxiv.org/abs/2504.09694",
        "author": "Jiachen Liu, Yuan Xue, Haomiao Ni, Rui Yu, Zihan Zhou, Sharon X. Huang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09694v1 Announce Type: new \nAbstract: Generating realistic building layouts for automatic building design has been studied in both the computer vision and architecture domains. Traditional approaches from the architecture domain, which are based on optimization techniques or heuristic design guidelines, can synthesize desirable layouts, but usually require post-processing and involve human interaction in the design pipeline, making them costly and timeconsuming. The advent of deep generative models has significantly improved the fidelity and diversity of the generated architecture layouts, reducing the workload by designers and making the process much more efficient. In this paper, we conduct a comprehensive review of three major research topics of architecture layout design and generation: floorplan layout generation, scene layout synthesis, and generation of some other formats of building layouts. For each topic, we present an overview of the leading paradigms, categorized either by research domains (architecture or machine learning) or by user input conditions or constraints. We then introduce the commonly-adopted benchmark datasets that are used to verify the effectiveness of the methods, as well as the corresponding evaluation metrics. Finally, we identify the well-solved problems and limitations of existing approaches, then propose new perspectives as promising directions for future research in this important research area. A project associated with this survey to maintain the resources is available at awesome-building-layout-generation."
      },
      {
        "id": "oai:arXiv.org:2504.09696v1",
        "title": "GRPO-LEAD: A Difficulty-Aware Reinforcement Learning Approach for Concise Mathematical Reasoning in Language Models",
        "link": "https://arxiv.org/abs/2504.09696",
        "author": "Jixiao Zhang, Chunsheng Zuo",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09696v1 Announce Type: new \nAbstract: Recent advances in R1-like reasoning models leveraging Group Relative Policy Optimization (GRPO) have significantly improved the performance of language models on mathematical reasoning tasks. However, current GRPO implementations encounter critical challenges, including reward sparsity due to binary accuracy metrics, limited incentives for conciseness, and insufficient focus on complex reasoning tasks. To address these issues, we propose GRPO-LEAD, a suite of novel enhancements tailored for mathematical reasoning. Specifically, GRPO-LEAD introduces (1) a length-dependent accuracy reward to encourage concise and precise solutions, (2) an explicit penalty mechanism for incorrect answers to sharpen decision boundaries, and (3) a difficulty-aware advantage reweighting strategy that amplifies learning signals for challenging problems. Furthermore, we systematically examine the impact of model scale and supervised fine-tuning (SFT) strategies, demonstrating that larger-scale base models and carefully curated datasets significantly enhance reinforcement learning effectiveness. Extensive empirical evaluations and ablation studies confirm that GRPO-LEAD substantially mitigates previous shortcomings, resulting in language models that produce more concise, accurate, and robust reasoning across diverse mathematical tasks."
      },
      {
        "id": "oai:arXiv.org:2504.09700v1",
        "title": "ToolTipNet: A Segmentation-Driven Deep Learning Baseline for Surgical Instrument Tip Detection",
        "link": "https://arxiv.org/abs/2504.09700",
        "author": "Zijian Wu, Shuojue Yang, Yueming Jin, Septimiu E Salcudean",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09700v1 Announce Type: new \nAbstract: In robot-assisted laparoscopic radical prostatectomy (RALP), the location of the instrument tip is important to register the ultrasound frame with the laparoscopic camera frame. A long-standing limitation is that the instrument tip position obtained from the da Vinci API is inaccurate and requires hand-eye calibration. Thus, directly computing the position of the tool tip in the camera frame using the vision-based method becomes an attractive solution. Besides, surgical instrument tip detection is the key component of other tasks, like surgical skill assessment and surgery automation. However, this task is challenging due to the small size of the tool tip and the articulation of the surgical instrument. Surgical instrument segmentation becomes relatively easy due to the emergence of the Segmentation Foundation Model, i.e., Segment Anything. Based on this advancement, we explore the deep learning-based surgical instrument tip detection approach that takes the part-level instrument segmentation mask as input. Comparison experiments with a hand-crafted image-processing approach demonstrate the superiority of the proposed method on simulated and real datasets."
      },
      {
        "id": "oai:arXiv.org:2504.09704v1",
        "title": "Transformer-Based Representation Learning for Robust Gene Expression Modeling and Cancer Prognosis",
        "link": "https://arxiv.org/abs/2504.09704",
        "author": "Shuai Jiang, Saeed Hassanpour",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09704v1 Announce Type: new \nAbstract: Transformer-based models have achieved remarkable success in natural language and vision tasks, but their application to gene expression analysis remains limited due to data sparsity, high dimensionality, and missing values. We present GexBERT, a transformer-based autoencoder framework for robust representation learning of gene expression data. GexBERT learns context-aware gene embeddings by pretraining on large-scale transcriptomic profiles with a masking and restoration objective that captures co-expression relationships among thousands of genes. We evaluate GexBERT across three critical tasks in cancer research: pan-cancer classification, cancer-specific survival prediction, and missing value imputation. GexBERT achieves state-of-the-art classification accuracy from limited gene subsets, improves survival prediction by restoring expression of prognostic anchor genes, and outperforms conventional imputation methods under high missingness. Furthermore, its attention-based interpretability reveals biologically meaningful gene patterns across cancer types. These findings demonstrate the utility of GexBERT as a scalable and effective tool for gene expression modeling, with translational potential in settings where gene coverage is limited or incomplete."
      },
      {
        "id": "oai:arXiv.org:2504.09710v1",
        "title": "DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM Post-training",
        "link": "https://arxiv.org/abs/2504.09710",
        "author": "Zhenting Wang, Guofeng Cui, Kun Wan, Wentian Zhao",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09710v1 Announce Type: new \nAbstract: Recent advances in reinforcement learning (RL)-based post-training have led to notable improvements in large language models (LLMs), particularly in enhancing their reasoning capabilities to handle complex tasks. However, most existing methods treat the training data as a unified whole, overlooking the fact that modern LLM training often involves a mixture of data from diverse distributions-varying in both source and difficulty. This heterogeneity introduces a key challenge: how to adaptively schedule training across distributions to optimize learning efficiency. In this paper, we present a principled curriculum learning framework grounded in the notion of distribution-level learnability. Our core insight is that the magnitude of policy advantages reflects how much a model can still benefit from further training on a given distribution. Based on this, we propose a distribution-level curriculum learning framework for RL-based LLM post-training, which leverages the Upper Confidence Bound (UCB) principle to dynamically adjust sampling probabilities for different distrubutions. This approach prioritizes distributions with either high average advantage (exploitation) or low sample count (exploration), yielding an adaptive and theoretically grounded training schedule. We instantiate our curriculum learning framework with GRPO as the underlying RL algorithm and demonstrate its effectiveness on logic reasoning datasets with multiple difficulties and sources. Our experiments show that our framework significantly improves convergence speed and final performance, highlighting the value of distribution-aware curriculum strategies in LLM post-training. Code: https://github.com/ZhentingWang/DUMP."
      },
      {
        "id": "oai:arXiv.org:2504.09714v1",
        "title": "Evaluating the Quality of Benchmark Datasets for Low-Resource Languages: A Case Study on Turkish",
        "link": "https://arxiv.org/abs/2504.09714",
        "author": "Ay\\c{s}e Aysu Cengiz, Ahmet Kaan Sever, Elif Ecem \\\"Um\\\"utl\\\"u, Naime \\c{S}eyma Erdem, Burak Aytan, B\\\"u\\c{s}ra Tufan, Abdullah Topraksoy, Esra Dar{\\i}c{\\i}, Cagri Toraman",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09714v1 Announce Type: new \nAbstract: The reliance on translated or adapted datasets from English or multilingual resources introduces challenges regarding linguistic and cultural suitability. This study addresses the need for robust and culturally appropriate benchmarks by evaluating the quality of 17 commonly used Turkish benchmark datasets. Using a comprehensive framework that assesses six criteria, both human and LLM-judge annotators provide detailed evaluations to identify dataset strengths and shortcomings.\n  Our results reveal that 70% of the benchmark datasets fail to meet our heuristic quality standards. The correctness of the usage of technical terms is the strongest criterion, but 85% of the criteria are not satisfied in the examined datasets. Although LLM judges demonstrate potential, they are less effective than human annotators, particularly in understanding cultural common sense knowledge and interpreting fluent, unambiguous text. GPT-4o has stronger labeling capabilities for grammatical and technical tasks, while Llama3.3-70B excels at correctness and cultural knowledge evaluation. Our findings emphasize the urgent need for more rigorous quality control in creating and adapting datasets for low-resource languages."
      },
      {
        "id": "oai:arXiv.org:2504.09724v1",
        "title": "A Survey on Efficient Vision-Language Models",
        "link": "https://arxiv.org/abs/2504.09724",
        "author": "Gaurav Shinde, Anuradha Ravi, Emon Dey, Shadman Sakib, Milind Rampure, Nirmalya Roy",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09724v1 Announce Type: new \nAbstract: Vision-language models (VLMs) integrate visual and textual information, enabling a wide range of applications such as image captioning and visual question answering, making them crucial for modern AI systems. However, their high computational demands pose challenges for real-time applications. This has led to a growing focus on developing efficient vision language models. In this survey, we review key techniques for optimizing VLMs on edge and resource-constrained devices. We also explore compact VLM architectures, frameworks and provide detailed insights into the performance-memory trade-offs of efficient VLMs. Furthermore, we establish a GitHub repository at https://github.com/MPSCUMBC/Efficient-Vision-Language-Models-A-Survey to compile all surveyed papers, which we will actively update. Our objective is to foster deeper research in this area."
      },
      {
        "id": "oai:arXiv.org:2504.09738v1",
        "title": "Automatic Detection of Intro and Credits in Video using CLIP and Multihead Attention",
        "link": "https://arxiv.org/abs/2504.09738",
        "author": "Vasilii Korolkov, Andrey Yanchenko",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09738v1 Announce Type: new \nAbstract: Detecting transitions between intro/credits and main content in videos is a crucial task for content segmentation, indexing, and recommendation systems. Manual annotation of such transitions is labor-intensive and error-prone, while heuristic-based methods often fail to generalize across diverse video styles. In this work, we introduce a deep learning-based approach that formulates the problem as a sequence-to-sequence classification task, where each second of a video is labeled as either \"intro\" or \"film.\" Our method extracts frames at a fixed rate of 1 FPS, encodes them using CLIP (Contrastive Language-Image Pretraining), and processes the resulting feature representations with a multihead attention model incorporating learned positional encoding. The system achieves an F1-score of 91.0%, Precision of 89.0%, and Recall of 97.0% on the test set, and is optimized for real-time inference, achieving 11.5 FPS on CPU and 107 FPS on high-end GPUs. This approach has practical applications in automated content indexing, highlight detection, and video summarization. Future work will explore multimodal learning, incorporating audio features and subtitles to further enhance detection accuracy."
      },
      {
        "id": "oai:arXiv.org:2504.09753v1",
        "title": "Improving Multilingual Capabilities with Cultural and Local Knowledge in Large Language Models While Enhancing Native Performance",
        "link": "https://arxiv.org/abs/2504.09753",
        "author": "Ram Mohan Rao Kadiyala, Siddartha Pullakhandam, Siddhant Gupta, Drishti Sharma, Jebish Purbey, Kanwal Mehreen, Muhammad Arham, Hamza Farooq",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09753v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have shown remarkable capabilities, but their development has primarily focused on English and other high-resource languages, leaving many languages underserved. We present our latest Hindi-English bi-lingual LLM \\textbf{Mantra-14B} with ~3\\% average improvement in benchmark scores over both languages, outperforming models twice its size. Using a curated dataset composed of English and Hindi instruction data of 485K samples, we instruction tuned models such as Qwen-2.5-14B-Instruct and Phi-4 to improve performance over both English and Hindi. Our experiments encompassing seven different LLMs of varying parameter sizes and over 140 training attempts with varying English-Hindi training data ratios demonstrated that it is possible to significantly improve multilingual performance without compromising native performance. Further, our approach avoids resource-intensive techniques like vocabulary expansion or architectural modifications, thus keeping the model size small. Our results indicate that modest fine-tuning with culturally and locally informed data can bridge performance gaps without incurring significant computational overhead. We release our training code, datasets, and models under mit and apache licenses to aid further research towards under-represented and low-resource languages."
      },
      {
        "id": "oai:arXiv.org:2504.09759v1",
        "title": "Enhancing Classifier Evaluation: A Fairer Benchmarking Strategy Based on Ability and Robustness",
        "link": "https://arxiv.org/abs/2504.09759",
        "author": "Lucas Cardoso, Vitor Santos, Jos\\'e Ribeiro, Regiane Kawasaki, Ricardo Prud\\^encio, Ronnie Alves",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09759v1 Announce Type: new \nAbstract: Benchmarking is a fundamental practice in machine learning (ML) for comparing the performance of classification algorithms. However, traditional evaluation methods often overlook a critical aspect: the joint consideration of dataset complexity and an algorithm's ability to generalize. Without this dual perspective, assessments may favor models that perform well on easy instances while failing to capture their true robustness. To address this limitation, this study introduces a novel evaluation methodology that combines Item Response Theory (IRT) with the Glicko-2 rating system, originally developed to measure player strength in competitive games. IRT assesses classifier ability based on performance over difficult instances, while Glicko-2 updates performance metrics - such as rating, deviation, and volatility - via simulated tournaments between classifiers. This combined approach provides a fairer and more nuanced measure of algorithm capability. A case study using the OpenML-CC18 benchmark showed that only 15% of the datasets are truly challenging and that a reduced subset with 50% of the original datasets offers comparable evaluation power. Among the algorithms tested, Random Forest achieved the highest ability score. The results highlight the importance of improving benchmark design by focusing on dataset quality and adopting evaluation strategies that reflect both difficulty and classifier proficiency."
      },
      {
        "id": "oai:arXiv.org:2504.09761v1",
        "title": "Dynamical symmetries in the fluctuation-driven regime: an application of Noether's theorem to noisy dynamical systems",
        "link": "https://arxiv.org/abs/2504.09761",
        "author": "John J. Vastola",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09761v1 Announce Type: new \nAbstract: Noether's theorem provides a powerful link between continuous symmetries and conserved quantities for systems governed by some variational principle. Perhaps unfortunately, most dynamical systems of interest in neuroscience and artificial intelligence cannot be described by any such principle. On the other hand, nonequilibrium physics provides a variational principle that describes how fairly generic noisy dynamical systems are most likely to transition between two states; in this work, we exploit this principle to apply Noether's theorem, and hence learn about how the continuous symmetries of dynamical systems constrain their most likely trajectories. We identify analogues of the conservation of energy, momentum, and angular momentum, and briefly discuss examples of each in the context of models of decision-making, recurrent neural networks, and diffusion generative models."
      },
      {
        "id": "oai:arXiv.org:2504.09763v1",
        "title": "Executable Functional Abstractions: Inferring Generative Programs for Advanced Math Problems",
        "link": "https://arxiv.org/abs/2504.09763",
        "author": "Zaid Khan, Elias Stengel-Eskin, Archiki Prasad, Jaemin Cho, Mohit Bansal",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09763v1 Announce Type: new \nAbstract: Scientists often infer abstract procedures from specific instances of problems and use the abstractions to generate new, related instances. For example, programs encoding the formal rules and properties of a system have been useful in fields ranging from RL (procedural environments) to physics (simulation engines). These programs can be seen as functions which execute to different outputs based on their parameterizations (e.g., gridworld configuration or initial physical conditions). We introduce the term EFA (Executable Functional Abstraction) to denote such programs for math problems. EFA-like constructs have been shown to be useful for math reasoning as problem generators for stress-testing models. However, prior work has been limited to abstractions for grade-school math (whose simple rules are easy to encode in programs), while generating EFAs for advanced math has thus far required human engineering. We explore the automatic construction of EFAs for advanced math problems. We operationalize the task of automatically constructing EFAs as a program synthesis task, and develop EFAGen, which conditions an LLM on a seed math problem and its step-by-step solution to generate candidate EFA programs that are faithful to the generalized problem and solution class underlying the seed problem. Furthermore, we formalize properties any valid EFA must possess in terms of executable unit tests, and show how the tests can be used as verifiable rewards to train LLMs to become better writers of EFAs. We demonstrate that EFAs constructed by EFAGen behave rationally by remaining faithful to seed problems, produce learnable problem variations, and that EFAGen can infer EFAs across multiple diverse sources of competition-level math problems. Finally, we show downstream uses of model-written EFAs e.g. finding problem variations that are harder or easier for a learner to solve, as well as data generation."
      },
      {
        "id": "oai:arXiv.org:2504.09764v1",
        "title": "Socratic Chart: Cooperating Multiple Agents for Robust SVG Chart Understanding",
        "link": "https://arxiv.org/abs/2504.09764",
        "author": "Yuyang Ji, Haohan Wang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09764v1 Announce Type: new \nAbstract: Multimodal Large Language Models (MLLMs) have shown remarkable versatility but face challenges in demonstrating true visual understanding, particularly in chart reasoning tasks. Existing benchmarks like ChartQA reveal significant reliance on text-based shortcuts and probabilistic pattern-matching rather than genuine visual reasoning. To rigorously evaluate visual reasoning, we introduce a more challenging test scenario by removing textual labels and introducing chart perturbations in the ChartQA dataset. Under these conditions, models like GPT-4o and Gemini-2.0 Pro experience up to a 30% performance drop, underscoring their limitations. To address these challenges, we propose Socratic Chart, a new framework that transforms chart images into Scalable Vector Graphics (SVG) representations, enabling MLLMs to integrate textual and visual modalities for enhanced chart understanding. Socratic Chart employs a multi-agent pipeline with specialized agent-generators to extract primitive chart attributes (e.g., bar heights, line coordinates) and an agent-critic to validate results, ensuring high-fidelity symbolic representations. Our framework surpasses state-of-the-art models in accurately capturing chart primitives and improving reasoning performance, establishing a robust pathway for advancing MLLM visual understanding."
      },
      {
        "id": "oai:arXiv.org:2504.09766v1",
        "title": "On the representation of stack operators by mathematical morphology",
        "link": "https://arxiv.org/abs/2504.09766",
        "author": "Diego Marcondes",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09766v1 Announce Type: new \nAbstract: This paper introduces the class of grey-scale image stack operators as those that (a) map binary-images into binary-images and (b) commute in average with cross-sectioning. We show that stack operators are 1-Lipchitz extensions of set operators which can be represented by applying a characteristic set operator to the cross-sections of the image and summing. In particular, they are a generalisation of stack filters, for which the characteristic set operators are increasing. Our main result is that stack operators inherit lattice properties of the characteristic set operators. We focus on the case of translation-invariant and locally defined stack operators and show the main result by deducing the characteristic function, kernel, and basis representation of stack operators. The results of this paper have implications on the design of image operators, since imply that to solve some grey-scale image processing problems it is enough to design an operator for performing the desired transformation on binary images, and then considering its extension given by a stack operator. We leave many topics for future research regarding the machine learning of stack operators and the characterisation of the image processing problems that can be solved by them."
      },
      {
        "id": "oai:arXiv.org:2504.09769v1",
        "title": "Identification of Community Structures in Networks Employing a Modified Divisive Algorithm",
        "link": "https://arxiv.org/abs/2504.09769",
        "author": "Ghazal Ghajari, Hooshang Jazayeri-Rad, Mashalla Abbasi Dezfooli",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09769v1 Announce Type: new \nAbstract: In numerous networks, it is vital to identify communities consisting of closely joined groups of individuals. Such communities often reveal the role of the networks or primary properties of the individuals. In this perspective, Newman and Girvan proposed a modularity score (Q) for quantifying the power of community structure and measuring the appropriateness of a division. The Q function has newly become a significant standard. In this paper, the strengths of the Q score and another technique known as the divisive algorithm are combined to enhance the efficiently of the identification of communities from a network. To achieve that goal, we have developed a new algorithm. The simulation results indicated that our algorithm achieved a division with a slightly higher Q score against some conventional methods."
      },
      {
        "id": "oai:arXiv.org:2504.09777v1",
        "title": "Reasoning without Regret",
        "link": "https://arxiv.org/abs/2504.09777",
        "author": "Tarun Chitra",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09777v1 Announce Type: new \nAbstract: Chain-of-thought reasoning enables large language models to solve multi-step tasks by framing problem solving as sequential decision problems. Outcome-based rewards, which provide feedback only on final answers, show impressive success, but face challenges with credit assignment and slow convergence. In contrast, procedure-based rewards offer efficient step-level feedback, but typically require costly human supervision. We introduce \\emph{Backwards Adaptive Reward Shaping} (BARS), a no-regret framework that converts sparse outcomes-based rewards into effective procedure-based signals. BARS uses sparse rewards generated from terminal-state priors and cover trees to scale rewards while preventing exploitation. With Bellman contraction and $(\\Delta, \\epsilon)$-gap rewards, our backward Euler solver achieves $\\epsilon$-accuracy in $O\\left((R_{\\max}/\\Delta)\\log(1/\\epsilon)\\right)$ iterations with $O(\\log T)$ dynamic regret over $T$ rounds. Our analysis, based on generic chaining, continuous scaling limits, and non-linear Feynman-Kac bounds, connects recent outcome-based methods' empirical successes with the benefits of intermediate supervision. Combined, this provides the first rigorous no-regret algorithm for outcome reward shaping, providing a theoretical foundation for the empirical success of DeepSeek's R1."
      },
      {
        "id": "oai:arXiv.org:2504.09781v1",
        "title": "Reasoning Court: Combining Reasoning, Action, and Judgment for Multi-Hop Reasoning",
        "link": "https://arxiv.org/abs/2504.09781",
        "author": "Jingtian Wu, Claire Cardie",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09781v1 Announce Type: new \nAbstract: While large language models (LLMs) have demonstrated strong capabilities in tasks like question answering and fact verification, they continue to suffer from hallucinations and reasoning errors, especially in multi-hop tasks that require integration of multiple information sources. Current methods address these issues through retrieval-based techniques (grounding reasoning in external evidence), reasoning-based approaches (enhancing coherence via improved prompting), or hybrid strategies combining both elements. One prominent hybrid method, ReAct, has outperformed purely retrieval-based or reasoning-based approaches; however, it lacks internal verification of intermediate reasoning steps, allowing potential errors to propagate through complex reasoning tasks. In this paper, we introduce Reasoning Court (RC), a novel framework that extends iterative reasoning-and-retrieval methods, such as ReAct, with a dedicated LLM judge. Unlike ReAct, RC employs this judge to independently evaluate multiple candidate answers and their associated reasoning generated by separate LLM agents. The judge is asked to select the answer that it considers the most factually grounded and logically coherent based on the presented reasoning and evidence, or synthesizes a new answer using available evidence and its pre-trained knowledge if all candidates are inadequate, flawed, or invalid. Evaluations on multi-hop benchmarks (HotpotQA, MuSiQue) and fact-verification (FEVER) demonstrate that RC consistently outperforms state-of-the-art few-shot prompting methods without task-specific fine-tuning."
      },
      {
        "id": "oai:arXiv.org:2504.09789v1",
        "title": "EquiVDM: Equivariant Video Diffusion Models with Temporally Consistent Noise",
        "link": "https://arxiv.org/abs/2504.09789",
        "author": "Chao Liu, Arash Vahdat",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09789v1 Announce Type: new \nAbstract: Temporally consistent video-to-video generation is essential for applications of video diffusion models in areas such as sim-to-real, style-transfer, video upsampling, etc. In this paper, we propose a video diffusion framework that leverages temporally consistent noise to generate coherent video frames without specialized modules or additional constraints. We show that the standard training objective of diffusion models, when applied with temporally consistent noise, encourages the model to be equivariant to spatial transformations in input video and noise. This enables our model to better follow motion patterns from the input video, producing aligned motion and high-fidelity frames. Furthermore, we extend our approach to 3D-consistent video generation by attaching noise as textures on 3D meshes, ensuring 3D consistency in sim-to-real applications. Experimental results demonstrate that our method surpasses state-of-the-art baselines in motion alignment, 3D consistency, and video quality while requiring only a few sampling steps in practice."
      },
      {
        "id": "oai:arXiv.org:2504.09792v1",
        "title": "A Tale of Two Learning Algorithms: Multiple Stream Random Walk and Asynchronous Gossip",
        "link": "https://arxiv.org/abs/2504.09792",
        "author": "Peyman Gholami, Hulya Seferoglu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09792v1 Announce Type: new \nAbstract: Although gossip and random walk-based learning algorithms are widely known for decentralized learning, there has been limited theoretical and experimental analysis to understand their relative performance for different graph topologies and data heterogeneity. We first design and analyze a random walk-based learning algorithm with multiple streams (walks), which we name asynchronous \"Multi-Walk (MW)\". We provide a convergence analysis for MW w.r.t iteration (computation), wall-clock time, and communication. We also present a convergence analysis for \"Asynchronous Gossip\", noting the lack of a comprehensive analysis of its convergence, along with the computation and communication overhead, in the literature. Our results show that MW has better convergence in terms of iterations as compared to Asynchronous Gossip in graphs with large diameters (e.g., cycles), while its relative performance, as compared to Asynchronous Gossip, depends on the number of walks and the data heterogeneity in graphs with small diameters (e.g., complete graphs). In wall-clock time analysis, we observe a linear speed-up with the number of walks and nodes in MW and Asynchronous Gossip, respectively. Finally, we show that MW outperforms Asynchronous Gossip in communication overhead, except in small-diameter topologies with extreme data heterogeneity. These results highlight the effectiveness of each algorithm in different graph topologies and data heterogeneity. Our codes are available for reproducibility."
      },
      {
        "id": "oai:arXiv.org:2504.09795v1",
        "title": "VDocRAG: Retrieval-Augmented Generation over Visually-Rich Documents",
        "link": "https://arxiv.org/abs/2504.09795",
        "author": "Ryota Tanaka, Taichi Iki, Taku Hasegawa, Kyosuke Nishida, Kuniko Saito, Jun Suzuki",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09795v1 Announce Type: new \nAbstract: We aim to develop a retrieval-augmented generation (RAG) framework that answers questions over a corpus of visually-rich documents presented in mixed modalities (e.g., charts, tables) and diverse formats (e.g., PDF, PPTX). In this paper, we introduce a new RAG framework, VDocRAG, which can directly understand varied documents and modalities in a unified image format to prevent missing information that occurs by parsing documents to obtain text. To improve the performance, we propose novel self-supervised pre-training tasks that adapt large vision-language models for retrieval by compressing visual information into dense token representations while aligning them with textual content in documents. Furthermore, we introduce OpenDocVQA, the first unified collection of open-domain document visual question answering datasets, encompassing diverse document types and formats. OpenDocVQA provides a comprehensive resource for training and evaluating retrieval and question answering models on visually-rich documents in an open-domain setting. Experiments show that VDocRAG substantially outperforms conventional text-based RAG and has strong generalization capability, highlighting the potential of an effective RAG paradigm for real-world documents."
      },
      {
        "id": "oai:arXiv.org:2504.09797v1",
        "title": "IGL-DT: Iterative Global-Local Feature Learning with Dual-Teacher Semantic Segmentation Framework under Limited Annotation Scheme",
        "link": "https://arxiv.org/abs/2504.09797",
        "author": "Dinh Dai Quan Tran, Hoang-Thien Nguyen. Thanh-Huy Nguyen, Gia-Van To, Tien-Huy Nguyen, Quan Nguyen",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09797v1 Announce Type: new \nAbstract: Semi-Supervised Semantic Segmentation (SSSS) aims to improve segmentation accuracy by leveraging a small set of labeled images alongside a larger pool of unlabeled data. Recent advances primarily focus on pseudo-labeling, consistency regularization, and co-training strategies. However, existing methods struggle to balance global semantic representation with fine-grained local feature extraction. To address this challenge, we propose a novel tri-branch semi-supervised segmentation framework incorporating a dual-teacher strategy, named IGL-DT. Our approach employs SwinUnet for high-level semantic guidance through Global Context Learning and ResUnet for detailed feature refinement via Local Regional Learning. Additionally, a Discrepancy Learning mechanism mitigates over-reliance on a single teacher, promoting adaptive feature learning. Extensive experiments on benchmark datasets demonstrate that our method outperforms state-of-the-art approaches, achieving superior segmentation performance across various data regimes."
      },
      {
        "id": "oai:arXiv.org:2504.09800v1",
        "title": "Multi-task Federated Learning with Encoder-Decoder Structure: Enabling Collaborative Learning Across Different Tasks",
        "link": "https://arxiv.org/abs/2504.09800",
        "author": "Jingxuan Zhou, Weidong Bao, Ji Wang, Dayu Zhang, Xiongtao Zhang, Yaohong Zhang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09800v1 Announce Type: new \nAbstract: Federated learning has been extensively studied and applied due to its ability to ensure data security in distributed environments while building better models. However, clients participating in federated learning still face limitations, as clients with different structures or tasks cannot participate in learning together. In view of this, constructing a federated learning framework that allows collaboration between clients with different model structures and performing different tasks, enabling them to share valuable knowledge to enhance model efficiency, holds significant practical implications for the widespread application of federated learning. To achieve this goal, we propose a multi-task federated learning with encoder-decoder structure (M-Fed). Specifically, given the widespread adoption of the encoder-decoder architecture in current models, we leverage this structure to share intra-task knowledge through traditional federated learning methods and extract general knowledge from the encoder to achieve cross-task knowledge sharing. The training process is similar to traditional federated learning, and we incorporate local decoder and global decoder information into the loss function. The local decoder iteratively updates and gradually approaches the global decoder until sufficient cross-task knowledge sharing is achieved. Our method is lightweight and modular, demonstrating innovation compared to previous research. It enables clients performing different tasks to share general knowledge while maintaining the efficiency of traditional federated learning systems. We conducted experiments on two widely used benchmark datasets to verify the feasibility of M-Fed and compared it with traditional methods. The experimental results demonstrate the effectiveness of M-Fed in multi-task federated learning."
      },
      {
        "id": "oai:arXiv.org:2504.09802v1",
        "title": "Training Small Reasoning LLMs with Cognitive Preference Alignment",
        "link": "https://arxiv.org/abs/2504.09802",
        "author": "Wenrui Cai, Chengyu Wang, Junbing Yan, Jun Huang, Xiangzhong Fang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09802v1 Announce Type: new \nAbstract: The reasoning capabilities of large language models (LLMs), such as OpenAI's o1 and DeepSeek-R1, have seen substantial advancements through deep thinking. However, these enhancements come with significant resource demands, underscoring the need to explore strategies to train effective reasoning LLMs with far fewer parameters. A critical challenge is that smaller models have different capacities and cognitive trajectories than their larger counterparts. Hence, direct distillation of chain-of-thought (CoT) results from large LLMs to smaller ones can be sometimes ineffective and requires a huge amount of annotated data. In this paper, we introduce a novel framework called Critique-Rethink-Verify (CRV), designed for training smaller yet powerful reasoning LLMs. Our CRV framework consists of multiple LLM agents, each specializing in unique abilities: (i) critiquing the CoTs according to the cognitive capabilities of smaller models, (ii) rethinking and refining these CoTs based on the critiques, and (iii) verifying the correctness of the refined results. We further propose the cognitive preference optimization (CogPO) algorithm to enhance the reasoning abilities of smaller models by aligning thoughts of these models with their cognitive capacities. Comprehensive evaluations on challenging reasoning benchmarks demonstrate the efficacy of CRV and CogPO, which outperforms other training methods by a large margin."
      },
      {
        "id": "oai:arXiv.org:2504.09803v1",
        "title": "CUT: Pruning Pre-Trained Multi-Task Models into Compact Models for Edge Devices",
        "link": "https://arxiv.org/abs/2504.09803",
        "author": "Jingxuan Zhou, Weidong Bao, Ji Wang, Zhengyi Zhong",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09803v1 Announce Type: new \nAbstract: Multi-task learning has garnered widespread attention in the industry due to its efficient data utilization and strong generalization capabilities, making it particularly suitable for providing high-quality intelligent services to users. Edge devices, as the primary platforms directly serving users, play a crucial role in delivering multi-task services. However, current multi-task models are often large, and user task demands are increasingly diverse. Deploying such models directly on edge devices not only increases the burden on these devices but also leads to task redundancy. To address this issue, this paper innovatively proposes a pre-trained multi-task model pruning method specifically designed for edge computing. The goal is to utilize existing pre-trained multi-task models to construct a compact multi-task model that meets the needs of edge devices. The specific implementation steps are as follows: First, decompose the tasks within the pre-trained multi-task model and select tasks based on actual user needs. Next, while retaining the knowledge of the original pre-trained model, evaluate parameter importance and use a parameter fusion method to effectively integrate shared parameters among tasks. Finally, obtain a compact multi-task model suitable for edge devices. To validate the effectiveness of the proposed method, we conducted experiments on three public image datasets. The experimental results fully demonstrate the superiority and efficiency of this method, providing a new solution for multi-task learning on edge devices."
      },
      {
        "id": "oai:arXiv.org:2504.09812v1",
        "title": "Efficient Multi-Task Modeling through Automated Fusion of Trained Models",
        "link": "https://arxiv.org/abs/2504.09812",
        "author": "Jingxuan Zhou, Weidong Bao, Ji Wang, Zhengyi Zhong, Dayu Zhang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09812v1 Announce Type: new \nAbstract: Although multi-task learning is widely applied in intelligent services, traditional multi-task modeling methods often require customized designs based on specific task combinations, resulting in a cumbersome modeling process. Inspired by the rapid development and excellent performance of single-task models, this paper proposes an efficient multi-task modeling method that can automatically fuse trained single-task models with different structures and tasks to form a multi-task model. As a general framework, this method allows modelers to simply prepare trained models for the required tasks, simplifying the modeling process while fully utilizing the knowledge contained in the trained models. This eliminates the need for excessive focus on task relationships and model structure design. To achieve this goal, we consider the structural differences among various trained models and employ model decomposition techniques to hierarchically decompose them into multiple operable model components. Furthermore, we have designed an Adaptive Knowledge Fusion (AKF) module based on Transformer, which adaptively integrates intra-task and inter-task knowledge based on model components. Through the proposed method, we achieve efficient and automated construction of multi-task models, and its effectiveness is verified through extensive experiments on three datasets."
      },
      {
        "id": "oai:arXiv.org:2504.09814v1",
        "title": "DUDA: Distilled Unsupervised Domain Adaptation for Lightweight Semantic Segmentation",
        "link": "https://arxiv.org/abs/2504.09814",
        "author": "Beomseok Kang, Niluthpol Chowdhury Mithun, Abhinav Rajvanshi, Han-Pang Chiu, Supun Samarasekera",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09814v1 Announce Type: new \nAbstract: Unsupervised Domain Adaptation (UDA) is essential for enabling semantic segmentation in new domains without requiring costly pixel-wise annotations. State-of-the-art (SOTA) UDA methods primarily use self-training with architecturally identical teacher and student networks, relying on Exponential Moving Average (EMA) updates. However, these approaches face substantial performance degradation with lightweight models due to inherent architectural inflexibility leading to low-quality pseudo-labels. To address this, we propose Distilled Unsupervised Domain Adaptation (DUDA), a novel framework that combines EMA-based self-training with knowledge distillation (KD). Our method employs an auxiliary student network to bridge the architectural gap between heavyweight and lightweight models for EMA-based updates, resulting in improved pseudo-label quality. DUDA employs a strategic fusion of UDA and KD, incorporating innovative elements such as gradual distillation from large to small networks, inconsistency loss prioritizing poorly adapted classes, and learning with multiple teachers. Extensive experiments across four UDA benchmarks demonstrate DUDA's superiority in achieving SOTA performance with lightweight models, often surpassing the performance of heavyweight models from other approaches."
      },
      {
        "id": "oai:arXiv.org:2504.09818v1",
        "title": "Transferable text data distillation by trajectory matching",
        "link": "https://arxiv.org/abs/2504.09818",
        "author": "Rong Yao, Hailin Hu, Yifei Fu, Hanting Chen, Wenyi Fang, Fanyi Du, Kai Han, Yunhe Wang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09818v1 Announce Type: new \nAbstract: In the realm of large language model (LLM), as the size of large models increases, it also brings higher training costs. There is a urgent need to minimize the data size in LLM training. Compared with data selection method, the data distillation method aims to synthesize a small number of data samples to achieve the training effect of the full data set and has better flexibility. Despite its successes in computer vision, the discreteness of text data has hitherto stymied its exploration in natural language processing (NLP). In this work, we proposed a method that involves learning pseudo prompt data based on trajectory matching and finding its nearest neighbor ID to achieve cross-architecture transfer. During the distillation process, we introduce a regularization loss to improve the robustness of our distilled data. To our best knowledge, this is the first data distillation work suitable for text generation tasks such as instruction tuning. Evaluations on two benchmarks, including ARC-Easy and MMLU instruction tuning datasets, established the superiority of our distillation approach over the SOTA data selection method LESS. Furthermore, our method demonstrates a good transferability over LLM structures (i.e., OPT to Llama)."
      },
      {
        "id": "oai:arXiv.org:2504.09819v1",
        "title": "Density-based Object Detection in Crowded Scenes",
        "link": "https://arxiv.org/abs/2504.09819",
        "author": "Chenyang Zhao, Jia Wan, Antoni B. Chan",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09819v1 Announce Type: new \nAbstract: Compared with the generic scenes, crowded scenes contain highly-overlapped instances, which result in: 1) more ambiguous anchors during training of object detectors, and 2) more predictions are likely to be mistakenly suppressed in post-processing during inference. To address these problems, we propose two new strategies, density-guided anchors (DGA) and density-guided NMS (DG-NMS), which uses object density maps to jointly compute optimal anchor assignments and reweighing, as well as an adaptive NMS. Concretely, based on an unbalanced optimal transport (UOT) problem, the density owned by each ground-truth object is transported to each anchor position at a minimal transport cost. And density on anchors comprises an instance-specific density distribution, from which DGA decodes the optimal anchor assignment and re-weighting strategy. Meanwhile, DG-NMS utilizes the predicted density map to adaptively adjust the NMS threshold to reduce mistaken suppressions. In the UOT, a novel overlap-aware transport cost is specifically designed for ambiguous anchors caused by overlapped neighboring objects. Extensive experiments on the challenging CrowdHuman dataset with Citypersons dataset demonstrate that our proposed density-guided detector is effective and robust to crowdedness. The code and pre-trained models will be made available later."
      },
      {
        "id": "oai:arXiv.org:2504.09824v1",
        "title": "Abacus-SQL: A Text-to-SQL System Empowering Cross-Domain and Open-Domain Database Retrieval",
        "link": "https://arxiv.org/abs/2504.09824",
        "author": "Keyan Xu, Dingzirui Wang, Xuanliang Zhang, Qingfu Zhu, Wanxiang Che",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09824v1 Announce Type: new \nAbstract: The existing text-to-SQL systems have made significant progress in SQL query generation, but they still face numerous challenges. Existing systems often lack retrieval capabilities for open-domain databases, requiring users to manually filter relevant databases. Additionally, their cross-domain transferability is limited, making it challenging to accommodate diverse query requirements. To address these issues, we propose Abacus-SQL. Abacus-SQL utilizes database retrieval technology to accurately locate the required databases in an open-domain database environment. It also enhances the system cross-domain transfer ability through data augmentation methods. Moreover, Abacus-SQL employs Pre-SQL and Self-debug methods, thereby enhancing the accuracy of SQL queries. Experimental results demonstrate that Abacus-SQL performs excellently in multi-turn text-to-SQL tasks, effectively validating the approach's effectiveness. Abacus-SQL is publicly accessible at https://huozi.8wss.com/abacus-sql/."
      },
      {
        "id": "oai:arXiv.org:2504.09828v1",
        "title": "FATE: A Prompt-Tuning-Based Semi-Supervised Learning Framework for Extremely Limited Labeled Data",
        "link": "https://arxiv.org/abs/2504.09828",
        "author": "Hezhao Liu, Yang Lu, Mengke Li, Yiqun Zhang, Shreyank N Gowda, Chen Gong, Hanzi Wang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09828v1 Announce Type: new \nAbstract: Semi-supervised learning (SSL) has achieved significant progress by leveraging both labeled data and unlabeled data. Existing SSL methods overlook a common real-world scenario when labeled data is extremely scarce, potentially as limited as a single labeled sample in the dataset. General SSL approaches struggle to train effectively from scratch under such constraints, while methods utilizing pre-trained models often fail to find an optimal balance between leveraging limited labeled data and abundant unlabeled data. To address this challenge, we propose Firstly Adapt, Then catEgorize (FATE), a novel SSL framework tailored for scenarios with extremely limited labeled data. At its core, the two-stage prompt tuning paradigm FATE exploits unlabeled data to compensate for scarce supervision signals, then transfers to downstream tasks. Concretely, FATE first adapts a pre-trained model to the feature distribution of downstream data using volumes of unlabeled samples in an unsupervised manner. It then applies an SSL method specifically designed for pre-trained models to complete the final classification task. FATE is designed to be compatible with both vision and vision-language pre-trained models. Extensive experiments demonstrate that FATE effectively mitigates challenges arising from the scarcity of labeled samples in SSL, achieving an average performance improvement of 33.74% across seven benchmarks compared to state-of-the-art SSL methods. Code is available at https://anonymous.4open.science/r/Semi-supervised-learning-BA72."
      },
      {
        "id": "oai:arXiv.org:2504.09843v1",
        "title": "ST-Booster: An Iterative SpatioTemporal Perception Booster for Vision-and-Language Navigation in Continuous Environments",
        "link": "https://arxiv.org/abs/2504.09843",
        "author": "Lu Yue, Dongliang Zhou, Liang Xie, Erwei Yin, Feitian Zhang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09843v1 Announce Type: new \nAbstract: Vision-and-Language Navigation in Continuous Environments (VLN-CE) requires agents to navigate unknown, continuous spaces based on natural language instructions. Compared to discrete settings, VLN-CE poses two core perception challenges. First, the absence of predefined observation points leads to heterogeneous visual memories and weakened global spatial correlations. Second, cumulative reconstruction errors in three-dimensional scenes introduce structural noise, impairing local feature perception. To address these challenges, this paper proposes ST-Booster, an iterative spatiotemporal booster that enhances navigation performance through multi-granularity perception and instruction-aware reasoning. ST-Booster consists of three key modules -- Hierarchical SpatioTemporal Encoding (HSTE), Multi-Granularity Aligned Fusion (MGAF), and ValueGuided Waypoint Generation (VGWG). HSTE encodes long-term global memory using topological graphs and captures shortterm local details via grid maps. MGAF aligns these dualmap representations with instructions through geometry-aware knowledge fusion. The resulting representations are iteratively refined through pretraining tasks. During reasoning, VGWG generates Guided Attention Heatmaps (GAHs) to explicitly model environment-instruction relevance and optimize waypoint selection. Extensive comparative experiments and performance analyses are conducted, demonstrating that ST-Booster outperforms existing state-of-the-art methods, particularly in complex, disturbance-prone environments."
      },
      {
        "id": "oai:arXiv.org:2504.09846v1",
        "title": "GlyTwin: Digital Twin for Glucose Control in Type 1 Diabetes Through Optimal Behavioral Modifications Using Patient-Centric Counterfactuals",
        "link": "https://arxiv.org/abs/2504.09846",
        "author": "Asiful Arefeen, Saman Khamesian, Maria Adela Grando, Bithika Thompson, Hassan Ghasemzadeh",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09846v1 Announce Type: new \nAbstract: Frequent and long-term exposure to hyperglycemia (i.e., high blood glucose) increases the risk of chronic complications such as neuropathy, nephropathy, and cardiovascular disease. Current technologies like continuous subcutaneous insulin infusion (CSII) and continuous glucose monitoring (CGM) primarily model specific aspects of glycemic control-like hypoglycemia prediction or insulin delivery. Similarly, most digital twin approaches in diabetes management simulate only physiological processes. These systems lack the ability to offer alternative treatment scenarios that support proactive behavioral interventions. To address this, we propose GlyTwin, a novel digital twin framework that uses counterfactual explanations to simulate optimal treatments for glucose regulation. Our approach helps patients and caregivers modify behaviors like carbohydrate intake and insulin dosing to avoid abnormal glucose events. GlyTwin generates behavioral treatment suggestions that proactively prevent hyperglycemia by recommending small adjustments to daily choices, reducing both frequency and duration of these events. Additionally, it incorporates stakeholder preferences into the intervention design, making recommendations patient-centric and tailored. We evaluate GlyTwin on AZT1D, a newly constructed dataset with longitudinal data from 21 type 1 diabetes (T1D) patients on automated insulin delivery systems over 26 days. Results show GlyTwin outperforms state-of-the-art counterfactual methods, generating 76.6% valid and 86% effective interventions. These findings demonstrate the promise of counterfactual-driven digital twins in delivering personalized healthcare."
      },
      {
        "id": "oai:arXiv.org:2504.09850v1",
        "title": "Accelerating Differentially Private Federated Learning via Adaptive Extrapolation",
        "link": "https://arxiv.org/abs/2504.09850",
        "author": "Shokichi Takakura, Seng Pei Liew, Satoshi Hasegawa",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09850v1 Announce Type: new \nAbstract: The federated learning (FL) framework enables multiple clients to collaboratively train machine learning models without sharing their raw data, but it remains vulnerable to privacy attacks. One promising approach is to incorporate differential privacy (DP)-a formal notion of privacy-into the FL framework. DP-FedAvg is one of the most popular algorithms for DP-FL, but it is known to suffer from the slow convergence in the presence of heterogeneity among clients' data. Most of the existing methods to accelerate DP-FL require 1) additional hyperparameters or 2) additional computational cost for clients, which is not desirable since 1) hyperparameter tuning is computationally expensive and data-dependent choice of hyperparameters raises the risk of privacy leakage, and 2) clients are often resource-constrained. To address this issue, we propose DP-FedEXP, which adaptively selects the global step size based on the diversity of the local updates without requiring any additional hyperparameters or client computational cost. We show that DP-FedEXP provably accelerates the convergence of DP-FedAvg and it empirically outperforms existing methods tailored for DP-FL."
      },
      {
        "id": "oai:arXiv.org:2504.09852v1",
        "title": "GFT: Gradient Focal Transformer",
        "link": "https://arxiv.org/abs/2504.09852",
        "author": "Boris Kriuk, Simranjit Kaur Gill, Shoaib Aslam, Amir Fakhrutdinov",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09852v1 Announce Type: new \nAbstract: Fine-Grained Image Classification (FGIC) remains a complex task in computer vision, as it requires models to distinguish between categories with subtle localized visual differences. Well-studied CNN-based models, while strong in local feature extraction, often fail to capture the global context required for fine-grained recognition, while more recent ViT-backboned models address FGIC with attention-driven mechanisms but lack the ability to adaptively focus on truly discriminative regions. TransFG and other ViT-based extensions introduced part-aware token selection to enhance attention localization, yet they still struggle with computational efficiency, attention region selection flexibility, and detail-focus narrative in complex environments. This paper introduces GFT (Gradient Focal Transformer), a new ViT-derived framework created for FGIC tasks. GFT integrates the Gradient Attention Learning Alignment (GALA) mechanism to dynamically prioritize class-discriminative features by analyzing attention gradient flow. Coupled with a Progressive Patch Selection (PPS) strategy, the model progressively filters out less informative regions, reducing computational overhead while enhancing sensitivity to fine details. GFT achieves SOTA accuracy on FGVC Aircraft, Food-101, and COCO datasets with 93M parameters, outperforming ViT-based advanced FGIC models in efficiency. By bridging global context and localized detail extraction, GFT sets a new benchmark in fine-grained recognition, offering interpretable solutions for real-world deployment scenarios."
      },
      {
        "id": "oai:arXiv.org:2504.09862v1",
        "title": "RadarLLM: Empowering Large Language Models to Understand Human Motion from Millimeter-wave Point Cloud Sequence",
        "link": "https://arxiv.org/abs/2504.09862",
        "author": "Zengyuan Lai, Jiarui Yang, Songpengcheng Xia, Lizhou Lin, Lan Sun, Renwen Wang, Jianran Liu, Qi Wu, Ling Pei",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09862v1 Announce Type: new \nAbstract: Millimeter-wave radar provides a privacy-preserving solution for human motion analysis, yet its sparse point clouds pose significant challenges for semantic understanding. We present Radar-LLM, the first framework that leverages large language models (LLMs) for human motion understanding using millimeter-wave radar as the sensing modality. Our approach introduces two key innovations: (1) a motion-guided radar tokenizer based on our Aggregate VQ-VAE architecture that incorporates deformable body templates and masked trajectory modeling to encode spatiotemporal point clouds into compact semantic tokens, and (2) a radar-aware language model that establishes cross-modal alignment between radar and text in a shared embedding space. To address data scarcity, we introduce a physics-aware synthesis pipeline that generates realistic radar-text pairs from motion-text datasets. Extensive experiments demonstrate that Radar-LLM achieves state-of-the-art performance across both synthetic and real-world benchmarks, enabling accurate translation of millimeter-wave signals to natural language descriptions. This breakthrough facilitates comprehensive motion understanding in privacy-sensitive applications like healthcare and smart homes. We will release the full implementation to support further research on https://inowlzy.github.io/RadarLLM/."
      },
      {
        "id": "oai:arXiv.org:2504.09866v1",
        "title": "PASS-FC: Progressive and Adaptive Search Scheme for Fact Checking of Comprehensive Claims",
        "link": "https://arxiv.org/abs/2504.09866",
        "author": "Ziyu Zhuang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09866v1 Announce Type: new \nAbstract: Automated fact-checking faces challenges in handling complex real-world claims. We present PASS-FC, a novel framework that addresses these issues through claim augmentation, adaptive question generation, and iterative verification. PASS-FC enhances atomic claims with temporal and entity context, employs advanced search techniques, and utilizes a reflection mechanism. We evaluate PASS-FC on six diverse datasets, demonstrating superior performance across general knowledge, scientific, real-world, and multilingual fact-checking tasks. Our framework often surpasses stronger baseline models. Hyperparameter analysis reveals optimal settings for evidence quantity and reflection label triggers, while ablation studies highlight the importance of claim augmentation and language-specific adaptations. PASS-FC's performance underscores its effectiveness in improving fact-checking accuracy and adaptability across various domains. We will open-source our code and experimental results to facilitate further research in this area."
      },
      {
        "id": "oai:arXiv.org:2504.09873v1",
        "title": "Truncated Matrix Completion - An Empirical Study",
        "link": "https://arxiv.org/abs/2504.09873",
        "author": "Rishhabh Naik, Nisarg Trivedi, Davoud Ataee Tarzanagh, Laura Balzano",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09873v1 Announce Type: new \nAbstract: Low-rank Matrix Completion (LRMC) describes the problem where we wish to recover missing entries of partially observed low-rank matrix. Most existing matrix completion work deals with sampling procedures that are independent of the underlying data values. While this assumption allows the derivation of nice theoretical guarantees, it seldom holds in real-world applications. In this paper, we consider various settings where the sampling mask is dependent on the underlying data values, motivated by applications in sensing, sequential decision-making, and recommender systems. Through a series of experiments, we study and compare the performance of various LRMC algorithms that were originally successful for data-independent sampling patterns."
      },
      {
        "id": "oai:arXiv.org:2504.09876v1",
        "title": "HDC: Hierarchical Distillation for Multi-level Noisy Consistency in Semi-Supervised Fetal Ultrasound Segmentation",
        "link": "https://arxiv.org/abs/2504.09876",
        "author": "Tran Quoc Khanh Le, Nguyen Lan Vi Vu, Ha-Hieu Pham, Xuan-Loc Huynh, Tien-Huy Nguyen, Minh Huu Nhat Le, Quan Nguyen, Hien D. Nguyen",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09876v1 Announce Type: new \nAbstract: Transvaginal ultrasound is a critical imaging modality for evaluating cervical anatomy and detecting physiological changes. However, accurate segmentation of cervical structures remains challenging due to low contrast, shadow artifacts, and fuzzy boundaries. While convolutional neural networks (CNNs) have shown promising results in medical image segmentation, their performance is often limited by the need for large-scale annotated datasets - an impractical requirement in clinical ultrasound imaging. Semi-supervised learning (SSL) offers a compelling solution by leveraging unlabeled data, but existing teacher-student frameworks often suffer from confirmation bias and high computational costs. We propose HDC, a novel semi-supervised segmentation framework that integrates Hierarchical Distillation and Consistency learning within a multi-level noise mean-teacher framework. Unlike conventional approaches that rely solely on pseudo-labeling, we introduce a hierarchical distillation mechanism that guides feature-level learning via two novel objectives: (1) Correlation Guidance Loss to align feature representations between the teacher and main student branch, and (2) Mutual Information Loss to stabilize representations between the main and noisy student branches. Our framework reduces model complexity while improving generalization. Extensive experiments on two fetal ultrasound datasets, FUGC and PSFH, demonstrate that our method achieves competitive performance with significantly lower computational overhead than existing multi-teacher models."
      },
      {
        "id": "oai:arXiv.org:2504.09878v1",
        "title": "MCBlock: Boosting Neural Radiance Field Training Speed by MCTS-based Dynamic-Resolution Ray Sampling",
        "link": "https://arxiv.org/abs/2504.09878",
        "author": "Yunpeng Tan, Junlin Hao, Jiangkai Wu, Liming Liu, Qingyang Li, Xinggong Zhang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09878v1 Announce Type: new \nAbstract: Neural Radiance Field (NeRF) is widely known for high-fidelity novel view synthesis. However, even the state-of-the-art NeRF model, Gaussian Splatting, requires minutes for training, far from the real-time performance required by multimedia scenarios like telemedicine. One of the obstacles is its inefficient sampling, which is only partially addressed by existing works. Existing point-sampling algorithms uniformly sample simple-texture regions (easy to fit) and complex-texture regions (hard to fit), while existing ray-sampling algorithms sample these regions all in the finest granularity (i.e. the pixel level), both wasting GPU training resources. Actually, regions with different texture intensities require different sampling granularities. To this end, we propose a novel dynamic-resolution ray-sampling algorithm, MCBlock, which employs Monte Carlo Tree Search (MCTS) to partition each training image into pixel blocks with different sizes for active block-wise training. Specifically, the trees are initialized according to the texture of training images to boost the initialization speed, and an expansion/pruning module dynamically optimizes the block partition. MCBlock is implemented in Nerfstudio, an open-source toolset, and achieves a training acceleration of up to 2.33x, surpassing other ray-sampling algorithms. We believe MCBlock can apply to any cone-tracing NeRF model and contribute to the multimedia community."
      },
      {
        "id": "oai:arXiv.org:2504.09881v1",
        "title": "Focus on Local: Finding Reliable Discriminative Regions for Visual Place Recognition",
        "link": "https://arxiv.org/abs/2504.09881",
        "author": "Changwei Wang, Shunpeng Chen, Yukun Song, Rongtao Xu, Zherui Zhang, Jiguang Zhang, Haoran Yang, Yu Zhang, Kexue Fu, Shide Du, Zhiwei Xu, Longxiang Gao, Li Guo, Shibiao Xu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09881v1 Announce Type: new \nAbstract: Visual Place Recognition (VPR) is aimed at predicting the location of a query image by referencing a database of geotagged images. For VPR task, often fewer discriminative local regions in an image produce important effects while mundane background regions do not contribute or even cause perceptual aliasing because of easy overlap. However, existing methods lack precisely modeling and full exploitation of these discriminative regions. In this paper, we propose the Focus on Local (FoL) approach to stimulate the performance of image retrieval and re-ranking in VPR simultaneously by mining and exploiting reliable discriminative local regions in images and introducing pseudo-correlation supervision. First, we design two losses, Extraction-Aggregation Spatial Alignment Loss (SAL) and Foreground-Background Contrast Enhancement Loss (CEL), to explicitly model reliable discriminative local regions and use them to guide the generation of global representations and efficient re-ranking. Second, we introduce a weakly-supervised local feature training strategy based on pseudo-correspondences obtained from aggregating global features to alleviate the lack of local correspondences ground truth for the VPR task. Third, we suggest an efficient re-ranking pipeline that is efficiently and precisely based on discriminative region guidance. Finally, experimental results show that our FoL achieves the state-of-the-art on multiple VPR benchmarks in both image retrieval and re-ranking stages and also significantly outperforms existing two-stage VPR methods in terms of computational efficiency. Code and models are available at https://github.com/chenshunpeng/FoL"
      },
      {
        "id": "oai:arXiv.org:2504.09886v1",
        "title": "Investigating Syntactic Biases in Multilingual Transformers with RC Attachment Ambiguities in Italian and English",
        "link": "https://arxiv.org/abs/2504.09886",
        "author": "Michael Kamerath, Aniello De Santo",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09886v1 Announce Type: new \nAbstract: This paper leverages past sentence processing studies to investigate whether monolingual and multilingual LLMs show human-like preferences when presented with examples of relative clause attachment ambiguities in Italian and English. Furthermore, we test whether these preferences can be modulated by lexical factors (the type of verb/noun in the matrix clause) which have been shown to be tied to subtle constraints on syntactic and semantic relations. Our results overall showcase how LLM behavior varies interestingly across models, but also general failings of these models in correctly capturing human-like preferences. In light of these results, we argue that RC attachment is the ideal benchmark for cross-linguistic investigations of LLMs' linguistic knowledge and biases."
      },
      {
        "id": "oai:arXiv.org:2504.09887v1",
        "title": "Enhanced Semantic Extraction and Guidance for UGC Image Super Resolution",
        "link": "https://arxiv.org/abs/2504.09887",
        "author": "Yiwen Wang, Ying Liang, Yuxuan Zhang, Xinning Chai, Zhengxue Cheng, Yingsheng Qin, Yucai Yang, Rong Xie, Li Song",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09887v1 Announce Type: new \nAbstract: Due to the disparity between real-world degradations in user-generated content(UGC) images and synthetic degradations, traditional super-resolution methods struggle to generalize effectively, necessitating a more robust approach to model real-world distortions. In this paper, we propose a novel approach to UGC image super-resolution by integrating semantic guidance into a diffusion framework. Our method addresses the inconsistency between degradations in wild and synthetic datasets by separately simulating the degradation processes on the LSDIR dataset and combining them with the official paired training set. Furthermore, we enhance degradation removal and detail generation by incorporating a pretrained semantic extraction model (SAM2) and fine-tuning key hyperparameters for improved perceptual fidelity. Extensive experiments demonstrate the superiority of our approach against state-of-the-art methods. Additionally, the proposed model won second place in the CVPR NTIRE 2025 Short-form UGC Image Super-Resolution Challenge, further validating its effectiveness. The code is available at https://github.c10pom/Moonsofang/NTIRE-2025-SRlab."
      },
      {
        "id": "oai:arXiv.org:2504.09895v1",
        "title": "Learning from Reference Answers: Versatile Language Model Alignment without Binary Human Preference Data",
        "link": "https://arxiv.org/abs/2504.09895",
        "author": "Shuai Zhao, Linchao Zhu, Yi Yang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09895v1 Announce Type: new \nAbstract: Large language models~(LLMs) are expected to be helpful, harmless, and honest. In various alignment scenarios, such as general human preference, safety, and confidence alignment, binary preference data collection and reward modeling are resource-intensive but necessary for human preference transferring. In this work, we explore using the similarity between sampled generations and high-quality reference answers as an alternative reward function for LLM alignment. Using similarity as a reward circumvents training reward models, and collecting a single reference answer potentially costs less time than constructing binary preference pairs when multiple candidates are available. Specifically, we develop \\textit{RefAlign}, a versatile REINFORCE-style alignment algorithm, which is free of reference and reward models. Instead, RefAlign utilizes BERTScore between sampled generations and high-quality reference answers as the surrogate reward. Beyond general human preference optimization, RefAlign can be readily extended to diverse scenarios, such as safety and confidence alignment, by incorporating the similarity reward with task-related objectives. In various scenarios, {RefAlign} demonstrates comparable performance to previous alignment methods while offering high efficiency."
      },
      {
        "id": "oai:arXiv.org:2504.09896v1",
        "title": "TWSSenti: A Novel Hybrid Framework for Topic-Wise Sentiment Analysis on Social Media Using Transformer Models",
        "link": "https://arxiv.org/abs/2504.09896",
        "author": "Aish Albladi, Md Kaosar Uddin, Minarul Islam, Cheryl Seals",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09896v1 Announce Type: new \nAbstract: Sentiment analysis is a crucial task in natural language processing (NLP) that enables the extraction of meaningful insights from textual data, particularly from dynamic platforms like Twitter and IMDB. This study explores a hybrid framework combining transformer-based models, specifically BERT, GPT-2, RoBERTa, XLNet, and DistilBERT, to improve sentiment classification accuracy and robustness. The framework addresses challenges such as noisy data, contextual ambiguity, and generalization across diverse datasets by leveraging the unique strengths of these models. BERT captures bidirectional context, GPT-2 enhances generative capabilities, RoBERTa optimizes contextual understanding with larger corpora and dynamic masking, XLNet models dependency through permutation-based learning, and DistilBERT offers efficiency with reduced computational overhead while maintaining high accuracy. We demonstrate text cleaning, tokenization, and feature extraction using Term Frequency Inverse Document Frequency (TF-IDF) and Bag of Words (BoW), ensure high-quality input data for the models. The hybrid approach was evaluated on benchmark datasets Sentiment140 and IMDB, achieving superior accuracy rates of 94\\% and 95\\%, respectively, outperforming standalone models. The results validate the effectiveness of combining multiple transformer models in ensemble-like setups to address the limitations of individual architectures. This research highlights its applicability to real-world tasks such as social media monitoring, customer sentiment analysis, and public opinion tracking which offers a pathway for future advancements in hybrid NLP frameworks."
      },
      {
        "id": "oai:arXiv.org:2504.09897v1",
        "title": "TAMP: Token-Adaptive Layerwise Pruning in Multimodal Large Language Models",
        "link": "https://arxiv.org/abs/2504.09897",
        "author": "Jaewoo Lee (May), Keyang Xuan (May), Chanakya Ekbote (May), Sandeep Polisetty (May), Yi R. (May),  Fung, Paul Pu Liang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09897v1 Announce Type: new \nAbstract: Multimodal Large Language Models (MLLMs) have shown remarkable versatility in understanding diverse multimodal data and tasks. However, these capabilities come with an increased model scale. While post-training pruning reduces model size in unimodal models, its application to MLLMs often yields limited success. Our analysis discovers that conventional methods fail to account for the unique token attributes across layers and modalities inherent to MLLMs. Inspired by this observation, we propose TAMP, a simple yet effective pruning framework tailored for MLLMs, featuring two key components: (1) Diversity-Aware Sparsity, which adjusts sparsity ratio per layer based on diversities among multimodal output tokens, preserving more parameters in high-diversity layers; and (2) Adaptive Multimodal Input Activation, which identifies representative multimodal input tokens using attention scores to guide unstructured weight pruning. We validate our method on two state-of-the-art MLLMs: LLaVA-NeXT, designed for vision-language tasks, and VideoLLaMA2, capable of processing audio, visual, and language modalities. Empirical experiments across various multimodal evaluation benchmarks demonstrate that each component of our approach substantially outperforms existing pruning techniques."
      },
      {
        "id": "oai:arXiv.org:2504.09899v1",
        "title": "Digital Staining with Knowledge Distillation: A Unified Framework for Unpaired and Paired-But-Misaligned Data",
        "link": "https://arxiv.org/abs/2504.09899",
        "author": "Ziwang Xu, Lanqing Guo, Satoshi Tsutsui, Shuyan Zhang, Alex C. Kot, Bihan Wen",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09899v1 Announce Type: new \nAbstract: Staining is essential in cell imaging and medical diagnostics but poses significant challenges, including high cost, time consumption, labor intensity, and irreversible tissue alterations. Recent advances in deep learning have enabled digital staining through supervised model training. However, collecting large-scale, perfectly aligned pairs of stained and unstained images remains difficult. In this work, we propose a novel unsupervised deep learning framework for digital cell staining that reduces the need for extensive paired data using knowledge distillation. We explore two training schemes: (1) unpaired and (2) paired-but-misaligned settings. For the unpaired case, we introduce a two-stage pipeline, comprising light enhancement followed by colorization, as a teacher model. Subsequently, we obtain a student staining generator through knowledge distillation with hybrid non-reference losses. To leverage the pixel-wise information between adjacent sections, we further extend to the paired-but-misaligned setting, adding the Learning to Align module to utilize pixel-level information. Experiment results on our dataset demonstrate that our proposed unsupervised deep staining method can generate stained images with more accurate positions and shapes of the cell targets in both settings. Compared with competing methods, our method achieves improved results both qualitatively and quantitatively (e.g., NIQE and PSNR).We applied our digital staining method to the White Blood Cell (WBC) dataset, investigating its potential for medical applications."
      },
      {
        "id": "oai:arXiv.org:2504.09900v1",
        "title": "Small Object Detection with YOLO: A Performance Analysis Across Model Versions and Hardware",
        "link": "https://arxiv.org/abs/2504.09900",
        "author": "Muhammad Fasih Tariq, Muhammad Azeem Javed",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09900v1 Announce Type: new \nAbstract: This paper provides an extensive evaluation of YOLO object detection models (v5, v8, v9, v10, v11) by com- paring their performance across various hardware platforms and optimization libraries. Our study investigates inference speed and detection accuracy on Intel and AMD CPUs using popular libraries such as ONNX and OpenVINO, as well as on GPUs through TensorRT and other GPU-optimized frameworks. Furthermore, we analyze the sensitivity of these YOLO models to object size within the image, examining performance when detecting objects that occupy 1%, 2.5%, and 5% of the total area of the image. By identifying the trade-offs in efficiency, accuracy, and object size adaptability, this paper offers insights for optimal model selection based on specific hardware constraints and detection requirements, aiding practitioners in deploying YOLO models effectively for real-world applications."
      },
      {
        "id": "oai:arXiv.org:2504.09903v1",
        "title": "Refining Financial Consumer Complaints through Multi-Scale Model Interaction",
        "link": "https://arxiv.org/abs/2504.09903",
        "author": "Bo-Wei Chen, An-Zi Yen, Chung-Chi Chen",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09903v1 Announce Type: new \nAbstract: Legal writing demands clarity, formality, and domain-specific precision-qualities often lacking in documents authored by individuals without legal training. To bridge this gap, this paper explores the task of legal text refinement that transforms informal, conversational inputs into persuasive legal arguments. We introduce FinDR, a Chinese dataset of financial dispute records, annotated with official judgments on claim reasonableness. Our proposed method, Multi-Scale Model Interaction (MSMI), leverages a lightweight classifier to evaluate outputs and guide iterative refinement by Large Language Models (LLMs). Experimental results demonstrate that MSMI significantly outperforms single-pass prompting strategies. Additionally, we validate the generalizability of MSMI on several short-text benchmarks, showing improved adversarial robustness. Our findings reveal the potential of multi-model collaboration for enhancing legal document generation and broader text refinement tasks."
      },
      {
        "id": "oai:arXiv.org:2504.09904v1",
        "title": "LiteTracker: Leveraging Temporal Causality for Accurate Low-latency Tissue Tracking",
        "link": "https://arxiv.org/abs/2504.09904",
        "author": "Mert Asim Karaoglu, Wenbo Ji, Ahmed Abbas, Nassir Navab, Benjamin Busam, Alexander Ladikos",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09904v1 Announce Type: new \nAbstract: Tissue tracking plays a critical role in various surgical navigation and extended reality (XR) applications. While current methods trained on large synthetic datasets achieve high tracking accuracy and generalize well to endoscopic scenes, their runtime performances fail to meet the low-latency requirements necessary for real-time surgical applications. To address this limitation, we propose LiteTracker, a low-latency method for tissue tracking in endoscopic video streams. LiteTracker builds on a state-of-the-art long-term point tracking method, and introduces a set of training-free runtime optimizations. These optimizations enable online, frame-by-frame tracking by leveraging a temporal memory buffer for efficient feature reuse and utilizing prior motion for accurate track initialization. LiteTracker demonstrates significant runtime improvements being around 7x faster than its predecessor and 2x than the state-of-the-art. Beyond its primary focus on efficiency, LiteTracker delivers high-accuracy tracking and occlusion prediction, performing competitively on both the STIR and SuPer datasets. We believe LiteTracker is an important step toward low-latency tissue tracking for real-time surgical applications in the operating room."
      },
      {
        "id": "oai:arXiv.org:2504.09906v1",
        "title": "Plasticity-Aware Mixture of Experts for Learning Under QoE Shifts in Adaptive Video Streaming",
        "link": "https://arxiv.org/abs/2504.09906",
        "author": "Zhiqiang He, Zhi Liu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09906v1 Announce Type: new \nAbstract: Adaptive video streaming systems are designed to optimize Quality of Experience (QoE) and, in turn, enhance user satisfaction. However, differences in user profiles and video content lead to different weights for QoE factors, resulting in user-specific QoE functions and, thus, varying optimization objectives. This variability poses significant challenges for neural networks, as they often struggle to generalize under evolving targets - a phenomenon known as plasticity loss that prevents conventional models from adapting effectively to changing optimization objectives. To address this limitation, we propose the Plasticity-Aware Mixture of Experts (PA-MoE), a novel learning framework that dynamically modulates network plasticity by balancing memory retention with selective forgetting. In particular, PA-MoE leverages noise injection to promote the selective forgetting of outdated knowledge, thereby endowing neural networks with enhanced adaptive capabilities. In addition, we present a rigorous theoretical analysis of PA-MoE by deriving a regret bound that quantifies its learning performance. Experimental evaluations demonstrate that PA-MoE achieves a 45.5% improvement in QoE over competitive baselines in dynamic streaming environments. Further analysis reveals that the model effectively mitigates plasticity loss by optimizing neuron utilization. Finally, a parameter sensitivity study is performed by injecting varying levels of noise, and the results align closely with our theoretical predictions."
      },
      {
        "id": "oai:arXiv.org:2504.09909v1",
        "title": "Quantum Natural Language Processing: A Comprehensive Review of Models, Methods, and Applications",
        "link": "https://arxiv.org/abs/2504.09909",
        "author": "Farha Nausheen, Khandakar Ahmed, M Imad Khan",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09909v1 Announce Type: new \nAbstract: In recent developments, deep learning methodologies applied to Natural Language Processing (NLP) have revealed a paradox: They improve performance but demand considerable data and resources for their training. Alternatively, quantum computing exploits the principles of quantum mechanics to overcome the computational limitations of current methodologies, thereby establishing an emerging field known as quantum natural language processing (QNLP). This domain holds the potential to attain a quantum advantage in the processing of linguistic structures, surpassing classical models in both efficiency and accuracy. In this paper, it is proposed to categorise QNLP models based on quantum computing principles, architecture, and computational approaches. This paper attempts to provide a survey on how quantum meets language by mapping state-of-the-art in this area, embracing quantum encoding techniques for classical data, QNLP models for prevalent NLP tasks, and quantum optimisation techniques for hyper parameter tuning. The landscape of quantum computing approaches applied to various NLP tasks is summarised by showcasing the specific QNLP methods used, and the popularity of these methods is indicated by their count. From the findings, it is observed that QNLP approaches are still limited to small data sets, with only a few models explored extensively, and there is increasing interest in the application of quantum computing to natural language processing tasks."
      },
      {
        "id": "oai:arXiv.org:2504.09910v1",
        "title": "Learning to Erase Private Knowledge from Multi-Documents for Retrieval-Augmented Large Language Models",
        "link": "https://arxiv.org/abs/2504.09910",
        "author": "Yujing Wang, Hainan Zhang, Liang Pang, Yongxin Tong, Binghui Guo, Hongwei Zheng, Zhiming Zheng",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09910v1 Announce Type: new \nAbstract: Retrieval-Augmented Generation (RAG) is a promising technique for applying LLMs to proprietary domains. However, retrieved documents may contain sensitive knowledge, posing risks of privacy leakage in generative results. Thus, effectively erasing private information from retrieved documents is a key challenge for RAG. Unlike traditional text anonymization, RAG should consider: (1) the inherent multi-document reasoning may face de-anonymization attacks; (2) private knowledge varies by scenarios, so users should be allowed to customize which information to erase; (3) preserving sufficient publicly available knowledge for generation tasks. This paper introduces the privacy erasure task for RAG and proposes Eraser4RAG, a private knowledge eraser which effectively removes user-defined private knowledge from documents while preserving sufficient public knowledge for generation. Specifically, we first construct a global knowledge graph to identify potential knowledge across documents, aiming to defend against de-anonymization attacks. Then we randomly split it into private and public sub-graphs, and fine-tune Flan-T5 to rewrite the retrieved documents excluding private triples. Finally, PPO algorithm optimizes the rewriting model to minimize private triples and maximize public triples retention. Experiments on four QA datasets demonstrate that Eraser4RAG achieves superior erase performance than GPT-4o."
      },
      {
        "id": "oai:arXiv.org:2504.09914v1",
        "title": "Improving Multimodal Hateful Meme Detection Exploiting LMM-Generated Knowledge",
        "link": "https://arxiv.org/abs/2504.09914",
        "author": "Maria Tzelepi, Vasileios Mezaris",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09914v1 Announce Type: new \nAbstract: Memes have become a dominant form of communication in social media in recent years. Memes are typically humorous and harmless, however there are also memes that promote hate speech, being in this way harmful to individuals and groups based on their identity. Therefore, detecting hateful content in memes has emerged as a task of critical importance. The need for understanding the complex interactions of images and their embedded text renders the hateful meme detection a challenging multimodal task. In this paper we propose to address the aforementioned task leveraging knowledge encoded in powerful Large Multimodal Models (LMM). Specifically, we propose to exploit LMMs in a two-fold manner. First, by extracting knowledge oriented to the hateful meme detection task in order to build strong meme representations. Specifically, generic semantic descriptions and emotions that the images along with their embedded texts elicit are extracted, which are then used to train a simple classification head for hateful meme detection. Second, by developing a novel hard mining approach introducing directly LMM-encoded knowledge to the training process, providing further improvements. We perform extensive experiments on two datasets that validate the effectiveness of the proposed method, achieving state-of-the-art performance. Our code and trained models are publicly available at: https://github.com/IDT-ITI/LMM-CLIP-meme."
      },
      {
        "id": "oai:arXiv.org:2504.09923v1",
        "title": "Guiding Reasoning in Small Language Models with LLM Assistance",
        "link": "https://arxiv.org/abs/2504.09923",
        "author": "Yujin Kim, Euiin Yi, Minu Kim, Se-Young Yun, Taehyeon Kim",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09923v1 Announce Type: new \nAbstract: The limited reasoning capabilities of small language models (SLMs) cast doubt on their suitability for tasks demanding deep, multi-step logical deduction. This paper introduces a framework called Small Reasons, Large Hints (SMART), which selectively augments SLM reasoning with targeted guidance from large language models (LLMs). Inspired by the concept of cognitive scaffolding, SMART employs a score-based evaluation to identify uncertain reasoning steps and injects corrective LLM-generated reasoning only when necessary. By framing structured reasoning as an optimal policy search, our approach steers the reasoning trajectory toward correct solutions without exhaustive sampling. Our experiments on mathematical reasoning datasets demonstrate that targeted external scaffolding significantly improves performance, paving the way for collaborative use of both SLM and LLM to tackle complex reasoning tasks that are currently unsolvable by SLMs alone."
      },
      {
        "id": "oai:arXiv.org:2504.09925v1",
        "title": "FUSION: Fully Integration of Vision-Language Representations for Deep Cross-Modal Understanding",
        "link": "https://arxiv.org/abs/2504.09925",
        "author": "Zheng Liu, Mengjie Liu, Jingzhou Chen, Jingwei Xu, Bin Cui, Conghui He, Wentao Zhang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09925v1 Announce Type: new \nAbstract: We introduce FUSION, a family of multimodal large language models (MLLMs) with a fully vision-language alignment and integration paradigm. Unlike existing methods that primarily rely on late-stage modality interaction during LLM decoding, our approach achieves deep, dynamic integration throughout the entire processing pipeline. To this end, we propose Text-Guided Unified Vision Encoding, incorporating textual information in vision encoding to achieve pixel-level integration. We further design Context-Aware Recursive Alignment Decoding that recursively aggregates visual features conditioned on textual context during decoding, enabling fine-grained, question-level semantic integration. To guide feature mapping and mitigate modality discrepancies, we develop Dual-Supervised Semantic Mapping Loss. Additionally, we construct a Synthesized Language-Driven Question-Answer (QA) dataset through a new data synthesis method, prioritizing high-quality QA pairs to optimize text-guided feature integration. Building on these foundations, we train FUSION at two scales-3B, 8B-and demonstrate that our full-modality integration approach significantly outperforms existing methods with only 630 vision tokens. Notably, FUSION 3B surpasses Cambrian-1 8B and Florence-VL 8B on most benchmarks. FUSION 3B continues to outperform Cambrian-1 8B even when limited to 300 vision tokens. Our ablation studies show that FUSION outperforms LLaVA-NeXT on over half of the benchmarks under same configuration without dynamic resolution, highlighting the effectiveness of our approach. We release our code, model weights, and dataset. https://github.com/starriver030515/FUSION"
      },
      {
        "id": "oai:arXiv.org:2504.09929v1",
        "title": "Moderate Actor-Critic Methods: Controlling Overestimation Bias via Expectile Loss",
        "link": "https://arxiv.org/abs/2504.09929",
        "author": "Ukjo Hwang, Songnam Hong",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09929v1 Announce Type: new \nAbstract: Overestimation is a fundamental characteristic of model-free reinforcement learning (MF-RL), arising from the principles of temporal difference learning and the approximation of the Q-function. To address this challenge, we propose a novel moderate target in the Q-function update, formulated as a convex optimization of an overestimated Q-function and its lower bound. Our primary contribution lies in the efficient estimation of this lower bound through the lower expectile of the Q-value distribution conditioned on a state. Notably, our moderate target integrates seamlessly into state-of-the-art (SOTA) MF-RL algorithms, including Deep Deterministic Policy Gradient (DDPG) and Soft Actor Critic (SAC). Experimental results validate the effectiveness of our moderate target in mitigating overestimation bias in DDPG, SAC, and distributional RL algorithms."
      },
      {
        "id": "oai:arXiv.org:2504.09930v1",
        "title": "Multi-objective Bayesian Optimization With Mixed-categorical Design Variables for Expensive-to-evaluate Aeronautical Applications",
        "link": "https://arxiv.org/abs/2504.09930",
        "author": "Nathalie Bartoli, Thierry Lefebvre, R\\'emi Lafage, Paul Saves, Youssef Diouane, Joseph Morlier, Jasper Bussemaker, Giuseppa Donelli, Joao Marcos Gomes de Mello, Massimo Mandorino, Pierluigi Della Vecchia",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09930v1 Announce Type: new \nAbstract: This work aims at developing new methodologies to optimize computational costly complex systems (e.g., aeronautical engineering systems). The proposed surrogate-based method (often called Bayesian optimization) uses adaptive sampling to promote a trade-off between exploration and exploitation. Our in-house implementation, called SEGOMOE, handles a high number of design variables (continuous, discrete or categorical) and nonlinearities by combining mixtures of experts for the objective and/or the constraints. Additionally, the method handles multi-objective optimization settings, as it allows the construction of accurate Pareto fronts with a minimal number of function evaluations. Different infill criteria have been implemented to handle multiple objectives with or without constraints. The effectiveness of the proposed method was tested on practical aeronautical applications within the context of the European Project AGILE 4.0 and demonstrated favorable results. A first example concerns a retrofitting problem where a comparison between two optimizers have been made. A second example introduces hierarchical variables to deal with architecture system in order to design an aircraft family. The third example increases drastically the number of categorical variables as it combines aircraft design, supply chain and manufacturing process. In this article, we show, on three different realistic problems, various aspects of our optimization codes thanks to the diversity of the treated aircraft problems."
      },
      {
        "id": "oai:arXiv.org:2504.09936v1",
        "title": "KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference",
        "link": "https://arxiv.org/abs/2504.09936",
        "author": "Yuxuan Tian, Zihan Wang, Yebo Peng, Aomufei Yuan, Zhiming Wang, Bairen Yi, Xin Liu, Yong Cui, Tong Yang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09936v1 Announce Type: new \nAbstract: Efficient inference of large language models (LLMs) is hindered by an ever-growing key-value (KV) cache, making KV cache compression a critical research direction. Traditional methods selectively evict less important KV cache entries based on attention scores or position heuristics, which leads to information loss and hallucinations. Recently, merging-based strategies have been explored to retain more information by merging KV pairs that would be discarded; however, these existing approaches inevitably introduce inconsistencies in attention distributions before and after merging, causing output perturbation and degraded generation quality. To overcome this challenge, we propose KeepKV, a novel adaptive KV cache merging method designed to eliminate output perturbation while preserving performance under strict memory constraints. KeepKV introduces the Electoral Votes mechanism that records merging history and adaptively adjusts attention scores. Moreover, it further leverages a novel Zero Inference-Perturbation Merging methods, keeping attention consistency and compensating for attention loss resulting from cache merging. KeepKV successfully retains essential context information within a significantly compressed cache. Extensive experiments on various benchmarks and LLM architectures demonstrate that KeepKV substantially reduces memory usage, enhances inference throughput by more than 2x and keeps superior generation quality even with 10% KV cache budgets."
      },
      {
        "id": "oai:arXiv.org:2504.09940v1",
        "title": "TianQuan-Climate: A Subseasonal-to-Seasonal Global Weather Model via Incorporate Climatology State",
        "link": "https://arxiv.org/abs/2504.09940",
        "author": "Guowen Li, Xintong Liu, Shilei Cao, Haoyuan Liang, Mengxuan Chen, Lixian Zhang, Jinxiao Zhang, Jiuke Wang, Meng Jin, Juepeng Zheng",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09940v1 Announce Type: new \nAbstract: Subseasonal forecasting serves as an important support for Sustainable Development Goals (SDGs), such as climate challenges, agricultural yield and sustainable energy production. However, subseasonal forecasting is a complex task in meteorology due to dissipating initial conditions and delayed external forces. Although AI models are increasingly pushing the boundaries of this forecasting limit, they face two major challenges: error accumulation and Smoothness. To address these two challenges, we propose Climate Furnace Subseasonal-to-Seasonal (TianQuan-Climate), a novel machine learning model designed to provide global daily mean forecasts up to 45 days, covering five upper-air atmospheric variables at 13 pressure levels and two surface variables. Our proposed TianQuan-Climate has two advantages: 1) it utilizes a multi-model prediction strategy to reduce system error impacts in long-term subseasonal forecasts; 2) it incorporates a Content Fusion Module for climatological integration and extends ViT with uncertainty blocks (UD-ViT) to improve generalization by learning from uncertainty. We demonstrate the effectiveness of TianQuan-Climate on benchmarks for weather forecasting and climate projections within the 15 to 45-day range, where TianQuan-Climate outperforms existing numerical and AI methods."
      },
      {
        "id": "oai:arXiv.org:2504.09941v1",
        "title": "FedRecon: Missing Modality Reconstruction in Distributed Heterogeneous Environments",
        "link": "https://arxiv.org/abs/2504.09941",
        "author": "Junming Liu, Guosun Zeng, Ding Wang, Yanting Gao, Yufei Jin",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09941v1 Announce Type: new \nAbstract: Multimodal data are often incomplete and exhibit Non-Independent and Identically Distributed (Non-IID) characteristics in real-world scenarios. These inherent limitations lead to both modality heterogeneity through partial modality absence and data heterogeneity from distribution divergence, creating fundamental challenges for effective federated learning (FL). To address these coupled challenges, we propose FedRecon, the first method targeting simultaneous missing modality reconstruction and Non-IID adaptation in multimodal FL. Our approach first employs a lightweight Multimodal Variational Autoencoder (MVAE) to reconstruct missing modalities while preserving cross-modal consistency. Distinct from conventional imputation methods, we achieve sample-level alignment through a novel distribution mapping mechanism that guarantees both data consistency and completeness. Additionally, we introduce a strategy employing global generator freezing to prevent catastrophic forgetting, which in turn mitigates Non-IID fluctuations. Extensive evaluations on multimodal datasets demonstrate FedRecon's superior performance in modality reconstruction under Non-IID conditions, surpassing state-of-the-art methods."
      },
      {
        "id": "oai:arXiv.org:2504.09948v1",
        "title": "Omni-Dish: Photorealistic and Faithful Image Generation and Editing for Arbitrary Chinese Dishes",
        "link": "https://arxiv.org/abs/2504.09948",
        "author": "Huijie Liu, Bingcan Wang, Jie Hu, Xiaoming Wei, Guoliang Kang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09948v1 Announce Type: new \nAbstract: Dish images play a crucial role in the digital era, with the demand for culturally distinctive dish images continuously increasing due to the digitization of the food industry and e-commerce. In general cases, existing text-to-image generation models excel in producing high-quality images; however, they struggle to capture diverse characteristics and faithful details of specific domains, particularly Chinese dishes. To address this limitation, we propose Omni-Dish, the first text-to-image generation model specifically tailored for Chinese dishes. We develop a comprehensive dish curation pipeline, building the largest dish dataset to date. Additionally, we introduce a recaption strategy and employ a coarse-to-fine training scheme to help the model better learn fine-grained culinary nuances. During inference, we enhance the user's textual input using a pre-constructed high-quality caption library and a large language model, enabling more photorealistic and faithful image generation. Furthermore, to extend our model's capability for dish editing tasks, we propose Concept-Enhanced P2P. Based on this approach, we build a dish editing dataset and train a specialized editing model. Extensive experiments demonstrate the superiority of our methods."
      },
      {
        "id": "oai:arXiv.org:2504.09953v1",
        "title": "Efficient 2D to Full 3D Human Pose Uplifting including Joint Rotations",
        "link": "https://arxiv.org/abs/2504.09953",
        "author": "Katja Ludwig, Yuliia Oksymets, Robin Sch\\\"on, Daniel Kienzle, Rainer Lienhart",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09953v1 Announce Type: new \nAbstract: In sports analytics, accurately capturing both the 3D locations and rotations of body joints is essential for understanding an athlete's biomechanics. While Human Mesh Recovery (HMR) models can estimate joint rotations, they often exhibit lower accuracy in joint localization compared to 3D Human Pose Estimation (HPE) models. Recent work addressed this limitation by combining a 3D HPE model with inverse kinematics (IK) to estimate both joint locations and rotations. However, IK is computationally expensive. To overcome this, we propose a novel 2D-to-3D uplifting model that directly estimates 3D human poses, including joint rotations, in a single forward pass. We investigate multiple rotation representations, loss functions, and training strategies - both with and without access to ground truth rotations. Our models achieve state-of-the-art accuracy in rotation estimation, are 150 times faster than the IK-based approach, and surpass HMR models in joint localization precision."
      },
      {
        "id": "oai:arXiv.org:2504.09956v1",
        "title": "Semantic Depth Matters: Explaining Errors of Deep Vision Networks through Perceived Class Similarities",
        "link": "https://arxiv.org/abs/2504.09956",
        "author": "Katarzyna Filus, Micha{\\l} Romaszewski, Mateusz \\.Zarski",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09956v1 Announce Type: new \nAbstract: Understanding deep neural network (DNN) behavior requires more than evaluating classification accuracy alone; analyzing errors and their predictability is equally crucial. Current evaluation methodologies lack transparency, particularly in explaining the underlying causes of network misclassifications. To address this, we introduce a novel framework that investigates the relationship between the semantic hierarchy depth perceived by a network and its real-data misclassification patterns. Central to our framework is the Similarity Depth (SD) metric, which quantifies the semantic hierarchy depth perceived by a network along with a method of evaluation of how closely the network's errors align with its internally perceived similarity structure. We also propose a graph-based visualization of model semantic relationships and misperceptions. A key advantage of our approach is that leveraging class templates -- representations derived from classifier layer weights -- is applicable to already trained networks without requiring additional data or experiments. Our approach reveals that deep vision networks encode specific semantic hierarchies and that high semantic depth improves the compliance between perceived class similarities and actual errors."
      },
      {
        "id": "oai:arXiv.org:2504.09958v1",
        "title": "C-MTCSD: A Chinese Multi-Turn Conversational Stance Detection Dataset",
        "link": "https://arxiv.org/abs/2504.09958",
        "author": "Fuqiang Niu, Yi Yang, Xianghua Fu, Genan Dai, Bowen Zhang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09958v1 Announce Type: new \nAbstract: Stance detection has become an essential tool for analyzing public discussions on social media. Current methods face significant challenges, particularly in Chinese language processing and multi-turn conversational analysis. To address these limitations, we introduce C-MTCSD, the largest Chinese multi-turn conversational stance detection dataset, comprising 24,264 carefully annotated instances from Sina Weibo, which is 4.2 times larger than the only prior Chinese conversational stance detection dataset. Our comprehensive evaluation using both traditional approaches and large language models reveals the complexity of C-MTCSD: even state-of-the-art models achieve only 64.07% F1 score in the challenging zero-shot setting, while performance consistently degrades with increasing conversation depth. Traditional models particularly struggle with implicit stance detection, achieving below 50% F1 score. This work establishes a challenging new benchmark for Chinese stance detection research, highlighting significant opportunities for future improvements."
      },
      {
        "id": "oai:arXiv.org:2504.09960v1",
        "title": "Dual-Path Enhancements in Event-Based Eye Tracking: Augmented Robustness and Adaptive Temporal Modeling",
        "link": "https://arxiv.org/abs/2504.09960",
        "author": "Hoang M. Truong, Vinh-Thuan Ly, Huy G. Tran, Thuan-Phat Nguyen, Tram T. Doan",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09960v1 Announce Type: new \nAbstract: Event-based eye tracking has become a pivotal technology for augmented reality and human-computer interaction. Yet, existing methods struggle with real-world challenges such as abrupt eye movements and environmental noise. Building on the efficiency of the Lightweight Spatiotemporal Network-a causal architecture optimized for edge devices-we introduce two key advancements. First, a robust data augmentation pipeline incorporating temporal shift, spatial flip, and event deletion improves model resilience, reducing Euclidean distance error by 12% (1.61 vs. 1.70 baseline) on challenging samples. Second, we propose KnightPupil, a hybrid architecture combining an EfficientNet-B3 backbone for spatial feature extraction, a bidirectional GRU for contextual temporal modeling, and a Linear Time-Varying State-Space Module to adapt to sparse inputs and noise dynamically. Evaluated on the 3ET+ benchmark, our framework achieved 1.61 Euclidean distance on the private test set of the Event-based Eye Tracking Challenge at CVPR 2025, demonstrating its effectiveness for practical deployment in AR/VR systems while providing a foundation for future innovations in neuromorphic vision."
      },
      {
        "id": "oai:arXiv.org:2504.09963v1",
        "title": "Towards Unbiased Federated Graph Learning: Label and Topology Perspectives",
        "link": "https://arxiv.org/abs/2504.09963",
        "author": "Zhengyu Wu, Boyang Pang, Xunkai Li, Yinlin Zhu, Daohan Su, Bowen Fan, Rong-Hua Li, Guoren Wang, Chenghu Zhou",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09963v1 Announce Type: new \nAbstract: Federated Graph Learning (FGL) enables privacy-preserving, distributed training of graph neural networks without sharing raw data. Among its approaches, subgraph-FL has become the dominant paradigm, with most work focused on improving overall node classification accuracy. However, these methods often overlook fairness due to the complexity of node features, labels, and graph structures. In particular, they perform poorly on nodes with disadvantaged properties, such as being in the minority class within subgraphs or having heterophilous connections (neighbors with dissimilar labels or misleading features). This reveals a critical issue: high accuracy can mask degraded performance on structurally or semantically marginalized nodes. To address this, we advocate for two fairness goals: (1) improving representation of minority class nodes for class-wise fairness and (2) mitigating topological bias from heterophilous connections for topology-aware fairness. We propose FairFGL, a novel framework that enhances fairness through fine-grained graph mining and collaborative learning. On the client side, the History-Preserving Module prevents overfitting to dominant local classes, while the Majority Alignment Module refines representations of heterophilous majority-class nodes. The Gradient Modification Module transfers minority-class knowledge from structurally favorable clients to improve fairness. On the server side, FairFGL uploads only the most influenced subset of parameters to reduce communication costs and better reflect local distributions. A cluster-based aggregation strategy reconciles conflicting updates and curbs global majority dominance . Extensive evaluations on eight benchmarks show FairFGL significantly improves minority-group performance , achieving up to a 22.62 percent Macro-F1 gain while enhancing convergence over state-of-the-art baselines."
      },
      {
        "id": "oai:arXiv.org:2504.09966v1",
        "title": "SemiETS: Integrating Spatial and Content Consistencies for Semi-Supervised End-to-end Text Spotting",
        "link": "https://arxiv.org/abs/2504.09966",
        "author": "Dongliang Luo, Hanshen Zhu, Ziyang Zhang, Dingkang Liang, Xudong Xie, Yuliang Liu, Xiang Bai",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09966v1 Announce Type: new \nAbstract: Most previous scene text spotting methods rely on high-quality manual annotations to achieve promising performance. To reduce their expensive costs, we study semi-supervised text spotting (SSTS) to exploit useful information from unlabeled images. However, directly applying existing semi-supervised methods of general scenes to SSTS will face new challenges: 1) inconsistent pseudo labels between detection and recognition tasks, and 2) sub-optimal supervisions caused by inconsistency between teacher/student. Thus, we propose a new Semi-supervised framework for End-to-end Text Spotting, namely SemiETS that leverages the complementarity of text detection and recognition. Specifically, it gradually generates reliable hierarchical pseudo labels for each task, thereby reducing noisy labels. Meanwhile, it extracts important information in locations and transcriptions from bidirectional flows to improve consistency. Extensive experiments on three datasets under various settings demonstrate the effectiveness of SemiETS on arbitrary-shaped text. For example, it outperforms previous state-of-the-art SSL methods by a large margin on end-to-end spotting (+8.7%, +5.6%, and +2.6% H-mean under 0.5%, 1%, and 2% labeled data settings on Total-Text, respectively). More importantly, it still improves upon a strongly supervised text spotter trained with plenty of labeled data by 2.0%. Compelling domain adaptation ability shows practical potential. Moreover, our method demonstrates consistent improvement on different text spotters."
      },
      {
        "id": "oai:arXiv.org:2504.09967v1",
        "title": "Enhancing Multi-task Learning Capability of Medical Generalist Foundation Model via Image-centric Multi-annotation Data",
        "link": "https://arxiv.org/abs/2504.09967",
        "author": "Xun Zhu, Fanbin Mo, Zheng Zhang, Jiaxi Wang, Yiming Shi, Ming Wu, Chuang Zhang, Miao Li, Ji Wu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09967v1 Announce Type: new \nAbstract: The emergence of medical generalist foundation models has revolutionized conventional task-specific model development paradigms, aiming to better handle multiple tasks through joint training on large-scale medical datasets. However, recent advances prioritize simple data scaling or architectural component enhancement, while neglecting to re-examine multi-task learning from a data-centric perspective. Critically, simply aggregating existing data resources leads to decentralized image-task alignment, which fails to cultivate comprehensive image understanding or align with clinical needs for multi-dimensional image interpretation. In this paper, we introduce the image-centric multi-annotation X-ray dataset (IMAX), the first attempt to enhance the multi-task learning capabilities of medical multi-modal large language models (MLLMs) from the data construction level. To be specific, IMAX is featured from the following attributes: 1) High-quality data curation. A comprehensive collection of more than 354K entries applicable to seven different medical tasks. 2) Image-centric dense annotation. Each X-ray image is associated with an average of 4.10 tasks and 7.46 training entries, ensuring multi-task representation richness per image. Compared to the general decentralized multi-annotation X-ray dataset (DMAX), IMAX consistently demonstrates significant multi-task average performance gains ranging from 3.20% to 21.05% across seven open-source state-of-the-art medical MLLMs. Moreover, we investigate differences in statistical patterns exhibited by IMAX and DMAX training processes, exploring potential correlations between optimization dynamics and multi-task performance. Finally, leveraging the core concept of IMAX data construction, we propose an optimized DMAX-based training strategy to alleviate the dilemma of obtaining high-quality IMAX data in practical scenarios."
      },
      {
        "id": "oai:arXiv.org:2504.09970v1",
        "title": "IsoSEL: Isometric Structural Entropy Learning for Deep Graph Clustering in Hyperbolic Space",
        "link": "https://arxiv.org/abs/2504.09970",
        "author": "Li Sun, Zhenhao Huang, Yujie Wang, Hongbo Lv, Chunyang Liu, Hao Peng, Philip S. Yu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09970v1 Announce Type: new \nAbstract: Graph clustering is a longstanding topic in machine learning. In recent years, deep learning methods have achieved encouraging results, but they still require predefined cluster numbers K, and typically struggle with imbalanced graphs, especially in identifying minority clusters. The limitations motivate us to study a challenging yet practical problem: deep graph clustering without K considering the imbalance in reality. We approach this problem from a fresh perspective of information theory (i.e., structural information). In the literature, structural information has rarely been touched in deep clustering, and the classic definition falls short in its discrete formulation, neglecting node attributes and exhibiting prohibitive complexity. In this paper, we first establish a new Differentiable Structural Information, generalizing the discrete formalism to continuous realm, so that the optimal partitioning tree, revealing the cluster structure, can be created by the gradient backpropagation. Theoretically, we demonstrate its capability in clustering without requiring K and identifying the minority clusters in imbalanced graphs, while reducing the time complexity to O(N) w.r.t. the number of nodes. Subsequently, we present a novel IsoSEL framework for deep graph clustering, where we design a hyperbolic neural network to learn the partitioning tree in the Lorentz model of hyperbolic space, and further conduct Lorentz Tree Contrastive Learning with isometric augmentation. As a result, the partitioning tree incorporates node attributes via mutual information maximization, while the cluster assignment is refined by the proposed tree contrastive learning. Extensive experiments on five benchmark datasets show the IsoSEL outperforms 14 recent baselines by an average of +1.3% in NMI."
      },
      {
        "id": "oai:arXiv.org:2504.09973v1",
        "title": "Beyond Degradation Redundancy: Contrastive Prompt Learning for All-in-One Image Restoration",
        "link": "https://arxiv.org/abs/2504.09973",
        "author": "Gang Wu, Junjun Jiang, Kui Jiang, Xianming Liu, Liqiang Nie",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09973v1 Announce Type: new \nAbstract: All-in-one image restoration, addressing diverse degradation types with a unified model, presents significant challenges in designing task-specific prompts that effectively guide restoration across multiple degradation scenarios. While adaptive prompt learning enables end-to-end optimization, it often yields overlapping or redundant task representations. Conversely, explicit prompts derived from pretrained classifiers enhance discriminability but may discard critical visual information for reconstruction. To address these limitations, we introduce Contrastive Prompt Learning (CPL), a novel framework that fundamentally enhances prompt-task alignment through two complementary innovations: a \\emph{Sparse Prompt Module (SPM)} that efficiently captures degradation-specific features while minimizing redundancy, and a \\emph{Contrastive Prompt Regularization (CPR)} that explicitly strengthens task boundaries by incorporating negative prompt samples across different degradation types. Unlike previous approaches that focus primarily on degradation classification, CPL optimizes the critical interaction between prompts and the restoration model itself. Extensive experiments across five comprehensive benchmarks demonstrate that CPL consistently enhances state-of-the-art all-in-one restoration models, achieving significant improvements in both standard multi-task scenarios and challenging composite degradation settings. Our framework establishes new state-of-the-art performance while maintaining parameter efficiency, offering a principled solution for unified image restoration."
      },
      {
        "id": "oai:arXiv.org:2504.09978v1",
        "title": "New exponential law for real networks",
        "link": "https://arxiv.org/abs/2504.09978",
        "author": "Mikhail Tuzhilin",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09978v1 Announce Type: new \nAbstract: In this article we have shown that the distributions of ksi satisfy an exponential law for real networks while the distributions of ksi for random networks are bell-shaped and closer to the normal distribution. The ksi distributions for Barabasi-Albert and Watts-Strogatz networks are similar to the ksi distributions for random networks (bell-shaped) for most parameters, but when these parameters become small enough, the Barabasi-Albert and Watts-Strogatz networks become more realistic with respect to the ksi distributions."
      },
      {
        "id": "oai:arXiv.org:2504.09979v1",
        "title": "Resampling Benchmark for Efficient Comprehensive Evaluation of Large Vision-Language Models",
        "link": "https://arxiv.org/abs/2504.09979",
        "author": "Teppei Suzuki, Keisuke Ozawa",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09979v1 Announce Type: new \nAbstract: We propose an efficient evaluation protocol for large vision-language models (VLMs). Given their broad knowledge and reasoning capabilities, multiple benchmarks are needed for comprehensive assessment, making evaluation computationally expensive. To improve efficiency, we construct a subset that yields results comparable to full benchmark evaluations. Our benchmark classification experiments reveal that no single benchmark fully covers all challenges. We then introduce a subset construction method using farthest point sampling (FPS). Our experiments show that FPS-based benchmarks maintain a strong correlation (> 0.96) with full evaluations while using only ~1\\% of the data. Additionally, applying FPS to an existing benchmark improves correlation with overall evaluation results, suggesting its potential to reduce unintended dataset biases."
      },
      {
        "id": "oai:arXiv.org:2504.09980v1",
        "title": "Turn-taking annotation for quantitative and qualitative analyses of conversation",
        "link": "https://arxiv.org/abs/2504.09980",
        "author": "Anneliese Kelterer, Barbara Schuppler",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09980v1 Announce Type: new \nAbstract: This paper has two goals. First, we present the turn-taking annotation layers created for 95 minutes of conversational speech of the Graz Corpus of Read and Spontaneous Speech (GRASS), available to the scientific community. Second, we describe the annotation system and the annotation process in more detail, so other researchers may use it for their own conversational data. The annotation system was developed with an interdisciplinary application in mind. It should be based on sequential criteria according to Conversation Analysis, suitable for subsequent phonetic analysis, thus time-aligned annotations were made Praat, and it should be suitable for automatic classification, which required the continuous annotation of speech and a label inventory that is not too large and results in a high inter-rater agreement. Turn-taking was annotated on two layers, Inter-Pausal Units (IPU) and points of potential completion (PCOMP; similar to transition relevance places). We provide a detailed description of the annotation process and of segmentation and labelling criteria. A detailed analysis of inter-rater agreement and common confusions shows that agreement for IPU annotation is near-perfect, that agreement for PCOMP annotations is substantial, and that disagreements often are either partial or can be explained by a different analysis of a sequence which also has merit. The annotation system can be applied to a variety of conversational data for linguistic studies and technological applications, and we hope that the annotations, as well as the annotation system will contribute to a stronger cross-fertilization between these disciplines."
      },
      {
        "id": "oai:arXiv.org:2504.09990v1",
        "title": "Correlative and Discriminative Label Grouping for Multi-Label Visual Prompt Tuning",
        "link": "https://arxiv.org/abs/2504.09990",
        "author": "LeiLei Ma, Shuo Xu, MingKun Xie, Lei Wang, Dengdi Sun, Haifeng Zhao",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09990v1 Announce Type: new \nAbstract: Modeling label correlations has always played a pivotal role in multi-label image classification (MLC), attracting significant attention from researchers. However, recent studies have overemphasized co-occurrence relationships among labels, which can lead to overfitting risk on this overemphasis, resulting in suboptimal models. To tackle this problem, we advocate for balancing correlative and discriminative relationships among labels to mitigate the risk of overfitting and enhance model performance. To this end, we propose the Multi-Label Visual Prompt Tuning framework, a novel and parameter-efficient method that groups classes into multiple class subsets according to label co-occurrence and mutual exclusivity relationships, and then models them respectively to balance the two relationships. In this work, since each group contains multiple classes, multiple prompt tokens are adopted within Vision Transformer (ViT) to capture the correlation or discriminative label relationship within each group, and effectively learn correlation or discriminative representations for class subsets. On the other hand, each group contains multiple group-aware visual representations that may correspond to multiple classes, and the mixture of experts (MoE) model can cleverly assign them from the group-aware to the label-aware, adaptively obtaining label-aware representation, which is more conducive to classification. Experiments on multiple benchmark datasets show that our proposed approach achieves competitive results and outperforms SOTA methods on multiple pre-trained models."
      },
      {
        "id": "oai:arXiv.org:2504.09993v1",
        "title": "AimTS: Augmented Series and Image Contrastive Learning for Time Series Classification",
        "link": "https://arxiv.org/abs/2504.09993",
        "author": "Yuxuan Chen, Shanshan Huang, Yunyao Cheng, Peng Chen, Zhongwen Rao, Yang Shu, Bin Yang, Lujia Pan, Chenjuan Guo",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09993v1 Announce Type: new \nAbstract: Time series classification (TSC) is an important task in time series analysis. Existing TSC methods mainly train on each single domain separately, suffering from a degradation in accuracy when the samples for training are insufficient in certain domains. The pre-training and fine-tuning paradigm provides a promising direction for solving this problem. However, time series from different domains are substantially divergent, which challenges the effective pre-training on multi-source data and the generalization ability of pre-trained models. To handle this issue, we introduce Augmented Series and Image Contrastive Learning for Time Series Classification (AimTS), a pre-training framework that learns generalizable representations from multi-source time series data. We propose a two-level prototype-based contrastive learning method to effectively utilize various augmentations in multi-source pre-training, which learns representations for TSC that can be generalized to different domains. In addition, considering augmentations within the single time series modality are insufficient to fully address classification problems with distribution shift, we introduce the image modality to supplement structural information and establish a series-image contrastive learning to improve the generalization of the learned representations for TSC tasks. Extensive experiments show that after multi-source pre-training, AimTS achieves good generalization performance, enabling efficient learning and even few-shot learning on various downstream TSC datasets."
      },
      {
        "id": "oai:arXiv.org:2504.09998v1",
        "title": "Metric-Guided Synthesis of Class Activation Mapping",
        "link": "https://arxiv.org/abs/2504.09998",
        "author": "Alejandro Luque-Cerpa, Elizabeth Polgreen, Ajitha Rajan, Hazem Torfah",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09998v1 Announce Type: new \nAbstract: Class activation mapping (CAM) is a widely adopted class of saliency methods used to explain the behavior of convolutional neural networks (CNNs). These methods generate heatmaps that highlight the parts of the input most relevant to the CNN output. Various CAM methods have been proposed, each distinguished by the expressions used to derive heatmaps. In general, users look for heatmaps with specific properties that reflect different aspects of CNN functionality. These may include similarity to ground truth, robustness, equivariance, and more. Although existing CAM methods implicitly encode some of these properties in their expressions, they do not allow for variability in heatmap generation following the user's intent or domain knowledge. In this paper, we address this limitation by introducing SyCAM, a metric-based approach for synthesizing CAM expressions. Given a predefined evaluation metric for saliency maps, SyCAM automatically generates CAM expressions optimized for that metric. We specifically explore a syntax-guided synthesis instantiation of SyCAM, where CAM expressions are derived based on predefined syntactic constraints and the given metric. Using several established evaluation metrics, we demonstrate the efficacy and flexibility of our approach in generating targeted heatmaps. We compare SyCAM with other well-known CAM methods on three prominent models: ResNet50, VGG16, and VGG19."
      },
      {
        "id": "oai:arXiv.org:2504.10001v1",
        "title": "GaussVideoDreamer: 3D Scene Generation with Video Diffusion and Inconsistency-Aware Gaussian Splatting",
        "link": "https://arxiv.org/abs/2504.10001",
        "author": "Junlin Hao, Peiheng Wang, Haoyang Wang, Xinggong Zhang, Zongming Guo",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10001v1 Announce Type: new \nAbstract: Single-image 3D scene reconstruction presents significant challenges due to its inherently ill-posed nature and limited input constraints. Recent advances have explored two promising directions: multiview generative models that train on 3D consistent datasets but struggle with out-of-distribution generalization, and 3D scene inpainting and completion frameworks that suffer from cross-view inconsistency and suboptimal error handling, as they depend exclusively on depth data or 3D smoothness, which ultimately degrades output quality and computational performance. Building upon these approaches, we present GaussVideoDreamer, which advances generative multimedia approaches by bridging the gap between image, video, and 3D generation, integrating their strengths through two key innovations: (1) A progressive video inpainting strategy that harnesses temporal coherence for improved multiview consistency and faster convergence. (2) A 3D Gaussian Splatting consistency mask to guide the video diffusion with 3D consistent multiview evidence. Our pipeline combines three core components: a geometry-aware initialization protocol, Inconsistency-Aware Gaussian Splatting, and a progressive video inpainting strategy. Experimental results demonstrate that our approach achieves 32% higher LLaVA-IQA scores and at least 2x speedup compared to existing methods while maintaining robust performance across diverse scenes."
      },
      {
        "id": "oai:arXiv.org:2504.10004v1",
        "title": "An Image is Worth $K$ Topics: A Visual Structural Topic Model with Pretrained Image Embeddings",
        "link": "https://arxiv.org/abs/2504.10004",
        "author": "Mat\\'ias Piqueras, Alexandra Segerberg, Matteo Magnani, M{\\aa}ns Magnusson, Nata\\v{s}a Sladoje",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10004v1 Announce Type: new \nAbstract: Political scientists are increasingly interested in analyzing visual content at scale. However, the existing computational toolbox is still in need of methods and models attuned to the specific challenges and goals of social and political inquiry. In this article, we introduce a visual Structural Topic Model (vSTM) that combines pretrained image embeddings with a structural topic model. This has important advantages compared to existing approaches. First, pretrained embeddings allow the model to capture the semantic complexity of images relevant to political contexts. Second, the structural topic model provides the ability to analyze how topics and covariates are related, while maintaining a nuanced representation of images as a mixture of multiple topics. In our empirical application, we show that the vSTM is able to identify topics that are interpretable, coherent, and substantively relevant to the study of online political communication."
      },
      {
        "id": "oai:arXiv.org:2504.10005v1",
        "title": "Session-based Recommender Systems: User Interest as a Stochastic Process in the Latent Space",
        "link": "https://arxiv.org/abs/2504.10005",
        "author": "Klaudia Balcer, Piotr Lipinski",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10005v1 Announce Type: new \nAbstract: This paper jointly addresses the problem of data uncertainty, popularity bias, and exposure bias in session-based recommender systems. We study the symptoms of this bias both in item embeddings and in recommendations. We propose treating user interest as a stochastic process in the latent space and providing a model-agnostic implementation of this mathematical concept. The proposed stochastic component consists of elements: debiasing item embeddings with regularization for embedding uniformity, modeling dense user interest from session prefixes, and introducing fake targets in the data to simulate extended exposure. We conducted computational experiments on two popular benchmark datasets, Diginetica and YooChoose 1/64, as well as several modifications of the YooChoose dataset with different ratios of popular items. The results show that the proposed approach allows us to mitigate the challenges mentioned."
      },
      {
        "id": "oai:arXiv.org:2504.10006v1",
        "title": "Improving Controller Generalization with Dimensionless Markov Decision Processes",
        "link": "https://arxiv.org/abs/2504.10006",
        "author": "Valentin Charvet, Sebastian Stein, Roderick Murray-Smith",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10006v1 Announce Type: new \nAbstract: Controllers trained with Reinforcement Learning tend to be very specialized and thus generalize poorly when their testing environment differs from their training one. We propose a Model-Based approach to increase generalization where both world model and policy are trained in a dimensionless state-action space. To do so, we introduce the Dimensionless Markov Decision Process ($\\Pi$-MDP): an extension of Contextual-MDPs in which state and action spaces are non-dimensionalized with the Buckingham-$\\Pi$ theorem. This procedure induces policies that are equivariant with respect to changes in the context of the underlying dynamics. We provide a generic framework for this approach and apply it to a model-based policy search algorithm using Gaussian Process models. We demonstrate the applicability of our method on simulated actuated pendulum and cartpole systems, where policies trained on a single environment are robust to shifts in the distribution of the context."
      },
      {
        "id": "oai:arXiv.org:2504.10007v1",
        "title": "Balancing Two Classifiers via A Simplex ETF Structure for Model Calibration",
        "link": "https://arxiv.org/abs/2504.10007",
        "author": "Jiani Ni, He Zhao, Jintong Gao, Dandan Guo, Hongyuan Zha",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10007v1 Announce Type: new \nAbstract: In recent years, deep neural networks (DNNs) have demonstrated state-of-the-art performance across various domains. However, despite their success, they often face calibration issues, particularly in safety-critical applications such as autonomous driving and healthcare, where unreliable predictions can have serious consequences. Recent research has started to improve model calibration from the view of the classifier. However, the exploration of designing the classifier to solve the model calibration problem is insufficient. Let alone most of the existing methods ignore the calibration errors arising from underconfidence. In this work, we propose a novel method by balancing learnable and ETF classifiers to solve the overconfidence or underconfidence problem for model Calibration named BalCAL. By introducing a confidence-tunable module and a dynamic adjustment method, we ensure better alignment between model confidence and its true accuracy. Extensive experimental validation shows that ours significantly improves model calibration performance while maintaining high predictive accuracy, outperforming existing techniques. This provides a novel solution to the calibration challenges commonly encountered in deep learning."
      },
      {
        "id": "oai:arXiv.org:2504.10012v1",
        "title": "EBAD-Gaussian: Event-driven Bundle Adjusted Deblur Gaussian Splatting",
        "link": "https://arxiv.org/abs/2504.10012",
        "author": "Yufei Deng, Yuanjian Wang, Rong Xiao, Chenwei Tang, Jizhe Zhou, Jiahao Fan, Deng Xiong, Jiancheng Lv, Huajin Tang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10012v1 Announce Type: new \nAbstract: While 3D Gaussian Splatting (3D-GS) achieves photorealistic novel view synthesis, its performance degrades with motion blur. In scenarios with rapid motion or low-light conditions, existing RGB-based deblurring methods struggle to model camera pose and radiance changes during exposure, reducing reconstruction accuracy. Event cameras, capturing continuous brightness changes during exposure, can effectively assist in modeling motion blur and improving reconstruction quality. Therefore, we propose Event-driven Bundle Adjusted Deblur Gaussian Splatting (EBAD-Gaussian), which reconstructs sharp 3D Gaussians from event streams and severely blurred images. This method jointly learns the parameters of these Gaussians while recovering camera motion trajectories during exposure time. Specifically, we first construct a blur loss function by synthesizing multiple latent sharp images during the exposure time, minimizing the difference between real and synthesized blurred images. Then we use event stream to supervise the light intensity changes between latent sharp images at any time within the exposure period, supplementing the light intensity dynamic changes lost in RGB images. Furthermore, we optimize the latent sharp images at intermediate exposure times based on the event-based double integral (EDI) prior, applying consistency constraints to enhance the details and texture information of the reconstructed images. Extensive experiments on synthetic and real-world datasets show that EBAD-Gaussian can achieve high-quality 3D scene reconstruction under the condition of blurred images and event stream inputs."
      },
      {
        "id": "oai:arXiv.org:2504.10014v1",
        "title": "Air Quality Prediction with A Meteorology-Guided Modality-Decoupled Spatio-Temporal Network",
        "link": "https://arxiv.org/abs/2504.10014",
        "author": "Hang Yin, Yan-Ming Zhang, Jian Xu, Jian-Long Chang, Yin Li, Cheng-Lin Liu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10014v1 Announce Type: new \nAbstract: Air quality prediction plays a crucial role in public health and environmental protection. Accurate air quality prediction is a complex multivariate spatiotemporal problem, that involves interactions across temporal patterns, pollutant correlations, spatial station dependencies, and particularly meteorological influences that govern pollutant dispersion and chemical transformations. Existing works underestimate the critical role of atmospheric conditions in air quality prediction and neglect comprehensive meteorological data utilization, thereby impairing the modeling of dynamic interdependencies between air quality and meteorological data. To overcome this, we propose MDSTNet, an encoder-decoder framework that explicitly models air quality observations and atmospheric conditions as distinct modalities, integrating multi-pressure-level meteorological data and weather forecasts to capture atmosphere-pollution dependencies for prediction. Meantime, we construct ChinaAirNet, the first nationwide dataset combining air quality records with multi-pressure-level meteorological observations. Experimental results on ChinaAirNet demonstrate MDSTNet's superiority, substantially reducing 48-hour prediction errors by 17.54\\% compared to the state-of-the-art model. The source code and dataset will be available on github."
      },
      {
        "id": "oai:arXiv.org:2504.10018v1",
        "title": "RGB-Event based Pedestrian Attribute Recognition: A Benchmark Dataset and An Asymmetric RWKV Fusion Framework",
        "link": "https://arxiv.org/abs/2504.10018",
        "author": "Xiao Wang, Haiyang Wang, Shiao Wang, Qiang Chen, Jiandong Jin, Haoyu Song, Bo Jiang, Chenglong Li",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10018v1 Announce Type: new \nAbstract: Existing pedestrian attribute recognition methods are generally developed based on RGB frame cameras. However, these approaches are constrained by the limitations of RGB cameras, such as sensitivity to lighting conditions and motion blur, which hinder their performance. Furthermore, current attribute recognition primarily focuses on analyzing pedestrians' external appearance and clothing, lacking an exploration of emotional dimensions. In this paper, we revisit these issues and propose a novel multi-modal RGB-Event attribute recognition task by drawing inspiration from the advantages of event cameras in low-light, high-speed, and low-power consumption. Specifically, we introduce the first large-scale multi-modal pedestrian attribute recognition dataset, termed EventPAR, comprising 100K paired RGB-Event samples that cover 50 attributes related to both appearance and six human emotions, diverse scenes, and various seasons. By retraining and evaluating mainstream PAR models on this dataset, we establish a comprehensive benchmark and provide a solid foundation for future research in terms of data and algorithmic baselines. In addition, we propose a novel RWKV-based multi-modal pedestrian attribute recognition framework, featuring an RWKV visual encoder and an asymmetric RWKV fusion module. Extensive experiments are conducted on our proposed dataset as well as two simulated datasets (MARS-Attribute and DukeMTMC-VID-Attribute), achieving state-of-the-art results. The source code and dataset will be released on https://github.com/Event-AHU/OpenPAR"
      },
      {
        "id": "oai:arXiv.org:2504.10020v1",
        "title": "The Mirage of Performance Gains: Why Contrastive Decoding Fails to Address Multimodal Hallucination",
        "link": "https://arxiv.org/abs/2504.10020",
        "author": "Hao Yin, Gunagzong Si, Zilei Wang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10020v1 Announce Type: new \nAbstract: Contrastive decoding strategies are widely used to reduce hallucinations in multimodal large language models (MLLMs). These methods work by constructing contrastive samples to induce hallucinations and then suppressing them in the output distribution. However, this paper demonstrates that such approaches fail to effectively mitigate the hallucination problem. The performance improvements observed on POPE Benchmark are largely driven by two misleading factors: (1) crude, unidirectional adjustments to the model's output distribution and (2) the adaptive plausibility constraint, which reduces the sampling strategy to greedy search. To further illustrate these issues, we introduce a series of spurious improvement methods and evaluate their performance against contrastive decoding techniques. Experimental results reveal that the observed performance gains in contrastive decoding are entirely unrelated to its intended goal of mitigating hallucinations. Our findings challenge common assumptions about the effectiveness of contrastive decoding strategies and pave the way for developing genuinely effective solutions to hallucinations in MLLMs."
      },
      {
        "id": "oai:arXiv.org:2504.10021v1",
        "title": "Masked Autoencoder Self Pre-Training for Defect Detection in Microelectronics",
        "link": "https://arxiv.org/abs/2504.10021",
        "author": "Nikolai R\\\"ohrich (XITASO GmbH,LMU Munich), Alwin Hoffmann (XITASO GmbH), Richard Nordsieck (XITASO GmbH), Emilio Zarbali (XITASO GmbH), Alireza Javanmardi (LMU Munich,Munich Center for Machine Learning)",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10021v1 Announce Type: new \nAbstract: Whereas in general computer vision, transformer-based architectures have quickly become the gold standard, microelectronics defect detection still heavily relies on convolutional neural networks (CNNs). We hypothesize that this is due to the fact that a) transformers have an increased need for data and b) labelled image generation procedures for microelectronics are costly, and labelled data is therefore sparse. Whereas in other domains, pre-training on large natural image datasets can mitigate this problem, in microelectronics transfer learning is hindered due to the dissimilarity of domain data and natural images. Therefore, we evaluate self pre-training, where models are pre-trained on the target dataset, rather than another dataset. We propose a vision transformer (ViT) pre-training framework for defect detection in microelectronics based on masked autoencoders (MAE). In MAE, a large share of image patches is masked and reconstructed by the model during pre-training. We perform pre-training and defect detection using a dataset of less than 10.000 scanning acoustic microscopy (SAM) images labelled using transient thermal analysis (TTA). Our experimental results show that our approach leads to substantial performance gains compared to a) supervised ViT, b) ViT pre-trained on natural image datasets, and c) state-of-the-art CNN-based defect detection models used in the literature. Additionally, interpretability analysis reveals that our self pre-trained models, in comparison to ViT baselines, correctly focus on defect-relevant features such as cracks in the solder material. This demonstrates that our approach yields fault-specific feature representations, making our self pre-trained models viable for real-world defect detection in microelectronics."
      },
      {
        "id": "oai:arXiv.org:2504.10024v1",
        "title": "Relative Illumination Fields: Learning Medium and Light Independent Underwater Scenes",
        "link": "https://arxiv.org/abs/2504.10024",
        "author": "Mengkun She, Felix Seegr\\\"aber, David Nakath, Patricia Sch\\\"ontag, Kevin K\\\"oser",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10024v1 Announce Type: new \nAbstract: We address the challenge of constructing a consistent and photorealistic Neural Radiance Field in inhomogeneously illuminated, scattering environments with unknown, co-moving light sources. While most existing works on underwater scene representation focus on a static homogeneous illumination, limited attention has been paid to scenarios such as when a robot explores water deeper than a few tens of meters, where sunlight becomes insufficient. To address this, we propose a novel illumination field locally attached to the camera, enabling the capture of uneven lighting effects within the viewing frustum. We combine this with a volumetric medium representation to an overall method that effectively handles interaction between dynamic illumination field and static scattering medium. Evaluation results demonstrate the effectiveness and flexibility of our approach."
      },
      {
        "id": "oai:arXiv.org:2504.10031v1",
        "title": "Using Reinforcement Learning to Integrate Subjective Wellbeing into Climate Adaptation Decision Making",
        "link": "https://arxiv.org/abs/2504.10031",
        "author": "Arthur Vandervoort, Miguel Costa, Morten W. Petersen, Martin Drews, Sonja Haustein, Karyn Morrissey, Francisco C. Pereira",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10031v1 Announce Type: new \nAbstract: Subjective wellbeing is a fundamental aspect of human life, influencing life expectancy and economic productivity, among others. Mobility plays a critical role in maintaining wellbeing, yet the increasing frequency and intensity of both nuisance and high-impact floods due to climate change are expected to significantly disrupt access to activities and destinations, thereby affecting overall wellbeing. Addressing climate adaptation presents a complex challenge for policymakers, who must select and implement policies from a broad set of options with varying effects while managing resource constraints and uncertain climate projections. In this work, we propose a multi-modular framework that uses reinforcement learning as a decision-support tool for climate adaptation in Copenhagen, Denmark. Our framework integrates four interconnected components: long-term rainfall projections, flood modeling, transport accessibility, and wellbeing modeling. This approach enables decision-makers to identify spatial and temporal policy interventions that help sustain or enhance subjective wellbeing over time. By modeling climate adaptation as an open-ended system, our framework provides a structured framework for exploring and evaluating adaptation policy pathways. In doing so, it supports policymakers to make informed decisions that maximize wellbeing in the long run."
      },
      {
        "id": "oai:arXiv.org:2504.10035v1",
        "title": "TT3D: Table Tennis 3D Reconstruction",
        "link": "https://arxiv.org/abs/2504.10035",
        "author": "Thomas Gossard, Andreas Ziegler, Andreas Zell",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10035v1 Announce Type: new \nAbstract: Sports analysis requires processing large amounts of data, which is time-consuming and costly. Advancements in neural networks have significantly alleviated this burden, enabling highly accurate ball tracking in sports broadcasts. However, relying solely on 2D ball tracking is limiting, as it depends on the camera's viewpoint and falls short of supporting comprehensive game analysis. To address this limitation, we propose a novel approach for reconstructing precise 3D ball trajectories from online table tennis match recordings. Our method leverages the underlying physics of the ball's motion to identify the bounce state that minimizes the reprojection error of the ball's flying trajectory, hence ensuring an accurate and reliable 3D reconstruction. A key advantage of our approach is its ability to infer ball spin without relying on human pose estimation or racket tracking, which are often unreliable or unavailable in broadcast footage. We developed an automated camera calibration method capable of reliably tracking camera movements. Additionally, we adapted an existing 3D pose estimation model, which lacks depth motion capture, to accurately track player movements. Together, these contributions enable the full 3D reconstruction of a table tennis rally."
      },
      {
        "id": "oai:arXiv.org:2504.10036v1",
        "title": "DataMosaic: Explainable and Verifiable Multi-Modal Data Analytics through Extract-Reason-Verify",
        "link": "https://arxiv.org/abs/2504.10036",
        "author": "Zhengxuan Zhang, Zhuowen Liang, Yin Wu, Teng Lin, Yuyu Luo, Nan Tang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10036v1 Announce Type: new \nAbstract: Large Language Models (LLMs) are transforming data analytics, but their widespread adoption is hindered by two critical limitations: they are not explainable (opaque reasoning processes) and not verifiable (prone to hallucinations and unchecked errors). While retrieval-augmented generation (RAG) improves accuracy by grounding LLMs in external data, it fails to address the core challenges of trustworthy analytics - especially when processing noisy, inconsistent, or multi-modal data (for example, text, tables, images). We propose DataMosaic, a framework designed to make LLM-powered analytics both explainable and verifiable. By dynamically extracting task-specific structures (for example, tables, graphs, trees) from raw data, DataMosaic provides transparent, step-by-step reasoning traces and enables validation of intermediate results. Built on a multi-agent framework, DataMosaic orchestrates self-adaptive agents that align with downstream task requirements, enhancing consistency, completeness, and privacy. Through this approach, DataMosaic not only tackles the limitations of current LLM-powered analytics systems but also lays the groundwork for a new paradigm of grounded, accurate, and explainable multi-modal data analytics."
      },
      {
        "id": "oai:arXiv.org:2504.10039v1",
        "title": "Investigating the Role of Bilateral Symmetry for Inpainting Brain MRI",
        "link": "https://arxiv.org/abs/2504.10039",
        "author": "Sergey Kuznetsov, Sanduni Pinnawala, Peter A. Wijeratne, Ivor J. A. Simpson",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10039v1 Announce Type: new \nAbstract: Inpainting has recently emerged as a valuable and interesting technology to employ in the analysis of medical imaging data, in particular brain MRI. A wide variety of methodologies for inpainting MRI have been proposed and demonstrated on tasks including anomaly detection. In this work we investigate the statistical relationship between inpainted brain structures and the amount of subject-specific conditioning information, i.e. the other areas of the image that are masked. In particular, we analyse the distribution of inpainting results when masking additional regions of the image, specifically the contra-lateral structure. This allows us to elucidate where in the brain the model is drawing information from, and in particular, what is the importance of hemispherical symmetry? Our experiments interrogate a diffusion inpainting model through analysing the inpainting of subcortical brain structures based on intensity and estimated area change. We demonstrate that some structures show a strong influence of symmetry in the conditioning of the inpainting process."
      },
      {
        "id": "oai:arXiv.org:2504.10044v1",
        "title": "Aligning Anime Video Generation with Human Feedback",
        "link": "https://arxiv.org/abs/2504.10044",
        "author": "Bingwen Zhu, Yudong Jiang, Baohan Xu, Siqian Yang, Mingyu Yin, Yidi Wu, Huyang Sun, Zuxuan Wu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10044v1 Announce Type: new \nAbstract: Anime video generation faces significant challenges due to the scarcity of anime data and unusual motion patterns, leading to issues such as motion distortion and flickering artifacts, which result in misalignment with human preferences. Existing reward models, designed primarily for real-world videos, fail to capture the unique appearance and consistency requirements of anime. In this work, we propose a pipeline to enhance anime video generation by leveraging human feedback for better alignment. Specifically, we construct the first multi-dimensional reward dataset for anime videos, comprising 30k human-annotated samples that incorporating human preferences for both visual appearance and visual consistency. Based on this, we develop AnimeReward, a powerful reward model that employs specialized vision-language models for different evaluation dimensions to guide preference alignment. Furthermore, we introduce Gap-Aware Preference Optimization (GAPO), a novel training method that explicitly incorporates preference gaps into the optimization process, enhancing alignment performance and efficiency. Extensive experiment results show that AnimeReward outperforms existing reward models, and the inclusion of GAPO leads to superior alignment in both quantitative benchmarks and human evaluations, demonstrating the effectiveness of our pipeline in enhancing anime video quality. Our dataset and code will be publicly available."
      },
      {
        "id": "oai:arXiv.org:2504.10048v1",
        "title": "Multi-Object Grounding via Hierarchical Contrastive Siamese Transformers",
        "link": "https://arxiv.org/abs/2504.10048",
        "author": "Chengyi Du, Keyan Jin",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10048v1 Announce Type: new \nAbstract: Multi-object grounding in 3D scenes involves localizing multiple objects based on natural language input. While previous work has primarily focused on single-object grounding, real-world scenarios often demand the localization of several objects. To tackle this challenge, we propose Hierarchical Contrastive Siamese Transformers (H-COST), which employs a Hierarchical Processing strategy to progressively refine object localization, enhancing the understanding of complex language instructions. Additionally, we introduce a Contrastive Siamese Transformer framework, where two networks with the identical structure are used: one auxiliary network processes robust object relations from ground-truth labels to guide and enhance the second network, the reference network, which operates on segmented point-cloud data. This contrastive mechanism strengthens the model' s semantic understanding and significantly enhances its ability to process complex point-cloud data. Our approach outperforms previous state-of-the-art methods by 9.5% on challenging multi-object grounding benchmarks."
      },
      {
        "id": "oai:arXiv.org:2504.10049v1",
        "title": "Summarization of Multimodal Presentations with Vision-Language Models: Study of the Effect of Modalities and Structure",
        "link": "https://arxiv.org/abs/2504.10049",
        "author": "Th\\'eo Gigant, Camille Guinaudeau, Fr\\'ed\\'eric Dufaux",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10049v1 Announce Type: new \nAbstract: Vision-Language Models (VLMs) can process visual and textual information in multiple formats: texts, images, interleaved texts and images, or even hour-long videos. In this work, we conduct fine-grained quantitative and qualitative analyses of automatic summarization of multimodal presentations using VLMs with various representations as input. From these experiments, we suggest cost-effective strategies for generating summaries from text-heavy multimodal documents under different input-length budgets using VLMs. We show that slides extracted from the video stream can be beneficially used as input against the raw video, and that a structured representation from interleaved slides and transcript provides the best performance. Finally, we reflect and comment on the nature of cross-modal interactions in multimodal presentations and share suggestions to improve the capabilities of VLMs to understand documents of this nature."
      },
      {
        "id": "oai:arXiv.org:2504.10058v1",
        "title": "Data Cooperatives: Democratic Models for Ethical Data Stewardship",
        "link": "https://arxiv.org/abs/2504.10058",
        "author": "Francisco Mendonca, Giovanna DiMarzo, Nabil Abdennadher",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10058v1 Announce Type: new \nAbstract: Data cooperatives offer a new model for fair data governance, enabling individuals to collectively control, manage, and benefit from their information while adhering to cooperative principles such as democratic member control, economic participation, and community concern. This paper reviews data cooperatives, distinguishing them from models like data trusts, data commons, and data unions, and defines them based on member ownership, democratic governance, and data sovereignty. It explores applications in sectors like healthcare, agriculture, and construction. Despite their potential, data cooperatives face challenges in coordination, scalability, and member engagement, requiring innovative governance strategies, robust technical systems, and mechanisms to align member interests with cooperative goals. The paper concludes by advocating for data cooperatives as a sustainable, democratic, and ethical model for the future data economy."
      },
      {
        "id": "oai:arXiv.org:2504.10063v1",
        "title": "Hallucination Detection in LLMs via Topological Divergence on Attention Graphs",
        "link": "https://arxiv.org/abs/2504.10063",
        "author": "Alexandra Bazarova, Aleksandr Yugay, Andrey Shulga, Alina Ermilova, Andrei Volodichev, Konstantin Polev, Julia Belikova, Rauf Parchiev, Dmitry Simakov, Maxim Savchenko, Andrey Savchenko, Serguei Barannikov, Alexey Zaytsev",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10063v1 Announce Type: new \nAbstract: Hallucination, i.e., generating factually incorrect content, remains a critical challenge for large language models (LLMs). We introduce TOHA, a TOpology-based HAllucination detector in the RAG setting, which leverages a topological divergence metric to quantify the structural properties of graphs induced by attention matrices. Examining the topological divergence between prompt and response subgraphs reveals consistent patterns: higher divergence values in specific attention heads correlate with hallucinated outputs, independent of the dataset. Extensive experiments, including evaluation on question answering and data-to-text tasks, show that our approach achieves state-of-the-art or competitive results on several benchmarks, two of which were annotated by us and are being publicly released to facilitate further research. Beyond its strong in-domain performance, TOHA maintains remarkable domain transferability across multiple open-source LLMs. Our findings suggest that analyzing the topological structure of attention matrices can serve as an efficient and robust indicator of factual reliability in LLMs."
      },
      {
        "id": "oai:arXiv.org:2504.10065v1",
        "title": "A Computational Cognitive Model for Processing Repetitions of Hierarchical Relations",
        "link": "https://arxiv.org/abs/2504.10065",
        "author": "Zeng Ren, Xinyi Guan, Martin Rohrmeier",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10065v1 Announce Type: new \nAbstract: Patterns are fundamental to human cognition, enabling the recognition of structure and regularity across diverse domains. In this work, we focus on structural repeats, patterns that arise from the repetition of hierarchical relations within sequential data, and develop a candidate computational model of how humans detect and understand such structural repeats. Based on a weighted deduction system, our model infers the minimal generative process of a given sequence in the form of a Template program, a formalism that enriches the context-free grammar with repetition combinators. Such representation efficiently encodes the repetition of sub-computations in a recursive manner. As a proof of concept, we demonstrate the expressiveness of our model on short sequences from music and action planning. The proposed model offers broader insights into the mental representations and cognitive mechanisms underlying human pattern recognition."
      },
      {
        "id": "oai:arXiv.org:2504.10067v1",
        "title": "Undermining Federated Learning Accuracy in EdgeIoT via Variational Graph Auto-Encoders",
        "link": "https://arxiv.org/abs/2504.10067",
        "author": "Kai Li, Shuyan Hu, Bochun Wu, Sai Zou, Wei Ni, Falko Dressler",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10067v1 Announce Type: new \nAbstract: EdgeIoT represents an approach that brings together mobile edge computing with Internet of Things (IoT) devices, allowing for data processing close to the data source. Sending source data to a server is bandwidth-intensive and may compromise privacy. Instead, federated learning allows each device to upload a shared machine-learning model update with locally processed data. However, this technique, which depends on aggregating model updates from various IoT devices, is vulnerable to attacks from malicious entities that may inject harmful data into the learning process. This paper introduces a new attack method targeting federated learning in EdgeIoT, known as data-independent model manipulation attack. This attack does not rely on training data from the IoT devices but instead uses an adversarial variational graph auto-encoder (AV-GAE) to create malicious model updates by analyzing benign model updates intercepted during communication. AV-GAE identifies and exploits structural relationships between benign models and their training data features. By manipulating these structural correlations, the attack maximizes the training loss of the federated learning system, compromising its overall effectiveness."
      },
      {
        "id": "oai:arXiv.org:2504.10068v1",
        "title": "Mavors: Multi-granularity Video Representation for Multimodal Large Language Model",
        "link": "https://arxiv.org/abs/2504.10068",
        "author": "Yang Shi, Jiaheng Liu, Yushuo Guan, Zhenhua Wu, Yuanxing Zhang, Zihao Wang, Weihong Lin, Jingyun Hua, Zekun Wang, Xinlong Chen, Bohan Zeng, Wentao Zhang, Fuzheng Zhang, Wenjing Yang, Di Zhang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10068v1 Announce Type: new \nAbstract: Long-context video understanding in multimodal large language models (MLLMs) faces a critical challenge: balancing computational efficiency with the retention of fine-grained spatio-temporal patterns. Existing approaches (e.g., sparse sampling, dense sampling with low resolution, and token compression) suffer from significant information loss in temporal dynamics, spatial details, or subtle interactions, particularly in videos with complex motion or varying resolutions. To address this, we propose $\\mathbf{Mavors}$, a novel framework that introduces $\\mathbf{M}$ulti-gr$\\mathbf{a}$nularity $\\mathbf{v}$ide$\\mathbf{o}$ $\\mathbf{r}$epre$\\mathbf{s}$entation for holistic long-video modeling. Specifically, Mavors directly encodes raw video content into latent representations through two core components: 1) an Intra-chunk Vision Encoder (IVE) that preserves high-resolution spatial features via 3D convolutions and Vision Transformers, and 2) an Inter-chunk Feature Aggregator (IFA) that establishes temporal coherence across chunks using transformer-based dependency modeling with chunk-level rotary position encodings. Moreover, the framework unifies image and video understanding by treating images as single-frame videos via sub-image decomposition. Experiments across diverse benchmarks demonstrate Mavors' superiority in maintaining both spatial fidelity and temporal continuity, significantly outperforming existing methods in tasks requiring fine-grained spatio-temporal reasoning."
      },
      {
        "id": "oai:arXiv.org:2504.10070v1",
        "title": "DTFSal: Audio-Visual Dynamic Token Fusion for Video Saliency Prediction",
        "link": "https://arxiv.org/abs/2504.10070",
        "author": "Kiana Hoshanfar, Alireza Hosseini, Ahmad Kalhor, Babak Nadjar Araabi",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10070v1 Announce Type: new \nAbstract: Audio-visual saliency prediction aims to mimic human visual attention by identifying salient regions in videos through the integration of both visual and auditory information. Although visual-only approaches have significantly advanced, effectively incorporating auditory cues remains challenging due to complex spatio-temporal interactions and high computational demands. To address these challenges, we propose Dynamic Token Fusion Saliency (DFTSal), a novel audio-visual saliency prediction framework designed to balance accuracy with computational efficiency. Our approach features a multi-scale visual encoder equipped with two novel modules: the Learnable Token Enhancement Block (LTEB), which adaptively weights tokens to emphasize crucial saliency cues, and the Dynamic Learnable Token Fusion Block (DLTFB), which employs a shifting operation to reorganize and merge features, effectively capturing long-range dependencies and detailed spatial information. In parallel, an audio branch processes raw audio signals to extract meaningful auditory features. Both visual and audio features are integrated using our Adaptive Multimodal Fusion Block (AMFB), which employs local, global, and adaptive fusion streams for precise cross-modal fusion. The resulting fused features are processed by a hierarchical multi-decoder structure, producing accurate saliency maps. Extensive evaluations on six audio-visual benchmarks demonstrate that DFTSal achieves SOTA performance while maintaining computational efficiency."
      },
      {
        "id": "oai:arXiv.org:2504.10076v1",
        "title": "Towards Scalable Bayesian Optimization via Gradient-Informed Bayesian Neural Networks",
        "link": "https://arxiv.org/abs/2504.10076",
        "author": "Georgios Makrygiorgos, Joshua Hang Sai Ip, Ali Mesbah",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10076v1 Announce Type: new \nAbstract: Bayesian optimization (BO) is a widely used method for data-driven optimization that generally relies on zeroth-order data of objective function to construct probabilistic surrogate models. These surrogates guide the exploration-exploitation process toward finding global optimum. While Gaussian processes (GPs) are commonly employed as surrogates of the unknown objective function, recent studies have highlighted the potential of Bayesian neural networks (BNNs) as scalable and flexible alternatives. Moreover, incorporating gradient observations into GPs, when available, has been shown to improve BO performance. However, the use of gradients within BNN surrogates remains unexplored. By leveraging automatic differentiation, gradient information can be seamlessly integrated into BNN training, resulting in more informative surrogates for BO. We propose a gradient-informed loss function for BNN training, effectively augmenting function observations with local gradient information. The effectiveness of this approach is demonstrated on well-known benchmarks in terms of improved BNN predictions and faster BO convergence as the number of decision variables increases."
      },
      {
        "id": "oai:arXiv.org:2504.10077v1",
        "title": "Towards Quantifying Commonsense Reasoning with Mechanistic Insights",
        "link": "https://arxiv.org/abs/2504.10077",
        "author": "Abhinav Joshi, Areeb Ahmad, Divyaksh Shukla, Ashutosh Modi",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10077v1 Announce Type: new \nAbstract: Commonsense reasoning deals with the implicit knowledge that is well understood by humans and typically acquired via interactions with the world. In recent times, commonsense reasoning and understanding of various LLMs have been evaluated using text-based tasks. In this work, we argue that a proxy of this understanding can be maintained as a graphical structure that can further help to perform a rigorous evaluation of commonsense reasoning abilities about various real-world activities. We create an annotation scheme for capturing this implicit knowledge in the form of a graphical structure for 37 daily human activities. We find that the created resource can be used to frame an enormous number of commonsense queries (~ 10^{17}), facilitating rigorous evaluation of commonsense reasoning in LLMs. Moreover, recently, the remarkable performance of LLMs has raised questions about whether these models are truly capable of reasoning in the wild and, in general, how reasoning occurs inside these models. In this resource paper, we bridge this gap by proposing design mechanisms that facilitate research in a similar direction. Our findings suggest that the reasoning components are localized in LLMs that play a prominent role in decision-making when prompted with a commonsense query."
      },
      {
        "id": "oai:arXiv.org:2504.10079v1",
        "title": "Hierarchical Relation-augmented Representation Generalization for Few-shot Action Recognition",
        "link": "https://arxiv.org/abs/2504.10079",
        "author": "Hongyu Qu, Ling Xing, Rui Yan, Yazhou Yao, Guo-Sen Xie, Xiangbo Shu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10079v1 Announce Type: new \nAbstract: Few-shot action recognition (FSAR) aims to recognize novel action categories with few exemplars. Existing methods typically learn frame-level representations independently for each video by designing various inter-frame temporal modeling strategies. However, they neglect explicit relation modeling between videos and tasks, thus failing to capture shared temporal patterns across videos and reuse temporal knowledge from historical tasks. In light of this, we propose HR2G-shot, a Hierarchical Relation-augmented Representation Generalization framework for FSAR, which unifies three types of relation modeling (inter-frame, inter-video, and inter-task) to learn task-specific temporal patterns from a holistic view. In addition to conducting inter-frame temporal interactions, we further devise two components to respectively explore inter-video and inter-task relationships: i) Inter-video Semantic Correlation (ISC) performs cross-video frame-level interactions in a fine-grained manner, thereby capturing task-specific query features and learning intra- and inter-class temporal correlations among support features; ii) Inter-task Knowledge Transfer (IKT) retrieves and aggregates relevant temporal knowledge from the bank, which stores diverse temporal patterns from historical tasks. Extensive experiments on five benchmarks show that HR2G-shot outperforms current top-leading FSAR methods."
      },
      {
        "id": "oai:arXiv.org:2504.10080v1",
        "title": "Learning to Harmonize Cross-vendor X-ray Images by Non-linear Image Dynamics Correction",
        "link": "https://arxiv.org/abs/2504.10080",
        "author": "Yucheng Lu, Shunxin Wang, Dovile Juodelyte, Veronika Cheplygina",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10080v1 Announce Type: new \nAbstract: In this paper, we explore how conventional image enhancement can improve model robustness in medical image analysis. By applying commonly used normalization methods to images from various vendors and studying their influence on model generalization in transfer learning, we show that the nonlinear characteristics of domain-specific image dynamics cannot be addressed by simple linear transforms. To tackle this issue, we reformulate the image harmonization task as an exposure correction problem and propose a method termed Global Deep Curve Estimation (GDCE) to reduce domain-specific exposure mismatch. GDCE performs enhancement via a pre-defined polynomial function and is trained with the help of a ``domain discriminator'', aiming to improve model transparency in downstream tasks compared to existing black-box methods."
      },
      {
        "id": "oai:arXiv.org:2504.10084v1",
        "title": "UP-Person: Unified Parameter-Efficient Transfer Learning for Text-based Person Retrieval",
        "link": "https://arxiv.org/abs/2504.10084",
        "author": "Yating Liu, Yaowei Li, Xiangyuan Lan, Wenming Yang, Zimo Liu, Qingmin Liao",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10084v1 Announce Type: new \nAbstract: Text-based Person Retrieval (TPR) as a multi-modal task, which aims to retrieve the target person from a pool of candidate images given a text description, has recently garnered considerable attention due to the progress of contrastive visual-language pre-trained model. Prior works leverage pre-trained CLIP to extract person visual and textual features and fully fine-tune the entire network, which have shown notable performance improvements compared to uni-modal pre-training models. However, full-tuning a large model is prone to overfitting and hinders the generalization ability. In this paper, we propose a novel Unified Parameter-Efficient Transfer Learning (PETL) method for Text-based Person Retrieval (UP-Person) to thoroughly transfer the multi-modal knowledge from CLIP. Specifically, UP-Person simultaneously integrates three lightweight PETL components including Prefix, LoRA and Adapter, where Prefix and LoRA are devised together to mine local information with task-specific information prompts, and Adapter is designed to adjust global feature representations. Additionally, two vanilla submodules are optimized to adapt to the unified architecture of TPR. For one thing, S-Prefix is proposed to boost attention of prefix and enhance the gradient propagation of prefix tokens, which improves the flexibility and performance of the vanilla prefix. For another thing, L-Adapter is designed in parallel with layer normalization to adjust the overall distribution, which can resolve conflicts caused by overlap and interaction among multiple submodules. Extensive experimental results demonstrate that our UP-Person achieves state-of-the-art results across various person retrieval datasets, including CUHK-PEDES, ICFG-PEDES and RSTPReid while merely fine-tuning 4.7\\% parameters. Code is available at https://github.com/Liu-Yating/UP-Person."
      },
      {
        "id": "oai:arXiv.org:2504.10090v1",
        "title": "CameraBench: Benchmarking Visual Reasoning in MLLMs via Photography",
        "link": "https://arxiv.org/abs/2504.10090",
        "author": "I-Sheng Fang, Jun-Cheng Chen",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10090v1 Announce Type: new \nAbstract: Large language models (LLMs) and multimodal large language models (MLLMs) have significantly advanced artificial intelligence. However, visual reasoning, reasoning involving both visual and textual inputs, remains underexplored. Recent advancements, including the reasoning models like OpenAI o1 and Gemini 2.0 Flash Thinking, which incorporate image inputs, have opened this capability. In this ongoing work, we focus specifically on photography-related tasks because a photo is a visual snapshot of the physical world where the underlying physics (i.e., illumination, blur extent, etc.) interplay with the camera parameters. Successfully reasoning from the visual information of a photo to identify these numerical camera settings requires the MLLMs to have a deeper understanding of the underlying physics for precise visual comprehension, representing a challenging and intelligent capability essential for practical applications like photography assistant agents. We aim to evaluate MLLMs on their ability to distinguish visual differences related to numerical camera settings, extending a methodology previously proposed for vision-language models (VLMs). Our preliminary results demonstrate the importance of visual reasoning in photography-related tasks. Moreover, these results show that no single MLLM consistently dominates across all evaluation tasks, demonstrating ongoing challenges and opportunities in developing MLLMs with better visual reasoning."
      },
      {
        "id": "oai:arXiv.org:2504.10097v1",
        "title": "STaRFormer: Semi-Supervised Task-Informed Representation Learning via Dynamic Attention-Based Regional Masking for Sequential Data",
        "link": "https://arxiv.org/abs/2504.10097",
        "author": "Maxmilian Forstenh\\\"ausler, Daniel K\\\"ulzer, Christos Anagnostopoulos, Shameem Puthiya Parambath, Natascha Weber",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10097v1 Announce Type: new \nAbstract: Accurate predictions using sequential spatiotemporal data are crucial for various applications. Utilizing real-world data, we aim to learn the intent of a smart device user within confined areas of a vehicle's surroundings. However, in real-world scenarios, environmental factors and sensor limitations result in non-stationary and irregularly sampled data, posing significant challenges. To address these issues, we developed a Transformer-based approach, STaRFormer, which serves as a universal framework for sequential modeling. STaRFormer employs a novel, dynamic attention-based regional masking scheme combined with semi-supervised contrastive learning to enhance task-specific latent representations. Comprehensive experiments on 15 datasets varying in types (including non-stationary and irregularly sampled), domains, sequence lengths, training samples, and applications, demonstrate the efficacy and practicality of STaRFormer. We achieve notable improvements over state-of-the-art approaches. Code and data will be made available."
      },
      {
        "id": "oai:arXiv.org:2504.10105v1",
        "title": "Global and Local Mamba Network for Multi-Modality Medical Image Super-Resolution",
        "link": "https://arxiv.org/abs/2504.10105",
        "author": "Zexin Ji, Beiji Zou, Xiaoyan Kui, Sebastien Thureau, Su Ruan",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10105v1 Announce Type: new \nAbstract: Convolutional neural networks and Transformer have made significant progresses in multi-modality medical image super-resolution. However, these methods either have a fixed receptive field for local learning or significant computational burdens for global learning, limiting the super-resolution performance. To solve this problem, State Space Models, notably Mamba, is introduced to efficiently model long-range dependencies in images with linear computational complexity. Relying on the Mamba and the fact that low-resolution images rely on global information to compensate for missing details, while high-resolution reference images need to provide more local details for accurate super-resolution, we propose a global and local Mamba network (GLMamba) for multi-modality medical image super-resolution. To be specific, our GLMamba is a two-branch network equipped with a global Mamba branch and a local Mamba branch. The global Mamba branch captures long-range relationships in low-resolution inputs, and the local Mamba branch focuses more on short-range details in high-resolution reference images. We also use the deform block to adaptively extract features of both branches to enhance the representation ability. A modulator is designed to further enhance deformable features in both global and local Mamba blocks. To fully integrate the reference image for low-resolution image super-resolution, we further develop a multi-modality feature fusion block to adaptively fuse features by considering similarities, differences, and complementary aspects between modalities. In addition, a contrastive edge loss (CELoss) is developed for sufficient enhancement of edge textures and contrast in medical images."
      },
      {
        "id": "oai:arXiv.org:2504.10106v1",
        "title": "SoccerNet-v3D: Leveraging Sports Broadcast Replays for 3D Scene Understanding",
        "link": "https://arxiv.org/abs/2504.10106",
        "author": "Marc Guti\\'errez-P\\'erez, Antonio Agudo",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10106v1 Announce Type: new \nAbstract: Sports video analysis is a key domain in computer vision, enabling detailed spatial understanding through multi-view correspondences. In this work, we introduce SoccerNet-v3D and ISSIA-3D, two enhanced and scalable datasets designed for 3D scene understanding in soccer broadcast analysis. These datasets extend SoccerNet-v3 and ISSIA by incorporating field-line-based camera calibration and multi-view synchronization, enabling 3D object localization through triangulation. We propose a monocular 3D ball localization task built upon the triangulation of ground-truth 2D ball annotations, along with several calibration and reprojection metrics to assess annotation quality on demand. Additionally, we present a single-image 3D ball localization method as a baseline, leveraging camera calibration and ball size priors to estimate the ball's position from a monocular viewpoint. To further refine 2D annotations, we introduce a bounding box optimization technique that ensures alignment with the 3D scene representation. Our proposed datasets establish new benchmarks for 3D soccer scene understanding, enhancing both spatial and temporal analysis in sports analytics. Finally, we provide code to facilitate access to our annotations and the generation pipelines for the datasets."
      },
      {
        "id": "oai:arXiv.org:2504.10117v1",
        "title": "AGO: Adaptive Grounding for Open World 3D Occupancy Prediction",
        "link": "https://arxiv.org/abs/2504.10117",
        "author": "Peizheng Li, Shuxiao Ding, You Zhou, Qingwen Zhang, Onat Inak, Larissa Triess, Niklas Hanselmann, Marius Cordts, Andreas Zell",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10117v1 Announce Type: new \nAbstract: Open-world 3D semantic occupancy prediction aims to generate a voxelized 3D representation from sensor inputs while recognizing both known and unknown objects. Transferring open-vocabulary knowledge from vision-language models (VLMs) offers a promising direction but remains challenging. However, methods based on VLM-derived 2D pseudo-labels with traditional supervision are limited by a predefined label space and lack general prediction capabilities. Direct alignment with pretrained image embeddings, on the other hand, fails to achieve reliable performance due to often inconsistent image and text representations in VLMs. To address these challenges, we propose AGO, a novel 3D occupancy prediction framework with adaptive grounding to handle diverse open-world scenarios. AGO first encodes surrounding images and class prompts into 3D and text embeddings, respectively, leveraging similarity-based grounding training with 3D pseudo-labels. Additionally, a modality adapter maps 3D embeddings into a space aligned with VLM-derived image embeddings, reducing modality gaps. Experiments on Occ3D-nuScenes show that AGO improves unknown object prediction in zero-shot and few-shot transfer while achieving state-of-the-art closed-world self-supervised performance, surpassing prior methods by 4.09 mIoU."
      },
      {
        "id": "oai:arXiv.org:2504.10123v1",
        "title": "M2S-RoAD: Multi-Modal Semantic Segmentation for Road Damage Using Camera and LiDAR Data",
        "link": "https://arxiv.org/abs/2504.10123",
        "author": "Tzu-Yun Tseng, Hongyu Lyu, Josephine Li, Julie Stephany Berrio, Mao Shan, Stewart Worrall",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10123v1 Announce Type: new \nAbstract: Road damage can create safety and comfort challenges for both human drivers and autonomous vehicles (AVs). This damage is particularly prevalent in rural areas due to less frequent surveying and maintenance of roads. Automated detection of pavement deterioration can be used as an input to AVs and driver assistance systems to improve road safety. Current research in this field has predominantly focused on urban environments driven largely by public datasets, while rural areas have received significantly less attention. This paper introduces M2S-RoAD, a dataset for the semantic segmentation of different classes of road damage. M2S-RoAD was collected in various towns across New South Wales, Australia, and labelled for semantic segmentation to identify nine distinct types of road damage. This dataset will be released upon the acceptance of the paper."
      },
      {
        "id": "oai:arXiv.org:2504.10136v1",
        "title": "Uncertainty Propagation in the Fast Fourier Transform",
        "link": "https://arxiv.org/abs/2504.10136",
        "author": "Luca Schmid, Charlotte Muth, Laurent Schmalen",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10136v1 Announce Type: new \nAbstract: We address the problem of uncertainty propagation in the discrete Fourier transform by modeling the fast Fourier transform as a factor graph. Building on this representation, we propose an efficient framework for approximate Bayesian inference using belief propagation (BP) and expectation propagation, extending its applicability beyond Gaussian assumptions. By leveraging an appropriate BP message representation and a suitable schedule, our method achieves stable convergence with accurate mean and variance estimates. Numerical experiments in representative scenarios from communications demonstrate the practical potential of the proposed framework for uncertainty-aware inference in probabilistic systems operating across both time and frequency domain."
      },
      {
        "id": "oai:arXiv.org:2504.10141v1",
        "title": "The Impact of Model Zoo Size and Composition on Weight Space Learning",
        "link": "https://arxiv.org/abs/2504.10141",
        "author": "Damian Falk, Konstantin Sch\\\"urholt, Damian Borth",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10141v1 Announce Type: new \nAbstract: Re-using trained neural network models is a common strategy to reduce training cost and transfer knowledge. Weight space learning - using the weights of trained models as data modality - is a promising new field to re-use populations of pre-trained models for future tasks. Approaches in this field have demonstrated high performance both on model analysis and weight generation tasks. However, until now their learning setup requires homogeneous model zoos where all models share the same exact architecture, limiting their capability to generalize beyond the population of models they saw during training. In this work, we remove this constraint and propose a modification to a common weight space learning method to accommodate training on heterogeneous populations of models. We further investigate the resulting impact of model diversity on generating unseen neural network model weights for zero-shot knowledge transfer. Our extensive experimental evaluation shows that including models with varying underlying image datasets has a high impact on performance and generalization, for both in- and out-of-distribution settings. Code is available on github.com/HSG-AIML/MultiZoo-SANE."
      },
      {
        "id": "oai:arXiv.org:2504.10143v1",
        "title": "Negate or Embrace: On How Misalignment Shapes Multimodal Representation Learning",
        "link": "https://arxiv.org/abs/2504.10143",
        "author": "Yichao Cai, Yuhang Liu, Erdun Gao, Tianjiao Jiang, Zhen Zhang, Anton van den Hengel, Javen Qinfeng Shi",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10143v1 Announce Type: new \nAbstract: Multimodal representation learning, exemplified by multimodal contrastive learning (MMCL) using image-text pairs, aims to learn powerful representations by aligning cues across modalities. This approach relies on the core assumption that the exemplar image-text pairs constitute two representations of an identical concept. However, recent research has revealed that real-world datasets often exhibit misalignment. There are two distinct viewpoints on how to address this issue: one suggests mitigating the misalignment, and the other leveraging it. We seek here to reconcile these seemingly opposing perspectives, and to provide a practical guide for practitioners. Using latent variable models we thus formalize misalignment by introducing two specific mechanisms: selection bias, where some semantic variables are missing, and perturbation bias, where semantic variables are distorted -- both affecting latent variables shared across modalities. Our theoretical analysis demonstrates that, under mild assumptions, the representations learned by MMCL capture exactly the information related to the subset of the semantic variables invariant to selection and perturbation biases. This provides a unified perspective for understanding misalignment. Based on this, we further offer actionable insights into how misalignment should inform the design of real-world ML systems. We validate our theoretical findings through extensive empirical studies on both synthetic data and real image-text datasets, shedding light on the nuanced impact of misalignment on multimodal representation learning."
      },
      {
        "id": "oai:arXiv.org:2504.10146v1",
        "title": "GeoUni: A Unified Model for Generating Geometry Diagrams, Problems and Problem Solutions",
        "link": "https://arxiv.org/abs/2504.10146",
        "author": "Jo-Ku Cheng, Zeren Zhang, Ran Chen, Jingyang Deng, Ziran Qin, Jinwen Ma",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10146v1 Announce Type: new \nAbstract: We propose GeoUni, the first unified geometry expert model capable of generating problem solutions and diagrams within a single framework in a way that enables the creation of unique and individualized geometry problems. Traditionally, solving geometry problems and generating diagrams have been treated as separate tasks in machine learning, with no models successfully integrating both to support problem creation. However, we believe that mastery in geometry requires frictionless integration of all of these skills, from solving problems to visualizing geometric relationships, and finally, crafting tailored problems. Our extensive experiments demonstrate that GeoUni, with only 1.5B parameters, achieves performance comparable to larger models such as DeepSeek-R1 with 671B parameters in geometric reasoning tasks. GeoUni also excels in generating precise geometric diagrams, surpassing both text-to-image models and unified models, including the GPT-4o image generation. Most importantly, GeoUni is the only model capable of successfully generating textual problems with matching diagrams based on specific knowledge points, thus offering a wider range of capabilities that extend beyond current models."
      },
      {
        "id": "oai:arXiv.org:2504.10148v1",
        "title": "Hierarchical and Step-Layer-Wise Tuning of Attention Specialty for Multi-Instance Synthesis in Diffusion Transformers",
        "link": "https://arxiv.org/abs/2504.10148",
        "author": "Chunyang Zhang, Zhenhong Sun, Zhicheng Zhang, Junyan Wang, Yu Zhang, Dong Gong, Huadong Mo, Daoyi Dong",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10148v1 Announce Type: new \nAbstract: Text-to-image (T2I) generation models often struggle with multi-instance synthesis (MIS), where they must accurately depict multiple distinct instances in a single image based on complex prompts detailing individual features. Traditional MIS control methods for UNet architectures like SD v1.5/SDXL fail to adapt to DiT-based models like FLUX and SD v3.5, which rely on integrated attention between image and text tokens rather than text-image cross-attention. To enhance MIS in DiT, we first analyze the mixed attention mechanism in DiT. Our token-wise and layer-wise analysis of attention maps reveals a hierarchical response structure: instance tokens dominate early layers, background tokens in middle layers, and attribute tokens in later layers. Building on this observation, we propose a training-free approach for enhancing MIS in DiT-based models with hierarchical and step-layer-wise attention specialty tuning (AST). AST amplifies key regions while suppressing irrelevant areas in distinct attention maps across layers and steps, guided by the hierarchical structure. This optimizes multimodal interactions by hierarchically decoupling the complex prompts with instance-based sketches. We evaluate our approach using upgraded sketch-based layouts for the T2I-CompBench and customized complex scenes. Both quantitative and qualitative results confirm our method enhances complex layout generation, ensuring precise instance placement and attribute representation in MIS."
      },
      {
        "id": "oai:arXiv.org:2504.10149v1",
        "title": "BoTTA: Benchmarking on-device Test Time Adaptation",
        "link": "https://arxiv.org/abs/2504.10149",
        "author": "Michal Danilowski, Soumyajit Chatterjee, Abhirup Ghosh",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10149v1 Announce Type: new \nAbstract: The performance of deep learning models depends heavily on test samples at runtime, and shifts from the training data distribution can significantly reduce accuracy. Test-time adaptation (TTA) addresses this by adapting models during inference without requiring labeled test data or access to the original training set. While research has explored TTA from various perspectives like algorithmic complexity, data and class distribution shifts, model architectures, and offline versus continuous learning, constraints specific to mobile and edge devices remain underexplored. We propose BoTTA, a benchmark designed to evaluate TTA methods under practical constraints on mobile and edge devices. Our evaluation targets four key challenges caused by limited resources and usage conditions: (i) limited test samples, (ii) limited exposure to categories, (iii) diverse distribution shifts, and (iv) overlapping shifts within a sample. We assess state-of-the-art TTA methods under these scenarios using benchmark datasets and report system-level metrics on a real testbed. Furthermore, unlike prior work, we align with on-device requirements by advocating periodic adaptation instead of continuous inference-time adaptation. Experiments reveal key insights: many recent TTA algorithms struggle with small datasets, fail to generalize to unseen categories, and depend on the diversity and complexity of distribution shifts. BoTTA also reports device-specific resource use. For example, while SHOT improves accuracy by $2.25\\times$ with $512$ adaptation samples, it uses $1.08\\times$ peak memory on Raspberry Pi versus the base model. BoTTA offers actionable guidance for TTA in real-world, resource-constrained deployments."
      },
      {
        "id": "oai:arXiv.org:2504.10151v1",
        "title": "Continual learning for rotating machinery fault diagnosis with cross-domain environmental and operational variations",
        "link": "https://arxiv.org/abs/2504.10151",
        "author": "Diogo Risca, Afonso Louren\\c{c}o, Goreti Marreiros",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10151v1 Announce Type: new \nAbstract: Although numerous machine learning models exist to detect issues like rolling bearing strain and deformation, typically caused by improper mounting, overloading, or poor lubrication, these models often struggle to isolate faults from the noise of real-world operational and environmental variability. Conditions such as variable loads, high temperatures, stress, and rotational speeds can mask early signs of failure, making reliable detection challenging. To address these limitations, this work proposes a continual deep learning approach capable of learning across domains that share underlying structure over time. This approach goes beyond traditional accuracy metrics by addressing four second-order challenges: catastrophic forgetting (where new learning overwrites past knowledge), lack of plasticity (where models fail to adapt to new data), forward transfer (using past knowledge to improve future learning), and backward transfer (refining past knowledge with insights from new domains). The method comprises a feature generator and domain-specific classifiers, allowing capacity to grow as new domains emerge with minimal interference, while an experience replay mechanism selectively revisits prior domains to mitigate forgetting. Moreover, nonlinear dependencies across domains are exploited by prioritizing replay from those with the highest prior errors, refining models based on most informative past experiences. Experiments show high average domain accuracy (up to 88.96%), with forgetting measures as low as .0027 across non-stationary class-incremental environments."
      },
      {
        "id": "oai:arXiv.org:2504.10157v1",
        "title": "SocioVerse: A World Model for Social Simulation Powered by LLM Agents and A Pool of 10 Million Real-World Users",
        "link": "https://arxiv.org/abs/2504.10157",
        "author": "Xinnong Zhang, Jiayu Lin, Xinyi Mou, Shiyue Yang, Xiawei Liu, Libo Sun, Hanjia Lyu, Yihang Yang, Weihong Qi, Yue Chen, Guanying Li, Ling Yan, Yao Hu, Siming Chen, Yu Wang, Jingxuan Huang, Jiebo Luo, Shiping Tang, Libo Wu, Baohua Zhou, Zhongyu Wei",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10157v1 Announce Type: new \nAbstract: Social simulation is transforming traditional social science research by modeling human behavior through interactions between virtual individuals and their environments. With recent advances in large language models (LLMs), this approach has shown growing potential in capturing individual differences and predicting group behaviors. However, existing methods face alignment challenges related to the environment, target users, interaction mechanisms, and behavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven world model for social simulation. Our framework features four powerful alignment components and a user pool of 10 million real individuals. To validate its effectiveness, we conducted large-scale simulation experiments across three distinct domains: politics, news, and economics. Results demonstrate that SocioVerse can reflect large-scale population dynamics while ensuring diversity, credibility, and representativeness through standardized procedures and minimal manual adjustments."
      },
      {
        "id": "oai:arXiv.org:2504.10158v1",
        "title": "COUNTS: Benchmarking Object Detectors and Multimodal Large Language Models under Distribution Shifts",
        "link": "https://arxiv.org/abs/2504.10158",
        "author": "Jiansheng Li, Xingxuan Zhang, Hao Zou, Yige Guo, Renzhe Xu, Yilong Liu, Chuzhao Zhu, Yue He, Peng Cui",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10158v1 Announce Type: new \nAbstract: Current object detectors often suffer significant perfor-mance degradation in real-world applications when encountering distributional shifts. Consequently, the out-of-distribution (OOD) generalization capability of object detectors has garnered increasing attention from researchers. Despite this growing interest, there remains a lack of a large-scale, comprehensive dataset and evaluation benchmark with fine-grained annotations tailored to assess the OOD generalization on more intricate tasks like object detection and grounding. To address this gap, we introduce COUNTS, a large-scale OOD dataset with object-level annotations. COUNTS encompasses 14 natural distributional shifts, over 222K samples, and more than 1,196K labeled bounding boxes. Leveraging COUNTS, we introduce two novel benchmarks: O(OD)2 and OODG. O(OD)2 is designed to comprehensively evaluate the OOD generalization capabilities of object detectors by utilizing controlled distribution shifts between training and testing data. OODG, on the other hand, aims to assess the OOD generalization of grounding abilities in multimodal large language models (MLLMs). Our findings reveal that, while large models and extensive pre-training data substantially en hance performance in in-distribution (IID) scenarios, significant limitations and opportunities for improvement persist in OOD contexts for both object detectors and MLLMs. In visual grounding tasks, even the advanced GPT-4o and Gemini-1.5 only achieve 56.7% and 28.0% accuracy, respectively. We hope COUNTS facilitates advancements in the development and assessment of robust object detectors and MLLMs capable of maintaining high performance under distributional shifts."
      },
      {
        "id": "oai:arXiv.org:2504.10160v1",
        "title": "MT-R1-Zero: Advancing LLM-based Machine Translation via R1-Zero-like Reinforcement Learning",
        "link": "https://arxiv.org/abs/2504.10160",
        "author": "Zhaopeng Feng, Shaosheng Cao, Jiahan Ren, Jiayuan Su, Ruizhe Chen, Yan Zhang, Zhe Xu, Yao Hu, Jian Wu, Zuozhu Liu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10160v1 Announce Type: new \nAbstract: Large-scale reinforcement learning (RL) methods have proven highly effective in enhancing the reasoning abilities of large language models (LLMs), particularly for tasks with verifiable solutions such as mathematics and coding. However, applying this idea to machine translation (MT), where outputs are flexibly formatted and difficult to automatically evaluate with explicit rules, remains underexplored. In this work, we introduce MT-R1-Zero, the first open-source adaptation of the R1-Zero RL framework for MT without supervised fine-tuning or cold-start. We propose a rule-metric mixed reward mechanism to guide LLMs towards improved translation quality via emergent reasoning. On the WMT 24 English-Chinese benchmark, our MT-R1-Zero-3B-Mix achieves competitive performance, surpassing TowerInstruct-7B-v0.2 by an average of 1.26 points. Meanwhile, our MT-R1-Zero-7B-Mix attains a high average score of 62.25 across all metrics, placing it on par with advanced proprietary models such as GPT-4o and Claude-3.5-Sonnet, while the MT-R1-Zero-7B-Sem variant achieves state-of-the-art scores on semantic metrics. Moreover, our work exhibits strong generalization capabilities on out-of-distribution MT tasks, robustly supporting multilingual and low-resource settings. Extensive analysis of model behavior across different initializations and reward metrics offers pioneering insight into the critical role of reward design, LLM adaptability, training dynamics, and emergent reasoning patterns within the R1-Zero paradigm for MT. Our code is available at https://github.com/fzp0424/MT-R1-Zero."
      },
      {
        "id": "oai:arXiv.org:2504.10165v1",
        "title": "WildLive: Near Real-time Visual Wildlife Tracking onboard UAVs",
        "link": "https://arxiv.org/abs/2504.10165",
        "author": "Nguyen Ngoc Dat, Tom Richardson, Matthew Watson, Kilian Meier, Jenna Kline, Sid Reid, Guy Maalouf, Duncan Hine, Majid Mirmehdi, Tilo Burghardt",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10165v1 Announce Type: new \nAbstract: Live tracking of wildlife via high-resolution video processing directly onboard drones is widely unexplored and most existing solutions rely on streaming video to ground stations to support navigation. Yet, both autonomous animal-reactive flight control beyond visual line of sight and/or mission-specific individual and behaviour recognition tasks rely to some degree on this capability. In response, we introduce WildLive -- a near real-time animal detection and tracking framework for high-resolution imagery running directly onboard uncrewed aerial vehicles (UAVs). The system performs multi-animal detection and tracking at 17fps+ for HD and 7fps+ on 4K video streams suitable for operation during higher altitude flights to minimise animal disturbance. Our system is optimised for Jetson Orin AGX onboard hardware. It integrates the efficiency of sparse optical flow tracking and mission-specific sampling with device-optimised and proven YOLO-driven object detection and segmentation techniques. Essentially, computational resource is focused onto spatio-temporal regions of high uncertainty to significantly improve UAV processing speeds without domain-specific loss of accuracy. Alongside, we introduce our WildLive dataset, which comprises 200k+ annotated animal instances across 19k+ frames from 4K UAV videos collected at the Ol Pejeta Conservancy in Kenya. All frames contain ground truth bounding boxes, segmentation masks, as well as individual tracklets and tracking point trajectories. We compare our system against current object tracking approaches including OC-SORT, ByteTrack, and SORT. Our multi-animal tracking experiments with onboard hardware confirm that near real-time high-resolution wildlife tracking is possible on UAVs whilst maintaining high accuracy levels as needed for future navigational and mission-specific animal-centric operational autonomy."
      },
      {
        "id": "oai:arXiv.org:2504.10166v1",
        "title": "Fact-Checking with Contextual Narratives: Leveraging Retrieval-Augmented LLMs for Social Media Analysis",
        "link": "https://arxiv.org/abs/2504.10166",
        "author": "Arka Ujjal Dey, Muhammad Junaid Awan, Georgia Channing, Christian Schroeder de Witt, John Collomosse",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10166v1 Announce Type: new \nAbstract: We propose CRAVE (Cluster-based Retrieval Augmented Verification with Explanation); a novel framework that integrates retrieval-augmented Large Language Models (LLMs) with clustering techniques to address fact-checking challenges on social media. CRAVE automatically retrieves multimodal evidence from diverse, often contradictory, sources. Evidence is clustered into coherent narratives, and evaluated via an LLM-based judge to deliver fact-checking verdicts explained by evidence summaries. By synthesizing evidence from both text and image modalities and incorporating agent-based refinement, CRAVE ensures consistency and diversity in evidence representation. Comprehensive experiments demonstrate CRAVE's efficacy in retrieval precision, clustering quality, and judgment accuracy, showcasing its potential as a robust decision-support tool for fact-checkers."
      },
      {
        "id": "oai:arXiv.org:2504.10167v1",
        "title": "C-FAITH: A Chinese Fine-Grained Benchmark for Automated Hallucination Evaluation",
        "link": "https://arxiv.org/abs/2504.10167",
        "author": "Xu Zhang, Zhifei Liu, Jiahao Wang, Huixuan Zhang, Fan Xu, Junzhe Zhang, Xiaojun Wan",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10167v1 Announce Type: new \nAbstract: Despite the rapid advancement of large language models, they remain highly susceptible to generating hallucinations, which significantly hinders their widespread application. Hallucination research requires dynamic and fine-grained evaluation. However, most existing hallucination benchmarks (especially in Chinese language) rely on human annotations, making automatical and cost-effective hallucination evaluation challenging. To address this, we introduce HaluAgent, an agentic framework that automatically constructs fine-grained QA dataset based on some knowledge documents. Our experiments demonstrate that the manually designed rules and prompt optimization can improve the quality of generated data. Using HaluAgent, we construct C-FAITH, a Chinese QA hallucination benchmark created from 1,399 knowledge documents obtained from web scraping, totaling 60,702 entries. We comprehensively evaluate 16 mainstream LLMs with our proposed C-FAITH, providing detailed experimental results and analysis."
      },
      {
        "id": "oai:arXiv.org:2504.10168v1",
        "title": "HalluSearch at SemEval-2025 Task 3: A Search-Enhanced RAG Pipeline for Hallucination Detection",
        "link": "https://arxiv.org/abs/2504.10168",
        "author": "Mohamed A. Abdallah, Samhaa R. El-Beltagy",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10168v1 Announce Type: new \nAbstract: In this paper, we present HalluSearch, a multilingual pipeline designed to detect fabricated text spans in Large Language Model (LLM) outputs. Developed as part of Mu-SHROOM, the Multilingual Shared-task on Hallucinations and Related Observable Overgeneration Mistakes, HalluSearch couples retrieval-augmented verification with fine-grained factual splitting to identify and localize hallucinations in fourteen different languages. Empirical evaluations show that HalluSearch performs competitively, placing fourth in both English (within the top ten percent) and Czech. While the system's retrieval-based strategy generally proves robust, it faces challenges in languages with limited online coverage, underscoring the need for further research to ensure consistent hallucination detection across diverse linguistic contexts."
      },
      {
        "id": "oai:arXiv.org:2504.10169v1",
        "title": "Challenges in interpretability of additive models",
        "link": "https://arxiv.org/abs/2504.10169",
        "author": "Xinyu Zhang, Julien Martinelli, ST John",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10169v1 Announce Type: new \nAbstract: We review generalized additive models as a type of ``transparent'' model that has recently seen renewed interest in the deep learning community as neural additive models. We highlight multiple types of nonidentifiability in this model class and discuss challenges in interpretability, arguing for restraint when claiming ``interpretability'' or ``suitability for safety-critical applications'' of such models."
      },
      {
        "id": "oai:arXiv.org:2504.10174v1",
        "title": "LLaVA-ReID: Selective Multi-image Questioner for Interactive Person Re-Identification",
        "link": "https://arxiv.org/abs/2504.10174",
        "author": "Yiding Lu, Mouxing Yang, Dezhong Peng, Peng Hu, Yijie Lin, Xi Peng",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10174v1 Announce Type: new \nAbstract: Traditional text-based person ReID assumes that person descriptions from witnesses are complete and provided at once. However, in real-world scenarios, such descriptions are often partial or vague. To address this limitation, we introduce a new task called interactive person re-identification (Inter-ReID). Inter-ReID is a dialogue-based retrieval task that iteratively refines initial descriptions through ongoing interactions with the witnesses. To facilitate the study of this new task, we construct a dialogue dataset that incorporates multiple types of questions by decomposing fine-grained attributes of individuals. We further propose LLaVA-ReID, a question model that generates targeted questions based on visual and textual contexts to elicit additional details about the target person. Leveraging a looking-forward strategy, we prioritize the most informative questions as supervision during training. Experimental results on both Inter-ReID and text-based ReID benchmarks demonstrate that LLaVA-ReID significantly outperforms baselines."
      },
      {
        "id": "oai:arXiv.org:2504.10185v1",
        "title": "LLM Unlearning Reveals a Stronger-Than-Expected Coreset Effect in Current Benchmarks",
        "link": "https://arxiv.org/abs/2504.10185",
        "author": "Soumyadeep Pal, Changsheng Wang, James Diffenderfer, Bhavya Kailkhura, Sijia Liu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10185v1 Announce Type: new \nAbstract: Large language model unlearning has become a critical challenge in ensuring safety and controlled model behavior by removing undesired data-model influences from the pretrained model while preserving general utility. Significant recent efforts have been dedicated to developing LLM unlearning benchmarks such as WMDP (Weapons of Mass Destruction Proxy) and MUSE (Machine Unlearning Six-way Evaluation), facilitating standardized unlearning performance assessment and method comparison. Despite their usefulness, we uncover for the first time a novel coreset effect within these benchmarks. Specifically, we find that LLM unlearning achieved with the original (full) forget set can be effectively maintained using a significantly smaller subset (functioning as a \"coreset\"), e.g., as little as 5% of the forget set, even when selected at random. This suggests that LLM unlearning in these benchmarks can be performed surprisingly easily, even in an extremely low-data regime. We demonstrate that this coreset effect remains strong, regardless of the LLM unlearning method used, such as NPO (Negative Preference Optimization) and RMU (Representation Misdirection Unlearning), the popular ones in these benchmarks. The surprisingly strong coreset effect is also robust across various data selection methods, ranging from random selection to more sophisticated heuristic approaches. We explain the coreset effect in LLM unlearning through a keyword-based perspective, showing that keywords extracted from the forget set alone contribute significantly to unlearning effectiveness and indicating that current unlearning is driven by a compact set of high-impact tokens rather than the entire dataset. We further justify the faithfulness of coreset-unlearned models along additional dimensions, such as mode connectivity and robustness to jailbreaking attacks. Codes are available at https://github.com/OPTML-Group/MU-Coreset."
      },
      {
        "id": "oai:arXiv.org:2504.10187v1",
        "title": "Deep Reasoning Translation via Reinforcement Learning",
        "link": "https://arxiv.org/abs/2504.10187",
        "author": "Jiaan Wang, Fandong Meng, Jie Zhou",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10187v1 Announce Type: new \nAbstract: Recently, deep reasoning LLMs (e.g., OpenAI o1/o3 and DeepSeek-R1) have shown promising performance in various complex tasks. Free translation is an important and interesting task in the multilingual world, which requires going beyond word-for-word translation and taking cultural differences into account. This task is still under-explored in deep reasoning LLMs. In this paper, we introduce DeepTrans, a deep reasoning translation model that learns free translation via reinforcement learning. Specifically, we carefully build a reward model with pre-defined scoring criteria on both the translation results and the thought process. Given the source sentences, the reward model teaches the deep translation model how to think and free-translate them during reinforcement learning. In this way, training DeepTrans does not need any labeled translations, avoiding the human-intensive annotation or resource-intensive data synthesis. Experimental results show the effectiveness of DeepTrans. Using Qwen2.5-7B as the backbone, DeepTrans improves performance by 16.3% in literature translation, and outperforms strong deep reasoning baselines as well as baselines that are fine-tuned with synthesized data. Moreover, we summarize the failures and interesting findings during our RL exploration. We hope this work could inspire other researchers in free translation."
      },
      {
        "id": "oai:arXiv.org:2504.10188v1",
        "title": "Efficient Generative Model Training via Embedded Representation Warmup",
        "link": "https://arxiv.org/abs/2504.10188",
        "author": "Deyuan Liu, Peng Sun, Xufeng Li, Tao Lin",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10188v1 Announce Type: new \nAbstract: Diffusion models excel at generating high-dimensional data but fall short in training efficiency and representation quality compared to self-supervised methods. We identify a key bottleneck: the underutilization of high-quality, semantically rich representations during training notably slows down convergence. Our systematic analysis reveals a critical representation processing region -- primarily in the early layers -- where semantic and structural pattern learning takes place before generation can occur. To address this, we propose Embedded Representation Warmup (ERW), a plug-and-play framework where in the first stage we get the ERW module serves as a warmup that initializes the early layers of the diffusion model with high-quality, pretrained representations. This warmup minimizes the burden of learning representations from scratch, thereby accelerating convergence and boosting performance. Our theoretical analysis demonstrates that ERW's efficacy depends on its precise integration into specific neural network layers -- termed the representation processing region -- where the model primarily processes and transforms feature representations for later generation. We further establish that ERW not only accelerates training convergence but also enhances representation quality: empirically, our method achieves a 40$\\times$ acceleration in training speed compared to REPA, the current state-of-the-art methods. Code is available at https://github.com/LINs-lab/ERW."
      },
      {
        "id": "oai:arXiv.org:2504.10190v1",
        "title": "Differentially Private 2D Human Pose Estimation",
        "link": "https://arxiv.org/abs/2504.10190",
        "author": "Kaushik Bhargav Sivangi, Idris Zakariyya, Paul Henderson, Fani Deligianni",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10190v1 Announce Type: new \nAbstract: Human pose estimation (HPE) has become essential in numerous applications including healthcare, activity recognition, and human-computer interaction. However, the privacy implications of processing sensitive visual data present significant deployment barriers in critical domains. While traditional anonymization techniques offer limited protection and often compromise data utility for broader motion analysis, Differential Privacy (DP) provides formal privacy guarantees but typically degrades model performance when applied naively. In this work, we present the first differentially private 2D human pose estimation (2D-HPE) by applying Differentially Private Stochastic Gradient Descent (DP-SGD) to this task. To effectively balance privacy with performance, we adopt Projected DP-SGD (PDP-SGD), which projects the noisy gradients to a low-dimensional subspace. Additionally, we adapt TinyViT, a compact and efficient vision transformer for coordinate classification in HPE, providing a lightweight yet powerful backbone that enhances privacy-preserving deployment feasibility on resource-limited devices. Our approach is particularly valuable for multimedia interpretation tasks, enabling privacy-safe analysis and understanding of human motion across diverse visual media while preserving the semantic meaning required for downstream applications. Comprehensive experiments on the MPII Human Pose Dataset demonstrate significant performance enhancement with PDP-SGD achieving 78.48% PCKh@0.5 at a strict privacy budget ($\\epsilon=0.2$), compared to 63.85% for standard DP-SGD. This work lays foundation for privacy-preserving human pose estimation in real-world, sensitive applications."
      },
      {
        "id": "oai:arXiv.org:2504.10191v1",
        "title": "Localized Cultural Knowledge is Conserved and Controllable in Large Language Models",
        "link": "https://arxiv.org/abs/2504.10191",
        "author": "Veniamin Veselovsky, Berke Argin, Benedikt Stroebl, Chris Wendler, Robert West, James Evans, Thomas L. Griffiths, Arvind Narayanan",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10191v1 Announce Type: new \nAbstract: Just as humans display language patterns influenced by their native tongue when speaking new languages, LLMs often default to English-centric responses even when generating in other languages. Nevertheless, we observe that local cultural information persists within the models and can be readily activated for cultural customization. We first demonstrate that explicitly providing cultural context in prompts significantly improves the models' ability to generate culturally localized responses. We term the disparity in model performance with versus without explicit cultural context the explicit-implicit localization gap, indicating that while cultural knowledge exists within LLMs, it may not naturally surface in multilingual interactions if cultural context is not explicitly provided. Despite the explicit prompting benefit, however, the answers reduce in diversity and tend toward stereotypes. Second, we identify an explicit cultural customization vector, conserved across all non-English languages we explore, which enables LLMs to be steered from the synthetic English cultural world-model toward each non-English cultural world. Steered responses retain the diversity of implicit prompting and reduce stereotypes to dramatically improve the potential for customization. We discuss the implications of explicit cultural customization for understanding the conservation of alternative cultural world models within LLMs, and their controllable utility for translation, cultural customization, and the possibility of making the explicit implicit through soft control for expanded LLM function and appeal."
      },
      {
        "id": "oai:arXiv.org:2504.10198v1",
        "title": "DioR: Adaptive Cognitive Detection and Contextual Retrieval Optimization for Dynamic Retrieval-Augmented Generation",
        "link": "https://arxiv.org/abs/2504.10198",
        "author": "Hanghui Guo, Jia Zhu, Shimin Di, Weijie Shi, Zhangze Chen, Jiajie Xu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10198v1 Announce Type: new \nAbstract: Dynamic Retrieval-augmented Generation (RAG) has shown great success in mitigating hallucinations in large language models (LLMs) during generation. However, existing dynamic RAG methods face significant limitations in two key aspects: 1) Lack of an effective mechanism to control retrieval triggers, and 2) Lack of effective scrutiny of retrieval content. To address these limitations, we propose an innovative dynamic RAG method, DioR (Adaptive Cognitive Detection and Contextual Retrieval Optimization), which consists of two main components: adaptive cognitive detection and contextual retrieval optimization, specifically designed to determine when retrieval is needed and what to retrieve for LLMs is useful. Experimental results demonstrate that DioR achieves superior performance on all tasks, demonstrating the effectiveness of our work."
      },
      {
        "id": "oai:arXiv.org:2504.10201v1",
        "title": "VibrantLeaves: A principled parametric image generator for training deep restoration models",
        "link": "https://arxiv.org/abs/2504.10201",
        "author": "Raphael Achddou, Yann Gousseau, Sa\\\"id Ladjal, Sabine S\\\"usstrunk",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10201v1 Announce Type: new \nAbstract: Even though Deep Neural Networks are extremely powerful for image restoration tasks, they have several limitations. They are poorly understood and suffer from strong biases inherited from the training sets. One way to address these shortcomings is to have a better control over the training sets, in particular by using synthetic sets. In this paper, we propose a synthetic image generator relying on a few simple principles. In particular, we focus on geometric modeling, textures, and a simple modeling of image acquisition. These properties, integrated in a classical Dead Leaves model, enable the creation of efficient training sets. Standard image denoising and super-resolution networks can be trained on such datasets, reaching performance almost on par with training on natural image datasets. As a first step towards explainability, we provide a careful analysis of the considered principles, identifying which image properties are necessary to obtain good performances. Besides, such training also yields better robustness to various geometric and radiometric perturbations of the test sets."
      },
      {
        "id": "oai:arXiv.org:2504.10214v1",
        "title": "Balancing Stability and Plasticity in Pretrained Detector: A Dual-Path Framework for Incremental Object Detection",
        "link": "https://arxiv.org/abs/2504.10214",
        "author": "Songze Li, Qixing Xu, Tonghua Su, Xu-Yao Zhang, Zhongjie Wang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10214v1 Announce Type: new \nAbstract: The balance between stability and plasticity remains a fundamental challenge in pretrained model-based incremental object detection (PTMIOD). While existing PTMIOD methods demonstrate strong performance on in-domain tasks aligned with pretraining data, their plasticity to cross-domain scenarios remains underexplored. Through systematic component-wise analysis of pretrained detectors, we reveal a fundamental discrepancy: the localization modules demonstrate inherent cross-domain stability-preserving precise bounding box estimation across distribution shifts-while the classification components require enhanced plasticity to mitigate discriminability degradation in cross-domain scenarios. Motivated by these findings, we propose a dual-path framework built upon pretrained DETR-based detectors which decouples localization stability and classification plasticity: the localization path maintains stability to preserve pretrained localization knowledge, while the classification path facilitates plasticity via parameter-efficient fine-tuning and resists forgetting with pseudo-feature replay. Extensive evaluations on both in-domain (MS COCO and PASCAL VOC) and cross-domain (TT100K) benchmarks show state-of-the-art performance, demonstrating our method's ability to effectively balance stability and plasticity in PTMIOD, achieving robust cross-domain adaptation and strong retention of anti-forgetting capabilities."
      },
      {
        "id": "oai:arXiv.org:2504.10222v1",
        "title": "PRM-BAS: Enhancing Multimodal Reasoning through PRM-guided Beam Annealing Search",
        "link": "https://arxiv.org/abs/2504.10222",
        "author": "Pengfei Hu, Zhenrong Zhang, Qikai Chang, Shuhang Liu, Jiefeng Ma, Jun Du, Jianshu Zhang, Quan Liu, Jianqing Gao, Feng Ma, Qingfeng Liu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10222v1 Announce Type: new \nAbstract: Recent work increasingly focuses on improving the reasoning capabilities of Multimodal Large Language Models (MLLMs). Among existing methods, Process Reward Models (PRMs) stand out for offering dense, step-wise supervision to guide intermediate reasoning. However, how to effectively integrate PRMs into search strategies remains an open question. In this paper, we introduce PRM-BAS (PRM-Guided Beam Annealing Search), a lightweight approach for PRM-guided reasoning that dynamically adjusts beam size -- starting with a broader search space and gradually narrowing it as contextual information accumulates, thereby balancing performance and efficiency. We further propose a unified framework for data construction and PRM training. Specifically, we construct the PRM-BAS-300k dataset by selecting 300k questions from existing datasets and performing rollouts at each step to estimate the probability of reaching a correct final answer. The PRM is then trained using a combination of value loss for absolute action quality and rank loss for relative action quality. Extensive experiments on challenging multimodal reasoning benchmarks demonstrate that PRM-BAS significantly improves reasoning performance while maintaining low computational cost. Moreover, it generalizes well across different model scales and architectures, showcasing strong robustness and plug-and-play capability."
      },
      {
        "id": "oai:arXiv.org:2504.10227v1",
        "title": "Probing then Editing Response Personality of Large Language Models",
        "link": "https://arxiv.org/abs/2504.10227",
        "author": "Tianjie Ju, Zhenyu Shao, Bowen Wang, Yujia Chen, Zhuosheng Zhang, Hao Fei, Mong-Li Lee, Wynne Hsu, Sufeng Duan, Gongshen Liu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10227v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have demonstrated promising capabilities to generate responses that exhibit consistent personality traits. Despite the major attempts to analyze personality expression through output-based evaluations, little is known about how such traits are internally encoded within LLM parameters. In this paper, we introduce a layer-wise probing framework to systematically investigate the layer-wise capability of LLMs in encoding personality for responding. We conduct probing experiments on 11 open-source LLMs over the PersonalityEdit benchmark and find that LLMs predominantly encode personality for responding in their middle and upper layers, with instruction-tuned models demonstrating a slightly clearer separation of personality traits. Furthermore, by interpreting the trained probing hyperplane as a layer-wise boundary for each personality category, we propose a layer-wise perturbation method to edit the personality expressed by LLMs during inference. Our results show that even when the prompt explicitly specifies a particular personality, our method can still successfully alter the response personality of LLMs. Interestingly, the difficulty of converting between certain personality traits varies substantially, which aligns with the representational distances in our probing experiments. Finally, we conduct a comprehensive MMLU benchmark evaluation and time overhead analysis, demonstrating that our proposed personality editing method incurs only minimal degradation in general capabilities while maintaining low training costs and acceptable inference latency. Our code is publicly available at https://github.com/universe-sky/probing-then-editing-personality."
      },
      {
        "id": "oai:arXiv.org:2504.10229v1",
        "title": "ROSFD: Robust Online Streaming Fraud Detection with Resilience to Concept Drift in Data Streams",
        "link": "https://arxiv.org/abs/2504.10229",
        "author": "Vivek Yelleti",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10229v1 Announce Type: new \nAbstract: Continuous generation of streaming data from diverse sources, such as online transactions and digital interactions, necessitates timely fraud detection. Traditional batch processing methods often struggle to capture the rapidly evolving patterns of fraudulent activities. This paper highlights the critical importance of processing streaming data for effective fraud detection. To address the inherent challenges of latency, scalability, and concept drift in streaming environments, we propose a robust online streaming fraud detection (ROSFD) framework. Our proposed framework comprises two key stages: (i) Stage One: Offline Model Initialization. In this initial stage, a model is built in offline settings using incremental learning principles to overcome the \"cold-start\" problem. (ii) Stage Two: Real-time Model Adaptation. In this dynamic stage, drift detection algorithms (viz.,, DDM, EDDM, and ADWIN) are employed to identify concept drift in the incoming data stream and incrementally train the model accordingly. This \"train-only-when-required\" strategy drastically reduces the number of retrains needed without significantly impacting the area under the receiver operating characteristic curve (AUC). Overall, ROSFD utilizing ADWIN as the drift detection method demonstrated the best performance among the employed methods. In terms of model efficacy, Adaptive Random Forest consistently outperformed other models, achieving the highest AUC in four out of five datasets."
      },
      {
        "id": "oai:arXiv.org:2504.10231v1",
        "title": "A Model Zoo of Vision Transformers",
        "link": "https://arxiv.org/abs/2504.10231",
        "author": "Damian Falk, L\\'eo Meynent, Florence Pfammatter, Konstantin Sch\\\"urholt, Damian Borth",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10231v1 Announce Type: new \nAbstract: The availability of large, structured populations of neural networks - called 'model zoos' - has led to the development of a multitude of downstream tasks ranging from model analysis, to representation learning on model weights or generative modeling of neural network parameters. However, existing model zoos are limited in size and architecture and neglect the transformer, which is among the currently most successful neural network architectures. We address this gap by introducing the first model zoo of vision transformers (ViT). To better represent recent training approaches, we develop a new blueprint for model zoo generation that encompasses both pre-training and fine-tuning steps, and publish 250 unique models. They are carefully generated with a large span of generating factors, and their diversity is validated using a thorough choice of weight-space and behavioral metrics. To further motivate the utility of our proposed dataset, we suggest multiple possible applications grounded in both extensive exploratory experiments and a number of examples from the existing literature. By extending previous lines of similar work, our model zoo allows researchers to push their model population-based methods from the small model regime to state-of-the-art architectures. We make our model zoo available at github.com/ModelZoos/ViTModelZoo."
      },
      {
        "id": "oai:arXiv.org:2504.10242v1",
        "title": "CAT: A Conditional Adaptation Tailor for Efficient and Effective Instance-Specific Pansharpening on Real-World Data",
        "link": "https://arxiv.org/abs/2504.10242",
        "author": "Tianyu Xin, Jin-Liang Xiao, Zeyu Xia, Shan Yin, Liang-Jian Deng",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10242v1 Announce Type: new \nAbstract: Pansharpening is a crucial remote sensing technique that fuses low-resolution multispectral (LRMS) images with high-resolution panchromatic (PAN) images to generate high-resolution multispectral (HRMS) imagery. Although deep learning techniques have significantly advanced pansharpening, many existing methods suffer from limited cross-sensor generalization and high computational overhead, restricting their real-time applications. To address these challenges, we propose an efficient framework that quickly adapts to a specific input instance, completing both training and inference in a short time. Our framework splits the input image into multiple patches, selects a subset for unsupervised CAT training, and then performs inference on all patches, stitching them into the final output. The CAT module, integrated between the feature extraction and channel transformation stages of a pre-trained network, tailors the fused features and fixes the parameters for efficient inference, generating improved results. Our approach offers two key advantages: (1) $\\textit{Improved Generalization Ability}$: by mitigating cross-sensor degradation, our model--although pre-trained on a specific dataset--achieves superior performance on datasets captured by other sensors; (2) $\\textit{Enhanced Computational Efficiency}$: the CAT-enhanced network can swiftly adapt to the test sample using the single LRMS-PAN pair input, without requiring extensive large-scale data retraining. Experiments on the real-world data from WorldView-3 and WorldView-2 datasets demonstrate that our method achieves state-of-the-art performance on cross-sensor real-world data, while achieving both training and inference of $512\\times512$ image within $\\textit{0.4 seconds}$ and $4000\\times4000$ image within $\\textit{3 seconds}$ at the fastest setting on a commonly used RTX 3090 GPU."
      },
      {
        "id": "oai:arXiv.org:2504.10254v1",
        "title": "MASSeg : 2nd Technical Report for 4th PVUW MOSE Track",
        "link": "https://arxiv.org/abs/2504.10254",
        "author": "Xuqiang Cao, Linnan Zhao, Jiaxuan Zhao, Fang Liu, Puhua Chen, Wenping Ma",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10254v1 Announce Type: new \nAbstract: Complex video object segmentation continues to face significant challenges in small object recognition, occlusion handling, and dynamic scene modeling. This report presents our solution, which ranked second in the MOSE track of CVPR 2025 PVUW Challenge. Based on an existing segmentation framework, we propose an improved model named MASSeg for complex video object segmentation, and construct an enhanced dataset, MOSE+, which includes typical scenarios with occlusions, cluttered backgrounds, and small target instances. During training, we incorporate a combination of inter-frame consistent and inconsistent data augmentation strategies to improve robustness and generalization. During inference, we design a mask output scaling strategy to better adapt to varying object sizes and occlusion levels. As a result, MASSeg achieves a J score of 0.8250, F score of 0.9007, and a J&amp;F score of 0.8628 on the MOSE test set."
      },
      {
        "id": "oai:arXiv.org:2504.10258v1",
        "title": "XY-Cut++: Advanced Layout Ordering via Hierarchical Mask Mechanism on a Novel Benchmark",
        "link": "https://arxiv.org/abs/2504.10258",
        "author": "Shuai Liu, Youmeng Li, Jizeng Wei",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10258v1 Announce Type: new \nAbstract: Document Reading Order Recovery is a fundamental task in document image understanding, playing a pivotal role in enhancing Retrieval-Augmented Generation (RAG) and serving as a critical preprocessing step for large language models (LLMs). Existing methods often struggle with complex layouts(e.g., multi-column newspapers), high-overhead interactions between cross-modal elements (visual regions and textual semantics), and a lack of robust evaluation benchmarks. We introduce XY-Cut++, an advanced layout ordering method that integrates pre-mask processing, multi-granularity segmentation, and cross-modal matching to address these challenges. Our method significantly enhances layout ordering accuracy compared to traditional XY-Cut techniques. Specifically, XY-Cut++ achieves state-of-the-art performance (98.8 BLEU overall) while maintaining simplicity and efficiency. It outperforms existing baselines by up to 24\\% and demonstrates consistent accuracy across simple and complex layouts on the newly introduced DocBench-100 dataset. This advancement establishes a reliable foundation for document structure recovery, setting a new standard for layout ordering tasks and facilitating more effective RAG and LLM preprocessing."
      },
      {
        "id": "oai:arXiv.org:2504.10267v1",
        "title": "Trade-offs in Privacy-Preserving Eye Tracking through Iris Obfuscation: A Benchmarking Study",
        "link": "https://arxiv.org/abs/2504.10267",
        "author": "Mengdi Wang, Efe Bozkir, Enkelejda Kasneci",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10267v1 Announce Type: new \nAbstract: Recent developments in hardware, computer graphics, and AI may soon enable AR/VR head-mounted displays (HMDs) to become everyday devices like smartphones and tablets. Eye trackers within HMDs provide a special opportunity for such setups as it is possible to facilitate gaze-based research and interaction. However, estimating users' gaze information often requires raw eye images and videos that contain iris textures, which are considered a gold standard biometric for user authentication, and this raises privacy concerns. Previous research in the eye-tracking community focused on obfuscating iris textures while keeping utility tasks such as gaze estimation accurate. Despite these attempts, there is no comprehensive benchmark that evaluates state-of-the-art approaches. Considering all, in this paper, we benchmark blurring, noising, downsampling, rubber sheet model, and iris style transfer to obfuscate user identity, and compare their impact on image quality, privacy, utility, and risk of imposter attack on two datasets. We use eye segmentation and gaze estimation as utility tasks, and reduction in iris recognition accuracy as a measure of privacy protection, and false acceptance rate to estimate risk of attack. Our experiments show that canonical image processing methods like blurring and noising cause a marginal impact on deep learning-based tasks. While downsampling, rubber sheet model, and iris style transfer are effective in hiding user identifiers, iris style transfer, with higher computation cost, outperforms others in both utility tasks, and is more resilient against spoof attacks. Our analyses indicate that there is no universal optimal approach to balance privacy, utility, and computation burden. Therefore, we recommend practitioners consider the strengths and weaknesses of each approach, and possible combinations of those to reach an optimal privacy-utility trade-off."
      },
      {
        "id": "oai:arXiv.org:2504.10273v1",
        "title": "Sidecar: A Structure-Preserving Framework for Solving Partial Differential Equations with Neural Networks",
        "link": "https://arxiv.org/abs/2504.10273",
        "author": "Gaohang Chen, Zhonghua Qiao",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10273v1 Announce Type: new \nAbstract: Solving partial differential equations (PDEs) with neural networks (NNs) has shown great potential in various scientific and engineering fields. However, most existing NN solvers mainly focus on satisfying the given PDEs, without explicitly considering intrinsic physical properties such as mass conservation or energy dissipation. This limitation can result in unstable or nonphysical solutions, particularly in long-term simulations. To address this issue, we propose Sidecar, a novel framework that enhances the accuracy and physical consistency of existing NN solvers by incorporating structure-preserving knowledge. Inspired by the Time-Dependent Spectral Renormalization (TDSR) approach, our Sidecar framework introduces a small copilot network, which is trained to guide the existing NN solver in preserving physical structure. This framework is designed to be highly flexible, enabling the incorporation of structure-preserving principles from diverse PDEs into a wide range of NN solvers. Our experimental results on benchmark PDEs demonstrate the improvement of the existing neural network solvers in terms of accuracy and consistency with structure-preserving properties."
      },
      {
        "id": "oai:arXiv.org:2504.10275v1",
        "title": "LMFormer: Lane based Motion Prediction Transformer",
        "link": "https://arxiv.org/abs/2504.10275",
        "author": "Harsh Yadav, Maximilian Schaefer, Kun Zhao, Tobias Meisen",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10275v1 Announce Type: new \nAbstract: Motion prediction plays an important role in autonomous driving. This study presents LMFormer, a lane-aware transformer network for trajectory prediction tasks. In contrast to previous studies, our work provides a simple mechanism to dynamically prioritize the lanes and shows that such a mechanism introduces explainability into the learning behavior of the network. Additionally, LMFormer uses the lane connection information at intersections, lane merges, and lane splits, in order to learn long-range dependency in lane structure. Moreover, we also address the issue of refining the predicted trajectories and propose an efficient method for iterative refinement through stacked transformer layers. For benchmarking, we evaluate LMFormer on the nuScenes dataset and demonstrate that it achieves SOTA performance across multiple metrics. Furthermore, the Deep Scenario dataset is used to not only illustrate cross-dataset network performance but also the unification capabilities of LMFormer to train on multiple datasets and achieve better performance."
      },
      {
        "id": "oai:arXiv.org:2504.10278v1",
        "title": "DiffMOD: Progressive Diffusion Point Denoising for Moving Object Detection in Remote Sensing",
        "link": "https://arxiv.org/abs/2504.10278",
        "author": "Jinyue Zhang, Xiangrong Zhang, Zhongjian Huang, Tianyang Zhang, Yifei Jiang, Licheng Jiao",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10278v1 Announce Type: new \nAbstract: Moving object detection (MOD) in remote sensing is significantly challenged by low resolution, extremely small object sizes, and complex noise interference. Current deep learning-based MOD methods rely on probability density estimation, which restricts flexible information interaction between objects and across temporal frames. To flexibly capture high-order inter-object and temporal relationships, we propose a point-based MOD in remote sensing. Inspired by diffusion models, the network optimization is formulated as a progressive denoising process that iteratively recovers moving object centers from sparse noisy points. Specifically, we sample scattered features from the backbone outputs as atomic units for subsequent processing, while global feature embeddings are aggregated to compensate for the limited coverage of sparse point features. By modeling spatial relative positions and semantic affinities, Spatial Relation Aggregation Attention is designed to enable high-order interactions among point-level features for enhanced object representation. To enhance temporal consistency, the Temporal Propagation and Global Fusion module is designed, which leverages an implicit memory reasoning mechanism for robust cross-frame feature integration. To align with the progressive denoising process, we propose a progressive MinK optimal transport assignment strategy that establishes specialized learning objectives at each denoising level. Additionally, we introduce a missing loss function to counteract the clustering tendency of denoised points around salient objects. Experiments on the RsData remote sensing MOD dataset show that our MOD method based on scattered point denoising can more effectively explore potential relationships between sparse moving objects and improve the detection capability and temporal consistency."
      },
      {
        "id": "oai:arXiv.org:2504.10283v1",
        "title": "$\\alpha$-Flow: A Unified Framework for Continuous-State Discrete Flow Matching Models",
        "link": "https://arxiv.org/abs/2504.10283",
        "author": "Chaoran Cheng, Jiahan Li, Jiajun Fan, Ge Liu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10283v1 Announce Type: new \nAbstract: Recent efforts have extended the flow-matching framework to discrete generative modeling. One strand of models directly works with the continuous probabilities instead of discrete tokens, which we colloquially refer to as Continuous-State Discrete Flow Matching (CS-DFM). Existing CS-DFM models differ significantly in their representations and geometric assumptions. This work presents a unified framework for CS-DFM models, under which the existing variants can be understood as operating on different $\\alpha$-representations of probabilities. Building upon the theory of information geometry, we introduce $\\alpha$-Flow, a family of CS-DFM models that adheres to the canonical $\\alpha$-geometry of the statistical manifold, and demonstrate its optimality in minimizing the generalized kinetic energy. Theoretically, we show that the flow matching loss for $\\alpha$-flow establishes a unified variational bound for the discrete negative log-likelihood. We comprehensively evaluate different instantiations of $\\alpha$-flow on various discrete generation domains to demonstrate their effectiveness in discrete generative modeling, including intermediate values whose geometries have never been explored before. $\\alpha$-flow significantly outperforms its discrete-state counterpart in image and protein sequence generation and better captures the entropy in language modeling."
      },
      {
        "id": "oai:arXiv.org:2504.10284v1",
        "title": "Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the Evaluation Protocol",
        "link": "https://arxiv.org/abs/2504.10284",
        "author": "Weiqi Wang, Jiefu Ou, Yangqiu Song, Benjamin Van Durme, Daniel Khashabi",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10284v1 Announce Type: new \nAbstract: Literature review tables are essential for summarizing and comparing collections of scientific papers. We explore the task of generating tables that best fulfill a user's informational needs given a collection of scientific papers. Building on recent work (Newman et al., 2024), we extend prior approaches to address real-world complexities through a combination of LLM-based methods and human annotations. Our contributions focus on three key challenges encountered in real-world use: (i) User prompts are often under-specified; (ii) Retrieved candidate papers frequently contain irrelevant content; and (iii) Task evaluation should move beyond shallow text similarity techniques and instead assess the utility of inferred tables for information-seeking tasks (e.g., comparing papers). To support reproducible evaluation, we introduce ARXIV2TABLE, a more realistic and challenging benchmark for this task, along with a novel approach to improve literature review table generation in real-world scenarios. Our extensive experiments on this benchmark show that both open-weight and proprietary LLMs struggle with the task, highlighting its difficulty and the need for further advancements. Our dataset and code are available at https://github.com/JHU-CLSP/arXiv2Table."
      },
      {
        "id": "oai:arXiv.org:2504.10286v1",
        "title": "Characterizing LLM-driven Social Network: The Chirper.ai Case",
        "link": "https://arxiv.org/abs/2504.10286",
        "author": "Yiming Zhu, Yupeng He, Ehsan-Ul Haq, Gareth Tyson, Pan Hui",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10286v1 Announce Type: new \nAbstract: Large language models (LLMs) demonstrate the ability to simulate human decision-making processes, enabling their use as agents in modeling sophisticated social networks, both offline and online. Recent research has explored collective behavioral patterns and structural characteristics of LLM agents within simulated networks. However, empirical comparisons between LLM-driven and human-driven online social networks remain scarce, limiting our understanding of how LLM agents differ from human users. This paper presents a large-scale analysis of Chirper.ai, an X/Twitter-like social network entirely populated by LLM agents, comprising over 65,000 agents and 7.7 million AI-generated posts. For comparison, we collect a parallel dataset from Mastodon, a human-driven decentralized social network, with over 117,000 users and 16 million posts. We examine key differences between LLM agents and humans in posting behaviors, abusive content, and social network structures. Our findings provide critical insights into the evolving landscape of online social network analysis in the AI era, offering a comprehensive profile of LLM agents in social simulations."
      },
      {
        "id": "oai:arXiv.org:2504.10288v1",
        "title": "Noise2Ghost: Self-supervised deep convolutional reconstruction for ghost imaging",
        "link": "https://arxiv.org/abs/2504.10288",
        "author": "Mathieu Manni, Dmitry Karpov, K. Joost Batenburg, Sharon Shwartz, Nicola Vigan\\`o",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10288v1 Announce Type: new \nAbstract: We present a new self-supervised deep-learning-based Ghost Imaging (GI) reconstruction method, which provides unparalleled reconstruction performance for noisy acquisitions among unsupervised methods. We present the supporting mathematical framework and results from theoretical and real data use cases. Self-supervision removes the need for clean reference data while offering strong noise reduction. This provides the necessary tools for addressing signal-to-noise ratio concerns for GI acquisitions in emerging and cutting-edge low-light GI scenarios. Notable examples include micro- and nano-scale x-ray emission imaging, e.g., x-ray fluorescence imaging of dose-sensitive samples. Their applications include in-vivo and in-operando case studies for biological samples and batteries."
      },
      {
        "id": "oai:arXiv.org:2504.10316v1",
        "title": "ESCT3D: Efficient and Selectively Controllable Text-Driven 3D Content Generation with Gaussian Splatting",
        "link": "https://arxiv.org/abs/2504.10316",
        "author": "Huiqi Wu, Jianbo Mei, Yingjie Huang, Yining Xu, Jingjiao You, Yilong Liu, Li Yao",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10316v1 Announce Type: new \nAbstract: In recent years, significant advancements have been made in text-driven 3D content generation. However, several challenges remain. In practical applications, users often provide extremely simple text inputs while expecting high-quality 3D content. Generating optimal results from such minimal text is a difficult task due to the strong dependency of text-to-3D models on the quality of input prompts. Moreover, the generation process exhibits high variability, making it difficult to control. Consequently, multiple iterations are typically required to produce content that meets user expectations, reducing generation efficiency. To address this issue, we propose GPT-4V for self-optimization, which significantly enhances the efficiency of generating satisfactory content in a single attempt. Furthermore, the controllability of text-to-3D generation methods has not been fully explored. Our approach enables users to not only provide textual descriptions but also specify additional conditions, such as style, edges, scribbles, poses, or combinations of multiple conditions, allowing for more precise control over the generated 3D content. Additionally, during training, we effectively integrate multi-view information, including multi-view depth, masks, features, and images, to address the common Janus problem in 3D content generation. Extensive experiments demonstrate that our method achieves robust generalization, facilitating the efficient and controllable generation of high-quality 3D content."
      },
      {
        "id": "oai:arXiv.org:2504.10317v1",
        "title": "Analysis of Attention in Video Diffusion Transformers",
        "link": "https://arxiv.org/abs/2504.10317",
        "author": "Yuxin Wen, Jim Wu, Ajay Jain, Tom Goldstein, Ashwinee Panda",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10317v1 Announce Type: new \nAbstract: We conduct an in-depth analysis of attention in video diffusion transformers (VDiTs) and report a number of novel findings. We identify three key properties of attention in VDiTs: Structure, Sparsity, and Sinks. Structure: We observe that attention patterns across different VDiTs exhibit similar structure across different prompts, and that we can make use of the similarity of attention patterns to unlock video editing via self-attention map transfer. Sparse: We study attention sparsity in VDiTs, finding that proposed sparsity methods do not work for all VDiTs, because some layers that are seemingly sparse cannot be sparsified. Sinks: We make the first study of attention sinks in VDiTs, comparing and contrasting them to attention sinks in language models. We propose a number of future directions that can make use of our insights to improve the efficiency-quality Pareto frontier for VDiTs."
      },
      {
        "id": "oai:arXiv.org:2504.10320v1",
        "title": "SlowFastVAD: Video Anomaly Detection via Integrating Simple Detector and RAG-Enhanced Vision-Language Model",
        "link": "https://arxiv.org/abs/2504.10320",
        "author": "Zongcan Ding, Haodong Zhang, Peng Wu, Guansong Pang, Zhiwei Yang, Peng Wang, Yanning Zhang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10320v1 Announce Type: new \nAbstract: Video anomaly detection (VAD) aims to identify unexpected events in videos and has wide applications in safety-critical domains. While semi-supervised methods trained on only normal samples have gained traction, they often suffer from high false alarm rates and poor interpretability. Recently, vision-language models (VLMs) have demonstrated strong multimodal reasoning capabilities, offering new opportunities for explainable anomaly detection. However, their high computational cost and lack of domain adaptation hinder real-time deployment and reliability. Inspired by dual complementary pathways in human visual perception, we propose SlowFastVAD, a hybrid framework that integrates a fast anomaly detector with a slow anomaly detector (namely a retrieval augmented generation (RAG) enhanced VLM), to address these limitations. Specifically, the fast detector first provides coarse anomaly confidence scores, and only a small subset of ambiguous segments, rather than the entire video, is further analyzed by the slower yet more interpretable VLM for elaborate detection and reasoning. Furthermore, to adapt VLMs to domain-specific VAD scenarios, we construct a knowledge base including normal patterns based on few normal samples and abnormal patterns inferred by VLMs. During inference, relevant patterns are retrieved and used to augment prompts for anomaly reasoning. Finally, we smoothly fuse the anomaly confidence of fast and slow detectors to enhance robustness of anomaly detection. Extensive experiments on four benchmarks demonstrate that SlowFastVAD effectively combines the strengths of both fast and slow detectors, and achieves remarkable detection accuracy and interpretability with significantly reduced computational overhead, making it well-suited for real-world VAD applications with high reliability requirements."
      },
      {
        "id": "oai:arXiv.org:2504.10322v1",
        "title": "Efficient Prompt Tuning for Hierarchical Ingredient Recognition",
        "link": "https://arxiv.org/abs/2504.10322",
        "author": "Yinxuan Gui, Bin Zhu, Jingjing Chen, Chong-Wah Ngo",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10322v1 Announce Type: new \nAbstract: Fine-grained ingredient recognition presents a significant challenge due to the diverse appearances of ingredients, resulting from different cutting and cooking methods. While existing approaches have shown promising results, they still require extensive training costs and focus solely on fine-grained ingredient recognition. In this paper, we address these limitations by introducing an efficient prompt-tuning framework that adapts pretrained visual-language models (VLMs), such as CLIP, to the ingredient recognition task without requiring full model finetuning. Additionally, we introduce three-level ingredient hierarchies to enhance both training performance and evaluation robustness. Specifically, we propose a hierarchical ingredient recognition task, designed to evaluate model performance across different hierarchical levels (e.g., chicken chunks, chicken, meat), capturing recognition capabilities from coarse- to fine-grained categories. Our method leverages hierarchical labels, training prompt-tuned models with both fine-grained and corresponding coarse-grained labels. Experimental results on the VireoFood172 dataset demonstrate the effectiveness of prompt-tuning with hierarchical labels, achieving superior performance. Moreover, the hierarchical ingredient recognition task provides valuable insights into the model's ability to generalize across different levels of ingredient granularity."
      },
      {
        "id": "oai:arXiv.org:2504.10329v1",
        "title": "InstructEngine: Instruction-driven Text-to-Image Alignment",
        "link": "https://arxiv.org/abs/2504.10329",
        "author": "Xingyu Lu, Yuhang Hu, YiFan Zhang, Kaiyu Jiang, Changyi Liu, Tianke Zhang, Jinpeng Wang, Bin Wen, Chun Yuan, Fan Yang, Tingting Gao, Di Zhang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10329v1 Announce Type: new \nAbstract: Reinforcement Learning from Human/AI Feedback (RLHF/RLAIF) has been extensively utilized for preference alignment of text-to-image models. Existing methods face certain limitations in terms of both data and algorithm. For training data, most approaches rely on manual annotated preference data, either by directly fine-tuning the generators or by training reward models to provide training signals. However, the high annotation cost makes them difficult to scale up, the reward model consumes extra computation and cannot guarantee accuracy. From an algorithmic perspective, most methods neglect the value of text and only take the image feedback as a comparative signal, which is inefficient and sparse. To alleviate these drawbacks, we propose the InstructEngine framework. Regarding annotation cost, we first construct a taxonomy for text-to-image generation, then develop an automated data construction pipeline based on it. Leveraging advanced large multimodal models and human-defined rules, we generate 25K text-image preference pairs. Finally, we introduce cross-validation alignment method, which refines data efficiency by organizing semantically analogous samples into mutually comparable pairs. Evaluations on DrawBench demonstrate that InstructEngine improves SD v1.5 and SDXL's performance by 10.53% and 5.30%, outperforming state-of-the-art baselines, with ablation study confirming the benefits of InstructEngine's all components. A win rate of over 50% in human reviews also proves that InstructEngine better aligns with human preferences."
      },
      {
        "id": "oai:arXiv.org:2504.10331v1",
        "title": "LL-Gaussian: Low-Light Scene Reconstruction and Enhancement via Gaussian Splatting for Novel View Synthesis",
        "link": "https://arxiv.org/abs/2504.10331",
        "author": "Hao Sun, Fenggen Yu, Huiyao Xu, Tao Zhang, Changqing Zou",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10331v1 Announce Type: new \nAbstract: Novel view synthesis (NVS) in low-light scenes remains a significant challenge due to degraded inputs characterized by severe noise, low dynamic range (LDR) and unreliable initialization. While recent NeRF-based approaches have shown promising results, most suffer from high computational costs, and some rely on carefully captured or pre-processed data--such as RAW sensor inputs or multi-exposure sequences--which severely limits their practicality. In contrast, 3D Gaussian Splatting (3DGS) enables real-time rendering with competitive visual fidelity; however, existing 3DGS-based methods struggle with low-light sRGB inputs, resulting in unstable Gaussian initialization and ineffective noise suppression. To address these challenges, we propose LL-Gaussian, a novel framework for 3D reconstruction and enhancement from low-light sRGB images, enabling pseudo normal-light novel view synthesis. Our method introduces three key innovations: 1) an end-to-end Low-Light Gaussian Initialization Module (LLGIM) that leverages dense priors from learning-based MVS approach to generate high-quality initial point clouds; 2) a dual-branch Gaussian decomposition model that disentangles intrinsic scene properties (reflectance and illumination) from transient interference, enabling stable and interpretable optimization; 3) an unsupervised optimization strategy guided by both physical constrains and diffusion prior to jointly steer decomposition and enhancement. Additionally, we contribute a challenging dataset collected in extreme low-light environments and demonstrate the effectiveness of LL-Gaussian. Compared to state-of-the-art NeRF-based methods, LL-Gaussian achieves up to 2,000 times faster inference and reduces training time to just 2%, while delivering superior reconstruction and rendering quality."
      },
      {
        "id": "oai:arXiv.org:2504.10335v1",
        "title": "MorphTok: Morphologically Grounded Tokenization for Indian Languages",
        "link": "https://arxiv.org/abs/2504.10335",
        "author": "Maharaj Brahma, N J Karthika, Atul Singh, Devaraj Adiga, Smruti Bhate, Ganesh Ramakrishnan, Rohit Saluja, Maunendra Sankar Desarkar",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10335v1 Announce Type: new \nAbstract: Tokenization is a crucial step in NLP, especially with the rise of large language models (LLMs), impacting downstream performance, computational cost, and efficiency. Existing LLMs rely on the classical Byte-pair Encoding (BPE) algorithm for subword tokenization that greedily merges frequent character bigrams. This often leads to segmentation that does not align with linguistically meaningful units. To address this, we propose morphology-aware segmentation as a pre-tokenization step prior to applying BPE. To facilitate morphology-aware segmentation, we create a novel dataset for Hindi and Marathi, incorporating sandhi splitting to enhance the subword tokenization. Experiments on downstream tasks show that morphologically grounded tokenization improves performance for machine translation and language modeling. Additionally, to handle the ambiguity in the Unicode characters for diacritics, particularly dependent vowels in syllable-based writing systems, we introduce Constrained BPE (CBPE), an extension to the traditional BPE algorithm that incorporates script-specific constraints. Specifically, CBPE handles dependent vowels. Our results show that CBPE achieves a 1.68\\% reduction in fertility scores while maintaining comparable or improved downstream performance in machine translation, offering a computationally efficient alternative to standard BPE. Moreover, to evaluate segmentation across different tokenization algorithms, we introduce a new human evaluation metric, \\textit{EvalTok}, enabling more human-grounded assessment."
      },
      {
        "id": "oai:arXiv.org:2504.10340v1",
        "title": "Forecasting from Clinical Textual Time Series: Adaptations of the Encoder and Decoder Language Model Families",
        "link": "https://arxiv.org/abs/2504.10340",
        "author": "Shahriar Noroozizadeh, Sayantan Kumar, Jeremy C. Weiss",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10340v1 Announce Type: new \nAbstract: Clinical case reports encode rich, temporal patient trajectories that are often underexploited by traditional machine learning methods relying on structured data. In this work, we introduce the forecasting problem from textual time series, where timestamped clinical findings--extracted via an LLM-assisted annotation pipeline--serve as the primary input for prediction. We systematically evaluate a diverse suite of models, including fine-tuned decoder-based large language models and encoder-based transformers, on tasks of event occurrence prediction, temporal ordering, and survival analysis. Our experiments reveal that encoder-based models consistently achieve higher F1 scores and superior temporal concordance for short- and long-horizon event forecasting, while fine-tuned masking approaches enhance ranking performance. In contrast, instruction-tuned decoder models demonstrate a relative advantage in survival analysis, especially in early prognosis settings. Our sensitivity analyses further demonstrate the importance of time ordering, which requires clinical time series construction, as compared to text ordering, the format of the text inputs that LLMs are classically trained on. This highlights the additional benefit that can be ascertained from time-ordered corpora, with implications for temporal tasks in the era of widespread LLM use."
      },
      {
        "id": "oai:arXiv.org:2504.10342v1",
        "title": "VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain Knowledge",
        "link": "https://arxiv.org/abs/2504.10342",
        "author": "Yueqi Song, Tianyue Ou, Yibo Kong, Zecheng Li, Graham Neubig, Xiang Yue",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10342v1 Announce Type: new \nAbstract: Current multimodal benchmarks often conflate reasoning with domain-specific knowledge, making it difficult to isolate and evaluate general reasoning abilities in non-expert settings. To address this, we introduce VisualPuzzles, a benchmark that targets visual reasoning while deliberately minimizing reliance on specialized knowledge. VisualPuzzles consists of diverse questions spanning five categories: algorithmic, analogical, deductive, inductive, and spatial reasoning. One major source of our questions is manually translated logical reasoning questions from the Chinese Civil Service Examination. Experiments show that VisualPuzzles requires significantly less intensive domain-specific knowledge and more complex reasoning compared to benchmarks like MMMU, enabling us to better evaluate genuine multimodal reasoning. Evaluations show that state-of-the-art multimodal large language models consistently lag behind human performance on VisualPuzzles, and that strong performance on knowledge-intensive benchmarks does not necessarily translate to success on reasoning-focused, knowledge-light tasks. Additionally, reasoning enhancements such as scaling up inference compute (with \"thinking\" modes) yield inconsistent gains across models and task types, and we observe no clear correlation between model size and performance. We also found that models exhibit different reasoning and answering patterns on VisualPuzzles compared to benchmarks with heavier emphasis on knowledge. VisualPuzzles offers a clearer lens through which to evaluate reasoning capabilities beyond factual recall and domain knowledge."
      },
      {
        "id": "oai:arXiv.org:2504.10343v1",
        "title": "Domain-Adversarial Neural Network and Explainable AI for Reducing Tissue-of-Origin Signal in Pan-cancer Mortality Classification",
        "link": "https://arxiv.org/abs/2504.10343",
        "author": "Cristian Padron-Manrique, Juan Jos\\'e Oropeza Valdez, Osbaldo Resendis-Antonio",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10343v1 Announce Type: new \nAbstract: Tissue-of-origin signals dominate pan-cancer gene expression, often obscuring molecular features linked to patient survival. This hampers the discovery of generalizable biomarkers, as models tend to overfit tissue-specific patterns rather than capture survival-relevant signals. To address this, we propose a Domain-Adversarial Neural Network (DANN) trained on TCGA RNA-seq data to learn representations less biased by tissue and more focused on survival. Identifying tissue-independent genetic profiles is key to revealing core cancer programs. We assess the DANN using: (1) Standard SHAP, based on the original input space and DANN's mortality classifier; (2) A layer-aware strategy applied to hidden activations, including an unsupervised manifold from raw activations and a supervised manifold from mortality-specific SHAP values. Standard SHAP remains confounded by tissue signals due to biases inherent in its computation. The raw activation manifold was dominated by high-magnitude activations, which masked subtle tissue and mortality-related signals. In contrast, the layer-aware SHAP manifold offers improved low-dimensional representations of both tissue and mortality signals, independent of activation strength, enabling subpopulation stratification and pan-cancer identification of survival-associated genes."
      },
      {
        "id": "oai:arXiv.org:2504.10350v1",
        "title": "Benchmarking 3D Human Pose Estimation Models Under Occlusions",
        "link": "https://arxiv.org/abs/2504.10350",
        "author": "Filipa Lino, Carlos Santiago, Manuel Marques",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10350v1 Announce Type: new \nAbstract: This paper addresses critical challenges in 3D Human Pose Estimation (HPE) by analyzing the robustness and sensitivity of existing models to occlusions, camera position, and action variability. Using a novel synthetic dataset, BlendMimic3D, which includes diverse scenarios with multi-camera setups and several occlusion types, we conduct specific tests on several state-of-the-art models. Our study focuses on the discrepancy in keypoint formats between common datasets such as Human3.6M, and 2D datasets such as COCO, commonly used for 2D detection models and frequently input of 3D HPE models. Our work explores the impact of occlusions on model performance and the generality of models trained exclusively under standard conditions. The findings suggest significant sensitivity to occlusions and camera settings, revealing a need for models that better adapt to real-world variability and occlusion scenarios. This research contributed to ongoing efforts to improve the fidelity and applicability of 3D HPE systems in complex environments."
      },
      {
        "id": "oai:arXiv.org:2504.10351v1",
        "title": "Multimodal Representation Learning Techniques for Comprehensive Facial State Analysis",
        "link": "https://arxiv.org/abs/2504.10351",
        "author": "Kaiwen Zheng, Xuri Ge, Junchen Fu, Jun Peng, Joemon M. Jose",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10351v1 Announce Type: new \nAbstract: Multimodal foundation models have significantly improved feature representation by integrating information from multiple modalities, making them highly suitable for a broader set of applications. However, the exploration of multimodal facial representation for understanding perception has been limited. Understanding and analyzing facial states, such as Action Units (AUs) and emotions, require a comprehensive and robust framework that bridges visual and linguistic modalities. In this paper, we present a comprehensive pipeline for multimodal facial state analysis. First, we compile a new Multimodal Face Dataset (MFA) by generating detailed multilevel language descriptions of face, incorporating Action Unit (AU) and emotion descriptions, by leveraging GPT-4o. Second, we introduce a novel Multilevel Multimodal Face Foundation model (MF^2) tailored for Action Unit (AU) and emotion recognition. Our model incorporates comprehensive visual feature modeling at both local and global levels of face image, enhancing its ability to represent detailed facial appearances. This design aligns visual representations with structured AU and emotion descriptions, ensuring effective cross-modal integration. Third, we develop a Decoupled Fine-Tuning Network (DFN) that efficiently adapts MF^2 across various tasks and datasets. This approach not only reduces computational overhead but also broadens the applicability of the foundation model to diverse scenarios. Experimentation show superior performance for AU and emotion detection tasks."
      },
      {
        "id": "oai:arXiv.org:2504.10353v1",
        "title": "Patch and Shuffle: A Preprocessing Technique for Texture Classification in Autonomous Cementitious Fabrication",
        "link": "https://arxiv.org/abs/2504.10353",
        "author": "Jeremiah Giordani",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10353v1 Announce Type: new \nAbstract: Autonomous fabrication systems are transforming construction and manufacturing, yet they remain vulnerable to print errors. Texture classification is a key component of computer vision systems that enable real-time monitoring and adjustment during cementitious fabrication. Traditional classification methods often rely on global image features, which can bias the model toward semantic content rather than low-level textures. In this paper, we introduce a novel preprocessing technique called \"patch and shuffle,\" which segments input images into smaller patches, shuffles them, and reconstructs a jumbled image before classification. This transformation removes semantic context, forcing the classifier to rely on local texture features.\n  We evaluate this approach on a dataset of extruded cement images, using a ResNet-18-based architecture. Our experiments compare the patch and shuffle method to a standard pipeline, holding all other factors constant. Results show a significant improvement in accuracy: the patch and shuffle model achieved 90.64% test accuracy versus 72.46% for the baseline. These findings suggest that disrupting global structure enhances performance in texture-based classification tasks.\n  This method has implications for broader vision tasks where low-level features matter more than high-level semantics. The technique may improve classification in applications ranging from fabrication monitoring to medical imaging."
      },
      {
        "id": "oai:arXiv.org:2504.10356v1",
        "title": "MultiLoKo: a multilingual local knowledge benchmark for LLMs spanning 31 languages",
        "link": "https://arxiv.org/abs/2504.10356",
        "author": "Dieuwke Hupkes, Nikolay Bogoychev",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10356v1 Announce Type: new \nAbstract: We present MultiLoKo, a new benchmark for evaluating multilinguality in LLMs covering 31 languages. MultiLoKo consists of three partitions: a main partition consisting of 500 questions per language, separately sourced to be locally relevant to the specific language, and two translated partitions, containing human-authored translations from 30 non-English languages to English and vice versa. For comparison, we also release corresponding machine-authored translations. The data is equally distributed over two splits: a dev split and a blind, out-of-distribution test split. MultiLoKo can be used to study a variety of questions regarding the multilinguality of LLMs as well as meta-questions about multilingual benchmark creation. We compute MultiLoKo scores for 11 base and chat models marketed to be multilingual and study their average performance, their performance parity across languages, how much their ability to answer questions depends on the question language, and which languages are most difficult. None of the models we studied performs well on MultiLoKo, as indicated by low average scores as well as large differences between the best and worst scoring languages. Furthermore, we find a substantial effect of the question language, indicating sub-optimal knowledge transfer between languages. Lastly, we find that using local vs English-translated data can result in differences more than 20 points for the best performing models, drastically change the estimated difficulty of some languages. For using machines instead of human translations, we find a weaker effect on ordering of language difficulty, a larger difference in model rankings, and a substantial drop in estimated performance for all models."
      },
      {
        "id": "oai:arXiv.org:2504.10358v1",
        "title": "FingER: Content Aware Fine-grained Evaluation with Reasoning for AI-Generated Videos",
        "link": "https://arxiv.org/abs/2504.10358",
        "author": "Rui Chen, Lei Sun, Jing Tang, Geng Li, Xiangxiang Chu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10358v1 Announce Type: new \nAbstract: Recent advances in video generation have posed great challenges in the assessment of AI-generated content, particularly with the emergence of increasingly sophisticated models. The various inconsistencies and defects observed in such videos are inherently complex, making overall scoring notoriously difficult. In this paper, we emphasize the critical importance of integrating fine-grained reasoning into video evaluation, and we propose $\\textbf{F}$ing$\\textbf{ER}$, a novel entity-level reasoning evaluation framework that first automatically generates $\\textbf{F}$ine-grained $\\textbf{E}$ntity-level questions, and then answers those questions by a $\\textbf{R}$easoning model with scores, which can be subsequently weighted summed to an overall score for different applications. Specifically, we leverage LLMs to derive entity-level questions across five distinct perspectives, which (i) often focus on some specific entities of the content, thereby making answering or scoring much easier by MLLMs, and (ii) are more interpretable. Then we construct a FingER dataset, consisting of approximately 3.3k videos and corresponding 60k fine-grained QA annotations, each with detailed reasons. Based on that, we further investigate various training protocols to best incentivize the reasoning capability of MLLMs for correct answer prediction. Extensive experiments demonstrate that a reasoning model trained using Group Relative Policy Optimization (GRPO) with a cold-start strategy achieves the best performance. Notably, our model surpasses existing methods by a relative margin of $11.8\\%$ on GenAI-Bench and $5.5\\%$ on MonetBench with only 3.3k training videos, which is at most one-tenth of the training samples utilized by other methods. Our code and dataset will be released soon."
      },
      {
        "id": "oai:arXiv.org:2504.10359v1",
        "title": "DICE: A Framework for Dimensional and Contextual Evaluation of Language Models",
        "link": "https://arxiv.org/abs/2504.10359",
        "author": "Aryan Shrivastava, Paula Akemi Aoyagui",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10359v1 Announce Type: new \nAbstract: Language models (LMs) are increasingly being integrated into a wide range of applications, yet the modern evaluation paradigm does not sufficiently reflect how they are actually being used. Current evaluations rely on benchmarks that often lack direct applicability to the real-world contexts in which LMs are being deployed. To address this gap, we propose Dimensional and Contextual Evaluation (DICE), an approach that evaluates LMs on granular, context-dependent dimensions. In this position paper, we begin by examining the insufficiency of existing LM benchmarks, highlighting their limited applicability to real-world use cases. Next, we propose a set of granular evaluation parameters that capture dimensions of LM behavior that are more meaningful to stakeholders across a variety of application domains. Specifically, we introduce the concept of context-agnostic parameters - such as robustness, coherence, and epistemic honesty - and context-specific parameters that must be tailored to the specific contextual constraints and demands of stakeholders choosing to deploy LMs into a particular setting. We then discuss potential approaches to operationalize this evaluation framework, finishing with the opportunities and challenges DICE presents to the LM evaluation landscape. Ultimately, this work serves as a practical and approachable starting point for context-specific and stakeholder-relevant evaluation of LMs."
      },
      {
        "id": "oai:arXiv.org:2504.10368v1",
        "title": "S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability of Large Reasoning Models",
        "link": "https://arxiv.org/abs/2504.10368",
        "author": "Wenyuan Zhang, Shuaiyi Nie, Xinghua Zhang, Zefeng Zhang, Tingwen Liu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10368v1 Announce Type: new \nAbstract: We introduce S1-Bench, a novel benchmark designed to evaluate Large Reasoning Models' (LRMs) performance on simple tasks that favor intuitive system 1 thinking rather than deliberative system 2 reasoning. While LRMs have achieved significant breakthroughs in complex reasoning tasks through explicit chains of thought, their reliance on deep analytical thinking may limit their system 1 thinking capabilities. Moreover, a lack of benchmark currently exists to evaluate LRMs' performance in tasks that require such capabilities. To fill this gap, S1-Bench presents a set of simple, diverse, and naturally clear questions across multiple domains and languages, specifically designed to assess LRMs' performance in such tasks. Our comprehensive evaluation of 22 LRMs reveals significant lower efficiency tendencies, with outputs averaging 15.5 times longer than those of traditional small LLMs. Additionally, LRMs often identify correct answers early but continue unnecessary deliberation, with some models even producing numerous errors. These findings highlight the rigid reasoning patterns of current LRMs and underscore the substantial development needed to achieve balanced dual-system thinking capabilities that can adapt appropriately to task complexity."
      },
      {
        "id": "oai:arXiv.org:2504.10373v1",
        "title": "DUE: A Deep Learning Framework and Library for Modeling Unknown Equations",
        "link": "https://arxiv.org/abs/2504.10373",
        "author": "Junfeng Chen, Kailiang Wu, Dongbin Xiu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10373v1 Announce Type: new \nAbstract: Equations, particularly differential equations, are fundamental for understanding natural phenomena and predicting complex dynamics across various scientific and engineering disciplines. However, the governing equations for many complex systems remain unknown due to intricate underlying mechanisms. Recent advancements in machine learning and data science offer a new paradigm for modeling unknown equations from measurement or simulation data. This paradigm shift, known as data-driven discovery or modeling, stands at the forefront of AI for science, with significant progress made in recent years. In this paper, we introduce a systematic framework for data-driven modeling of unknown equations using deep learning. This versatile framework is capable of learning unknown ODEs, PDEs, DAEs, IDEs, SDEs, reduced or partially observed systems, and non-autonomous differential equations. Based on this framework, we have developed Deep Unknown Equations (DUE), an open-source software package designed to facilitate the data-driven modeling of unknown equations using modern deep learning techniques. DUE serves as an educational tool for classroom instruction, enabling students and newcomers to gain hands-on experience with differential equations, data-driven modeling, and contemporary deep learning approaches such as FNN, ResNet, generalized ResNet, operator semigroup networks (OSG-Net), and Transformers. Additionally, DUE is a versatile and accessible toolkit for researchers across various scientific and engineering fields. It is applicable not only for learning unknown equations from data but also for surrogate modeling of known, yet complex, equations that are costly to solve using traditional numerical methods. We provide detailed descriptions of DUE and demonstrate its capabilities through diverse examples, which serve as templates that can be easily adapted for other applications."
      },
      {
        "id": "oai:arXiv.org:2504.10374v1",
        "title": "Ctrl-Z: Controlling AI Agents via Resampling",
        "link": "https://arxiv.org/abs/2504.10374",
        "author": "Aryan Bhatt, Cody Rushing, Adam Kaufman, Tyler Tracy, Vasil Georgiev, David Matolcsi, Akbir Khan, Buck Shlegeris",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10374v1 Announce Type: new \nAbstract: Control evaluations measure whether monitoring and security protocols for AI systems prevent intentionally subversive AI models from causing harm. Our work presents the first control evaluation performed in an agent environment. We construct BashBench, a dataset of 257 challenging multi-step system administration tasks, and evaluate whether various safety measures can prevent an adversarially constructed AI agent from covertly downloading and executing malicious code in this environment. This multi-step setting introduces new attack and defense dynamics, which we investigate in order to design novel control protocols that prevent safety failures without hindering the ability of non-malicious agents to perform useful work. We introduce a class of control protocols called resample protocols that dynamically take additional samples of certain actions. We find these protocols significantly improve on existing techniques by selectively blocking the AI agent from executing suspicious code and incriminating the agent by generating additional examples of dangerous behavior. We measure the tradeoff between attack prevention and usefulness; our best protocol combines resampling with analysis of previous steps, reducing the success rate of attacks from 58% to 7% at a 5% cost to the performance of a non-malicious agent."
      },
      {
        "id": "oai:arXiv.org:2504.10375v1",
        "title": "PG-DPIR: An efficient plug-and-play method for high-count Poisson-Gaussian inverse problems",
        "link": "https://arxiv.org/abs/2504.10375",
        "author": "Maud Biquard, Marie Chabert, Florence Genin, Christophe Latry, Thomas Oberlin",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10375v1 Announce Type: new \nAbstract: Poisson-Gaussian noise describes the noise of various imaging systems thus the need of efficient algorithms for Poisson-Gaussian image restoration. Deep learning methods offer state-of-the-art performance but often require sensor-specific training when used in a supervised setting. A promising alternative is given by plug-and-play (PnP) methods, which consist in learning only a regularization through a denoiser, allowing to restore images from several sources with the same network. This paper introduces PG-DPIR, an efficient PnP method for high-count Poisson-Gaussian inverse problems, adapted from DPIR. While DPIR is designed for white Gaussian noise, a naive adaptation to Poisson-Gaussian noise leads to prohibitively slow algorithms due to the absence of a closed-form proximal operator. To address this, we adapt DPIR for the specificities of Poisson-Gaussian noise and propose in particular an efficient initialization of the gradient descent required for the proximal step that accelerates convergence by several orders of magnitude. Experiments are conducted on satellite image restoration and super-resolution problems. High-resolution realistic Pleiades images are simulated for the experiments, which demonstrate that PG-DPIR achieves state-of-the-art performance with improved efficiency, which seems promising for on-ground satellite processing chains."
      },
      {
        "id": "oai:arXiv.org:2504.10391v1",
        "title": "LLM-driven Constrained Copy Generation through Iterative Refinement",
        "link": "https://arxiv.org/abs/2504.10391",
        "author": "Varun Vasudevan, Faezeh Akhavizadegan, Abhinav Prakash, Yokila Arora, Jason Cho, Tanya Mendiratta, Sushant Kumar, Kannan Achan",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10391v1 Announce Type: new \nAbstract: Crafting a marketing message (copy), or copywriting is a challenging generation task, as the copy must adhere to various constraints. Copy creation is inherently iterative for humans, starting with an initial draft followed by successive refinements. However, manual copy creation is time-consuming and expensive, resulting in only a few copies for each use case. This limitation restricts our ability to personalize content to customers. Contrary to the manual approach, LLMs can generate copies quickly, but the generated content does not consistently meet all the constraints on the first attempt (similar to humans). While recent studies have shown promise in improving constrained generation through iterative refinement, they have primarily addressed tasks with only a few simple constraints. Consequently, the effectiveness of iterative refinement for tasks such as copy generation, which involves many intricate constraints, remains unclear. To address this gap, we propose an LLM-based end-to-end framework for scalable copy generation using iterative refinement. To the best of our knowledge, this is the first study to address multiple challenging constraints simultaneously in copy generation. Examples of these constraints include length, topics, keywords, preferred lexical ordering, and tone of voice. We demonstrate the performance of our framework by creating copies for e-commerce banners for three different use cases of varying complexity. Our results show that iterative refinement increases the copy success rate by $16.25-35.91$% across use cases. Furthermore, the copies generated using our approach outperformed manually created content in multiple pilot studies using a multi-armed bandit framework. The winning copy improved the click-through rate by $38.5-45.21$%."
      },
      {
        "id": "oai:arXiv.org:2504.10395v1",
        "title": "Better Coherence, Better Height: Fusing Physical Models and Deep Learning for Forest Height Estimation from Interferometric SAR Data",
        "link": "https://arxiv.org/abs/2504.10395",
        "author": "Ragini Bal Mahesh, Ronny H\\\"ansch",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10395v1 Announce Type: new \nAbstract: Estimating forest height from Synthetic Aperture Radar (SAR) images often relies on traditional physical models, which, while interpretable and data-efficient, can struggle with generalization. In contrast, Deep Learning (DL) approaches lack physical insight. To address this, we propose CoHNet - an end-to-end framework that combines the best of both worlds: DL optimized with physics-informed constraints. We leverage a pre-trained neural surrogate model to enforce physical plausibility through a unique training loss. Our experiments show that this approach not only improves forest height estimation accuracy but also produces meaningful features that enhance the reliability of predictions."
      },
      {
        "id": "oai:arXiv.org:2504.10400v1",
        "title": "Towards Low-Latency Event-based Obstacle Avoidance on a FPGA-Drone",
        "link": "https://arxiv.org/abs/2504.10400",
        "author": "Pietro Bonazzi, Christian Vogt, Michael Jost, Lyes Khacef, Federico Paredes-Vall\\'es, Michele Magno",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10400v1 Announce Type: new \nAbstract: This work quantitatively evaluates the performance of event-based vision systems (EVS) against conventional RGB-based models for action prediction in collision avoidance on an FPGA accelerator. Our experiments demonstrate that the EVS model achieves a significantly higher effective frame rate (1 kHz) and lower temporal (-20 ms) and spatial prediction errors (-20 mm) compared to the RGB-based model, particularly when tested on out-of-distribution data. The EVS model also exhibits superior robustness in selecting optimal evasion maneuvers. In particular, in distinguishing between movement and stationary states, it achieves a 59 percentage point advantage in precision (78% vs. 19%) and a substantially higher F1 score (0.73 vs. 0.06), highlighting the susceptibility of the RGB model to overfitting. Further analysis in different combinations of spatial classes confirms the consistent performance of the EVS model in both test data sets. Finally, we evaluated the system end-to-end and achieved a latency of approximately 2.14 ms, with event aggregation (1 ms) and inference on the processing unit (0.94 ms) accounting for the largest components. These results underscore the advantages of event-based vision for real-time collision avoidance and demonstrate its potential for deployment in resource-constrained environments."
      },
      {
        "id": "oai:arXiv.org:2504.10403v1",
        "title": "Satellite Federated Fine-Tuning for Foundation Models in Space Computing Power Networks",
        "link": "https://arxiv.org/abs/2504.10403",
        "author": "Yan zhu, Jingyang zhu, Ting Wang, Yuanming Shi, Chunxiao Jiang, Khaled Ben Letaief",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10403v1 Announce Type: new \nAbstract: Advancements in artificial intelligence (AI) and low-earth orbit (LEO) satellites have promoted the application of large remote sensing foundation models for various downstream tasks. However, direct downloading of these models for fine-tuning on the ground is impeded by privacy concerns and limited bandwidth. Satellite federated learning (FL) offers a solution by enabling model fine-tuning directly on-board satellites and aggregating model updates without data downloading. Nevertheless, for large foundation models, the computational capacity of satellites is insufficient to support effective on-board fine-tuning in traditional satellite FL frameworks. To address these challenges, we propose a satellite-ground collaborative federated fine-tuning framework. The key of the framework lies in how to reasonably decompose and allocate model components to alleviate insufficient on-board computation capabilities. During fine-tuning, satellites exchange intermediate results with ground stations or other satellites for forward propagation and back propagation, which brings communication challenges due to the special communication topology of space transmission networks, such as intermittent satellite-ground communication, short duration of satellite-ground communication windows, and unstable inter-orbit inter-satellite links (ISLs). To reduce transmission delays, we further introduce tailored communication strategies that integrate both communication and computing resources. Specifically, we propose a parallel intra-orbit communication strategy, a topology-aware satellite-ground communication strategy, and a latency-minimalization inter-orbit communication strategy to reduce space communication costs. Simulation results demonstrate significant reductions in training time with improvements of approximately 33%."
      },
      {
        "id": "oai:arXiv.org:2504.10405v1",
        "title": "Performance of Large Language Models in Supporting Medical Diagnosis and Treatment",
        "link": "https://arxiv.org/abs/2504.10405",
        "author": "Diogo Sousa, Guilherme Barbosa, Catarina Rocha, Dulce Oliveira",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10405v1 Announce Type: new \nAbstract: The integration of Large Language Models (LLMs) into healthcare holds significant potential to enhance diagnostic accuracy and support medical treatment planning. These AI-driven systems can analyze vast datasets, assisting clinicians in identifying diseases, recommending treatments, and predicting patient outcomes. This study evaluates the performance of a range of contemporary LLMs, including both open-source and closed-source models, on the 2024 Portuguese National Exam for medical specialty access (PNA), a standardized medical knowledge assessment. Our results highlight considerable variation in accuracy and cost-effectiveness, with several models demonstrating performance exceeding human benchmarks for medical students on this specific task. We identify leading models based on a combined score of accuracy and cost, discuss the implications of reasoning methodologies like Chain-of-Thought, and underscore the potential for LLMs to function as valuable complementary tools aiding medical professionals in complex clinical decision-making."
      },
      {
        "id": "oai:arXiv.org:2504.10409v1",
        "title": "GPS: Distilling Compact Memories via Grid-based Patch Sampling for Efficient Online Class-Incremental Learning",
        "link": "https://arxiv.org/abs/2504.10409",
        "author": "Mingchuan Ma, Yuhao Zhou, Jindi Lv, Yuxin Tian, Dan Si, Shujian Li, Qing Ye, Jiancheng Lv",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10409v1 Announce Type: new \nAbstract: Online class-incremental learning aims to enable models to continuously adapt to new classes with limited access to past data, while mitigating catastrophic forgetting. Replay-based methods address this by maintaining a small memory buffer of previous samples, achieving competitive performance. For effective replay under constrained storage, recent approaches leverage distilled data to enhance the informativeness of memory. However, such approaches often involve significant computational overhead due to the use of bi-level optimization. Motivated by these limitations, we introduce Grid-based Patch Sampling (GPS), a lightweight and effective strategy for distilling informative memory samples without relying on a trainable model. GPS generates informative samples by sampling a subset of pixels from the original image, yielding compact low-resolution representations that preserve both semantic content and structural information. During replay, these representations are reassembled to support training and evaluation. Experiments on extensive benchmarks demonstrate that GRS can be seamlessly integrated into existing replay frameworks, leading to 3%-4% improvements in average end accuracy under memory-constrained settings, with limited computational overhead."
      },
      {
        "id": "oai:arXiv.org:2504.10414v1",
        "title": "HUMOTO: A 4D Dataset of Mocap Human Object Interactions",
        "link": "https://arxiv.org/abs/2504.10414",
        "author": "Jiaxin Lu, Chun-Hao Paul Huang, Uttaran Bhattacharya, Qixing Huang, Yi Zhou",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10414v1 Announce Type: new \nAbstract: We present Human Motions with Objects (HUMOTO), a high-fidelity dataset of human-object interactions for motion generation, computer vision, and robotics applications. Featuring 736 sequences (7,875 seconds at 30 fps), HUMOTO captures interactions with 63 precisely modeled objects and 72 articulated parts. Our innovations include a scene-driven LLM scripting pipeline creating complete, purposeful tasks with natural progression, and a mocap-and-camera recording setup to effectively handle occlusions. Spanning diverse activities from cooking to outdoor picnics, HUMOTO preserves both physical accuracy and logical task flow. Professional artists rigorously clean and verify each sequence, minimizing foot sliding and object penetrations. We also provide benchmarks compared to other datasets. HUMOTO's comprehensive full-body motion and simultaneous multi-object interactions address key data-capturing challenges and provide opportunities to advance realistic human-object interaction modeling across research domains with practical applications in animation, robotics, and embodied AI systems. Project: https://jiaxin-lu.github.io/humoto/ ."
      },
      {
        "id": "oai:arXiv.org:2504.10415v1",
        "title": "LLM-SRBench: A New Benchmark for Scientific Equation Discovery with Large Language Models",
        "link": "https://arxiv.org/abs/2504.10415",
        "author": "Parshin Shojaee, Ngoc-Hieu Nguyen, Kazem Meidani, Amir Barati Farimani, Khoa D Doan, Chandan K Reddy",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10415v1 Announce Type: new \nAbstract: Scientific equation discovery is a fundamental task in the history of scientific progress, enabling the derivation of laws governing natural phenomena. Recently, Large Language Models (LLMs) have gained interest for this task due to their potential to leverage embedded scientific knowledge for hypothesis generation. However, evaluating the true discovery capabilities of these methods remains challenging, as existing benchmarks often rely on common equations that are susceptible to memorization by LLMs, leading to inflated performance metrics that do not reflect discovery. In this paper, we introduce LLM-SRBench, a comprehensive benchmark with 239 challenging problems across four scientific domains specifically designed to evaluate LLM-based scientific equation discovery methods while preventing trivial memorization. Our benchmark comprises two main categories: LSR-Transform, which transforms common physical models into less common mathematical representations to test reasoning beyond memorized forms, and LSR-Synth, which introduces synthetic, discovery-driven problems requiring data-driven reasoning. Through extensive evaluation of several state-of-the-art methods, using both open and closed LLMs, we find that the best-performing system so far achieves only 31.5% symbolic accuracy. These findings highlight the challenges of scientific equation discovery, positioning LLM-SRBench as a valuable resource for future research."
      },
      {
        "id": "oai:arXiv.org:2504.10418v1",
        "title": "CliniChat: A Multi-Source Knowledge-Driven Framework for Clinical Interview Dialogue Reconstruction and Evaluation",
        "link": "https://arxiv.org/abs/2504.10418",
        "author": "Jing Chen, Zhihua Wei, Wei Zhang, Yingying Hu, Qiong Zhang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10418v1 Announce Type: new \nAbstract: Large language models (LLMs) hold great promise for assisting clinical interviews due to their fluent interactive capabilities and extensive medical knowledge. However, the lack of high-quality interview dialogue data and widely accepted evaluation methods has significantly impeded this process. So we propose CliniChat, a framework that integrates multi-source knowledge to enable LLMs to simulate real-world clinical interviews. It consists of two modules: Clini-Recon and Clini-Eval, each responsible for reconstructing and evaluating interview dialogues, respectively. By incorporating three sources of knowledge, Clini-Recon transforms clinical notes into systematic, professional, and empathetic interview dialogues. Clini-Eval combines a comprehensive evaluation metric system with a two-phase automatic evaluation approach, enabling LLMs to assess interview performance like experts. We contribute MedQA-Dialog, a high-quality synthetic interview dialogue dataset, and CliniChatGLM, a model specialized for clinical interviews. Experimental results demonstrate that CliniChatGLM's interview capabilities undergo a comprehensive upgrade, particularly in history-taking, achieving state-of-the-art performance."
      },
      {
        "id": "oai:arXiv.org:2504.10419v1",
        "title": "Unchecked and Overlooked: Addressing the Checkbox Blind Spot in Large Language Models with CheckboxQA",
        "link": "https://arxiv.org/abs/2504.10419",
        "author": "Micha{\\l} Turski, Mateusz Chili\\'nski, {\\L}ukasz Borchmann",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10419v1 Announce Type: new \nAbstract: Checkboxes are critical in real-world document processing where the presence or absence of ticks directly informs data extraction and decision-making processes. Yet, despite the strong performance of Large Vision and Language Models across a wide range of tasks, they struggle with interpreting checkable content. This challenge becomes particularly pressing in industries where a single overlooked checkbox may lead to costly regulatory or contractual oversights. To address this gap, we introduce the CheckboxQA dataset, a targeted resource designed to evaluate and improve model performance on checkbox-related tasks. It reveals the limitations of current models and serves as a valuable tool for advancing document comprehension systems, with significant implications for applications in sectors such as legal tech and finance.\n  The dataset is publicly available at: https://github.com/Snowflake-Labs/CheckboxQA"
      },
      {
        "id": "oai:arXiv.org:2504.10421v1",
        "title": "Can We Edit LLMs for Long-Tail Biomedical Knowledge?",
        "link": "https://arxiv.org/abs/2504.10421",
        "author": "Xinhao Yi, Jake Lever, Kevin Bryson, Zaiqiao Meng",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10421v1 Announce Type: new \nAbstract: Knowledge editing has emerged as an effective approach for updating large language models (LLMs) by modifying their internal knowledge. However, their application to the biomedical domain faces unique challenges due to the long-tailed distribution of biomedical knowledge, where rare and infrequent information is prevalent. In this paper, we conduct the first comprehensive study to investigate the effectiveness of knowledge editing methods for editing long-tail biomedical knowledge. Our results indicate that, while existing editing methods can enhance LLMs' performance on long-tail biomedical knowledge, their performance on long-tail knowledge remains inferior to that on high-frequency popular knowledge, even after editing. Our further analysis reveals that long-tail biomedical knowledge contains a significant amount of one-to-many knowledge, where one subject and relation link to multiple objects. This high prevalence of one-to-many knowledge limits the effectiveness of knowledge editing in improving LLMs' understanding of long-tail biomedical knowledge, highlighting the need for tailored strategies to bridge this performance gap."
      },
      {
        "id": "oai:arXiv.org:2504.10422v1",
        "title": "Foundation models for electronic health records: representation dynamics and transferability",
        "link": "https://arxiv.org/abs/2504.10422",
        "author": "Michael C. Burkhart, Bashar Ramadan, Zewei Liao, Kaveri Chhikara, Juan C. Rojas, William F. Parker, Brett K. Beaulieu-Jones",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10422v1 Announce Type: new \nAbstract: Foundation models (FMs) trained on electronic health records (EHRs) have shown strong performance on a range of clinical prediction tasks. However, adapting these models to local health systems remains challenging due to limited data availability and resource constraints. In this study, we investigated what these models learn and evaluated the transferability of an FM trained on MIMIC-IV to an institutional EHR dataset at the University of Chicago Medical Center. We assessed their ability to identify outlier patients and examined representation-space patient trajectories in relation to future clinical outcomes. We also evaluated the performance of supervised fine-tuned classifiers on both source and target datasets. Our findings offer insights into the adaptability of FMs across different healthcare systems, highlight considerations for their effective implementation, and provide an empirical analysis of the underlying factors that contribute to their predictive performance."
      },
      {
        "id": "oai:arXiv.org:2504.10430v1",
        "title": "LLM Can be a Dangerous Persuader: Empirical Study of Persuasion Safety in Large Language Models",
        "link": "https://arxiv.org/abs/2504.10430",
        "author": "Minqian Liu, Zhiyang Xu, Xinyi Zhang, Heajun An, Sarvech Qadir, Qi Zhang, Pamela J. Wisniewski, Jin-Hee Cho, Sang Won Lee, Ruoxi Jia, Lifu Huang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10430v1 Announce Type: new \nAbstract: Recent advancements in Large Language Models (LLMs) have enabled them to approach human-level persuasion capabilities. However, such potential also raises concerns about the safety risks of LLM-driven persuasion, particularly their potential for unethical influence through manipulation, deception, exploitation of vulnerabilities, and many other harmful tactics. In this work, we present a systematic investigation of LLM persuasion safety through two critical aspects: (1) whether LLMs appropriately reject unethical persuasion tasks and avoid unethical strategies during execution, including cases where the initial persuasion goal appears ethically neutral, and (2) how influencing factors like personality traits and external pressures affect their behavior. To this end, we introduce PersuSafety, the first comprehensive framework for the assessment of persuasion safety which consists of three stages, i.e., persuasion scene creation, persuasive conversation simulation, and persuasion safety assessment. PersuSafety covers 6 diverse unethical persuasion topics and 15 common unethical strategies. Through extensive experiments across 8 widely used LLMs, we observe significant safety concerns in most LLMs, including failing to identify harmful persuasion tasks and leveraging various unethical persuasion strategies. Our study calls for more attention to improve safety alignment in progressive and goal-driven conversations such as persuasion."
      },
      {
        "id": "oai:arXiv.org:2504.10433v1",
        "title": "MonoDiff9D: Monocular Category-Level 9D Object Pose Estimation via Diffusion Model",
        "link": "https://arxiv.org/abs/2504.10433",
        "author": "Jian Liu, Wei Sun, Hui Yang, Jin Zheng, Zichen Geng, Hossein Rahmani, Ajmal Mian",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10433v1 Announce Type: new \nAbstract: Object pose estimation is a core means for robots to understand and interact with their environment. For this task, monocular category-level methods are attractive as they require only a single RGB camera. However, current methods rely on shape priors or CAD models of the intra-class known objects. We propose a diffusion-based monocular category-level 9D object pose generation method, MonoDiff9D. Our motivation is to leverage the probabilistic nature of diffusion models to alleviate the need for shape priors, CAD models, or depth sensors for intra-class unknown object pose estimation. We first estimate coarse depth via DINOv2 from the monocular image in a zero-shot manner and convert it into a point cloud. We then fuse the global features of the point cloud with the input image and use the fused features along with the encoded time step to condition MonoDiff9D. Finally, we design a transformer-based denoiser to recover the object pose from Gaussian noise. Extensive experiments on two popular benchmark datasets show that MonoDiff9D achieves state-of-the-art monocular category-level 9D object pose estimation accuracy without the need for shape priors or CAD models at any stage. Our code will be made public at https://github.com/CNJianLiu/MonoDiff9D."
      },
      {
        "id": "oai:arXiv.org:2504.10434v1",
        "title": "Anchor Token Matching: Implicit Structure Locking for Training-free AR Image Editing",
        "link": "https://arxiv.org/abs/2504.10434",
        "author": "Taihang Hu, Linxuan Li, Kai Wang, Yaxing Wang, Jian Yang, Ming-Ming Cheng",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10434v1 Announce Type: new \nAbstract: Text-to-image generation has seen groundbreaking advancements with diffusion models, enabling high-fidelity synthesis and precise image editing through cross-attention manipulation. Recently, autoregressive (AR) models have re-emerged as powerful alternatives, leveraging next-token generation to match diffusion models. However, existing editing techniques designed for diffusion models fail to translate directly to AR models due to fundamental differences in structural control. Specifically, AR models suffer from spatial poverty of attention maps and sequential accumulation of structural errors during image editing, which disrupt object layouts and global consistency. In this work, we introduce Implicit Structure Locking (ISLock), the first training-free editing strategy for AR visual models. Rather than relying on explicit attention manipulation or fine-tuning, ISLock preserves structural blueprints by dynamically aligning self-attention patterns with reference images through the Anchor Token Matching (ATM) protocol. By implicitly enforcing structural consistency in latent space, our method ISLock enables structure-aware editing while maintaining generative autonomy. Extensive experiments demonstrate that ISLock achieves high-quality, structure-consistent edits without additional training and is superior or comparable to conventional editing techniques. Our findings pioneer the way for efficient and flexible AR-based image editing, further bridging the performance gap between diffusion and autoregressive generative models. The code will be publicly available at https://github.com/hutaiHang/ATM"
      },
      {
        "id": "oai:arXiv.org:2504.10443v1",
        "title": "Multimodal Long Video Modeling Based on Temporal Dynamic Context",
        "link": "https://arxiv.org/abs/2504.10443",
        "author": "Haoran Hao, Jiaming Han, Yiyuan Zhang, Xiangyu Yue",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10443v1 Announce Type: new \nAbstract: Recent advances in Large Language Models (LLMs) have led to significant breakthroughs in video understanding. However, existing models still struggle with long video processing due to the context length constraint of LLMs and the vast amount of information within the video. Although some recent methods are designed for long video understanding, they often lose crucial information during token compression and struggle with additional modality like audio. In this work, we propose a dynamic long video encoding method utilizing the temporal relationship between frames, named Temporal Dynamic Context (TDC). Firstly, we segment the video into semantically consistent scenes based on inter-frame similarities, then encode each frame into tokens using visual-audio encoders. Secondly, we propose a novel temporal context compressor to reduce the number of tokens within each segment. Specifically, we employ a query-based Transformer to aggregate video, audio, and instruction text tokens into a limited set of temporal context tokens. Finally, we feed the static frame tokens and the temporal context tokens into the LLM for video understanding. Furthermore, to handle extremely long videos, we propose a training-free chain-of-thought strategy that progressively extracts answers from multiple video segments. These intermediate answers serve as part of the reasoning process and contribute to the final answer. We conduct extensive experiments on general video understanding and audio-video understanding benchmarks, where our method demonstrates strong performance. The code and models are available at https://github.com/Hoar012/TDC-Video."
      },
      {
        "id": "oai:arXiv.org:2504.10449v1",
        "title": "M1: Towards Scalable Test-Time Compute with Mamba Reasoning Models",
        "link": "https://arxiv.org/abs/2504.10449",
        "author": "Junxiong Wang, Wen-Ding Li, Daniele Paliotta, Daniel Ritter, Alexander M. Rush, Tri Dao",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10449v1 Announce Type: new \nAbstract: Effective reasoning is crucial to solving complex mathematical problems. Recent large language models (LLMs) have boosted performance by scaling test-time computation through long chain-of-thought reasoning. However, transformer-based models are inherently limited in extending context length due to their quadratic computational complexity and linear memory requirements. In this paper, we introduce a novel hybrid linear RNN reasoning model, M1, built on the Mamba architecture, which allows memory-efficient inference. Our approach leverages a distillation process from existing reasoning models and is further enhanced through RL training. Experimental results on the AIME and MATH benchmarks show that M1 not only outperforms previous linear RNN models but also matches the performance of state-of-the-art Deepseek R1 distilled reasoning models at a similar scale. We also compare our generation speed with a highly performant general purpose inference engine, vLLM, and observe more than a 3x speedup compared to a same size transformer. With throughput speedup, we are able to achieve higher accuracy compared to DeepSeek R1 distilled transformer reasoning models under a fixed generation time budget using self-consistency voting. Overall, we introduce a hybrid Mamba reasoning model and provide a more effective approach to scaling test-time generation using self-consistency or long chain of thought reasoning."
      },
      {
        "id": "oai:arXiv.org:2504.10452v1",
        "title": "Integrating Vision and Location with Transformers: A Multimodal Deep Learning Framework for Medical Wound Analysis",
        "link": "https://arxiv.org/abs/2504.10452",
        "author": "Ramin Mousa, Hadis Taherinia, Khabiba Abdiyeva, Amir Ali Bengari, Mohammadmahdi Vahediahmar",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10452v1 Announce Type: new \nAbstract: Effective recognition of acute and difficult-to-heal wounds is a necessary step in wound diagnosis. An efficient classification model can help wound specialists classify wound types with less financial and time costs and also help in deciding on the optimal treatment method. Traditional machine learning models suffer from feature selection and are usually cumbersome models for accurate recognition. Recently, deep learning (DL) has emerged as a powerful tool in wound diagnosis. Although DL seems promising for wound type recognition, there is still a large scope for improving the efficiency and accuracy of the model. In this study, a DL-based multimodal classifier was developed using wound images and their corresponding locations to classify them into multiple classes, including diabetic, pressure, surgical, and venous ulcers. A body map was also created to provide location data, which can help wound specialists label wound locations more effectively. The model uses a Vision Transformer to extract hierarchical features from input images, a Discrete Wavelet Transform (DWT) layer to capture low and high frequency components, and a Transformer to extract spatial features. The number of neurons and weight vector optimization were performed using three swarm-based optimization techniques (Monster Gorilla Toner (MGTO), Improved Gray Wolf Optimization (IGWO), and Fox Optimization Algorithm). The evaluation results show that weight vector optimization using optimization algorithms can increase diagnostic accuracy and make it a very effective approach for wound detection. In the classification using the original body map, the proposed model was able to achieve an accuracy of 0.8123 using image data and an accuracy of 0.8007 using a combination of image data and wound location. Also, the accuracy of the model in combination with the optimization models varied from 0.7801 to 0.8342."
      },
      {
        "id": "oai:arXiv.org:2504.10456v1",
        "title": "Privacy-Preserving Distributed Link Predictions Among Peers in Online Classrooms Using Federated Learning",
        "link": "https://arxiv.org/abs/2504.10456",
        "author": "Anurata Prabha Hridi, Muntasir Hoq, Zhikai Gao, Collin Lynch, Rajeev Sahay, Seyyedali Hosseinalipour, Bita Akram",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10456v1 Announce Type: new \nAbstract: Social interactions among classroom peers, represented as social learning networks (SLNs), play a crucial role in enhancing learning outcomes. While SLN analysis has recently garnered attention, most existing approaches rely on centralized training, where data is aggregated and processed on a local/cloud server with direct access to raw data. However, in real-world educational settings, such direct access across multiple classrooms is often restricted due to privacy concerns. Furthermore, training models on isolated classroom data prevents the identification of common interaction patterns that exist across multiple classrooms, thereby limiting model performance. To address these challenges, we propose one of the first frameworks that integrates Federated Learning (FL), a distributed and collaborative machine learning (ML) paradigm, with SLNs derived from students' interactions in multiple classrooms' online forums to predict future link formations (i.e., interactions) among students. By leveraging FL, our approach enables collaborative model training across multiple classrooms while preserving data privacy, as it eliminates the need for raw data centralization. Recognizing that each classroom may exhibit unique student interaction dynamics, we further employ model personalization techniques to adapt the FL model to individual classroom characteristics. Our results demonstrate the effectiveness of our approach in capturing both shared and classroom-specific representations of student interactions in SLNs. Additionally, we utilize explainable AI (XAI) techniques to interpret model predictions, identifying key factors that influence link formation across different classrooms. These insights unveil the drivers of social learning interactions within a privacy-preserving, collaborative, and distributed ML framework -- an aspect that has not been explored before."
      },
      {
        "id": "oai:arXiv.org:2504.10458v1",
        "title": "GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI Agents",
        "link": "https://arxiv.org/abs/2504.10458",
        "author": "Xiaobo Xia, Run Luo",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10458v1 Announce Type: new \nAbstract: Existing efforts in building Graphical User Interface (GUI) agents largely rely on the training paradigm of supervised fine-tuning on Large Vision-Language Models (LVLMs). However, this approach not only demands extensive amounts of training data but also struggles to effectively understand GUI screenshots and generalize to unseen interfaces. The issue significantly limits its application in real-world scenarios, especially for high-level tasks. Inspired by Reinforcement Fine-Tuning (RFT) in large reasoning models (e.g., DeepSeek-R1), which efficiently enhances the problem-solving capabilities of large language models in real-world settings, we propose \\name, the first reinforcement learning framework designed to enhance the GUI capabilities of LVLMs in high-level real-world task scenarios, through unified action space rule modeling. By leveraging a small amount of carefully curated high-quality data across multiple platforms (including Windows, Linux, MacOS, Android, and Web) and employing policy optimization algorithms such as Group Relative Policy Optimization (GRPO) to update the model, \\name achieves superior performance using only 0.02\\% of the data (3K vs. 13M) compared to previous state-of-the-art methods like OS-Atlas across eight benchmarks spanning three different platforms (mobile, desktop, and web). These results demonstrate the immense potential of reinforcement learning based on unified action space rule modeling in improving the execution capabilities of LVLMs for real-world GUI agent tasks."
      },
      {
        "id": "oai:arXiv.org:2504.10462v1",
        "title": "The Scalability of Simplicity: Empirical Analysis of Vision-Language Learning with a Single Transformer",
        "link": "https://arxiv.org/abs/2504.10462",
        "author": "Weixian Lei, Jiacong Wang, Haochen Wang, Xiangtai Li, Jun Hao Liew, Jiashi Feng, Zilong Huang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10462v1 Announce Type: new \nAbstract: This paper introduces SAIL, a single transformer unified multimodal large language model (MLLM) that integrates raw pixel encoding and language decoding within a singular architecture. Unlike existing modular MLLMs, which rely on a pre-trained vision transformer (ViT), SAIL eliminates the need for a separate vision encoder, presenting a more minimalist architecture design. Instead of introducing novel architectural components, SAIL adapts mix-attention mechanisms and multimodal positional encodings to better align with the distinct characteristics of visual and textual modalities. We systematically compare SAIL's properties-including scalability, cross-modal information flow patterns, and visual representation capabilities-with those of modular MLLMs. By scaling both training data and model size, SAIL achieves performance comparable to modular MLLMs. Notably, the removal of pretrained ViT components enhances SAIL's scalability and results in significantly different cross-modal information flow patterns. Moreover, SAIL demonstrates strong visual representation capabilities, achieving results on par with ViT-22B in vision tasks such as semantic segmentation. Code and models are available at https://github.com/bytedance/SAIL."
      },
      {
        "id": "oai:arXiv.org:2504.10465v1",
        "title": "Pixel-SAIL: Single Transformer For Pixel-Grounded Understanding",
        "link": "https://arxiv.org/abs/2504.10465",
        "author": "Tao Zhang, Xiangtai Li, Zilong Huang, Yanwei Li, Weixian Lei, Xueqing Deng, Shihao Chen, Shunping Ji, Jiashi Feng",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10465v1 Announce Type: new \nAbstract: Multimodal Large Language Models (MLLMs) achieve remarkable performance for fine-grained pixel-level understanding tasks. However, all the works rely heavily on extra components, such as vision encoder (CLIP), segmentation experts, leading to high system complexity and limiting model scaling. In this work, our goal is to explore a highly simplified MLLM without introducing extra components. Our work is motivated by the recent works on Single trAnsformer as a unified vIsion-Language Model (SAIL) design, where these works jointly learn vision tokens and text tokens in transformers. We present Pixel-SAIL, a single transformer for pixel-wise MLLM tasks. In particular, we present three technical improvements on the plain baseline. First, we design a learnable upsampling module to refine visual token features. Secondly, we propose a novel visual prompt injection strategy to enable the single transformer to understand visual prompt inputs and benefit from the early fusion of visual prompt embeddings and vision tokens. Thirdly, we introduce a vision expert distillation strategy to efficiently enhance the single transformer's fine-grained feature extraction capability. In addition, we have collected a comprehensive pixel understanding benchmark (PerBench), using a manual check. It includes three tasks: detailed object description, visual prompt-based question answering, and visual-text referring segmentation. Extensive experiments on four referring segmentation benchmarks, one visual prompt benchmark, and our PerBench show that our Pixel-SAIL achieves comparable or even better results with a much simpler pipeline. Code and model will be released at https://github.com/magic-research/Sa2VA."
      },
      {
        "id": "oai:arXiv.org:2504.10466v1",
        "title": "Art3D: Training-Free 3D Generation from Flat-Colored Illustration",
        "link": "https://arxiv.org/abs/2504.10466",
        "author": "Xiaoyan Cong, Jiayi Shen, Zekun Li, Rao Fu, Tao Lu, Srinath Sridhar",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10466v1 Announce Type: new \nAbstract: Large-scale pre-trained image-to-3D generative models have exhibited remarkable capabilities in diverse shape generations. However, most of them struggle to synthesize plausible 3D assets when the reference image is flat-colored like hand drawings due to the lack of 3D illusion, which are often the most user-friendly input modalities in art content creation. To this end, we propose Art3D, a training-free method that can lift flat-colored 2D designs into 3D. By leveraging structural and semantic features with pre- trained 2D image generation models and a VLM-based realism evaluation, Art3D successfully enhances the three-dimensional illusion in reference images, thus simplifying the process of generating 3D from 2D, and proves adaptable to a wide range of painting styles. To benchmark the generalization performance of existing image-to-3D models on flat-colored images without 3D feeling, we collect a new dataset, Flat-2D, with over 100 samples. Experimental results demonstrate the performance and robustness of Art3D, exhibiting superior generalizable capacity and promising practical applicability. Our source code and dataset will be publicly available on our project page: https://joy-jy11.github.io/ ."
      },
      {
        "id": "oai:arXiv.org:2504.10471v1",
        "title": "MIEB: Massive Image Embedding Benchmark",
        "link": "https://arxiv.org/abs/2504.10471",
        "author": "Chenghao Xiao, Isaac Chung, Imene Kerboua, Jamie Stirling, Xin Zhang, M\\'arton Kardos, Roman Solomatin, Noura Al Moubayed, Kenneth Enevoldsen, Niklas Muennighoff",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10471v1 Announce Type: new \nAbstract: Image representations are often evaluated through disjointed, task-specific protocols, leading to a fragmented understanding of model capabilities. For instance, it is unclear whether an image embedding model adept at clustering images is equally good at retrieving relevant images given a piece of text. We introduce the Massive Image Embedding Benchmark (MIEB) to evaluate the performance of image and image-text embedding models across the broadest spectrum to date. MIEB spans 38 languages across 130 individual tasks, which we group into 8 high-level categories. We benchmark 50 models across our benchmark, finding that no single method dominates across all task categories. We reveal hidden capabilities in advanced vision models such as their accurate visual representation of texts, and their yet limited capabilities in interleaved encodings and matching images and texts in the presence of confounders. We also show that the performance of vision encoders on MIEB correlates highly with their performance when used in multimodal large language models. Our code, dataset, and leaderboard are publicly available at https://github.com/embeddings-benchmark/mteb."
      },
      {
        "id": "oai:arXiv.org:2504.10478v1",
        "title": "Weight Ensembling Improves Reasoning in Language Models",
        "link": "https://arxiv.org/abs/2504.10478",
        "author": "Xingyu Dang, Christina Baek, Kaiyue Wen, Zico Kolter, Aditi Raghunathan",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10478v1 Announce Type: new \nAbstract: We investigate a failure mode that arises during the training of reasoning models, where the diversity of generations begins to collapse, leading to suboptimal test-time scaling. Notably, the Pass@1 rate reliably improves during supervised finetuning (SFT), but Pass@k rapidly deteriorates. Surprisingly, a simple intervention of interpolating the weights of the latest SFT checkpoint with an early checkpoint, otherwise known as WiSE-FT, almost completely recovers Pass@k while also improving Pass@1. The WiSE-FT variant achieves better test-time scaling (Best@k, majority vote) and achieves superior results with less data when tuned further by reinforcement learning. Finally, we find that WiSE-FT provides complementary performance gains that cannot be achieved only through diversity-inducing decoding strategies, like temperature scaling. We formalize a bias-variance tradeoff of Pass@k with respect to the expectation and variance of Pass@1 over the test distribution. We find that WiSE-FT can reduce bias and variance simultaneously, while temperature scaling inherently trades-off between bias and variance."
      },
      {
        "id": "oai:arXiv.org:2504.10479v1",
        "title": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models",
        "link": "https://arxiv.org/abs/2504.10479",
        "author": "Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, Zhangwei Gao, Erfei Cui, Yue Cao, Yangzhou Liu, Weiye Xu, Hao Li, Jiahao Wang, Han Lv, Dengnian Chen, Songze Li, Yinan He, Tan Jiang, Jiapeng Luo, Yi Wang, Conghui He, Botian Shi, Xingcheng Zhang, Wenqi Shao, Junjun He, Yingtong Xiong, Wenwen Qu, Peng Sun, Penglong Jiao, Lijun Wu, Kaipeng Zhang, Huipeng Deng, Jiaye Ge, Kai Chen, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, Wenhai Wang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10479v1 Announce Type: new \nAbstract: We introduce InternVL3, a significant advancement in the InternVL series featuring a native multimodal pre-training paradigm. Rather than adapting a text-only large language model (LLM) into a multimodal large language model (MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and linguistic capabilities from both diverse multimodal data and pure-text corpora during a single pre-training stage. This unified training paradigm effectively addresses the complexities and alignment challenges commonly encountered in conventional post-hoc training pipelines for MLLMs. To further improve performance and scalability, InternVL3 incorporates variable visual position encoding (V2PE) to support extended multimodal contexts, employs advanced post-training techniques such as supervised fine-tuning (SFT) and mixed preference optimization (MPO), and adopts test-time scaling strategies alongside an optimized training infrastructure. Extensive empirical evaluations demonstrate that InternVL3 delivers superior performance across a wide range of multi-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the MMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its capabilities remain highly competitive with leading proprietary models, including ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also maintaining strong pure-language proficiency. In pursuit of open-science principles, we will publicly release both the training data and model weights to foster further research and development in next-generation MLLMs."
      },
      {
        "id": "oai:arXiv.org:2504.10481v1",
        "title": "xVerify: Efficient Answer Verifier for Reasoning Model Evaluations",
        "link": "https://arxiv.org/abs/2504.10481",
        "author": "Ding Chen, Qingchen Yu, Pengyuan Wang, Wentao Zhang, Bo Tang, Feiyu Xiong, Xinchi Li, Minchuan Yang, Zhiyu Li",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10481v1 Announce Type: new \nAbstract: With the release of the o1 model by OpenAI, reasoning models adopting slow thinking strategies have gradually emerged. As the responses generated by such models often include complex reasoning, intermediate steps, and self-reflection, existing evaluation methods are often inadequate. They struggle to determine whether the LLM output is truly equivalent to the reference answer, and also have difficulty identifying and extracting the final answer from long, complex responses. To address this issue, we propose xVerify, an efficient answer verifier for reasoning model evaluations. xVerify demonstrates strong capability in equivalence judgment, enabling it to effectively determine whether the answers produced by reasoning models are equivalent to reference answers across various types of objective questions. To train and evaluate xVerify, we construct the VAR dataset by collecting question-answer pairs generated by multiple LLMs across various datasets, leveraging multiple reasoning models and challenging evaluation sets designed specifically for reasoning model assessment. A multi-round annotation process is employed to ensure label accuracy. Based on the VAR dataset, we train multiple xVerify models of different scales. In evaluation experiments conducted on both the test set and generalization set, all xVerify models achieve overall F1 scores and accuracy exceeding 95\\%. Notably, the smallest variant, xVerify-0.5B-I, outperforms all evaluation methods except GPT-4o, while xVerify-3B-Ib surpasses GPT-4o in overall performance. These results validate the effectiveness and generalizability of xVerify."
      },
      {
        "id": "oai:arXiv.org:2504.10483v1",
        "title": "REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion Transformers",
        "link": "https://arxiv.org/abs/2504.10483",
        "author": "Xingjian Leng, Jaskirat Singh, Yunzhong Hou, Zhenchang Xing, Saining Xie, Liang Zheng",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10483v1 Announce Type: new \nAbstract: In this paper we tackle a fundamental question: \"Can we train latent diffusion models together with the variational auto-encoder (VAE) tokenizer in an end-to-end manner?\" Traditional deep-learning wisdom dictates that end-to-end training is often preferable when possible. However, for latent diffusion transformers, it is observed that end-to-end training both VAE and diffusion-model using standard diffusion-loss is ineffective, even causing a degradation in final performance. We show that while diffusion loss is ineffective, end-to-end training can be unlocked through the representation-alignment (REPA) loss -- allowing both VAE and diffusion model to be jointly tuned during the training process. Despite its simplicity, the proposed training recipe (REPA-E) shows remarkable performance; speeding up diffusion model training by over 17x and 45x over REPA and vanilla training recipes, respectively. Interestingly, we observe that end-to-end tuning with REPA-E also improves the VAE itself; leading to improved latent space structure and downstream generation performance. In terms of final performance, our approach sets a new state-of-the-art; achieving FID of 1.26 and 1.83 with and without classifier-free guidance on ImageNet 256 x 256. Code is available at https://end2end-diffusion.github.io."
      },
      {
        "id": "oai:arXiv.org:2504.10485v1",
        "title": "Decoupled Diffusion Sparks Adaptive Scene Generation",
        "link": "https://arxiv.org/abs/2504.10485",
        "author": "Yunsong Zhou, Naisheng Ye, William Ljungbergh, Tianyu Li, Jiazhi Yang, Zetong Yang, Hongzi Zhu, Christoffer Petersson, Hongyang Li",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10485v1 Announce Type: new \nAbstract: Controllable scene generation could reduce the cost of diverse data collection substantially for autonomous driving. Prior works formulate the traffic layout generation as predictive progress, either by denoising entire sequences at once or by iteratively predicting the next frame. However, full sequence denoising hinders online reaction, while the latter's short-sighted next-frame prediction lacks precise goal-state guidance. Further, the learned model struggles to generate complex or challenging scenarios due to a large number of safe and ordinal driving behaviors from open datasets. To overcome these, we introduce Nexus, a decoupled scene generation framework that improves reactivity and goal conditioning by simulating both ordinal and challenging scenarios from fine-grained tokens with independent noise states. At the core of the decoupled pipeline is the integration of a partial noise-masking training strategy and a noise-aware schedule that ensures timely environmental updates throughout the denoising process. To complement challenging scenario generation, we collect a dataset consisting of complex corner cases. It covers 540 hours of simulated data, including high-risk interactions such as cut-in, sudden braking, and collision. Nexus achieves superior generation realism while preserving reactivity and goal orientation, with a 40% reduction in displacement error. We further demonstrate that Nexus improves closed-loop planning by 20% through data augmentation and showcase its capability in safety-critical data generation."
      },
      {
        "id": "oai:arXiv.org:2504.10486v1",
        "title": "DNF-Avatar: Distilling Neural Fields for Real-time Animatable Avatar Relighting",
        "link": "https://arxiv.org/abs/2504.10486",
        "author": "Zeren Jiang, Shaofei Wang, Siyu Tang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10486v1 Announce Type: new \nAbstract: Creating relightable and animatable human avatars from monocular videos is a rising research topic with a range of applications, e.g. virtual reality, sports, and video games. Previous works utilize neural fields together with physically based rendering (PBR), to estimate geometry and disentangle appearance properties of human avatars. However, one drawback of these methods is the slow rendering speed due to the expensive Monte Carlo ray tracing. To tackle this problem, we proposed to distill the knowledge from implicit neural fields (teacher) to explicit 2D Gaussian splatting (student) representation to take advantage of the fast rasterization property of Gaussian splatting. To avoid ray-tracing, we employ the split-sum approximation for PBR appearance. We also propose novel part-wise ambient occlusion probes for shadow computation. Shadow prediction is achieved by querying these probes only once per pixel, which paves the way for real-time relighting of avatars. These techniques combined give high-quality relighting results with realistic shadow effects. Our experiments demonstrate that the proposed student model achieves comparable or even better relighting results with our teacher model while being 370 times faster at inference time, achieving a 67 FPS rendering speed."
      },
      {
        "id": "oai:arXiv.org:2504.10487v1",
        "title": "FLOSS: Free Lunch in Open-vocabulary Semantic Segmentation",
        "link": "https://arxiv.org/abs/2504.10487",
        "author": "Yasser Benigmim, Mohammad Fahes, Tuan-Hung Vu, Andrei Bursuc, Raoul de Charette",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10487v1 Announce Type: new \nAbstract: Recent Open-Vocabulary Semantic Segmentation (OVSS) models extend the CLIP model to segmentation while maintaining the use of multiple templates (e.g., a photo of , a sketch of a , etc.) for constructing class-wise averaged text embeddings, acting as a classifier. In this paper, we challenge this status quo and investigate the impact of templates for OVSS. Empirically, we observe that for each class, there exist single-template classifiers significantly outperforming the conventional averaged classifier. We refer to them as class-experts. Given access to unlabeled images and without any training involved, we estimate these experts by leveraging the class-wise prediction entropy of single-template classifiers, selecting as class-wise experts those which yield the lowest entropy. All experts, each specializing in a specific class, collaborate in a newly proposed fusion method to generate more accurate OVSS predictions. Our plug-and-play method, coined FLOSS, is orthogonal and complementary to existing OVSS methods, offering a ''free lunch'' to systematically improve OVSS without labels and additional training. Extensive experiments demonstrate that FLOSS consistently boosts state-of-the-art methods on various OVSS benchmarks. Moreover, the selected expert templates can generalize well from one dataset to others sharing the same semantic categories, yet exhibiting distribution shifts. Additionally, we obtain satisfactory improvements under a low-data regime, where only a few unlabeled images are available. Our code is available at https://github.com/yasserben/FLOSS ."
      },
      {
        "id": "oai:arXiv.org:2402.04621v2",
        "title": "Feature Distribution on Graph Topology Mediates the Effect of Graph Convolution: Homophily Perspective",
        "link": "https://arxiv.org/abs/2402.04621",
        "author": "Soo Yong Lee, Sunwoo Kim, Fanchen Bu, Jaemin Yoo, Jiliang Tang, Kijung Shin",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2402.04621v2 Announce Type: cross \nAbstract: How would randomly shuffling feature vectors among nodes from the same class affect graph neural networks (GNNs)? The feature shuffle, intuitively, perturbs the dependence between graph topology and features (A-X dependence) for GNNs to learn from. Surprisingly, we observe a consistent and significant improvement in GNN performance following the feature shuffle. Having overlooked the impact of A-X dependence on GNNs, the prior literature does not provide a satisfactory understanding of the phenomenon. Thus, we raise two research questions. First, how should A-X dependence be measured, while controlling for potential confounds? Second, how does A-X dependence affect GNNs? In response, we (i) propose a principled measure for A-X dependence, (ii) design a random graph model that controls A-X dependence, (iii) establish a theory on how A-X dependence relates to graph convolution, and (iv) present empirical analysis on real-world graphs that align with the theory. We conclude that A-X dependence mediates the effect of graph convolution, such that smaller dependence improves GNN-based node classification."
      },
      {
        "id": "oai:arXiv.org:2504.08016v1",
        "title": "Emergence of psychopathological computations in large language models",
        "link": "https://arxiv.org/abs/2504.08016",
        "author": "Soo Yong Lee, Hyunjin Hwang, Taekwan Kim, Yuyeong Kim, Kyuri Park, Jaemin Yoo, Denny Borsboom, Kijung Shin",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08016v1 Announce Type: cross \nAbstract: Can large language models (LLMs) implement computations of psychopathology? An effective approach to the question hinges on addressing two factors. First, for conceptual validity, we require a general and computational account of psychopathology that is applicable to computational entities without biological embodiment or subjective experience. Second, mechanisms underlying LLM behaviors need to be studied for better methodological validity. Thus, we establish a computational-theoretical framework to provide an account of psychopathology applicable to LLMs. To ground the theory for empirical analysis, we also propose a novel mechanistic interpretability method alongside a tailored empirical analytic framework. Based on the frameworks, we conduct experiments demonstrating three key claims: first, that distinct dysfunctional and problematic representational states are implemented in LLMs; second, that their activations can spread and self-sustain to trap LLMs; and third, that dynamic, cyclic structural causal models encoded in the LLMs underpin these patterns. In concert, the empirical results corroborate our hypothesis that network-theoretic computations of psychopathology have already emerged in LLMs. This suggests that certain LLM behaviors mirroring psychopathology may not be a superficial mimicry but a feature of their internal processing. Thus, our work alludes to the possibility of AI systems with psychopathological behaviors in the near future."
      },
      {
        "id": "oai:arXiv.org:2504.08740v1",
        "title": "Recommendation System in Advertising and Streaming Media: Unsupervised Data Enhancement Sequence Suggestions",
        "link": "https://arxiv.org/abs/2504.08740",
        "author": "Kowei Shih, Yi Han, Li Tan",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08740v1 Announce Type: cross \nAbstract: Sequential recommendation is an extensively explored approach to capturing users' evolving preferences based on past interactions, aimed at predicting their next likely choice. Despite significant advancements in this domain, including methods based on RNNs and self-attention, challenges like limited supervised signals and noisy data caused by unintentional clicks persist. To address these challenges, some studies have incorporated unsupervised learning by leveraging local item contexts within individual sequences. However, these methods often overlook the intricate associations between items across multiple sequences and are susceptible to noise in item co-occurrence patterns. In this context, we introduce a novel framework, Global Unsupervised Data-Augmentation (UDA4SR), which adopts a graph contrastive learning perspective to generate more robust item embeddings for sequential recommendation. Our approach begins by integrating Generative Adversarial Networks (GANs) for data augmentation, which serves as the first step to enhance the diversity and richness of the training data. Then, we build a Global Item Relationship Graph (GIG) based on all user interaction sequences. Subsequently, we employ graph contrastive learning on the refined graph to enhance item embeddings by capturing complex global associations. To model users' dynamic and diverse interests more effectively, we enhance the CapsNet module with a novel target-attention mechanism. Extensive experiments show that UDA4SR significantly outperforms state-of-the-art approaches."
      },
      {
        "id": "oai:arXiv.org:2504.08743v1",
        "title": "Dynamic Topic Analysis in Academic Journals using Convex Non-negative Matrix Factorization Method",
        "link": "https://arxiv.org/abs/2504.08743",
        "author": "Yang Yang, Tong Zhang, Jian Wu, Lijie Su",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08743v1 Announce Type: cross \nAbstract: With the rapid advancement of large language models, academic topic identification and topic evolution analysis are crucial for enhancing AI's understanding capabilities. Dynamic topic analysis provides a powerful approach to capturing and understanding the temporal evolution of topics in large-scale datasets. This paper presents a two-stage dynamic topic analysis framework that incorporates convex optimization to improve topic consistency, sparsity, and interpretability. In Stage 1, a two-layer non-negative matrix factorization (NMF) model is employed to extract annual topics and identify key terms. In Stage 2, a convex optimization algorithm refines the dynamic topic structure using the convex NMF (cNMF) model, further enhancing topic integration and stability. Applying the proposed method to IEEE journal abstracts from 2004 to 2022 effectively identifies and quantifies emerging research topics, such as COVID-19 and digital twins. By optimizing sparsity differences in the clustering feature space between traditional and emerging research topics, the framework provides deeper insights into topic evolution and ranking analysis. Moreover, the NMF-cNMF model demonstrates superior stability in topic consistency. At sparsity levels of 0.4, 0.6, and 0.9, the proposed approach improves topic ranking stability by 24.51%, 56.60%, and 36.93%, respectively. The source code (to be open after publication) is available at https://github.com/meetyangyang/CDNMF."
      },
      {
        "id": "oai:arXiv.org:2504.08744v1",
        "title": "ExpertRAG: Efficient RAG with Mixture of Experts -- Optimizing Context Retrieval for Adaptive LLM Responses",
        "link": "https://arxiv.org/abs/2504.08744",
        "author": "Esmail Gumaan",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08744v1 Announce Type: cross \nAbstract: ExpertRAG is a novel theoretical framework that integrates Mixture-of-Experts (MoE) architectures with Retrieval Augmented Generation (RAG) to advance the efficiency and accuracy of knowledge-intensive language modeling. We propose a dynamic retrieval gating mechanism coupled with expert routing, enabling the model to selectively consult an external knowledge store or rely on specialized internal experts based on the query's needs. The paper lays out the theoretical foundations of ExpertRAG, including a probabilistic formulation that treats retrieval and expert selection as latent decisions, and mathematical justifications for its efficiency in both computation and knowledge utilization. We derive formulae to quantify the expected computational cost savings from selective retrieval and the capacity gains from sparse expert utilization. A comparative analysis positions ExpertRAG against standard RAG (with always-on retrieval) and pure MoE models (e.g., Switch Transformer, Mixtral) to highlight its unique balance between parametric knowledge and non-parametric retrieval. We also outline an experimental validation strategy, proposing benchmarks and evaluation protocols to test ExpertRAG's performance on factual recall, generalization, and inference efficiency. The proposed framework, although presented theoretically, is supported by insights from prior work in RAG and MoE, and is poised to provide more factual, efficient, and adaptive generation by leveraging the best of both paradigms. In summary, ExpertRAG contributes a new perspective on scaling and augmenting language models, backed by a thorough analysis and a roadmap for empirical validation."
      },
      {
        "id": "oai:arXiv.org:2504.08745v1",
        "title": "Improving RAG for Personalization with Author Features and Contrastive Examples",
        "link": "https://arxiv.org/abs/2504.08745",
        "author": "Mert Yazan, Suzan Verberne, Frederik Situmeang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08745v1 Announce Type: cross \nAbstract: Personalization with retrieval-augmented generation (RAG) often fails to capture fine-grained features of authors, making it hard to identify their unique traits. To enrich the RAG context, we propose providing Large Language Models (LLMs) with author-specific features, such as average sentiment polarity and frequently used words, in addition to past samples from the author's profile. We introduce a new feature called Contrastive Examples: documents from other authors are retrieved to help LLM identify what makes an author's style unique in comparison to others. Our experiments show that adding a couple of sentences about the named entities, dependency patterns, and words a person uses frequently significantly improves personalized text generation. Combining features with contrastive examples boosts the performance further, achieving a relative 15% improvement over baseline RAG while outperforming the benchmarks. Our results show the value of fine-grained features for better personalization, while opening a new research dimension for including contrastive examples as a complement with RAG. We release our code publicly."
      },
      {
        "id": "oai:arXiv.org:2504.08748v1",
        "title": "A Survey of Multimodal Retrieval-Augmented Generation",
        "link": "https://arxiv.org/abs/2504.08748",
        "author": "Lang Mei, Siyu Mo, Zhihan Yang, Chong Chen",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08748v1 Announce Type: cross \nAbstract: Multimodal Retrieval-Augmented Generation (MRAG) enhances large language models (LLMs) by integrating multimodal data (text, images, videos) into retrieval and generation processes, overcoming the limitations of text-only Retrieval-Augmented Generation (RAG). While RAG improves response accuracy by incorporating external textual knowledge, MRAG extends this framework to include multimodal retrieval and generation, leveraging contextual information from diverse data types. This approach reduces hallucinations and enhances question-answering systems by grounding responses in factual, multimodal knowledge. Recent studies show MRAG outperforms traditional RAG, especially in scenarios requiring both visual and textual understanding. This survey reviews MRAG's essential components, datasets, evaluation methods, and limitations, providing insights into its construction and improvement. It also identifies challenges and future research directions, highlighting MRAG's potential to revolutionize multimodal information retrieval and generation. By offering a comprehensive perspective, this work encourages further exploration into this promising paradigm."
      },
      {
        "id": "oai:arXiv.org:2504.08753v1",
        "title": "Domain Specific Question to SQL Conversion with Embedded Data Balancing Technique",
        "link": "https://arxiv.org/abs/2504.08753",
        "author": "Jyothi, T. Satyanarayana Murthy",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08753v1 Announce Type: cross \nAbstract: The rise of deep learning in natural language processing has fostered the creation of text to structured query language models composed of an encoder and a decoder. Researchers have experimented with various intermediate processing like schema linking, table type aware, value extract. To generate accurate SQL results for the user question. However error analysis performed on the failed cases on these systems shows, 29 percentage of the errors would be because the system was unable to understand the values expressed by the user in their question. This challenge affects the generation of accurate SQL queries, especially when dealing with domain-specific terms and specific value conditions, where traditional methods struggle to maintain consistency and precision. To overcome these obstacles, proposed two intermediations like implementing data balancing technique and over sampling domain-specific queries which would refine the model architecture to enhance value recognition and fine tuning the model for domain-specific questions. This proposed solution achieved 10.98 percentage improvement in accuracy of the model performance compared to the state of the art model tested on WikiSQL dataset. to convert the user question accurately to SQL queries. Applying oversampling technique on the domain-specific questions shown a significant improvement as compared with traditional approaches."
      },
      {
        "id": "oai:arXiv.org:2504.08763v1",
        "title": "WebMap -- Large Language Model-assisted Semantic Link Induction in the Web",
        "link": "https://arxiv.org/abs/2504.08763",
        "author": "Shiraj Pokharel, Georg P. Ro{\\ss}rucker, Mario M. Kubek",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08763v1 Announce Type: cross \nAbstract: Carrying out research tasks is only inadequately supported, if not hindered, by current web search engines. This paper therefore proposes functional extensions of WebMap, a semantically induced overlay linking structure on the web to inherently facilitate research activities. These add-ons support the dynamic determination and regrouping of document clusters, the creation of a semantic signpost in the web, and the interactive tracing of topics back to their origins."
      },
      {
        "id": "oai:arXiv.org:2504.08764v1",
        "title": "Evaluation of the phi-3-mini SLM for identification of texts related to medicine, health, and sports injuries",
        "link": "https://arxiv.org/abs/2504.08764",
        "author": "Chris Brogly, Saif Rjaibi, Charlotte Liang, Erica Lam, Edward Wang, Adam Levitan, Sarah Paleczny, Michael Cusimano",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08764v1 Announce Type: cross \nAbstract: Small Language Models (SLMs) have potential to be used for automatically labelling and identifying aspects of text data for medicine/health-related purposes from documents and the web. As their resource requirements are significantly lower than Large Language Models (LLMs), these can be deployed potentially on more types of devices. SLMs often are benchmarked on health/medicine-related tasks, such as MedQA, although performance on these can vary especially depending on the size of the model in terms of number of parameters. Furthermore, these test results may not necessarily reflect real-world performance regarding the automatic labelling or identification of texts in documents and the web. As a result, we compared topic-relatedness scores from Microsofts phi-3-mini-4k-instruct SLM to the topic-relatedness scores from 7 human evaluators on 1144 samples of medical/health-related texts and 1117 samples of sports injury-related texts. These texts were from a larger dataset of about 9 million news headlines, each of which were processed and assigned scores by phi-3-mini-4k-instruct. Our sample was selected (filtered) based on 1 (low filtering) or more (high filtering) Boolean conditions on the phi-3 SLM scores. We found low-moderate significant correlations between the scores from the SLM and human evaluators for sports injury texts with low filtering (\\r{ho} = 0.3413, p < 0.001) and medicine/health texts with high filtering (\\r{ho} = 0.3854, p < 0.001), and low significant correlation for medicine/health texts with low filtering (\\r{ho} = 0.2255, p < 0.001). There was negligible, insignificant correlation for sports injury-related texts with high filtering (\\r{ho} = 0.0318, p = 0.4466)."
      },
      {
        "id": "oai:arXiv.org:2504.08766v1",
        "title": "Towards scientific machine learning for granular material simulations -- challenges and opportunities",
        "link": "https://arxiv.org/abs/2504.08766",
        "author": "Marc Fransen, Andreas F\\\"urst, Deepak Tunuguntla, Daniel N. Wilke, Benedikt Alkin, Daniel Barreto, Johannes Brandstetter, Miguel Angel Cabrera, Xinyan Fan, Mengwu Guo, Bram Kieskamp, Krishna Kumar, John Morrissey, Jonathan Nuttall, Jin Ooi, Luisa Orozco, Stefanos-Aldo Papanicolopulos, Tongming Qu, Dingena Schott, Takayuki Shuku, WaiChing Sun, Thomas Weinhart, Dongwei Ye, Hongyang Cheng",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08766v1 Announce Type: cross \nAbstract: Micro-scale mechanisms, such as inter-particle and particle-fluid interactions, govern the behaviour of granular systems. While particle-scale simulations provide detailed insights into these interactions, their computational cost is often prohibitive. Attended by researchers from both the granular materials (GM) and machine learning (ML) communities, a recent Lorentz Center Workshop on \"Machine Learning for Discrete Granular Media\" brought the ML community up to date with GM challenges.\n  This position paper emerged from the workshop discussions. We define granular materials and identify seven key challenges that characterise their distinctive behaviour across various scales and regimes, ranging from gas-like to fluid-like and solid-like. Addressing these challenges is essential for developing robust and efficient digital twins for granular systems in various industrial applications. To showcase the potential of ML to the GM community, we present classical and emerging machine/deep learning techniques that have been, or could be, applied to granular materials. We reviewed sequence-based learning models for path-dependent constitutive behaviour, followed by encoder-decoder type models for representing high-dimensional data. We then explore graph neural networks and recent advances in neural operator learning. Lastly, we discuss model-order reduction and probabilistic learning techniques for high-dimensional parameterised systems, which are crucial for quantifying uncertainties arising from physics-based and data-driven models.\n  We present a workflow aimed at unifying data structures and modelling pipelines and guiding readers through the selection, training, and deployment of ML surrogates for granular material simulations. Finally, we illustrate the workflow's practical use with two representative examples, focusing on granular materials in solid-like and fluid-like regimes."
      },
      {
        "id": "oai:arXiv.org:2504.08769v1",
        "title": "High-order expansion of Neural Ordinary Differential Equations flows",
        "link": "https://arxiv.org/abs/2504.08769",
        "author": "Dario Izzo, Sebastien Origer, Giacomo Acciarini, Francesco Biscani",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08769v1 Announce Type: cross \nAbstract: Artificial neural networks, widely recognised for their role in machine learning, are now transforming the study of ordinary differential equations (ODEs), bridging data-driven modelling with classical dynamical systems and enabling the development of infinitely deep neural models. However, the practical applicability of these models remains constrained by the opacity of their learned dynamics, which operate as black-box systems with limited explainability, thereby hindering trust in their deployment. Existing approaches for the analysis of these dynamical systems are predominantly restricted to first-order gradient information due to computational constraints, thereby limiting the depth of achievable insight. Here, we introduce Event Transition Tensors, a framework based on high-order differentials that provides a rigorous mathematical description of neural ODE dynamics on event manifolds. We demonstrate its versatility across diverse applications: characterising uncertainties in a data-driven prey-predator control model, analysing neural optimal feedback dynamics, and mapping landing trajectories in a three-body neural Hamiltonian system. In all cases, our method enhances the interpretability and rigour of neural ODEs by expressing their behaviour through explicit mathematical structures. Our findings contribute to a deeper theoretical foundation for event-triggered neural differential equations and provide a mathematical construct for explaining complex system dynamics."
      },
      {
        "id": "oai:arXiv.org:2504.08773v1",
        "title": "Counterfactual Inference under Thompson Sampling",
        "link": "https://arxiv.org/abs/2504.08773",
        "author": "Olivier Jeunen",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08773v1 Announce Type: cross \nAbstract: Recommender systems exemplify sequential decision-making under uncertainty, strategically deciding what content to serve to users, to optimise a range of potential objectives. To balance the explore-exploit trade-off successfully, Thompson sampling provides a natural and widespread paradigm to probabilistically select which action to take. Questions of causal and counterfactual inference, which underpin use-cases like offline evaluation, are not straightforward to answer in these contexts. Specifically, whilst most existing estimators rely on action propensities, these are not readily available under Thompson sampling procedures.\n  We derive exact and efficiently computable expressions for action propensities under a variety of parameter and outcome distributions, enabling the use of off-policy estimators in Thompson sampling scenarios. This opens up a range of practical use-cases where counterfactual inference is crucial, including unbiased offline evaluation of recommender systems, as well as general applications of causal inference in online advertising, personalisation, and beyond."
      },
      {
        "id": "oai:arXiv.org:2504.08777v1",
        "title": "The Lyme Disease Controversy: An AI-Driven Discourse Analysis of a Quarter Century of Academic Debate and Divides",
        "link": "https://arxiv.org/abs/2504.08777",
        "author": "Teo Susnjak, Cole Palffy, Tatiana Zimina, Nazgul Altynbekova, Kunal Garg, Leona Gilbert",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08777v1 Announce Type: cross \nAbstract: The scientific discourse surrounding Chronic Lyme Disease (CLD) and Post-Treatment Lyme Disease Syndrome (PTLDS) has evolved over the past twenty-five years into a complex and polarised debate, shaped by shifting research priorities, institutional influences, and competing explanatory models. This study presents the first large-scale, systematic examination of this discourse using an innovative hybrid AI-driven methodology, combining large language models with structured human validation to analyse thousands of scholarly abstracts spanning 25 years. By integrating Large Language Models (LLMs) with expert oversight, we developed a quantitative framework for tracking epistemic shifts in contested medical fields, with applications to other content analysis domains. Our analysis revealed a progressive transition from infection-based models of Lyme disease to immune-mediated explanations for persistent symptoms. This study offers new empirical insights into the structural and epistemic forces shaping Lyme disease research, providing a scalable and replicable methodology for analysing discourse, while underscoring the value of AI-assisted methodologies in social science and medical research."
      },
      {
        "id": "oai:arXiv.org:2504.08780v1",
        "title": "How Relevance Emerges: Interpreting LoRA Fine-Tuning in Reranking LLMs",
        "link": "https://arxiv.org/abs/2504.08780",
        "author": "Atharva Nijasure, Tanya Chowdhury, James Allan",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08780v1 Announce Type: cross \nAbstract: We conduct a behavioral exploration of LoRA fine-tuned LLMs for Passage Reranking to understand how relevance signals are learned and deployed by Large Language Models. By fine-tuning Mistral-7B, LLaMA3.1-8B, and Pythia-6.9B on MS MARCO under diverse LoRA configurations, we investigate how relevance modeling evolves across checkpoints, the impact of LoRA rank (1, 2, 8, 32), and the relative importance of updated MHA vs. MLP components. Our ablations reveal which layers and projections within LoRA transformations are most critical for reranking accuracy. These findings offer fresh explanations into LoRA's adaptation mechanisms, setting the stage for deeper mechanistic studies in Information Retrieval. All models used in this study have been shared."
      },
      {
        "id": "oai:arXiv.org:2504.08784v1",
        "title": "SLOs-Serve: Optimized Serving of Multi-SLO LLMs",
        "link": "https://arxiv.org/abs/2504.08784",
        "author": "Siyuan Chen, Zhipeng Jia, Samira Khan, Arvind Krishnamurthy, Phillip B. Gibbons",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08784v1 Announce Type: cross \nAbstract: This paper introduces SLOs-Serve, a system designed for serving multi-stage large language model (LLM) requests with application- and stage-specific service level objectives (SLOs). The key idea behind SLOs-Serve is to customize the allocation of tokens to meet these SLO requirements. SLOs-Serve uses a multi-SLO dynamic programming-based algorithm to continuously optimize token allocations under SLO constraints by exploring the full design space of chunked prefill and (optional) speculative decoding. Leveraging this resource planning algorithm, SLOs-Serve effectively supports multi-SLOs and multi-replica serving with dynamic request routing while being resilient to bursty arrivals. Our evaluation across 6 LLM application scenarios (including summarization, coding, chatbot, tool calling, and reasoning) demonstrates that SLOs-Serve improves per-GPU serving capacity by 2.2x on average compared to prior state-of-the-art systems."
      },
      {
        "id": "oai:arXiv.org:2504.08804v1",
        "title": "Estimating Item Difficulty Using Large Language Models and Tree-Based Machine Learning Algorithms",
        "link": "https://arxiv.org/abs/2504.08804",
        "author": "Pooya Razavi, Sonya J. Powers",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08804v1 Announce Type: cross \nAbstract: Estimating item difficulty through field-testing is often resource-intensive and time-consuming. As such, there is strong motivation to develop methods that can predict item difficulty at scale using only the item content. Large Language Models (LLMs) represent a new frontier for this goal. The present research examines the feasibility of using an LLM to predict item difficulty for K-5 mathematics and reading assessment items (N = 5170). Two estimation approaches were implemented: (a) a direct estimation method that prompted the LLM to assign a single difficulty rating to each item, and (b) a feature-based strategy where the LLM extracted multiple cognitive and linguistic features, which were then used in ensemble tree-based models (random forests and gradient boosting) to predict difficulty. Overall, direct LLM estimates showed moderate to strong correlations with true item difficulties. However, their accuracy varied by grade level, often performing worse for early grades. In contrast, the feature-based method yielded stronger predictive accuracy, with correlations as high as r = 0.87 and lower error estimates compared to both direct LLM predictions and baseline regressors. These findings highlight the promise of LLMs in streamlining item development and reducing reliance on extensive field testing and underscore the importance of structured feature extraction. We provide a seven-step workflow for testing professionals who would want to implement a similar item difficulty estimation approach with their item pool."
      },
      {
        "id": "oai:arXiv.org:2504.08805v1",
        "title": "Generative AI in Live Operations: Evidence of Productivity Gains in Cybersecurity and Endpoint Management",
        "link": "https://arxiv.org/abs/2504.08805",
        "author": "James Bono, Justin Grana, Kleanthis Karakolios, Pruthvi Hanumanthapura Ramakrishna, Ankit Srivastava",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08805v1 Announce Type: cross \nAbstract: We measure the association between generative AI (GAI) tool adoption and four metrics spanning security operations, information protection, and endpoint management: 1) number of security alerts per incident, 2) probability of security incident reopenings, 3) time to classify a data loss prevention alert, and 4) time to resolve device policy conflicts. We find that GAI is associated with robust and statistically and practically significant improvements in the four metrics. Although unobserved confounders inhibit causal identification, these results are among the first to use observational data from live operations to investigate the relationship between GAI adoption and security operations, data loss prevention, and device policy management."
      },
      {
        "id": "oai:arXiv.org:2504.08814v1",
        "title": "When Federated Learning Meets Quantum Computing: Survey and Research Opportunities",
        "link": "https://arxiv.org/abs/2504.08814",
        "author": "Aakar Mathur, Ashish Gupta, Sajal K. Das",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08814v1 Announce Type: cross \nAbstract: Quantum Federated Learning (QFL) is an emerging field that harnesses advances in Quantum Computing (QC) to improve the scalability and efficiency of decentralized Federated Learning (FL) models. This paper provides a systematic and comprehensive survey of the emerging problems and solutions when FL meets QC, from research protocol to a novel taxonomy, particularly focusing on both quantum and federated limitations, such as their architectures, Noisy Intermediate Scale Quantum (NISQ) devices, and privacy preservation, so on. This work explores key developments and integration strategies, along with the impact of quantum computing on FL, keeping a sharp focus on hybrid quantum-classical approaches. The paper offers an in-depth understanding of how the strengths of QC, such as gradient hiding, state entanglement, quantum key distribution, quantum security, and quantum-enhanced differential privacy, have been integrated into FL to ensure the privacy of participants in an enhanced, fast, and secure framework. Finally, this study proposes potential future directions to address the identified research gaps and challenges, aiming to inspire faster and more secure QFL models for practical use."
      },
      {
        "id": "oai:arXiv.org:2504.08817v1",
        "title": "Exploring utilization of generative AI for research and education in data-driven materials science",
        "link": "https://arxiv.org/abs/2504.08817",
        "author": "Takahiro Misawa, Ai Koizumi, Ryo Tamura, Kazuyoshi Yoshimi",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08817v1 Announce Type: cross \nAbstract: Generative AI has recently had a profound impact on various fields, including daily life, research, and education. To explore its efficient utilization in data-driven materials science, we organized a hackathon -- AIMHack2024 -- in July 2024. In this hackathon, researchers from fields such as materials science, information science, bioinformatics, and condensed matter physics worked together to explore how generative AI can facilitate research and education. Based on the results of the hackathon, this paper presents topics related to (1) conducting AI-assisted software trials, (2) building AI tutors for software, and (3) developing GUI applications for software. While generative AI continues to evolve rapidly, this paper provides an early record of its application in data-driven materials science and highlights strategies for integrating AI into research and education."
      },
      {
        "id": "oai:arXiv.org:2504.08836v1",
        "title": "Double Machine Learning for Causal Inference under Shared-State Interference",
        "link": "https://arxiv.org/abs/2504.08836",
        "author": "Chris Hays, Manish Raghavan",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08836v1 Announce Type: cross \nAbstract: Researchers and practitioners often wish to measure treatment effects in settings where units interact via markets and recommendation systems. In these settings, units are affected by certain shared states, like prices, algorithmic recommendations or social signals. We formalize this structure, calling it shared-state interference, and argue that our formulation captures many relevant applied settings. Our key modeling assumption is that individuals' potential outcomes are independent conditional on the shared state. We then prove an extension of a double machine learning (DML) theorem providing conditions for achieving efficient inference under shared-state interference. We also instantiate our general theorem in several models of interest where it is possible to efficiently estimate the average direct effect (ADE) or global average treatment effect (GATE)."
      },
      {
        "id": "oai:arXiv.org:2504.08846v1",
        "title": "AI-University: An LLM-based platform for instructional alignment to scientific classrooms",
        "link": "https://arxiv.org/abs/2504.08846",
        "author": "Mostafa Faghih Shojaei, Rahul Gulati, Benjamin A. Jasperson, Shangshang Wang, Simone Cimolato, Dangli Cao, Willie Neiswanger, Krishna Garikipati",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08846v1 Announce Type: cross \nAbstract: We introduce AI University (AI-U), a flexible framework for AI-driven course content delivery that adapts to instructors' teaching styles. At its core, AI-U fine-tunes a large language model (LLM) with retrieval-augmented generation (RAG) to generate instructor-aligned responses from lecture videos, notes, and textbooks. Using a graduate-level finite-element-method (FEM) course as a case study, we present a scalable pipeline to systematically construct training data, fine-tune an open-source LLM with Low-Rank Adaptation (LoRA), and optimize its responses through RAG-based synthesis. Our evaluation - combining cosine similarity, LLM-based assessment, and expert review - demonstrates strong alignment with course materials. We also have developed a prototype web application, available at https://my-ai-university.com, that enhances traceability by linking AI-generated responses to specific sections of the relevant course material and time-stamped instances of the open-access video lectures. Our expert model is found to have greater cosine similarity with a reference on 86% of test cases. An LLM judge also found our expert model to outperform the base Llama 3.2 model approximately four times out of five. AI-U offers a scalable approach to AI-assisted education, paving the way for broader adoption in higher education. Here, our framework has been presented in the setting of a class on FEM - a subject that is central to training PhD and Master students in engineering science. However, this setting is a particular instance of a broader context: fine-tuning LLMs to research content in science."
      },
      {
        "id": "oai:arXiv.org:2504.08861v1",
        "title": "Diachronic and synchronic variation in the performance of adaptive machine learning systems: The ethical challenges",
        "link": "https://arxiv.org/abs/2504.08861",
        "author": "Joshua Hatherley, Robert Sparrow",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08861v1 Announce Type: cross \nAbstract: Objectives: Machine learning (ML) has the potential to facilitate \"continual learning\" in medicine, in which an ML system continues to evolve in response to exposure to new data over time, even after being deployed in a clinical setting. In this paper, we provide a tutorial on the range of ethical issues raised by the use of such \"adaptive\" ML systems in medicine that have, thus far, been neglected in the literature.\n  Target audience: The target audiences for this tutorial are the developers of machine learning AI systems, healthcare regulators, the broader medical informatics community, and practicing clinicians.\n  Scope: Discussions of adaptive ML systems to date have overlooked the distinction between two sorts of variance that such systems may exhibit -- diachronic evolution (change over time) and synchronic variation (difference between cotemporaneous instantiations of the algorithm at different sites) -- and under-estimated the significance of the latter. We highlight the challenges that diachronic evolution and synchronic variation present for the quality of patient care, informed consent, and equity, and discuss the complex ethical trade-offs involved in the design of such systems."
      },
      {
        "id": "oai:arXiv.org:2504.08875v1",
        "title": "DataMap: A Portable Application for Visualizing High-Dimensional Data",
        "link": "https://arxiv.org/abs/2504.08875",
        "author": "Xijin Ge",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08875v1 Announce Type: cross \nAbstract: Motivation: The visualization and analysis of high-dimensional data are essential in biomedical research. There is a need for secure, scalable, and reproducible tools to facilitate data exploration and interpretation. Results: We introduce DataMap, a browser-based application for visualization of high-dimensional data using heatmaps, principal component analysis (PCA), and t-distributed stochastic neighbor embedding (t-SNE). DataMap runs in the web browser, ensuring data privacy while eliminating the need for installation or a server. The application has an intuitive user interface for data transformation, annotation, and generation of reproducible R code. Availability and Implementation: Freely available as a GitHub page https://gexijin.github.io/datamap/. The source code can be found at https://github.com/gexijin/datamap, and can also be installed as an R package. Contact: Xijin.Ge@sdstate.ed"
      },
      {
        "id": "oai:arXiv.org:2504.08907v1",
        "title": "Spatial Audio Processing with Large Language Model on Wearable Devices",
        "link": "https://arxiv.org/abs/2504.08907",
        "author": "Ayushi Mishra, Yang Bai, Priyadarshan Narayanasamy, Nakul Garg, Nirupam Roy",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08907v1 Announce Type: cross \nAbstract: Integrating spatial context into large language models (LLMs) has the potential to revolutionize human-computer interaction, particularly in wearable devices. In this work, we present a novel system architecture that incorporates spatial speech understanding into LLMs, enabling contextually aware and adaptive applications for wearable technologies. Our approach leverages microstructure-based spatial sensing to extract precise Direction of Arrival (DoA) information using a monaural microphone. To address the lack of existing dataset for microstructure-assisted speech recordings, we synthetically create a dataset called OmniTalk by using the LibriSpeech dataset. This spatial information is fused with linguistic embeddings from OpenAI's Whisper model, allowing each modality to learn complementary contextual representations. The fused embeddings are aligned with the input space of LLaMA-3.2 3B model and fine-tuned with lightweight adaptation technique LoRA to optimize for on-device processing. SING supports spatially-aware automatic speech recognition (ASR), achieving a mean error of $25.72^\\circ$-a substantial improvement compared to the 88.52$^\\circ$ median error in existing work-with a word error rate (WER) of 5.3. SING also supports soundscaping, for example, inference how many people were talking and their directions, with up to 5 people and a median DoA error of 16$^\\circ$. Our system demonstrates superior performance in spatial speech understanding while addressing the challenges of power efficiency, privacy, and hardware constraints, paving the way for advanced applications in augmented reality, accessibility, and immersive experiences."
      },
      {
        "id": "oai:arXiv.org:2504.08909v1",
        "title": "Hybrid AI-Physical Modeling for Penetration Bias Correction in X-band InSAR DEMs: A Greenland Case Study",
        "link": "https://arxiv.org/abs/2504.08909",
        "author": "Islam Mansour, Georg Fischer, Ronny Haensch, Irena Hajnsek",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08909v1 Announce Type: cross \nAbstract: Digital elevation models derived from Interferometric Synthetic Aperture Radar (InSAR) data over glacial and snow-covered regions often exhibit systematic elevation errors, commonly termed \"penetration bias.\" We leverage existing physics-based models and propose an integrated correction framework that combines parametric physical modeling with machine learning. We evaluate the approach across three distinct training scenarios - each defined by a different set of acquisition parameters - to assess overall performance and the model's ability to generalize. Our experiments on Greenland's ice sheet using TanDEM-X data show that the proposed hybrid model corrections significantly reduce the mean and standard deviation of DEM errors compared to a purely physical modeling baseline. The hybrid framework also achieves significantly improved generalization than a pure ML approach when trained on data with limited diversity in acquisition parameters."
      },
      {
        "id": "oai:arXiv.org:2504.08916v1",
        "title": "Improving the evaluation of samplers on multi-modal targets",
        "link": "https://arxiv.org/abs/2504.08916",
        "author": "Louis Grenioux, Maxence Noble, Marylou Gabri\\'e",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08916v1 Announce Type: cross \nAbstract: Addressing multi-modality constitutes one of the major challenges of sampling. In this reflection paper, we advocate for a more systematic evaluation of samplers towards two sources of difficulty that are mode separation and dimension. For this, we propose a synthetic experimental setting that we illustrate on a selection of samplers, focusing on the challenging criterion of recovery of the mode relative importance. These evaluations are crucial to diagnose the potential of samplers to handle multi-modality and therefore to drive progress in the field."
      },
      {
        "id": "oai:arXiv.org:2504.08937v1",
        "title": "Rethinking Few-Shot Fusion: Granular Ball Priors Enable General-Purpose Deep Image Fusion",
        "link": "https://arxiv.org/abs/2504.08937",
        "author": "Minjie Deng, Yan Wei, Hao Zhai, An Wu, Yuncan Ouyang, Qianyao Peng",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08937v1 Announce Type: cross \nAbstract: In image fusion tasks, due to the lack of real fused images as priors, most deep learning-based fusion methods obtain global weight features from original images in large-scale data pairs to generate images that approximate real fused images. However, unlike previous studies, this paper utilizes Granular Ball adaptation to extract features in the brightness space as priors for deep networks, enabling the fusion network to converge quickly and complete the fusion task. This leads to few-shot training for a general image fusion network, and based on this, we propose the GBFF fusion method. According to the information expression division of pixel pairs in the original fused image, we classify pixel pairs with significant performance as the positive domain and non-significant pixel pairs as the boundary domain. We perform split inference in the brightness space using Granular Ball adaptation to compute weights for pixels that express information to varying degrees, generating approximate supervision images that provide priors for the neural network in the structural brightness space. Additionally, the extracted global saliency features also adaptively provide priors for setting the loss function weights of each image in the network, guiding the network to converge quickly at both global and pixel levels alongside the supervised images, thereby enhancing the expressiveness of the fused images. Each modality only used 10 pairs of images as the training set, completing the fusion task with a limited number of iterations. Experiments validate the effectiveness of the algorithm and theory, and qualitative and quantitative comparisons with SOTA methods show that this approach is highly competitive in terms of fusion time and image expressiveness."
      },
      {
        "id": "oai:arXiv.org:2504.08949v1",
        "title": "Large Language Model Empowered Recommendation Meets All-domain Continual Pre-Training",
        "link": "https://arxiv.org/abs/2504.08949",
        "author": "Haokai Ma, Yunshan Ma, Ruobing Xie, Lei Meng, Jialie Shen, Xingwu Sun, Zhanhui Kang, Tat-Seng Chua",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08949v1 Announce Type: cross \nAbstract: Recent research efforts have investigated how to integrate Large Language Models (LLMs) into recommendation, capitalizing on their semantic comprehension and open-world knowledge for user behavior understanding. These approaches predominantly employ supervised fine-tuning on single-domain user interactions to adapt LLMs for specific recommendation tasks. However, they typically encounter dual challenges: the mismatch between general language representations and domain-specific preference patterns, as well as the limited adaptability to multi-domain recommendation scenarios. To bridge these gaps, we introduce CPRec -- an All-domain Continual Pre-Training framework for Recommendation -- designed to holistically align LLMs with universal user behaviors through the continual pre-training paradigm. Specifically, we first design a unified prompt template and organize users' multi-domain behaviors into domain-specific behavioral sequences and all-domain mixed behavioral sequences that emulate real-world user decision logic. To optimize behavioral knowledge infusion, we devise a Warmup-Stable-Annealing learning rate schedule tailored for the continual pre-training paradigm in recommendation to progressively enhance the LLM's capability in knowledge adaptation from open-world knowledge to universal recommendation tasks. To evaluate the effectiveness of our CPRec, we implement it on a large-scale dataset covering seven domains and conduct extensive experiments on five real-world datasets from two distinct platforms. Experimental results confirm that our continual pre-training paradigm significantly mitigates the semantic-behavioral discrepancy and achieves state-of-the-art performance in all recommendation scenarios. The source code will be released upon acceptance."
      },
      {
        "id": "oai:arXiv.org:2504.08974v1",
        "title": "Mixed Signals: Decoding VLMs' Reasoning and Underlying Bias in Vision-Language Conflict",
        "link": "https://arxiv.org/abs/2504.08974",
        "author": "Pouya Pezeshkpour, Moin Aminnaseri, Estevam Hruschka",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08974v1 Announce Type: cross \nAbstract: Vision-language models (VLMs) have demonstrated impressive performance by effectively integrating visual and textual information to solve complex tasks. However, it is not clear how these models reason over the visual and textual data together, nor how the flow of information between modalities is structured. In this paper, we examine how VLMs reason by analyzing their biases when confronted with scenarios that present conflicting image and text cues, a common occurrence in real-world applications. To uncover the extent and nature of these biases, we build upon existing benchmarks to create five datasets containing mismatched image-text pairs, covering topics in mathematics, science, and visual descriptions. Our analysis shows that VLMs favor text in simpler queries but shift toward images as query complexity increases. This bias correlates with model scale, with the difference between the percentage of image- and text-preferred responses ranging from +56.8% (image favored) to -74.4% (text favored), depending on the task and model. In addition, we explore three mitigation strategies: simple prompt modifications, modifications that explicitly instruct models on how to handle conflicting information (akin to chain-of-thought prompting), and a task decomposition strategy that analyzes each modality separately before combining their results. Our findings indicate that the effectiveness of these strategies in identifying and mitigating bias varies significantly and is closely linked to the model's overall performance on the task and the specific modality in question."
      },
      {
        "id": "oai:arXiv.org:2504.08989v1",
        "title": "RouterKT: Mixture-of-Experts for Knowledge Tracing",
        "link": "https://arxiv.org/abs/2504.08989",
        "author": "Han Liao, Shuaishuai Zu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08989v1 Announce Type: cross \nAbstract: Knowledge Tracing (KT) is a fundamental task in Intelligent Tutoring Systems (ITS), which aims to model the dynamic knowledge states of students based on their interaction histories. However, existing KT models often rely on a global forgetting decay mechanism for capturing learning patterns, assuming that students' performance is predominantly influenced by their most recent interactions. Such approaches fail to account for the diverse and complex learning patterns arising from individual differences and varying learning stages. To address this limitation, we propose RouterKT, a novel Mixture-of-Experts (MoE) architecture designed to capture heterogeneous learning patterns by enabling experts to specialize in different patterns without any handcrafted learning pattern bias such as forgetting decay. Specifically, RouterKT introduces a \\textbf{person-wise routing mechanism} to effectively model individual-specific learning behaviors and employs \\textbf{multi-heads as experts} to enhance the modeling of complex and diverse patterns. Comprehensive experiments on ten benchmark datasets demonstrate that RouterKT exhibits significant flexibility and improves the performance of various KT backbone models, with a maximum average AUC improvement of 3.29\\% across different backbones and datasets, outperforming other state-of-the-art models. Moreover, RouterKT demonstrates consistently superior inference efficiency compared to existing approaches based on handcrafted learning pattern bias, highlighting its usability for real-world educational applications. The source code is available at https://github.com/derek-liao/RouterKT.git."
      },
      {
        "id": "oai:arXiv.org:2504.09006v1",
        "title": "Learning in Structured Stackelberg Games",
        "link": "https://arxiv.org/abs/2504.09006",
        "author": "Maria-Florina Balcan, Kiriaki Fragkia, Keegan Harris",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09006v1 Announce Type: cross \nAbstract: We study structured Stackelberg games, in which both players (the leader and the follower) observe information about the state of the world at time of play. Importantly, this information may contain information about the follower, which the leader may use when deciding her strategy. Under this setting, we show that no-regret learning is possible if and only if the set of mappings from contexts to follower types that the leader uses to learn is not ``too complex''. Specifically, we find that standard learning theoretic measures of complexity do not characterize learnability in our setting and we give a new dimension which does, which we term the Stackelberg-Littlestone dimension. In the distributional setting, we give analogous results by showing that standard complexity measures do not characterize the sample complexity of learning, but a new dimension called the Stackelberg-Natarajan dimension does. We then show that an appropriate empirical risk minimization procedure achieves the corresponding sample complexity."
      },
      {
        "id": "oai:arXiv.org:2504.09035v1",
        "title": "InterQ: A DQN Framework for Optimal Intermittent Control",
        "link": "https://arxiv.org/abs/2504.09035",
        "author": "Shubham Aggarwal, Dipankar Maity, Tamer Ba\\c{s}ar",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09035v1 Announce Type: cross \nAbstract: In this letter, we explore the communication-control co-design of discrete-time stochastic linear systems through reinforcement learning. Specifically, we examine a closed-loop system involving two sequential decision-makers: a scheduler and a controller. The scheduler continuously monitors the system's state but transmits it to the controller intermittently to balance the communication cost and control performance. The controller, in turn, determines the control input based on the intermittently received information. Given the partially nested information structure, we show that the optimal control policy follows a certainty-equivalence form. Subsequently, we analyze the qualitative behavior of the scheduling policy. To develop the optimal scheduling policy, we propose InterQ, a deep reinforcement learning algorithm which uses a deep neural network to approximate the Q-function. Through extensive numerical evaluations, we analyze the scheduling landscape and further compare our approach against two baseline strategies: (a) a multi-period periodic scheduling policy, and (b) an event-triggered policy. The results demonstrate that our proposed method outperforms both baselines. The open source implementation can be found at https://github.com/AC-sh/InterQ."
      },
      {
        "id": "oai:arXiv.org:2504.09037v1",
        "title": "A Survey of Frontiers in LLM Reasoning: Inference Scaling, Learning to Reason, and Agentic Systems",
        "link": "https://arxiv.org/abs/2504.09037",
        "author": "Zixuan Ke, Fangkai Jiao, Yifei Ming, Xuan-Phi Nguyen, Austin Xu, Do Xuan Long, Minzhi Li, Chengwei Qin, Peifeng Wang, Silvio Savarese, Caiming Xiong, Shafiq Joty",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09037v1 Announce Type: cross \nAbstract: Reasoning is a fundamental cognitive process that enables logical inference, problem-solving, and decision-making. With the rapid advancement of large language models (LLMs), reasoning has emerged as a key capability that distinguishes advanced AI systems from conventional models that empower chatbots. In this survey, we categorize existing methods along two orthogonal dimensions: (1) Regimes, which define the stage at which reasoning is achieved (either at inference time or through dedicated training); and (2) Architectures, which determine the components involved in the reasoning process, distinguishing between standalone LLMs and agentic compound systems that incorporate external tools, and multi-agent collaborations. Within each dimension, we analyze two key perspectives: (1) Input level, which focuses on techniques that construct high-quality prompts that the LLM condition on; and (2) Output level, which methods that refine multiple sampled candidates to enhance reasoning quality. This categorization provides a systematic understanding of the evolving landscape of LLM reasoning, highlighting emerging trends such as the shift from inference-scaling to learning-to-reason (e.g., DeepSeek-R1), and the transition to agentic workflows (e.g., OpenAI Deep Research, Manus Agent). Additionally, we cover a broad spectrum of learning algorithms, from supervised fine-tuning to reinforcement learning such as PPO and GRPO, and the training of reasoners and verifiers. We also examine key designs of agentic workflows, from established patterns like generator-evaluator and LLM debate to recent innovations. ..."
      },
      {
        "id": "oai:arXiv.org:2504.09058v1",
        "title": "Towards Stepwise Domain Knowledge-Driven Reasoning Optimization and Reflection Improvement",
        "link": "https://arxiv.org/abs/2504.09058",
        "author": "Chengyuan Liu, Shihang Wang, Lizhi Qing, Kaisong Song, Junjie Cao, Jun Lin, Ji Zhang, Ang Li, Kun Kuang, Fei Wu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09058v1 Announce Type: cross \nAbstract: Recently, stepwise supervision on Chain of Thoughts (CoTs) presents an enhancement on the logical reasoning tasks such as coding and math, with the help of Monte Carlo Tree Search (MCTS). However, its contribution to tasks requiring domain-specific expertise and knowledge remains unexplored. Motivated by the interest, we identify several potential challenges of vanilla MCTS within this context, and propose the framework of Stepwise Domain Knowledge-Driven Reasoning Optimization, employing the MCTS algorithm to develop step-level supervision for problems that require essential comprehension, reasoning, and specialized knowledge. Additionally, we also introduce the Preference Optimization towards Reflection Paths, which iteratively learns self-reflection on the reasoning thoughts from better perspectives. We have conducted extensive experiments to evaluate the advantage of the methodologies. Empirical results demonstrate the effectiveness on various legal-domain problems. We also report a diverse set of valuable findings, hoping to encourage the enthusiasm to the research of domain-specific LLMs and MCTS."
      },
      {
        "id": "oai:arXiv.org:2504.09068v1",
        "title": "An Incremental Non-Linear Manifold Approximation Method",
        "link": "https://arxiv.org/abs/2504.09068",
        "author": "Praveen T. W. Hettige, Benjamin W. Ong",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09068v1 Announce Type: cross \nAbstract: Analyzing high-dimensional data presents challenges due to the \"curse of dimensionality'', making computations intensive. Dimension reduction techniques, categorized as linear or non-linear, simplify such data. Non-linear methods are particularly essential for efficiently visualizing and processing complex data structures in interactive and graphical applications. This research develops an incremental non-linear dimension reduction method using the Geometric Multi-Resolution Analysis (GMRA) framework for streaming data. The proposed method enables real-time data analysis and visualization by incrementally updating the cluster map, PCA basis vectors, and wavelet coefficients. Numerical experiments show that the incremental GMRA accurately represents non-linear manifolds even with small initial samples and aligns closely with batch GMRA, demonstrating efficient updates and maintaining the multiscale structure. The findings highlight the potential of Incremental GMRA for real-time visualization and interactive graphics applications that require adaptive high-dimensional data representations."
      },
      {
        "id": "oai:arXiv.org:2504.09072v1",
        "title": "MGS: Markov Greedy Sums for Accurate Low-Bitwidth Floating-Point Accumulation",
        "link": "https://arxiv.org/abs/2504.09072",
        "author": "Vikas Natesh, H. T. Kung, David Kong",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09072v1 Announce Type: cross \nAbstract: We offer a novel approach, MGS (Markov Greedy Sums), to improve the accuracy of low-bitwidth floating-point dot products in neural network computations. In conventional 32-bit floating-point summation, adding values with different exponents may lead to loss of precision in the mantissa of the smaller term, which is right-shifted to align with the larger term's exponent. Such shifting (a.k.a. 'swamping') is a significant source of numerical errors in accumulation when implementing low-bitwidth dot products (e.g., 8-bit floating point) as the mantissa has a small number of bits. We avoid most swamping errors by arranging the terms in dot product summation based on their exponents and summing the mantissas without overflowing the low-bitwidth accumulator. We design, analyze, and implement the algorithm to minimize 8-bit floating point error at inference time for several neural networks. In contrast to traditional sequential summation, our method has significantly lowered numerical errors, achieving classification accuracy on par with high-precision floating-point baselines for multiple image classification tasks. Our dMAC hardware units can reduce power consumption by up to 34.1\\% relative to conventional MAC units."
      },
      {
        "id": "oai:arXiv.org:2504.09081v1",
        "title": "SIFT-50M: A Large-Scale Multilingual Dataset for Speech Instruction Fine-Tuning",
        "link": "https://arxiv.org/abs/2504.09081",
        "author": "Prabhat Pandey, Rupak Vignesh Swaminathan, K V Vijay Girish, Arunasish Sen, Jian Xie, Grant P. Strimel, Andreas Schwarz",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09081v1 Announce Type: cross \nAbstract: We introduce SIFT (Speech Instruction Fine-Tuning), a 50M-example dataset designed for instruction fine-tuning and pre-training of speech-text large language models (LLMs). SIFT-50M is built from publicly available speech corpora, which collectively contain 14K hours of speech, and leverages LLMs along with off-the-shelf expert models. The dataset spans five languages, encompassing a diverse range of speech understanding as well as controllable speech generation instructions. Using SIFT-50M, we train SIFT-LLM, which outperforms existing speech-text LLMs on instruction-following benchmarks while achieving competitive performance on foundational speech tasks. To support further research, we also introduce EvalSIFT, a benchmark dataset specifically designed to evaluate the instruction-following capabilities of speech-text LLMs."
      },
      {
        "id": "oai:arXiv.org:2504.09088v1",
        "title": "Multi-Modal Brain Tumor Segmentation via 3D Multi-Scale Self-attention and Cross-attention",
        "link": "https://arxiv.org/abs/2504.09088",
        "author": "Yonghao Huang, Leiting Chen, Chuan Zhou",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09088v1 Announce Type: cross \nAbstract: Due to the success of CNN-based and Transformer-based models in various computer vision tasks, recent works study the applicability of CNN-Transformer hybrid architecture models in 3D multi-modality medical segmentation tasks. Introducing Transformer brings long-range dependent information modeling ability in 3D medical images to hybrid models via the self-attention mechanism. However, these models usually employ fixed receptive fields of 3D volumetric features within each self-attention layer, ignoring the multi-scale volumetric lesion features. To address this issue, we propose a CNN-Transformer hybrid 3D medical image segmentation model, named TMA-TransBTS, based on an encoder-decoder structure. TMA-TransBTS realizes simultaneous extraction of multi-scale 3D features and modeling of long-distance dependencies by multi-scale division and aggregation of 3D tokens in a self-attention layer. Furthermore, TMA-TransBTS proposes a 3D multi-scale cross-attention module to establish a link between the encoder and the decoder for extracting rich volume representations by exploiting the mutual attention mechanism of cross-attention and multi-scale aggregation of 3D tokens. Extensive experimental results on three public 3D medical segmentation datasets show that TMA-TransBTS achieves higher averaged segmentation results than previous state-of-the-art CNN-based 3D methods and CNN-Transform hybrid 3D methods for the segmentation of 3D multi-modality brain tumors."
      },
      {
        "id": "oai:arXiv.org:2504.09100v1",
        "title": "A Short Survey on Small Reasoning Models: Training, Inference, Applications and Research Directions",
        "link": "https://arxiv.org/abs/2504.09100",
        "author": "Chengyu Wang, Taolin Zhang, Richang Hong, Jun Huang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09100v1 Announce Type: cross \nAbstract: Recently, the reasoning capabilities of large reasoning models (LRMs), such as DeepSeek-R1, have seen significant advancements through the slow thinking process. Despite these achievements, the substantial computational demands of LRMs present considerable challenges. In contrast, small reasoning models (SRMs), often distilled from larger ones, offer greater efficiency and can exhibit distinct capabilities and cognitive trajectories compared to LRMs. This work surveys around 170 recently published papers on SRMs for tackling various complex reasoning tasks. We review the current landscape of SRMs and analyze diverse training and inference techniques related to SRMs. Furthermore, we provide a comprehensive review of SRMs for domain-specific applications and discuss possible future research directions. This survey serves as an essential reference for researchers to leverage or develop SRMs for advanced reasoning functionalities with high efficiency."
      },
      {
        "id": "oai:arXiv.org:2504.09157v1",
        "title": "Dose-finding design based on level set estimation in phase I cancer clinical trials",
        "link": "https://arxiv.org/abs/2504.09157",
        "author": "Keiichiro Seno, Kota Matsui, Shogo Iwazaki, Yu Inatsu, Shion Takeno, Shigeyuki Matsui",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09157v1 Announce Type: cross \nAbstract: The primary objective of phase I cancer clinical trials is to evaluate the safety of a new experimental treatment and to find the maximum tolerated dose (MTD). We show that the MTD estimation problem can be regarded as a level set estimation (LSE) problem whose objective is to determine the regions where an unknown function value is above or below a given threshold. Then, we propose a novel dose-finding design in the framework of LSE. The proposed design determines the next dose on the basis of an acquisition function incorporating uncertainty in the posterior distribution of the dose-toxicity curve as well as overdose control. Simulation experiments show that the proposed LSE design achieves a higher accuracy in estimating the MTD and involves a lower risk of overdosing allocation compared to existing designs, thereby indicating that it provides an effective methodology for phase I cancer clinical trial design."
      },
      {
        "id": "oai:arXiv.org:2504.09182v1",
        "title": "seg2med: a segmentation-based medical image generation framework using denoising diffusion probabilistic models",
        "link": "https://arxiv.org/abs/2504.09182",
        "author": "Zeyu Yang, Zhilin Chen, Yipeng Sun, Anika Strittmatter, Anish Raj, Ahmad Allababidi, Johann S. Rink, Frank G. Z\\\"ollner",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09182v1 Announce Type: cross \nAbstract: In this study, we present seg2med, an advanced medical image synthesis framework that uses Denoising Diffusion Probabilistic Models (DDPM) to generate high-quality synthetic medical images conditioned on anatomical masks from TotalSegmentator. The framework synthesizes CT and MR images from segmentation masks derived from real patient data and XCAT digital phantoms, achieving a Structural Similarity Index Measure (SSIM) of 0.94 +/- 0.02 for CT and 0.89 +/- 0.04 for MR images compared to ground-truth images of real patients. It also achieves a Feature Similarity Index Measure (FSIM) of 0.78 +/- 0.04 for CT images from XCAT. The generative quality is further supported by a Fr\\'echet Inception Distance (FID) of 3.62 for CT image generation.\n  Additionally, seg2med can generate paired CT and MR images with consistent anatomical structures and convert images between CT and MR modalities, achieving SSIM values of 0.91 +/- 0.03 for MR-to-CT and 0.77 +/- 0.04 for CT-to-MR conversion. Despite the limitations of incomplete anatomical details in segmentation masks, the framework shows strong performance in cross-modality synthesis and multimodal imaging.\n  seg2med also demonstrates high anatomical fidelity in CT synthesis, achieving a mean Dice coefficient greater than 0.90 for 11 abdominal organs and greater than 0.80 for 34 organs out of 59 in 58 test cases. The highest Dice of 0.96 +/- 0.01 was recorded for the right scapula. Leveraging the TotalSegmentator toolkit, seg2med enables segmentation mask generation across diverse datasets, supporting applications in clinical imaging, data augmentation, multimodal synthesis, and diagnostic algorithm development."
      },
      {
        "id": "oai:arXiv.org:2504.09209v1",
        "title": "EchoMask: Speech-Queried Attention-based Mask Modeling for Holistic Co-Speech Motion Generation",
        "link": "https://arxiv.org/abs/2504.09209",
        "author": "Xiangyue Zhang, Jianfang Li, Jiaxu Zhang, Jianqiang Ren, Liefeng Bo, Zhigang Tu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09209v1 Announce Type: cross \nAbstract: Masked modeling framework has shown promise in co-speech motion generation. However, it struggles to identify semantically significant frames for effective motion masking. In this work, we propose a speech-queried attention-based mask modeling framework for co-speech motion generation. Our key insight is to leverage motion-aligned speech features to guide the masked motion modeling process, selectively masking rhythm-related and semantically expressive motion frames. Specifically, we first propose a motion-audio alignment module (MAM) to construct a latent motion-audio joint space. In this space, both low-level and high-level speech features are projected, enabling motion-aligned speech representation using learnable speech queries. Then, a speech-queried attention mechanism (SQA) is introduced to compute frame-level attention scores through interactions between motion keys and speech queries, guiding selective masking toward motion frames with high attention scores. Finally, the motion-aligned speech features are also injected into the generation network to facilitate co-speech motion generation. Qualitative and quantitative evaluations confirm that our method outperforms existing state-of-the-art approaches, successfully producing high-quality co-speech motion."
      },
      {
        "id": "oai:arXiv.org:2504.09213v1",
        "title": "Spiking Neural Network for Intra-cortical Brain Signal Decoding",
        "link": "https://arxiv.org/abs/2504.09213",
        "author": "Song Yang, Haotian Fu, Herui Zhang, Peng Zhang, Wei Li, Dongrui Wu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09213v1 Announce Type: cross \nAbstract: Decoding brain signals accurately and efficiently is crucial for intra-cortical brain-computer interfaces. Traditional decoding approaches based on neural activity vector features suffer from low accuracy, whereas deep learning based approaches have high computational cost. To improve both the decoding accuracy and efficiency, this paper proposes a spiking neural network (SNN) for effective and energy-efficient intra-cortical brain signal decoding. We also propose a feature fusion approach, which integrates the manually extracted neural activity vector features with those extracted by a deep neural network, to further improve the decoding accuracy. Experiments in decoding motor-related intra-cortical brain signals of two rhesus macaques demonstrated that our SNN model achieved higher accuracy than traditional artificial neural networks; more importantly, it was tens or hundreds of times more efficient. The SNN model is very suitable for high precision and low power applications like intra-cortical brain-computer interfaces."
      },
      {
        "id": "oai:arXiv.org:2504.09221v1",
        "title": "CMCRD: Cross-Modal Contrastive Representation Distillation for Emotion Recognition",
        "link": "https://arxiv.org/abs/2504.09221",
        "author": "Siyuan Kan, Huanyu Wu, Zhenyao Cui, Fan Huang, Xiaolong Xu, Dongrui Wu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09221v1 Announce Type: cross \nAbstract: Emotion recognition is an important component of affective computing, and also human-machine interaction. Unimodal emotion recognition is convenient, but the accuracy may not be high enough; on the contrary, multi-modal emotion recognition may be more accurate, but it also increases the complexity and cost of the data collection system. This paper considers cross-modal emotion recognition, i.e., using both electroencephalography (EEG) and eye movement in training, but only EEG or eye movement in test. We propose cross-modal contrastive representation distillation (CMCRD), which uses a pre-trained eye movement classification model to assist the training of an EEG classification model, improving feature extraction from EEG signals, or vice versa. During test, only EEG signals (or eye movement signals) are acquired, eliminating the need for multi-modal data. CMCRD not only improves the emotion recognition accuracy, but also makes the system more simplified and practical. Experiments using three different neural network architectures on three multi-modal emotion recognition datasets demonstrated the effectiveness of CMCRD. Compared with the EEG-only model, it improved the average classification accuracy by about 6.2%."
      },
      {
        "id": "oai:arXiv.org:2504.09250v1",
        "title": "Adiabatic Encoding of Pre-trained MPS Classifiers into Quantum Circuits",
        "link": "https://arxiv.org/abs/2504.09250",
        "author": "Keisuke Murota",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09250v1 Announce Type: cross \nAbstract: Although Quantum Neural Networks (QNNs) offer powerful methods for classification tasks, the training of QNNs faces two major training obstacles: barren plateaus and local minima. A promising solution is to first train a tensor-network (TN) model classically and then embed it into a QNN.\\ However, embedding TN-classifiers into quantum circuits generally requires postselection whose success probability may decay exponentially with the system size. We propose an \\emph{adiabatic encoding} framework that encodes pre-trained MPS-classifiers into quantum MPS (qMPS) circuits with postselection, and gradually removes the postselection while retaining performance. We prove that training qMPS-classifiers from scratch on a certain artificial dataset is exponentially hard due to barren plateaus, but our adiabatic encoding circumvents this issue. Additional numerical experiments on binary MNIST also confirm its robustness."
      },
      {
        "id": "oai:arXiv.org:2504.09260v1",
        "title": "NetTAG: A Multimodal RTL-and-Layout-Aligned Netlist Foundation Model via Text-Attributed Graph",
        "link": "https://arxiv.org/abs/2504.09260",
        "author": "Wenji Fang, Wenkai Li, Shang Liu, Yao Lu, Hongce Zhang, Zhiyao Xie",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09260v1 Announce Type: cross \nAbstract: Circuit representation learning has shown promise in advancing Electronic Design Automation (EDA) by capturing structural and functional circuit properties for various tasks. Existing pre-trained solutions rely on graph learning with complex functional supervision, such as truth table simulation. However, they only handle simple and-inverter graphs (AIGs), struggling to fully encode other complex gate functionalities. While large language models (LLMs) excel at functional understanding, they lack the structural awareness for flattened netlists. To advance netlist representation learning, we present NetTAG, a netlist foundation model that fuses gate semantics with graph structure, handling diverse gate types and supporting a variety of functional and physical tasks. Moving beyond existing graph-only methods, NetTAG formulates netlists as text-attributed graphs, with gates annotated by symbolic logic expressions and physical characteristics as text attributes. Its multimodal architecture combines an LLM-based text encoder for gate semantics and a graph transformer for global structure. Pre-trained with gate and graph self-supervised objectives and aligned with RTL and layout stages, NetTAG captures comprehensive circuit intrinsics. Experimental results show that NetTAG consistently outperforms each task-specific method on four largely different functional and physical tasks and surpasses state-of-the-art AIG encoders, demonstrating its versatility."
      },
      {
        "id": "oai:arXiv.org:2504.09271v1",
        "title": "Linguistic Comparison of AI- and Human-Written Responses to Online Mental Health Queries",
        "link": "https://arxiv.org/abs/2504.09271",
        "author": "Koustuv Saha, Yoshee Jain, Munmun De Choudhury",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09271v1 Announce Type: cross \nAbstract: The ubiquity and widespread use of digital and online technologies have transformed mental health support, with online mental health communities (OMHCs) providing safe spaces for peer support. More recently, generative AI and large language models (LLMs) have introduced new possibilities for scalable, around-the-clock mental health assistance that could potentially augment and supplement the capabilities of OMHCs. Although genAI shows promise in delivering immediate and personalized responses, their effectiveness in replicating the nuanced, experience-based support of human peers remains an open question. In this study, we harnessed 24,114 posts and 138,758 online community (OC) responses from 55 OMHCs on Reddit. We prompted several state-of-the-art LLMs (GPT-4-Turbo, Llama-3, and Mistral-7B) with these posts, and compared their (AI) responses to human-written (OC) responses based on a variety of linguistic measures across psycholinguistics and lexico-semantics. Our findings revealed that AI responses are more verbose, readable, and analytically structured, but lack linguistic diversity and personal narratives inherent in human-human interactions. Through a qualitative examination, we found validation as well as complementary insights into the nature of AI responses, such as its neutrality of stance and the absence of seeking back-and-forth clarifications. We discuss the ethical and practical implications of integrating generative AI into OMHCs, advocating for frameworks that balance AI's scalability and timeliness with the irreplaceable authenticity, social interactiveness, and expertise of human connections that form the ethos of online support communities."
      },
      {
        "id": "oai:arXiv.org:2504.09279v1",
        "title": "No-Regret Generative Modeling via Parabolic Monge-Amp\\`ere PDE",
        "link": "https://arxiv.org/abs/2504.09279",
        "author": "Nabarun Deb, Tengyuan Liang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09279v1 Announce Type: cross \nAbstract: We introduce a novel generative modeling framework based on a discretized parabolic Monge-Amp\\`ere PDE, which emerges as a continuous limit of the Sinkhorn algorithm commonly used in optimal transport. Our method performs iterative refinement in the space of Brenier maps using a mirror gradient descent step. We establish theoretical guarantees for generative modeling through the lens of no-regret analysis, demonstrating that the iterates converge to the optimal Brenier map under a variety of step-size schedules. As a technical contribution, we derive a new Evolution Variational Inequality tailored to the parabolic Monge-Amp\\`ere PDE, connecting geometry, transportation cost, and regret. Our framework accommodates non-log-concave target distributions, constructs an optimal sampling process via the Brenier map, and integrates favorable learning techniques from generative adversarial networks and score-based diffusion models. As direct applications, we illustrate how our theory paves new pathways for generative modeling and variational inference."
      },
      {
        "id": "oai:arXiv.org:2504.09310v1",
        "title": "Conformal Calibration: Ensuring the Reliability of Black-Box AI in Wireless Systems",
        "link": "https://arxiv.org/abs/2504.09310",
        "author": "Osvaldo Simeone, Sangwoo Park, Matteo Zecchin",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09310v1 Announce Type: cross \nAbstract: AI is poised to revolutionize telecommunication networks by boosting efficiency, automation, and decision-making. However, the black-box nature of most AI models introduces substantial risk, possibly deterring adoption by network operators. These risks are not addressed by the current prevailing deployment strategy, which typically follows a best-effort train-and-deploy paradigm. This paper reviews conformal calibration, a general framework that moves beyond the state of the art by adopting computationally lightweight, advanced statistical tools that offer formal reliability guarantees without requiring further training or fine-tuning. Conformal calibration encompasses pre-deployment calibration via uncertainty quantification or hyperparameter selection; online monitoring to detect and mitigate failures in real time; and counterfactual post-deployment performance analysis to address \"what if\" diagnostic questions after deployment. By weaving conformal calibration into the AI model lifecycle, network operators can establish confidence in black-box AI models as a dependable enabling technology for wireless systems."
      },
      {
        "id": "oai:arXiv.org:2504.09340v1",
        "title": "Building Machine Learning Limited Area Models: Kilometer-Scale Weather Forecasting in Realistic Settings",
        "link": "https://arxiv.org/abs/2504.09340",
        "author": "Simon Adamov, Joel Oskarsson, Leif Denby, Tomas Landelius, Kasper Hintz, Simon Christiansen, Irene Schicker, Carlos Osuna, Fredrik Lindsten, Oliver Fuhrer, Sebastian Schemm",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09340v1 Announce Type: cross \nAbstract: Machine learning is revolutionizing global weather forecasting, with models that efficiently produce highly accurate forecasts. Apart from global forecasting there is also a large value in high-resolution regional weather forecasts, focusing on accurate simulations of the atmosphere for a limited area. Initial attempts have been made to use machine learning for such limited area scenarios, but these experiments do not consider realistic forecasting settings and do not investigate the many design choices involved. We present a framework for building kilometer-scale machine learning limited area models with boundary conditions imposed through a flexible boundary forcing method. This enables boundary conditions defined either from reanalysis or operational forecast data. Our approach employs specialized graph constructions with rectangular and triangular meshes, along with multi-step rollout training strategies to improve temporal consistency. We perform systematic evaluation of different design choices, including the boundary width, graph construction and boundary forcing integration. Models are evaluated across both a Danish and a Swiss domain, two regions that exhibit different orographical characteristics. Verification is performed against both gridded analysis data and in-situ observations, including a case study for the storm Ciara in February 2020. Both models achieve skillful predictions across a wide range of variables, with our Swiss model outperforming the numerical weather prediction baseline for key surface variables. With their substantially lower computational cost, our findings demonstrate great potential for machine learning limited area models in the future of regional weather forecasting."
      },
      {
        "id": "oai:arXiv.org:2504.09347v1",
        "title": "Inferring Outcome Means of Exponential Family Distributions Estimated by Deep Neural Networks",
        "link": "https://arxiv.org/abs/2504.09347",
        "author": "Xuran Meng, Yi Li",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09347v1 Announce Type: cross \nAbstract: Despite the widespread use of deep neural networks (DNNs) for prediction, inference on estimated means for categorical or exponential family outcomes remains underexplored. We address this gap by framing the problem within the generalized linear models (GLMs) framework and developing a rigorous statistical approach for inference on DNN-estimated means. To address a key limitation of assuming independence between prediction errors and input variables in the literature, which often fails in GLMs, we introduce a truncation technique that partitions the problem into regimes with distinct noise behaviors, enabling refined analysis and robust theoretical guarantees under general GLM frameworks. To implement inference, we consider an Ensemble Subsampling Method (ESM) that leverages U-statistics and the Hoeffding decomposition to construct reliable confidence intervals. This method enables model-free variance estimation and accounts for heterogeneity among individuals in the population. Through extensive simulations across Binary, Poisson and Binomial models, we demonstrate the effectiveness and efficiency of our method. We further apply the method to real-world data from the eICU dataset to predict patient readmission risks, providing actionable insights for clinical decision-making."
      },
      {
        "id": "oai:arXiv.org:2504.09348v1",
        "title": "Graph-Based Prediction Models for Data Debiasing",
        "link": "https://arxiv.org/abs/2504.09348",
        "author": "Dongze Wu, Hanyang Jiang, Yao Xie",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09348v1 Announce Type: cross \nAbstract: Bias in data collection, arising from both under-reporting and over-reporting, poses significant challenges in critical applications such as healthcare and public safety. In this work, we introduce Graph-based Over- and Under-reporting Debiasing (GROUD), a novel graph-based optimization framework that debiases reported data by jointly estimating the true incident counts and the associated reporting bias probabilities. By modeling the bias as a smooth signal over a graph constructed from geophysical or feature-based similarities, our convex formulation not only ensures a unique solution but also comes with theoretical recovery guarantees under certain assumptions. We validate GROUD on both challenging simulated experiments and real-world datasets -- including Atlanta emergency calls and COVID-19 vaccine adverse event reports -- demonstrating its robustness and superior performance in accurately recovering debiased counts. This approach paves the way for more reliable downstream decision-making in systems affected by reporting irregularities."
      },
      {
        "id": "oai:arXiv.org:2504.09352v1",
        "title": "Explorer: Robust Collection of Interactable GUI Elements",
        "link": "https://arxiv.org/abs/2504.09352",
        "author": "Iason Chaimalas, Arnas Vy\\v{s}niauskas, Gabriel Brostow",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09352v1 Announce Type: cross \nAbstract: Automation of existing Graphical User Interfaces (GUIs) is important but hard to achieve. Upstream of making the GUI user-accessible or somehow scriptable, even the data-collection to understand the original interface poses significant challenges. For example, large quantities of general UI data seem helpful for training general machine learning (ML) models, but accessibility for each person can hinge on the ML's precision on a specific app. We therefore take the perspective that a given user needs confidence, that the relevant UI elements are being detected correctly throughout one app or digital environment. We mostly assume that the target application is known in advance, so that data collection and ML-training can be personalized for the test-time target domain. The proposed Explorer system focuses on detecting on-screen buttons and text-entry fields, i.e. interactables, where the training process has access to a live version of the application. The live application can run on almost any popular platform except iOS phones, and the collection is especially streamlined for Android phones or for desktop Chrome browsers. Explorer also enables the recording of interactive user sessions, and subsequent mapping of how such sessions overlap and sometimes loop back to similar states. We show how having such a map enables a kind of path planning through the GUI, letting a user issue audio commands to get to their destination. Critically, we are releasing our code for Explorer openly at https://github.com/varnelis/Explorer."
      },
      {
        "id": "oai:arXiv.org:2504.09363v1",
        "title": "Machine Learning-Based Cyberattack Detection and Identification for Automatic Generation Control Systems Considering Nonlinearities",
        "link": "https://arxiv.org/abs/2504.09363",
        "author": "Nour M. Shabar, Ahmad Mohammad Saber, Deepa Kundur",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09363v1 Announce Type: cross \nAbstract: Automatic generation control (AGC) systems play a crucial role in maintaining system frequency across power grids. However, AGC systems' reliance on communicated measurements exposes them to false data injection attacks (FDIAs), which can compromise the overall system stability. This paper proposes a machine learning (ML)-based detection framework that identifies FDIAs and determines the compromised measurements. The approach utilizes an ML model trained offline to accurately detect attacks and classify the manipulated signals based on a comprehensive set of statistical and time-series features extracted from AGC measurements before and after disturbances. For the proposed approach, we compare the performance of several powerful ML algorithms. Our results demonstrate the efficacy of the proposed method in detecting FDIAs while maintaining a low false alarm rate, with an F1-score of up to 99.98%, outperforming existing approaches."
      },
      {
        "id": "oai:arXiv.org:2504.09409v1",
        "title": "Bregman Linearized Augmented Lagrangian Method for Nonconvex Constrained Stochastic Zeroth-order Optimization",
        "link": "https://arxiv.org/abs/2504.09409",
        "author": "Qiankun Shi, Xiao Wang, Hao Wang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09409v1 Announce Type: cross \nAbstract: In this paper, we study nonconvex constrained stochastic zeroth-order optimization problems, for which we have access to exact information of constraints and noisy function values of the objective. We propose a Bregman linearized augmented Lagrangian method that utilizes stochastic zeroth-order gradient estimators combined with a variance reduction technique. We analyze its oracle complexity, in terms of the total number of stochastic function value evaluations required to achieve an \\(\\epsilon\\)-KKT point in \\(\\ell_p\\)-norm metrics with \\(p \\ge 2\\), where \\(p\\) is a parameter associated with the selected Bregman distance. In particular, starting from a near-feasible initial point and using Rademacher smoothing, the oracle complexity is in order \\(O(p d^{2/p} \\epsilon^{-3})\\) for \\(p \\in [2, 2 \\ln d]\\), and \\(O(\\ln d \\cdot \\epsilon^{-3})\\) for \\(p > 2 \\ln d\\), where \\(d\\) denotes the problem dimension. Those results show that the complexity of the proposed method can achieve a dimensional dependency lower than \\(O(d)\\) without requiring additional assumptions, provided that a Bregman distance is chosen properly. This offers a significant improvement in the high-dimensional setting over existing work, and matches the lowest complexity order with respect to the tolerance \\(\\epsilon\\) reported in the literature. Numerical experiments on constrained Lasso and black-box adversarial attack problems highlight the promising performances of the proposed method."
      },
      {
        "id": "oai:arXiv.org:2504.09430v1",
        "title": "Predicting ulcer in H&E images of inflammatory bowel disease using domain-knowledge-driven graph neural network",
        "link": "https://arxiv.org/abs/2504.09430",
        "author": "Ruiwen Ding, Lin Li, Rajath Soans, Tosha Shah, Radha Krishnan, Marc Alexander Sze, Sasha Lukyanov, Yash Deshpande, Antong Chen",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09430v1 Announce Type: cross \nAbstract: Inflammatory bowel disease (IBD) involves chronic inflammation of the digestive tract, with treatment options often burdened by adverse effects. Identifying biomarkers for personalized treatment is crucial. While immune cells play a key role in IBD, accurately identifying ulcer regions in whole slide images (WSIs) is essential for characterizing these cells and exploring potential therapeutics. Multiple instance learning (MIL) approaches have advanced WSI analysis but they lack spatial context awareness. In this work, we propose a weakly-supervised model called DomainGCN that employs a graph convolution neural network (GCN) and incorporates domain-specific knowledge of ulcer features, specifically, the presence of epithelium, lymphocytes, and debris for WSI-level ulcer prediction in IBD. We demonstrate that DomainGCN outperforms various state-of-the-art (SOTA) MIL methods and show the added value of domain knowledge."
      },
      {
        "id": "oai:arXiv.org:2504.09456v1",
        "title": "Don't Deceive Me: Mitigating Gaslighting through Attention Reallocation in LMMs",
        "link": "https://arxiv.org/abs/2504.09456",
        "author": "Pengkun Jiao, Bin Zhu, Jingjing Chen, Chong-Wah Ngo, Yu-Gang Jiang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09456v1 Announce Type: cross \nAbstract: Large Multimodal Models (LMMs) have demonstrated remarkable capabilities across a wide range of tasks. However, their vulnerability to user gaslighting-the deliberate use of misleading or contradictory inputs-raises critical concerns about their reliability in real-world applications. In this paper, we address the novel and challenging issue of mitigating the negative impact of negation-based gaslighting on LMMs, where deceptive user statements lead to significant drops in model accuracy. Specifically, we introduce GasEraser, a training-free approach that reallocates attention weights from misleading textual tokens to semantically salient visual regions. By suppressing the influence of \"attention sink\" tokens and enhancing focus on visually grounded cues, GasEraser significantly improves LMM robustness without requiring retraining or additional supervision. Extensive experimental results demonstrate that GasEraser is effective across several leading open-source LMMs on the GaslightingBench. Notably, for LLaVA-v1.5-7B, GasEraser reduces the misguidance rate by 48.2%, demonstrating its potential for more trustworthy LMMs."
      },
      {
        "id": "oai:arXiv.org:2504.09466v1",
        "title": "AdaSteer: Your Aligned LLM is Inherently an Adaptive Jailbreak Defender",
        "link": "https://arxiv.org/abs/2504.09466",
        "author": "Weixiang Zhao, Jiahe Guo, Yulin Hu, Yang Deng, An Zhang, Xingyu Sui, Xinyang Han, Yanyan Zhao, Bing Qin, Tat-Seng Chua, Ting Liu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09466v1 Announce Type: cross \nAbstract: Despite extensive efforts in safety alignment, large language models (LLMs) remain vulnerable to jailbreak attacks. Activation steering offers a training-free defense method but relies on fixed steering coefficients, resulting in suboptimal protection and increased false rejections of benign inputs. To address this, we propose AdaSteer, an adaptive activation steering method that dynamically adjusts model behavior based on input characteristics. We identify two key properties: Rejection Law (R-Law), which shows that stronger steering is needed for jailbreak inputs opposing the rejection direction, and Harmfulness Law (H-Law), which differentiates adversarial and benign inputs. AdaSteer steers input representations along both the Rejection Direction (RD) and Harmfulness Direction (HD), with adaptive coefficients learned via logistic regression, ensuring robust jailbreak defense while preserving benign input handling. Experiments on LLaMA-3.1, Gemma-2, and Qwen2.5 show that AdaSteer outperforms baseline methods across multiple jailbreak attacks with minimal impact on utility. Our results highlight the potential of interpretable model internals for real-time, flexible safety enforcement in LLMs."
      },
      {
        "id": "oai:arXiv.org:2504.09479v1",
        "title": "Draw with Thought: Unleashing Multimodal Reasoning for Scientific Diagram Generation",
        "link": "https://arxiv.org/abs/2504.09479",
        "author": "Zhiqing Cui, Jiahao Yuan, Hanqing Wang, Yanshu Li, Chenxu Du, Zhenglong Ding",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09479v1 Announce Type: cross \nAbstract: Scientific diagrams are vital tools for communicating structured knowledge across disciplines. However, they are often published as static raster images, losing symbolic semantics and limiting reuse. While Multimodal Large Language Models (MLLMs) offer a pathway to bridging vision and structure, existing methods lack semantic control and structural interpretability, especially on complex diagrams. We propose Draw with Thought (DwT), a training-free framework that guides MLLMs to reconstruct diagrams into editable mxGraph XML code through cognitively-grounded Chain-of-Thought reasoning. DwT enables interpretable and controllable outputs without model fine-tuning by dividing the task into two stages: Coarse-to-Fine Planning, which handles perceptual structuring and semantic specification, and Structure-Aware Code Generation, enhanced by format-guided refinement. To support evaluation, we release Plot2XML, a benchmark of 247 real-world scientific diagrams with gold-standard XML annotations. Extensive experiments across eight MLLMs show that our approach yields high-fidelity, semantically aligned, and structurally valid reconstructions, with human evaluations confirming strong alignment in both accuracy and visual aesthetics, offering a scalable solution for converting static visuals into executable representations and advancing machine understanding of scientific graphics."
      },
      {
        "id": "oai:arXiv.org:2504.09509v1",
        "title": "Optimal sparse phase retrieval via a quasi-Bayesian approach",
        "link": "https://arxiv.org/abs/2504.09509",
        "author": "The Tien Mai",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09509v1 Announce Type: cross \nAbstract: This paper addresses the problem of sparse phase retrieval, a fundamental inverse problem in applied mathematics, physics, and engineering, where a signal need to be reconstructed using only the magnitude of its transformation while phase information remains inaccessible. Leveraging the inherent sparsity of many real-world signals, we introduce a novel sparse quasi-Bayesian approach and provide the first theoretical guarantees for such an approach. Specifically, we employ a scaled Student distribution as a continuous shrinkage prior to enforce sparsity and analyze the method using the PAC-Bayesian inequality framework. Our results establish that the proposed Bayesian estimator achieves minimax-optimal convergence rates under sub-exponential noise, matching those of state-of-the-art frequentist methods. To ensure computational feasibility, we develop an efficient Langevin Monte Carlo sampling algorithm. Through numerical experiments, we demonstrate that our method performs comparably to existing frequentist techniques, highlighting its potential as a principled alternative for sparse phase retrieval in noisy settings."
      },
      {
        "id": "oai:arXiv.org:2504.09516v1",
        "title": "FSSUAVL: A Discriminative Framework using Vision Models for Federated Self-Supervised Audio and Image Understanding",
        "link": "https://arxiv.org/abs/2504.09516",
        "author": "Yasar Abbas Ur Rehman, Kin Wai Lau, Yuyang Xie, Ma Lan, JiaJun Shen",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09516v1 Announce Type: cross \nAbstract: Recent studies have demonstrated that vision models can effectively learn multimodal audio-image representations when paired. However, the challenge of enabling deep models to learn representations from unpaired modalities remains unresolved. This issue is especially pertinent in scenarios like Federated Learning (FL), where data is often decentralized, heterogeneous, and lacks a reliable guarantee of paired data. Previous attempts tackled this issue through the use of auxiliary pretrained encoders or generative models on local clients, which invariably raise computational cost with increasing number modalities. Unlike these approaches, in this paper, we aim to address the task of unpaired audio and image recognition using \\texttt{FSSUAVL}, a single deep model pretrained in FL with self-supervised contrastive learning (SSL). Instead of aligning the audio and image modalities, \\texttt{FSSUAVL} jointly discriminates them by projecting them into a common embedding space using contrastive SSL. This extends the utility of \\texttt{FSSUAVL} to paired and unpaired audio and image recognition tasks. Our experiments with CNN and ViT demonstrate that \\texttt{FSSUAVL} significantly improves performance across various image- and audio-based downstream tasks compared to using separate deep models for each modality. Additionally, \\texttt{FSSUAVL}'s capacity to learn multimodal feature representations allows for integrating auxiliary information, if available, to enhance recognition accuracy."
      },
      {
        "id": "oai:arXiv.org:2504.09567v1",
        "title": "Conditional Independence Test Based on Transport Maps",
        "link": "https://arxiv.org/abs/2504.09567",
        "author": "Chenxuan He, Yuan Gao, Liping Zhu, Jian Huang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09567v1 Announce Type: cross \nAbstract: Testing conditional independence between two random vectors given a third is a fundamental and challenging problem in statistics, particularly in multivariate nonparametric settings due to the complexity of conditional structures. We propose a novel framework for testing conditional independence using transport maps. At the population level, we show that two well-defined transport maps can transform the conditional independence test into an unconditional independence test, this substantially simplifies the problem. These transport maps are estimated from data using conditional continuous normalizing flow models. Within this framework, we derive a test statistic and prove its consistency under both the null and alternative hypotheses. A permutation-based procedure is employed to evaluate the significance of the test. We validate the proposed method through extensive simulations and real-data analysis. Our numerical studies demonstrate the practical effectiveness of the proposed method for conditional independence testing."
      },
      {
        "id": "oai:arXiv.org:2504.09582v1",
        "title": "Reduction of Supervision for Biomedical Knowledge Discovery",
        "link": "https://arxiv.org/abs/2504.09582",
        "author": "Christos Theodoropoulos, Andrei Catalin Coman, James Henderson, Marie-Francine Moens",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09582v1 Announce Type: cross \nAbstract: Knowledge discovery is hindered by the increasing volume of publications and the scarcity of extensive annotated data. To tackle the challenge of information overload, it is essential to employ automated methods for knowledge extraction and processing. Finding the right balance between the level of supervision and the effectiveness of models poses a significant challenge. While supervised techniques generally result in better performance, they have the major drawback of demanding labeled data. This requirement is labor-intensive and time-consuming and hinders scalability when exploring new domains. In this context, our study addresses the challenge of identifying semantic relationships between biomedical entities (e.g., diseases, proteins) in unstructured text while minimizing dependency on supervision. We introduce a suite of unsupervised algorithms based on dependency trees and attention mechanisms and employ a range of pointwise binary classification methods. Transitioning from weakly supervised to fully unsupervised settings, we assess the methods' ability to learn from data with noisy labels. The evaluation on biomedical benchmark datasets explores the effectiveness of the methods. Our approach tackles a central issue in knowledge discovery: balancing performance with minimal supervision. By gradually decreasing supervision, we assess the robustness of pointwise binary classification techniques in handling noisy labels, revealing their capability to shift from weakly supervised to entirely unsupervised scenarios. Comprehensive benchmarking offers insights into the effectiveness of these techniques, suggesting an encouraging direction toward adaptable knowledge discovery systems, representing progress in creating data-efficient methodologies for extracting useful insights when annotated data is limited."
      },
      {
        "id": "oai:arXiv.org:2504.09589v1",
        "title": "Knowledge Independence Breeds Disruption but Limits Recognition",
        "link": "https://arxiv.org/abs/2504.09589",
        "author": "Xiaoyao Yu, Talal Rahwan, Tao Jia",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09589v1 Announce Type: cross \nAbstract: Recombinant growth theory highlights the pivotal role of cumulative knowledge in driving innovation. Although interconnected knowledge facilitates smoother dissemination, its connection to scientific disruption remains poorly understood. Here, we quantify knowledge dependence based on the degree to which references within a given paper's bibliography cite one another. Analyzing 53.8 million papers spanning six decades, we observe that papers built on independent knowledge have decreased over time. However, propensity score matching and regression analyses reveal that such papers are associated with greater scientific disruption, as those who cite them are less likely to cite their references. Moreover, a team's preference for independent knowledge amplifies its disruptive potential, regardless of team size, geographic distance, or collaboration freshness. Despite the disruptive nature, papers built on independent knowledge receive fewer citations and delayed recognition. Taken together, these findings fill a critical gap in our fundamental understanding of scientific innovation, revealing a universal law in peer recognition: Knowledge independence breeds disruption at the cost of impact."
      },
      {
        "id": "oai:arXiv.org:2504.09593v1",
        "title": "ControlNET: A Firewall for RAG-based LLM System",
        "link": "https://arxiv.org/abs/2504.09593",
        "author": "Hongwei Yao, Haoran Shi, Yidou Chen, Yixin Jiang, Cong Wang, Zhan Qin, Kui Ren, Chun Chen",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09593v1 Announce Type: cross \nAbstract: Retrieval-Augmented Generation (RAG) has significantly enhanced the factual accuracy and domain adaptability of Large Language Models (LLMs). This advancement has enabled their widespread deployment across sensitive domains such as healthcare, finance, and enterprise applications. RAG mitigates hallucinations by integrating external knowledge, yet introduces privacy risk and security risk, notably data breaching risk and data poisoning risk. While recent studies have explored prompt injection and poisoning attacks, there remains a significant gap in comprehensive research on controlling inbound and outbound query flows to mitigate these threats. In this paper, we propose an AI firewall, ControlNET, designed to safeguard RAG-based LLM systems from these vulnerabilities. ControlNET controls query flows by leveraging activation shift phenomena to detect adversarial queries and mitigate their impact through semantic divergence. We conduct comprehensive experiments on four different benchmark datasets including Msmarco, HotpotQA, FinQA, and MedicalSys using state-of-the-art open source LLMs (Llama3, Vicuna, and Mistral). Our results demonstrate that ControlNET achieves over 0.909 AUROC in detecting and mitigating security threats while preserving system harmlessness. Overall, ControlNET offers an effective, robust, harmless defense mechanism, marking a significant advancement toward the secure deployment of RAG-based LLM systems."
      },
      {
        "id": "oai:arXiv.org:2504.09597v1",
        "title": "Understanding LLM Behaviors via Compression: Data Generation, Knowledge Acquisition and Scaling Laws",
        "link": "https://arxiv.org/abs/2504.09597",
        "author": "Zhixuan Pan, Shaowen Wang, Jian Li",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09597v1 Announce Type: cross \nAbstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across numerous tasks, yet principled explanations for their underlying mechanisms and several phenomena, such as scaling laws, hallucinations, and related behaviors, remain elusive. In this work, we revisit the classical relationship between compression and prediction, grounded in Kolmogorov complexity and Shannon information theory, to provide deeper insights into LLM behaviors. By leveraging the Kolmogorov Structure Function and interpreting LLM compression as a two-part coding process, we offer a detailed view of how LLMs acquire and store information across increasing model and data scales -- from pervasive syntactic patterns to progressively rarer knowledge elements. Motivated by this theoretical perspective and natural assumptions inspired by Heap's and Zipf's laws, we introduce a simplified yet representative hierarchical data-generation framework called the Syntax-Knowledge model. Under the Bayesian setting, we show that prediction and compression within this model naturally lead to diverse learning and scaling behaviors of LLMs. In particular, our theoretical analysis offers intuitive and principled explanations for both data and model scaling laws, the dynamics of knowledge acquisition during training and fine-tuning, factual knowledge hallucinations in LLMs. The experimental results validate our theoretical predictions."
      },
      {
        "id": "oai:arXiv.org:2504.09602v1",
        "title": "Fine-tuning an Large Language Model for Automating Computational Fluid Dynamics Simulations",
        "link": "https://arxiv.org/abs/2504.09602",
        "author": "Zhehao Dong, Zhen Lu, Yue Yang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09602v1 Announce Type: cross \nAbstract: Configuring computational fluid dynamics (CFD) simulations typically demands extensive domain expertise, limiting broader access. Although large language models (LLMs) have advanced scientific computing, their use in automating CFD workflows is underdeveloped. We introduce a novel approach centered on domain-specific LLM adaptation. By fine-tuning Qwen2.5-7B-Instruct on NL2FOAM, our custom dataset of 28716 natural language-to-OpenFOAM configuration pairs with chain-of-thought (CoT) annotations, we enable direct translation from natural language descriptions to executable CFD setups. A multi-agent framework orchestrates the process, autonomously verifying inputs, generating configurations, running simulations, and correcting errors. Evaluation on a benchmark of 21 diverse flow cases demonstrates state-of-the-art performance, achieving 88.7% solution accuracy and 82.6% first-attempt success rate. This significantly outperforms larger general-purpose models like Qwen2.5-72B-Instruct, DeepSeek-R1, and Llama3.3-70B-Instruct, while also requiring fewer correction iterations and maintaining high computational efficiency. The results highlight the critical role of domain-specific adaptation in deploying LLM assistants for complex engineering workflows."
      },
      {
        "id": "oai:arXiv.org:2504.09655v1",
        "title": "OmniMamba4D: Spatio-temporal Mamba for longitudinal CT lesion segmentation",
        "link": "https://arxiv.org/abs/2504.09655",
        "author": "Justin Namuk Kim, Yiqiao Liu, Rajath Soans, Keith Persson, Sarah Halek, Michal Tomaszewski, Jianda Yuan, Gregory Goldmacher, Antong Chen",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09655v1 Announce Type: cross \nAbstract: Accurate segmentation of longitudinal CT scans is important for monitoring tumor progression and evaluating treatment responses. However, existing 3D segmentation models solely focus on spatial information. To address this gap, we propose OmniMamba4D, a novel segmentation model designed for 4D medical images (3D images over time). OmniMamba4D utilizes a spatio-temporal tetra-orientated Mamba block to effectively capture both spatial and temporal features. Unlike traditional 3D models, which analyze single-time points, OmniMamba4D processes 4D CT data, providing comprehensive spatio-temporal information on lesion progression. Evaluated on an internal dataset comprising of 3,252 CT scans, OmniMamba4D achieves a competitive Dice score of 0.682, comparable to state-of-the-arts (SOTA) models, while maintaining computational efficiency and better detecting disappeared lesions. This work demonstrates a new framework to leverage spatio-temporal information for longitudinal CT lesion segmentation."
      },
      {
        "id": "oai:arXiv.org:2504.09689v1",
        "title": "EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental Health Safety",
        "link": "https://arxiv.org/abs/2504.09689",
        "author": "Jiahao Qiu, Yinghui He, Xinzhe Juan, Yiming Wang, Yuhan Liu, Zixin Yao, Yue Wu, Xun Jiang, Ling Yang, Mengdi Wang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09689v1 Announce Type: cross \nAbstract: The rise of LLM-driven AI characters raises safety concerns, particularly for vulnerable human users with psychological disorders. To address these risks, we propose EmoAgent, a multi-agent AI framework designed to evaluate and mitigate mental health hazards in human-AI interactions. EmoAgent comprises two components: EmoEval simulates virtual users, including those portraying mentally vulnerable individuals, to assess mental health changes before and after interactions with AI characters. It uses clinically proven psychological and psychiatric assessment tools (PHQ-9, PDI, PANSS) to evaluate mental risks induced by LLM. EmoGuard serves as an intermediary, monitoring users' mental status, predicting potential harm, and providing corrective feedback to mitigate risks. Experiments conducted in popular character-based chatbots show that emotionally engaging dialogues can lead to psychological deterioration in vulnerable users, with mental state deterioration in more than 34.4% of the simulations. EmoGuard significantly reduces these deterioration rates, underscoring its role in ensuring safer AI-human interactions. Our code is available at: https://github.com/1akaman/EmoAgent"
      },
      {
        "id": "oai:arXiv.org:2504.09697v1",
        "title": "SPICE: A Synergistic, Precise, Iterative, and Customizable Image Editing Workflow",
        "link": "https://arxiv.org/abs/2504.09697",
        "author": "Kenan Tang, Yanhong Li, Yao Qin",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09697v1 Announce Type: cross \nAbstract: Recent prompt-based image editing models have demonstrated impressive prompt-following capability at structural editing tasks. However, existing models still fail to perform local edits, follow detailed editing prompts, or maintain global image quality beyond a single editing step. To address these challenges, we introduce SPICE, a training-free workflow that accepts arbitrary resolutions and aspect ratios, accurately follows user requirements, and improves image quality consistently during more than 100 editing steps. By synergizing the strengths of a base diffusion model and a Canny edge ControlNet model, SPICE robustly handles free-form editing instructions from the user. SPICE outperforms state-of-the-art baselines on a challenging realistic image-editing dataset consisting of semantic editing (object addition, removal, replacement, and background change), stylistic editing (texture changes), and structural editing (action change) tasks. Not only does SPICE achieve the highest quantitative performance according to standard evaluation metrics, but it is also consistently preferred by users over existing image-editing methods. We release the workflow implementation for popular diffusion model Web UIs to support further research and artistic exploration."
      },
      {
        "id": "oai:arXiv.org:2504.09707v1",
        "title": "InfoMAE: Pair-Efficient Cross-Modal Alignment for Multimodal Time-Series Sensing Signals",
        "link": "https://arxiv.org/abs/2504.09707",
        "author": "Tomoyoshi Kimura, Xinlin Li, Osama Hanna, Yatong Chen, Yizhuo Chen, Denizhan Kara, Tianshi Wang, Jinyang Li, Xiaomin Ouyang, Shengzhong Liu, Mani Srivastava, Suhas Diggavi, Tarek Abdelzaher",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09707v1 Announce Type: cross \nAbstract: Standard multimodal self-supervised learning (SSL) algorithms regard cross-modal synchronization as implicit supervisory labels during pretraining, thus posing high requirements on the scale and quality of multimodal samples. These constraints significantly limit the performance of sensing intelligence in IoT applications, as the heterogeneity and the non-interpretability of time-series signals result in abundant unimodal data but scarce high-quality multimodal pairs. This paper proposes InfoMAE, a cross-modal alignment framework that tackles the challenge of multimodal pair efficiency under the SSL setting by facilitating efficient cross-modal alignment of pretrained unimodal representations. InfoMAE achieves \\textit{efficient cross-modal alignment} with \\textit{limited data pairs} through a novel information theory-inspired formulation that simultaneously addresses distribution-level and instance-level alignment. Extensive experiments on two real-world IoT applications are performed to evaluate InfoMAE's pairing efficiency to bridge pretrained unimodal models into a cohesive joint multimodal model. InfoMAE enhances downstream multimodal tasks by over 60% with significantly improved multimodal pairing efficiency. It also improves unimodal task accuracy by an average of 22%."
      },
      {
        "id": "oai:arXiv.org:2504.09708v1",
        "title": "Preconditioned Gradient Descent for Over-Parameterized Nonconvex Matrix Factorization",
        "link": "https://arxiv.org/abs/2504.09708",
        "author": "Gavin Zhang, Salar Fattahi, Richard Y. Zhang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09708v1 Announce Type: cross \nAbstract: In practical instances of nonconvex matrix factorization, the rank of the true solution $r^{\\star}$ is often unknown, so the rank $r$ of the model can be overspecified as $r>r^{\\star}$. This over-parameterized regime of matrix factorization significantly slows down the convergence of local search algorithms, from a linear rate with $r=r^{\\star}$ to a sublinear rate when $r>r^{\\star}$. We propose an inexpensive preconditioner for the matrix sensing variant of nonconvex matrix factorization that restores the convergence rate of gradient descent back to linear, even in the over-parameterized case, while also making it agnostic to possible ill-conditioning in the ground truth. Classical gradient descent in a neighborhood of the solution slows down due to the need for the model matrix factor to become singular. Our key result is that this singularity can be corrected by $\\ell_{2}$ regularization with a specific range of values for the damping parameter. In fact, a good damping parameter can be inexpensively estimated from the current iterate. The resulting algorithm, which we call preconditioned gradient descent or PrecGD, is stable under noise, and converges linearly to an information theoretically optimal error bound. Our numerical experiments find that PrecGD works equally well in restoring the linear convergence of other variants of nonconvex matrix factorization in the over-parameterized regime."
      },
      {
        "id": "oai:arXiv.org:2504.09712v1",
        "title": "The Structural Safety Generalization Problem",
        "link": "https://arxiv.org/abs/2504.09712",
        "author": "Julius Broomfield, Tom Gibbs, Ethan Kosak-Hine, George Ingebretsen, Tia Nasir, Jason Zhang, Reihaneh Iranmanesh, Sara Pieri, Reihaneh Rabbany, Kellin Pelrine",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09712v1 Announce Type: cross \nAbstract: LLM jailbreaks are a widespread safety challenge. Given this problem has not yet been tractable, we suggest targeting a key failure mechanism: the failure of safety to generalize across semantically equivalent inputs. We further focus the target by requiring desirable tractability properties of attacks to study: explainability, transferability between models, and transferability between goals. We perform red-teaming within this framework by uncovering new vulnerabilities to multi-turn, multi-image, and translation-based attacks. These attacks are semantically equivalent by our design to their single-turn, single-image, or untranslated counterparts, enabling systematic comparisons; we show that the different structures yield different safety outcomes. We then demonstrate the potential for this framework to enable new defenses by proposing a Structure Rewriting Guardrail, which converts an input to a structure more conducive to safety assessment. This guardrail significantly improves refusal of harmful inputs, without over-refusing benign ones. Thus, by framing this intermediate challenge - more tractable than universal defenses but essential for long-term safety - we highlight a critical milestone for AI safety research."
      },
      {
        "id": "oai:arXiv.org:2504.09723v1",
        "title": "AgentA/B: Automated and Scalable Web A/BTesting with Interactive LLM Agents",
        "link": "https://arxiv.org/abs/2504.09723",
        "author": "Dakuo Wang, Ting-Yao Hsu, Yuxuan Lu, Limeng Cui, Yaochen Xie, William Headean, Bingsheng Yao, Akash Veeragouni, Jiapeng Liu, Sreyashi Nag, Jessie Wang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09723v1 Announce Type: cross \nAbstract: A/B testing experiment is a widely adopted method for evaluating UI/UX design decisions in modern web applications. Yet, traditional A/B testing remains constrained by its dependence on the large-scale and live traffic of human participants, and the long time of waiting for the testing result. Through formative interviews with six experienced industry practitioners, we identified critical bottlenecks in current A/B testing workflows. In response, we present AgentA/B, a novel system that leverages Large Language Model-based autonomous agents (LLM Agents) to automatically simulate user interaction behaviors with real webpages. AgentA/B enables scalable deployment of LLM agents with diverse personas, each capable of navigating the dynamic webpage and interactively executing multi-step interactions like search, clicking, filtering, and purchasing. In a demonstrative controlled experiment, we employ AgentA/B to simulate a between-subject A/B testing with 1,000 LLM agents Amazon.com, and compare agent behaviors with real human shopping behaviors at a scale. Our findings suggest AgentA/B can emulate human-like behavior patterns."
      },
      {
        "id": "oai:arXiv.org:2504.09733v1",
        "title": "Epsilon-Neighborhood Decision-Boundary Governed Estimation (EDGE) of 2D Black Box Classifier Functions",
        "link": "https://arxiv.org/abs/2504.09733",
        "author": "Mithun Goutham, Riccardo DalferroNucci, Stephanie Stockar, Meghna Menon, Sneha Nayak, Harshad Zade, Chetan Patel, Mario Santillo",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09733v1 Announce Type: cross \nAbstract: Accurately estimating decision boundaries in black box systems is critical when ensuring safety, quality, and feasibility in real-world applications. However, existing methods iteratively refine boundary estimates by sampling in regions of uncertainty, without providing guarantees on the closeness to the decision boundary and also result in unnecessary exploration that is especially disadvantageous when evaluations are costly. This paper presents the Epsilon-Neighborhood Decision-Boundary Governed Estimation (EDGE), a sample efficient and function-agnostic algorithm that leverages the intermediate value theorem to estimate the location of the decision boundary of a black box binary classifier within a user-specified epsilon-neighborhood. Evaluations are conducted on three nonlinear test functions and a case study of an electric grid stability problem with uncertain renewable power injection. The EDGE algorithm demonstrates superior sample efficiency and better boundary approximation than adaptive sampling techniques and grid-based searches."
      },
      {
        "id": "oai:arXiv.org:2504.09737v1",
        "title": "Can LLM feedback enhance review quality? A randomized study of 20K reviews at ICLR 2025",
        "link": "https://arxiv.org/abs/2504.09737",
        "author": "Nitya Thakkar, Mert Yuksekgonul, Jake Silberg, Animesh Garg, Nanyun Peng, Fei Sha, Rose Yu, Carl Vondrick, James Zou",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09737v1 Announce Type: cross \nAbstract: Peer review at AI conferences is stressed by rapidly rising submission volumes, leading to deteriorating review quality and increased author dissatisfaction. To address these issues, we developed Review Feedback Agent, a system leveraging multiple large language models (LLMs) to improve review clarity and actionability by providing automated feedback on vague comments, content misunderstandings, and unprofessional remarks to reviewers. Implemented at ICLR 2025 as a large randomized control study, our system provided optional feedback to more than 20,000 randomly selected reviews. To ensure high-quality feedback for reviewers at this scale, we also developed a suite of automated reliability tests powered by LLMs that acted as guardrails to ensure feedback quality, with feedback only being sent to reviewers if it passed all the tests. The results show that 27% of reviewers who received feedback updated their reviews, and over 12,000 feedback suggestions from the agent were incorporated by those reviewers. This suggests that many reviewers found the AI-generated feedback sufficiently helpful to merit updating their reviews. Incorporating AI feedback led to significantly longer reviews (an average increase of 80 words among those who updated after receiving feedback) and more informative reviews, as evaluated by blinded researchers. Moreover, reviewers who were selected to receive AI feedback were also more engaged during paper rebuttals, as seen in longer author-reviewer discussions. This work demonstrates that carefully designed LLM-generated review feedback can enhance peer review quality by making reviews more specific and actionable while increasing engagement between reviewers and authors. The Review Feedback Agent is publicly available at https://github.com/zou-group/review_feedback_agent."
      },
      {
        "id": "oai:arXiv.org:2504.09750v1",
        "title": "Stochastic generative methods for stable and accurate closure modeling of chaotic dynamical systems",
        "link": "https://arxiv.org/abs/2504.09750",
        "author": "Emily Williams, David Darmofal",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09750v1 Announce Type: cross \nAbstract: Traditional deterministic subgrid-scale (SGS) models are often dissipative and unstable, especially in regions of chaotic and turbulent flow. Ongoing work in climate science and ocean modeling motivates the use of stochastic SGS models for chaotic dynamics. Further, developing stochastic generative models of underlying dynamics is a rapidly expanding field. In this work, we aim to incorporate stochastic integration toward closure modeling for chaotic dynamical systems. Further, we want to explore the potential stabilizing effect that stochastic models could have on linearized chaotic systems. We propose parametric and generative approaches for closure modeling using stochastic differential equations (SDEs). We derive and implement a quadratic diffusion model based on the fluctuations, demonstrating increased accuracy from bridging theoretical models with generative approaches. Results are demonstrated on the Lorenz-63 dynamical system."
      },
      {
        "id": "oai:arXiv.org:2504.09754v1",
        "title": "Integrating Large Language Models for Automated Structural Analysis",
        "link": "https://arxiv.org/abs/2504.09754",
        "author": "Haoran Liang, Mohammad Talebi Kalaleh, Qipei Mei",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09754v1 Announce Type: cross \nAbstract: Automated analysis for engineering structures offers considerable potential for boosting efficiency by minimizing repetitive tasks. Although AI-driven methods are increasingly common, no systematic framework yet leverages Large Language Models (LLMs) for automatic structural analysis. To address this gap, we propose a novel framework that integrates LLMs with structural analysis software. LLMs serve as the core engine: they parse structural descriptions from text and translate them into executable Python scripts. Moreover, the framework integrates the generative capabilities of LLMs with code-based finite element (FE) tools like OpenSeesPy. It employs domain-specific prompt design and in-context learning strategies to enhance the LLM's problem-solving capabilities and generative stability, enabling fully automated structural analysis from descriptive text to model outputs. In our experiments, we introduce a well-curated small-scale benchmark dataset of 20 structural analysis word problems (SAWPs) with ground-truth solutions and evaluate the performance of different LLMs within our framework in solving these SAWPs. The role of system instructions, crafted by structural engineers, is also investigated to understand their impact on LLM-driven structural analysis. Additionally, the generative stability of our framework is examined. Through multiple validation experiments on the benchmark, our results demonstrate that the proposed framework can substantially increase the level of automation in solving SAWPs compared to traditional methods. Quantitatively, the framework, built on GPT-4o, achieved 100% accuracy, surpassing GPT-4 (85%), Gemini 1.5 Pro (80%), and Llama-3.3 (30%) on the test examples. Furthermore, integrating domain-specific instructions enhanced performance by 30% on problems with asymmetrical structural configurations."
      },
      {
        "id": "oai:arXiv.org:2504.09775v1",
        "title": "Understanding and Optimizing Multi-Stage AI Inference Pipelines",
        "link": "https://arxiv.org/abs/2504.09775",
        "author": "Abhimanyu Rajeshkumar Bambhaniya, Hanjiang Wu, Suvinay Subramanian, Sudarshan Srinivasan, Souvik Kundu, Amir Yazdanbakhsh, Midhilesh Elavazhagan, Madhu Kumar, Tushar Krishna",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09775v1 Announce Type: cross \nAbstract: The rapid evolution of Large Language Models (LLMs) has driven the need for increasingly sophisticated inference pipelines and hardware platforms. Modern LLM serving extends beyond traditional prefill-decode workflows, incorporating multi-stage processes such as Retrieval Augmented Generation (RAG), key-value (KV) cache retrieval, dynamic model routing, and multi step reasoning. These stages exhibit diverse computational demands, requiring distributed systems that integrate GPUs, ASICs, CPUs, and memory-centric architectures. However, existing simulators lack the fidelity to model these heterogeneous, multi-engine workflows, limiting their ability to inform architectural decisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM inference Execution Simulator. HERMES models diverse request stages; including RAG, KV retrieval, reasoning, prefill, and decode across complex hardware hierarchies. HERMES supports heterogeneous clients executing multiple models concurrently unlike prior frameworks while incorporating advanced batching strategies and multi-level memory hierarchies. By integrating real hardware traces with analytical modeling, HERMES captures critical trade-offs such as memory bandwidth contention, inter-cluster communication latency, and batching efficiency in hybrid CPU-accelerator deployments. Through case studies, we explore the impact of reasoning stages on end-to-end latency, optimal batching strategies for hybrid pipelines, and the architectural implications of remote KV cache retrieval. HERMES empowers system designers to navigate the evolving landscape of LLM inference, providing actionable insights into optimizing hardware-software co-design for next-generation AI workloads."
      },
      {
        "id": "oai:arXiv.org:2504.09804v1",
        "title": "BO-SA-PINNs: Self-adaptive physics-informed neural networks based on Bayesian optimization for automatically designing PDE solvers",
        "link": "https://arxiv.org/abs/2504.09804",
        "author": "Rui Zhang, Liang Li, St\\'ephane Lanteri, Hao Kang, Jiaqi Li",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09804v1 Announce Type: cross \nAbstract: Physics-informed neural networks (PINNs) is becoming a popular alternative method for solving partial differential equations (PDEs). However, they require dedicated manual modifications to the hyperparameters of the network, the sampling methods and loss function weights for different PDEs, which reduces the efficiency of the solvers. In this paper, we pro- pose a general multi-stage framework, i.e. BO-SA-PINNs to alleviate this issue. In the first stage, Bayesian optimization (BO) is used to select hyperparameters for the training process, and based on the results of the pre-training, the network architecture, learning rate, sampling points distribution and loss function weights suitable for the PDEs are automatically determined. The proposed hyperparameters search space based on experimental results can enhance the efficiency of BO in identifying optimal hyperparameters. After selecting the appropriate hyperparameters, we incorporate a global self-adaptive (SA) mechanism the second stage. Using the pre-trained model and loss information in the second-stage training, the exponential moving average (EMA) method is employed to optimize the loss function weights, and residual-based adaptive refinement with distribution (RAR-D) is used to optimize the sampling points distribution. In the third stage, L-BFGS is used for stable training. In addition, we introduce a new activation function that enables BO-SA-PINNs to achieve higher accuracy. In numerical experiments, we conduct comparative and ablation experiments to verify the performance of the model on Helmholtz, Maxwell, Burgers and high-dimensional Poisson equations. The comparative experiment results show that our model can achieve higher accuracy and fewer iterations in test cases, and the ablation experiments demonstrate the positive impact of every improvement."
      },
      {
        "id": "oai:arXiv.org:2504.09807v1",
        "title": "Virtual domain extension for imposing boundary conditions in flow simulation using pre-trained local neural operator",
        "link": "https://arxiv.org/abs/2504.09807",
        "author": "Ximeng Ye, Hongyu Li, Zhen-Guo Yan",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09807v1 Announce Type: cross \nAbstract: This paper builds up a virtual domain extension (VDE) framework for imposing boundary conditions (BCs) in flow simulation using pre-trained local neural operator (LNO). It creates extended virtual domains to the input function to compensate for the corrosion nature of computational domains during LNO inference, thus turns the implementation of BC into the determination of field values on the extended domain. Several strategies to calculate the field values are proposed and validated in solving numerical examples, including padding operation, direct imposition, pressure symmetry, and optimization by backpropagation, and compared with boundary imposition in traditional solvers. It is found that the large time interval of LNO induces a relatively wide near-boundary domain to be processed, thus imposing BC on only a few nodes near the boundary following the immersed boundary conception in traditional solvers can hardly achieve high accuracy. With appropriate values assigned on the extended virtual domains, VDE can accurately impose BCs and lead to reasonable flow field predictions. This work provides a guidance for imposing BCs reliably in LNO prediction, which could facilitate the reuse of pre-trained LNO in more applications."
      },
      {
        "id": "oai:arXiv.org:2504.09816v1",
        "title": "Augmented Relevance Datasets with Fine-Tuned Small LLMs",
        "link": "https://arxiv.org/abs/2504.09816",
        "author": "Quentin Fitte-Rey, Matyas Amrouche, Romain Deveaud",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09816v1 Announce Type: cross \nAbstract: Building high-quality datasets and labeling query-document relevance are essential yet resource-intensive tasks, requiring detailed guidelines and substantial effort from human annotators. This paper explores the use of small, fine-tuned large language models (LLMs) to automate relevance assessment, with a focus on improving ranking models' performance by augmenting their training dataset. We fine-tuned small LLMs to enhance relevance assessments, thereby improving dataset creation quality for downstream ranking model training. Our experiments demonstrate that these fine-tuned small LLMs not only outperform certain closed source models on our dataset but also lead to substantial improvements in ranking model performance. These results highlight the potential of leveraging small LLMs for efficient and scalable dataset augmentation, providing a practical solution for search engine optimization."
      },
      {
        "id": "oai:arXiv.org:2504.09831v1",
        "title": "Offline Dynamic Inventory and Pricing Strategy: Addressing Censored and Dependent Demand",
        "link": "https://arxiv.org/abs/2504.09831",
        "author": "Korel Gundem, Zhengling Qi",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09831v1 Announce Type: cross \nAbstract: In this paper, we study the offline sequential feature-based pricing and inventory control problem where the current demand depends on the past demand levels and any demand exceeding the available inventory is lost. Our goal is to leverage the offline dataset, consisting of past prices, ordering quantities, inventory levels, covariates, and censored sales levels, to estimate the optimal pricing and inventory control policy that maximizes long-term profit. While the underlying dynamic without censoring can be modeled by Markov decision process (MDP), the primary obstacle arises from the observed process where demand censoring is present, resulting in missing profit information, the failure of the Markov property, and a non-stationary optimal policy. To overcome these challenges, we first approximate the optimal policy by solving a high-order MDP characterized by the number of consecutive censoring instances, which ultimately boils down to solving a specialized Bellman equation tailored for this problem. Inspired by offline reinforcement learning and survival analysis, we propose two novel data-driven algorithms to solving these Bellman equations and, thus, estimate the optimal policy. Furthermore, we establish finite sample regret bounds to validate the effectiveness of these algorithms. Finally, we conduct numerical experiments to demonstrate the efficacy of our algorithms in estimating the optimal policy. To the best of our knowledge, this is the first data-driven approach to learning optimal pricing and inventory control policies in a sequential decision-making environment characterized by censored and dependent demand. The implementations of the proposed algorithms are available at https://github.com/gundemkorel/Inventory_Pricing_Control"
      },
      {
        "id": "oai:arXiv.org:2504.09835v1",
        "title": "Laugh at Your Own Pace: Basic Performance Evaluation of Language Learning Assistance by Adjustment of Video Playback Speeds Based on Laughter Detection",
        "link": "https://arxiv.org/abs/2504.09835",
        "author": "Naoto Nishida, Hinako Nozaki, Buntarou Shizuki",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09835v1 Announce Type: cross \nAbstract: Among various methods to learn a second language (L2), such as listening and shadowing, Extensive Viewing involves learning L2 by watching many videos. However, it is difficult for many L2 learners to smoothly and effortlessly comprehend video contents made for native speakers at the original speed. Therefore, we developed a language learning assistance system that automatically adjusts the playback speed according to the learner's comprehension. Our system judges that learners understand the contents if they laugh at the punchlines of comedy dramas, and vice versa. Experimental results show that this system supports learners with relatively low L2 ability (under 700 in TOEIC Score in the experimental condition) to understand video contents. Our system can widen learners' possible options of native speakers' videos as Extensive Viewing material."
      },
      {
        "id": "oai:arXiv.org:2504.09836v1",
        "title": "Score Matching Diffusion Based Feedback Control and Planning of Nonlinear Systems",
        "link": "https://arxiv.org/abs/2504.09836",
        "author": "Karthik Elamvazhuthi, Darshan Gadginmath, Fabio Pasqualetti",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09836v1 Announce Type: cross \nAbstract: We propose a novel control-theoretic framework that leverages principles from generative modeling -- specifically, Denoising Diffusion Probabilistic Models (DDPMs) -- to stabilize control-affine systems with nonholonomic constraints. Unlike traditional stochastic approaches, which rely on noise-driven dynamics in both forward and reverse processes, our method crucially eliminates the need for noise in the reverse phase, making it particularly relevant for control applications. We introduce two formulations: one where noise perturbs all state dimensions during the forward phase while the control system enforces time reversal deterministically, and another where noise is restricted to the control channels, embedding system constraints directly into the forward process.\n  For controllable nonlinear drift-free systems, we prove that deterministic feedback laws can exactly reverse the forward process, ensuring that the system's probability density evolves correctly without requiring artificial diffusion in the reverse phase. Furthermore, for linear time-invariant systems, we establish a time-reversal result under the second formulation. By eliminating noise in the backward process, our approach provides a more practical alternative to machine learning-based denoising methods, which are unsuitable for control applications due to the presence of stochasticity. We validate our results through numerical simulations on benchmark systems, including a unicycle model in a domain with obstacles, a driftless five-dimensional system, and a four-dimensional linear system, demonstrating the potential for applying diffusion-inspired techniques in linear, nonlinear, and settings with state space constraints."
      },
      {
        "id": "oai:arXiv.org:2504.09839v1",
        "title": "SafeSpeech: Robust and Universal Voice Protection Against Malicious Speech Synthesis",
        "link": "https://arxiv.org/abs/2504.09839",
        "author": "Zhisheng Zhang, Derui Wang, Qianyi Yang, Pengyang Huang, Junhan Pu, Yuxin Cao, Kai Ye, Jie Hao, Yixian Yang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09839v1 Announce Type: cross \nAbstract: Speech synthesis technology has brought great convenience, while the widespread usage of realistic deepfake audio has triggered hazards. Malicious adversaries may unauthorizedly collect victims' speeches and clone a similar voice for illegal exploitation (\\textit{e.g.}, telecom fraud). However, the existing defense methods cannot effectively prevent deepfake exploitation and are vulnerable to robust training techniques. Therefore, a more effective and robust data protection method is urgently needed. In response, we propose a defensive framework, \\textit{\\textbf{SafeSpeech}}, which protects the users' audio before uploading by embedding imperceptible perturbations on original speeches to prevent high-quality synthetic speech. In SafeSpeech, we devise a robust and universal proactive protection technique, \\textbf{S}peech \\textbf{PE}rturbative \\textbf{C}oncealment (\\textbf{SPEC}), that leverages a surrogate model to generate universally applicable perturbation for generative synthetic models. Moreover, we optimize the human perception of embedded perturbation in terms of time and frequency domains. To evaluate our method comprehensively, we conduct extensive experiments across advanced models and datasets, both subjectively and objectively. Our experimental results demonstrate that SafeSpeech achieves state-of-the-art (SOTA) voice protection effectiveness and transferability and is highly robust against advanced adaptive adversaries. Moreover, SafeSpeech has real-time capability in real-world tests. The source code is available at \\href{https://github.com/wxzyd123/SafeSpeech}{https://github.com/wxzyd123/SafeSpeech}."
      },
      {
        "id": "oai:arXiv.org:2504.09848v1",
        "title": "A Survey of Large Language Model-Powered Spatial Intelligence Across Scales: Advances in Embodied Agents, Smart Cities, and Earth Science",
        "link": "https://arxiv.org/abs/2504.09848",
        "author": "Jie Feng, Jinwei Zeng, Qingyue Long, Hongyi Chen, Jie Zhao, Yanxin Xi, Zhilun Zhou, Yuan Yuan, Shengyuan Wang, Qingbin Zeng, Songwei Li, Yunke Zhang, Yuming Lin, Tong Li, Jingtao Ding, Chen Gao, Fengli Xu, Yong Li",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09848v1 Announce Type: cross \nAbstract: Over the past year, the development of large language models (LLMs) has brought spatial intelligence into focus, with much attention on vision-based embodied intelligence. However, spatial intelligence spans a broader range of disciplines and scales, from navigation and urban planning to remote sensing and earth science. What are the differences and connections between spatial intelligence across these fields? In this paper, we first review human spatial cognition and its implications for spatial intelligence in LLMs. We then examine spatial memory, knowledge representations, and abstract reasoning in LLMs, highlighting their roles and connections. Finally, we analyze spatial intelligence across scales -- from embodied to urban and global levels -- following a framework that progresses from spatial memory and understanding to spatial reasoning and intelligence. Through this survey, we aim to provide insights into interdisciplinary spatial intelligence research and inspire future studies."
      },
      {
        "id": "oai:arXiv.org:2504.09858v1",
        "title": "Reasoning Models Can Be Effective Without Thinking",
        "link": "https://arxiv.org/abs/2504.09858",
        "author": "Wenjie Ma, Jingxuan He, Charlie Snell, Tyler Griggs, Sewon Min, Matei Zaharia",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09858v1 Announce Type: cross \nAbstract: Recent LLMs have significantly improved reasoning capabilities, primarily by including an explicit, lengthy Thinking process as part of generation. In this paper, we question whether this explicit thinking is necessary. Using the state-of-the-art DeepSeek-R1-Distill-Qwen, we find that bypassing the thinking process via simple prompting, denoted as NoThinking, can be surprisingly effective. When controlling for the number of tokens, NoThinking outperforms Thinking across a diverse set of seven challenging reasoning datasets--including mathematical problem solving, formal theorem proving, and coding--especially in low-budget settings, e.g., 51.3 vs. 28.9 on ACM 23 with 700 tokens. Notably, the performance of NoThinking becomes more competitive with pass@k as k increases. Building on this observation, we demonstrate that a parallel scaling approach that uses NoThinking to generate N outputs independently and aggregates them is highly effective. For aggregation, we use task-specific verifiers when available, or we apply simple best-of-N strategies such as confidence-based selection. Our method outperforms a range of baselines with similar latency using Thinking, and is comparable to Thinking with significantly longer latency (up to 9x). Together, our research encourages a reconsideration of the necessity of lengthy thinking processes, while also establishing a competitive reference for achieving strong reasoning performance in low-budget settings or at low latency using parallel scaling."
      },
      {
        "id": "oai:arXiv.org:2504.09870v1",
        "title": "Ember: A Compiler for Efficient Embedding Operations on Decoupled Access-Execute Architectures",
        "link": "https://arxiv.org/abs/2504.09870",
        "author": "Marco Siracusa (Barcelona Supercomputing Center), Olivia Hsu (Stanford University), Victor Soria-Pardos (Barcelona Supercomputing Center), Joshua Randall (Arm), Arnaud Grasset (Arm), Eric Biscondi (Arm), Doug Joseph (Arm), Randy Allen (Barcelona Supercomputing Center), Fredrik Kjolstad (Stanford University), Miquel Moret\\'o Planas (Barcelona Supercomputing Center, Universitat Polit\\`ecnica de Catalunya), Adri\\`a Armejach (Barcelona Supercomputing Center, Universitat Polit\\`ecnica de Catalunya)",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09870v1 Announce Type: cross \nAbstract: Irregular embedding lookups are a critical bottleneck in recommender models, sparse large language models, and graph learning models. In this paper, we first demonstrate that, by offloading these lookups to specialized access units, Decoupled Access-Execute (DAE) processors achieve 2.6$\\times$ higher performance and 6.4$\\times$ higher performance/watt than GPUs on end-to-end models. Then, we propose the Ember compiler for automatically generating optimized DAE code from PyTorch and TensorFlow. Conversely from other DAE compilers, Ember features multiple intermediate representations specifically designed for different optimization levels. In this way, Ember can implement all optimizations to match the performance of hand-written code, unlocking the full potential of DAE architectures at scale."
      },
      {
        "id": "oai:arXiv.org:2504.09885v1",
        "title": "Separate to Collaborate: Dual-Stream Diffusion Model for Coordinated Piano Hand Motion Synthesis",
        "link": "https://arxiv.org/abs/2504.09885",
        "author": "Zihao Liu, Mingwen Ou, Zunnan Xu, Jiaqi Huang, Haonan Han, Ronghui Li, Xiu Li",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09885v1 Announce Type: cross \nAbstract: Automating the synthesis of coordinated bimanual piano performances poses significant challenges, particularly in capturing the intricate choreography between the hands while preserving their distinct kinematic signatures. In this paper, we propose a dual-stream neural framework designed to generate synchronized hand gestures for piano playing from audio input, addressing the critical challenge of modeling both hand independence and coordination. Our framework introduces two key innovations: (i) a decoupled diffusion-based generation framework that independently models each hand's motion via dual-noise initialization, sampling distinct latent noise for each while leveraging a shared positional condition, and (ii) a Hand-Coordinated Asymmetric Attention (HCAA) mechanism suppresses symmetric (common-mode) noise to highlight asymmetric hand-specific features, while adaptively enhancing inter-hand coordination during denoising. The system operates hierarchically: it first predicts 3D hand positions from audio features and then generates joint angles through position-aware diffusion models, where parallel denoising streams interact via HCAA. Comprehensive evaluations demonstrate that our framework outperforms existing state-of-the-art methods across multiple metrics."
      },
      {
        "id": "oai:arXiv.org:2504.09915v1",
        "title": "StePO-Rec: Towards Personalized Outfit Styling Assistant via Knowledge-Guided Multi-Step Reasoning",
        "link": "https://arxiv.org/abs/2504.09915",
        "author": "Yuxi Bi, Yunfan Gao, Haofen Wang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09915v1 Announce Type: cross \nAbstract: Advancements in Generative AI offers new opportunities for FashionAI, surpassing traditional recommendation systems that often lack transparency and struggle to integrate expert knowledge, leaving the potential for personalized fashion styling remain untapped. To address these challenges, we present PAFA (Principle-Aware Fashion), a multi-granular knowledge base that organizes professional styling expertise into three levels of metadata, domain principles, and semantic relationships. Using PAFA, we develop StePO-Rec, a knowledge-guided method for multi-step outfit recommendation. StePO-Rec provides structured suggestions using a scenario-dimension-attribute framework, employing recursive tree construction to align recommendations with both professional principles and individual preferences. A preference-trend re-ranking system further adapts to fashion trends while maintaining the consistency of the user's original style. Experiments on the widely used personalized outfit dataset IQON show a 28% increase in Recall@1 and 32.8% in MAP. Furthermore, case studies highlight improved explainability, traceability, result reliability, and the seamless integration of expertise and personalization."
      },
      {
        "id": "oai:arXiv.org:2504.09946v1",
        "title": "Assessing Judging Bias in Large Reasoning Models: An Empirical Study",
        "link": "https://arxiv.org/abs/2504.09946",
        "author": "Qian Wang, Zhanzhi Lou, Zhenheng Tang, Nuo Chen, Xuandong Zhao, Wenxuan Zhang, Dawn Song, Bingsheng He",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09946v1 Announce Type: cross \nAbstract: Large Reasoning Models (LRMs) like DeepSeek-R1 and OpenAI-o1 have demonstrated remarkable reasoning capabilities, raising important questions about their biases in LLM-as-a-judge settings. We present a comprehensive benchmark comparing judging biases between LLMs and LRMs across both subjective preference-alignment datasets and objective fact-based datasets. Through investigation of bandwagon, authority, position, and distraction biases, we uncover four key findings: (1) despite their advanced reasoning capabilities, LRMs remain susceptible to the above biases; (2) LRMs demonstrate better robustness than LLMs specifically on fact-related datasets; (3) LRMs exhibit notable position bias, preferring options in later positions; and (4) we identify a novel \"superficial reflection bias\" where phrases mimicking reasoning (e.g., \"wait, let me think...\") significantly influence model judgments. To address these biases, we design and evaluate three mitigation strategies: specialized system prompts that reduce judging biases by up to 19\\% in preference alignment datasets and 14\\% in fact-related datasets, in-context learning that provides up to 27\\% improvement on preference tasks but shows inconsistent results on factual tasks, and a self-reflection mechanism that reduces biases by up to 10\\% in preference datasets and 16\\% in fact-related datasets, with self-reflection proving particularly effective for LRMs. Our work provides crucial insights for developing more reliable LLM-as-a-Judge frameworks, especially as LRMs become increasingly deployed as automated judges."
      },
      {
        "id": "oai:arXiv.org:2504.09949v1",
        "title": "Pseudo-Label Guided Real-World Image De-weathering: A Learning Framework with Imperfect Supervision",
        "link": "https://arxiv.org/abs/2504.09949",
        "author": "Heming Xu, Xiaohui Liu, Zhilu Zhang, Hongzhi Zhang, Xiaohe Wu, Wangmeng Zuo",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09949v1 Announce Type: cross \nAbstract: Real-world image de-weathering aims at removingvarious undesirable weather-related artifacts, e.g., rain, snow,and fog. To this end, acquiring ideal training pairs is crucial.Existing real-world datasets are typically constructed paired databy extracting clean and degraded images from live streamsof landscape scene on the Internet. Despite the use of strictfiltering mechanisms during collection, training pairs inevitablyencounter inconsistency in terms of lighting, object position, scenedetails, etc, making de-weathering models possibly suffer fromdeformation artifacts under non-ideal supervision. In this work,we propose a unified solution for real-world image de-weatheringwith non-ideal supervision, i.e., a pseudo-label guided learningframework, to address various inconsistencies within the realworld paired dataset. Generally, it consists of a de-weatheringmodel (De-W) and a Consistent Label Constructor (CLC), bywhich restoration result can be adaptively supervised by originalground-truth image to recover sharp textures while maintainingconsistency with the degraded inputs in non-weather contentthrough the supervision of pseudo-labels. Particularly, a Crossframe Similarity Aggregation (CSA) module is deployed withinCLC to enhance the quality of pseudo-labels by exploring thepotential complementary information of multi-frames throughgraph model. Moreover, we introduce an Information AllocationStrategy (IAS) to integrate the original ground-truth imagesand pseudo-labels, thereby facilitating the joint supervision forthe training of de-weathering model. Extensive experimentsdemonstrate that our method exhibits significant advantageswhen trained on imperfectly aligned de-weathering datasets incomparison with other approaches."
      },
      {
        "id": "oai:arXiv.org:2504.09951v1",
        "title": "Towards Weaker Variance Assumptions for Stochastic Optimization",
        "link": "https://arxiv.org/abs/2504.09951",
        "author": "Ahmet Alacaoglu, Yura Malitsky, Stephen J. Wright",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09951v1 Announce Type: cross \nAbstract: We revisit a classical assumption for analyzing stochastic gradient algorithms where the squared norm of the stochastic subgradient (or the variance for smooth problems) is allowed to grow as fast as the squared norm of the optimization variable. We contextualize this assumption in view of its inception in the 1960s, its seemingly independent appearance in the recent literature, its relationship to weakest-known variance assumptions for analyzing stochastic gradient algorithms, and its relevance in deterministic problems for non-Lipschitz nonsmooth convex optimization. We build on and extend a connection recently made between this assumption and the Halpern iteration. For convex nonsmooth, and potentially stochastic, optimization, we analyze horizon-free, anytime algorithms with last-iterate rates. For problems beyond simple constrained optimization, such as convex problems with functional constraints or regularized convex-concave min-max problems, we obtain rates for optimality measures that do not require boundedness of the feasible set."
      },
      {
        "id": "oai:arXiv.org:2504.09975v1",
        "title": "OctGPT: Octree-based Multiscale Autoregressive Models for 3D Shape Generation",
        "link": "https://arxiv.org/abs/2504.09975",
        "author": "Si-Tong Wei, Rui-Huan Wang, Chuan-Zhi Zhou, Baoquan Chen, Peng-Shuai Wang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09975v1 Announce Type: cross \nAbstract: Autoregressive models have achieved remarkable success across various domains, yet their performance in 3D shape generation lags significantly behind that of diffusion models. In this paper, we introduce OctGPT, a novel multiscale autoregressive model for 3D shape generation that dramatically improves the efficiency and performance of prior 3D autoregressive approaches, while rivaling or surpassing state-of-the-art diffusion models. Our method employs a serialized octree representation to efficiently capture the hierarchical and spatial structures of 3D shapes. Coarse geometry is encoded via octree structures, while fine-grained details are represented by binary tokens generated using a vector quantized variational autoencoder (VQVAE), transforming 3D shapes into compact \\emph{multiscale binary sequences} suitable for autoregressive prediction. To address the computational challenges of handling long sequences, we incorporate octree-based transformers enhanced with 3D rotary positional encodings, scale-specific embeddings, and token-parallel generation schemes. These innovations reduce training time by 13 folds and generation time by 69 folds, enabling the efficient training of high-resolution 3D shapes, e.g.,$1024^3$, on just four NVIDIA 4090 GPUs only within days. OctGPT showcases exceptional versatility across various tasks, including text-, sketch-, and image-conditioned generation, as well as scene-level synthesis involving multiple objects. Extensive experiments demonstrate that OctGPT accelerates convergence and improves generation quality over prior autoregressive methods, offering a new paradigm for high-quality, scalable 3D content creation."
      },
      {
        "id": "oai:arXiv.org:2504.09994v1",
        "title": "Physical Scales Matter: The Role of Receptive Fields and Advection in Satellite-Based Thunderstorm Nowcasting with Convolutional Neural Networks",
        "link": "https://arxiv.org/abs/2504.09994",
        "author": "Christoph Metzl, Kianusch Vahid Yousefnia, Richard M\\\"uller, Virginia Poli, Miria Celano, Tobias B\\\"olle",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09994v1 Announce Type: cross \nAbstract: The focus of nowcasting development is transitioning from physically motivated advection methods to purely data-driven Machine Learning (ML) approaches. Nevertheless, recent work indicates that incorporating advection into the ML value chain has improved skill for radar-based precipitation nowcasts. However, the generality of this approach and the underlying causes remain unexplored. This study investigates the generality by probing the approach on satellite-based thunderstorm nowcasts for the first time. Resorting to a scale argument, we then put forth an explanation when and why skill improvements can be expected. In essence, advection guarantees that thunderstorm patterns relevant for nowcasting are contained in the receptive field at long lead times. To test our hypotheses, we train ResU-Nets solving segmentation tasks with lightning observations as ground truth. The input of the Baseline Neural Network (BNN) are short time series of multispectral satellite imagery and lightning observations, whereas the Advection-Informed Neural Network (AINN) additionally receives the Lagrangian persistence nowcast of all input channels at the desired lead time. Overall, we find only a minor skill improvement of the AINN over the BNN when considering fully averaged scores. However, assessing skill conditioned on lead time and wind speed, we demonstrate that our scale argument correctly predicts the onset of skill improvement of the AINN over the BNN after 2h lead time. We confirm that generally advection becomes gradually more important with longer lead times and higher wind speeds. Our work accentuates the importance of considering and incorporating the underlying physical scales when designing ML based forecasting models."
      },
      {
        "id": "oai:arXiv.org:2504.10000v1",
        "title": "Do We Really Need Curated Malicious Data for Safety Alignment in Multi-modal Large Language Models?",
        "link": "https://arxiv.org/abs/2504.10000",
        "author": "Yanbo Wang, Jiyang Guan, Jian Liang, Ran He",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10000v1 Announce Type: cross \nAbstract: Multi-modal large language models (MLLMs) have made significant progress, yet their safety alignment remains limited. Typically, current open-source MLLMs rely on the alignment inherited from their language module to avoid harmful generations. However, the lack of safety measures specifically designed for multi-modal inputs creates an alignment gap, leaving MLLMs vulnerable to vision-domain attacks such as typographic manipulation. Current methods utilize a carefully designed safety dataset to enhance model defense capability, while the specific knowledge or patterns acquired from the high-quality dataset remain unclear. Through comparison experiments, we find that the alignment gap primarily arises from data distribution biases, while image content, response quality, or the contrastive behavior of the dataset makes little contribution to boosting multi-modal safety. To further investigate this and identify the key factors in improving MLLM safety, we propose finetuning MLLMs on a small set of benign instruct-following data with responses replaced by simple, clear rejection sentences. Experiments show that, without the need for labor-intensive collection of high-quality malicious data, model safety can still be significantly improved, as long as a specific fraction of rejection data exists in the finetuning set, indicating the security alignment is not lost but rather obscured during multi-modal pretraining or instruction finetuning. Simply correcting the underlying data bias could narrow the safety gap in the vision domain."
      },
      {
        "id": "oai:arXiv.org:2504.10002v1",
        "title": "FLoRA: Sample-Efficient Preference-based RL via Low-Rank Style Adaptation of Reward Functions",
        "link": "https://arxiv.org/abs/2504.10002",
        "author": "Daniel Marta, Simon Holk, Miguel Vasco, Jens Lundell, Timon Homberger, Finn Busch, Olov Andersson, Danica Kragic, Iolanda Leite",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10002v1 Announce Type: cross \nAbstract: Preference-based reinforcement learning (PbRL) is a suitable approach for style adaptation of pre-trained robotic behavior: adapting the robot's policy to follow human user preferences while still being able to perform the original task. However, collecting preferences for the adaptation process in robotics is often challenging and time-consuming. In this work we explore the adaptation of pre-trained robots in the low-preference-data regime. We show that, in this regime, recent adaptation approaches suffer from catastrophic reward forgetting (CRF), where the updated reward model overfits to the new preferences, leading the agent to become unable to perform the original task. To mitigate CRF, we propose to enhance the original reward model with a small number of parameters (low-rank matrices) responsible for modeling the preference adaptation. Our evaluation shows that our method can efficiently and effectively adjust robotic behavior to human preferences across simulation benchmark tasks and multiple real-world robotic tasks."
      },
      {
        "id": "oai:arXiv.org:2504.10003v1",
        "title": "NaviDiffusor: Cost-Guided Diffusion Model for Visual Navigation",
        "link": "https://arxiv.org/abs/2504.10003",
        "author": "Yiming Zeng, Hao Ren, Shuhang Wang, Junlong Huang, Hui Cheng",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10003v1 Announce Type: cross \nAbstract: Visual navigation, a fundamental challenge in mobile robotics, demands versatile policies to handle diverse environments. Classical methods leverage geometric solutions to minimize specific costs, offering adaptability to new scenarios but are prone to system errors due to their multi-modular design and reliance on hand-crafted rules. Learning-based methods, while achieving high planning success rates, face difficulties in generalizing to unseen environments beyond the training data and often require extensive training. To address these limitations, we propose a hybrid approach that combines the strengths of learning-based methods and classical approaches for RGB-only visual navigation. Our method first trains a conditional diffusion model on diverse path-RGB observation pairs. During inference, it integrates the gradients of differentiable scene-specific and task-level costs, guiding the diffusion model to generate valid paths that meet the constraints. This approach alleviates the need for retraining, offering a plug-and-play solution. Extensive experiments in both indoor and outdoor settings, across simulated and real-world scenarios, demonstrate zero-shot transfer capability of our approach, achieving higher success rates and fewer collisions compared to baseline methods. Code will be released at https://github.com/SYSU-RoboticsLab/NaviD."
      },
      {
        "id": "oai:arXiv.org:2504.10025v1",
        "title": "Progressive Transfer Learning for Multi-Pass Fundus Image Restoration",
        "link": "https://arxiv.org/abs/2504.10025",
        "author": "Uyen Phan, Ozer Can Devecioglu, Serkan Kiranyaz, Moncef Gabbouj",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10025v1 Announce Type: cross \nAbstract: Diabetic retinopathy is a leading cause of vision impairment, making its early diagnosis through fundus imaging critical for effective treatment planning. However, the presence of poor quality fundus images caused by factors such as inadequate illumination, noise, blurring and other motion artifacts yields a significant challenge for accurate DR screening. In this study, we propose progressive transfer learning for multi pass restoration to iteratively enhance the quality of degraded fundus images, ensuring more reliable DR screening. Unlike previous methods that often focus on a single pass restoration, multi pass restoration via PTL can achieve a superior blind restoration performance that can even improve most of the good quality fundus images in the dataset. Initially, a Cycle GAN model is trained to restore low quality images, followed by PTL induced restoration passes over the latest restored outputs to improve overall quality in each pass. The proposed method can learn blind restoration without requiring any paired data while surpassing its limitations by leveraging progressive learning and fine tuning strategies to minimize distortions and preserve critical retinal features. To evaluate PTL's effectiveness on multi pass restoration, we conducted experiments on DeepDRiD, a large scale fundus imaging dataset specifically curated for diabetic retinopathy detection. Our result demonstrates state of the art performance, showcasing PTL's potential as a superior approach to iterative image quality restoration."
      },
      {
        "id": "oai:arXiv.org:2504.10041v1",
        "title": "Prior Does Matter: Visual Navigation via Denoising Diffusion Bridge Models",
        "link": "https://arxiv.org/abs/2504.10041",
        "author": "Hao Ren, Yiming Zeng, Zetong Bi, Zhaoliang Wan, Junlong Huang, Hui Cheng",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10041v1 Announce Type: cross \nAbstract: Recent advancements in diffusion-based imitation learning, which show impressive performance in modeling multimodal distributions and training stability, have led to substantial progress in various robot learning tasks. In visual navigation, previous diffusion-based policies typically generate action sequences by initiating from denoising Gaussian noise. However, the target action distribution often diverges significantly from Gaussian noise, leading to redundant denoising steps and increased learning complexity. Additionally, the sparsity of effective action distributions makes it challenging for the policy to generate accurate actions without guidance. To address these issues, we propose a novel, unified visual navigation framework leveraging the denoising diffusion bridge models named NaviBridger. This approach enables action generation by initiating from any informative prior actions, enhancing guidance and efficiency in the denoising process. We explore how diffusion bridges can enhance imitation learning in visual navigation tasks and further examine three source policies for generating prior actions. Extensive experiments in both simulated and real-world indoor and outdoor scenarios demonstrate that NaviBridger accelerates policy inference and outperforms the baselines in generating target action sequences. Code is available at https://github.com/hren20/NaiviBridger."
      },
      {
        "id": "oai:arXiv.org:2504.10045v1",
        "title": "CHARM: Calibrating Reward Models With Chatbot Arena Scores",
        "link": "https://arxiv.org/abs/2504.10045",
        "author": "Xiao Zhu, Chenmien Tan, Pinzhen Chen, Rico Sennrich, Yanlin Zhang, Hanxu Hu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10045v1 Announce Type: cross \nAbstract: Reward models (RMs) play a crucial role in Reinforcement Learning from Human Feedback by serving as proxies for human preferences in aligning large language models. In this paper, we identify a model preference bias in RMs, where they systematically assign disproportionately high scores to responses from certain policy models. This bias distorts ranking evaluations and leads to unfair judgments. To address this issue, we propose a calibration method named CHatbot Arena calibrated Reward Modeling (CHARM) that leverages Elo scores from the Chatbot Arena leaderboard to mitigate RM overvaluation. We also introduce a Mismatch Degree metric to measure this preference bias. Our approach is computationally efficient, requiring only a small preference dataset for continued training of the RM. We conduct extensive experiments on reward model benchmarks and human preference alignment. Results demonstrate that our calibrated RMs (1) achieve improved evaluation accuracy on RM-Bench and the Chat-Hard domain of RewardBench, and (2) exhibit a stronger correlation with human preferences by producing scores more closely aligned with Elo rankings. By mitigating model preference bias, our method provides a generalizable and efficient solution for building fairer and more reliable reward models."
      },
      {
        "id": "oai:arXiv.org:2504.10055v1",
        "title": "Joint Action Language Modelling for Transparent Policy Execution",
        "link": "https://arxiv.org/abs/2504.10055",
        "author": "Theodor Wulff, Rahul Singh Maharjan, Xinyun Chi, Angelo Cangelosi",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10055v1 Announce Type: cross \nAbstract: An agent's intention often remains hidden behind the black-box nature of embodied policies. Communication using natural language statements that describe the next action can provide transparency towards the agent's behavior. We aim to insert transparent behavior directly into the learning process, by transforming the problem of policy learning into a language generation problem and combining it with traditional autoregressive modelling. The resulting model produces transparent natural language statements followed by tokens representing the specific actions to solve long-horizon tasks in the Language-Table environment. Following previous work, the model is able to learn to produce a policy represented by special discretized tokens in an autoregressive manner. We place special emphasis on investigating the relationship between predicting actions and producing high-quality language for a transparent agent. We find that in many cases both the quality of the action trajectory and the transparent statement increase when they are generated simultaneously."
      },
      {
        "id": "oai:arXiv.org:2504.10081v1",
        "title": "RealSafe-R1: Safety-Aligned DeepSeek-R1 without Compromising Reasoning Capability",
        "link": "https://arxiv.org/abs/2504.10081",
        "author": "Yichi Zhang, Zihao Zeng, Dongbai Li, Yao Huang, Zhijie Deng, Yinpeng Dong",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10081v1 Announce Type: cross \nAbstract: Large Reasoning Models (LRMs), such as OpenAI o1 and DeepSeek-R1, have been rapidly progressing and achieving breakthrough performance on complex reasoning tasks such as mathematics and coding. However, the open-source R1 models have raised safety concerns in wide applications, such as the tendency to comply with malicious queries, which greatly impacts the utility of these powerful models in their applications. In this paper, we introduce RealSafe-R1 as safety-aligned versions of DeepSeek-R1 distilled models. To train these models, we construct a dataset of 15k safety-aware reasoning trajectories generated by DeepSeek-R1, under explicit instructions for expected refusal behavior. Both quantitative experiments and qualitative case studies demonstrate the models' improvements, which are shown in their safety guardrails against both harmful queries and jailbreak attacks. Importantly, unlike prior safety alignment efforts that often compromise reasoning performance, our method preserves the models' reasoning capabilities by maintaining the training data within the original distribution of generation. Model weights of RealSafe-R1 are open-source at https://huggingface.co/RealSafe."
      },
      {
        "id": "oai:arXiv.org:2504.10127v1",
        "title": "Breaking the Data Barrier -- Building GUI Agents Through Task Generalization",
        "link": "https://arxiv.org/abs/2504.10127",
        "author": "Junlei Zhang, Zichen Ding, Chang Ma, Zijie Chen, Qiushi Sun, Zhenzhong Lan, Junxian He",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10127v1 Announce Type: cross \nAbstract: Graphical User Interface (GUI) agents offer cross-platform solutions for automating complex digital tasks, with significant potential to transform productivity workflows. However, their performance is often constrained by the scarcity of high-quality trajectory data. To address this limitation, we propose training Vision Language Models (VLMs) on data-rich, reasoning-intensive tasks during a dedicated mid-training stage, and then examine how incorporating these tasks facilitates generalization to GUI planning scenarios. Specifically, we explore a range of tasks with readily available instruction-tuning data, including GUI perception, multimodal reasoning, and textual reasoning. Through extensive experiments across 11 mid-training tasks, we demonstrate that: (1) Task generalization proves highly effective, yielding substantial improvements across most settings. For instance, multimodal mathematical reasoning enhances performance on AndroidWorld by an absolute 6.3%. Remarkably, text-only mathematical data significantly boosts GUI web agent performance, achieving a 5.6% improvement on WebArena and 5.4% improvement on AndroidWorld, underscoring notable cross-modal generalization from text-based to visual domains; (2) Contrary to prior assumptions, GUI perception data - previously considered closely aligned with GUI agent tasks and widely utilized for training - has a comparatively limited impact on final performance; (3) Building on these insights, we identify the most effective mid-training tasks and curate optimized mixture datasets, resulting in absolute performance gains of 8.0% on WebArena and 12.2% on AndroidWorld. Our work provides valuable insights into cross-domain knowledge transfer for GUI agents and offers a practical approach to addressing data scarcity challenges in this emerging field. The code, data and models will be available at https://github.com/hkust-nlp/GUIMid."
      },
      {
        "id": "oai:arXiv.org:2504.10139v1",
        "title": "Conditional Distribution Compression via the Kernel Conditional Mean Embedding",
        "link": "https://arxiv.org/abs/2504.10139",
        "author": "Dominic Broadbent, Nick Whiteley, Robert Allison, Tom Lovett",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10139v1 Announce Type: cross \nAbstract: Existing distribution compression methods, like Kernel Herding (KH), were originally developed for unlabelled data. However, no existing approach directly compresses the conditional distribution of labelled data. To address this gap, we first introduce the Average Maximum Conditional Mean Discrepancy (AMCMD), a natural metric for comparing conditional distributions. We then derive a consistent estimator for the AMCMD and establish its rate of convergence. Next, we make a key observation: in the context of distribution compression, the cost of constructing a compressed set targeting the AMCMD can be reduced from $\\mathcal{O}(n^3)$ to $\\mathcal{O}(n)$. Building on this, we extend the idea of KH to develop Average Conditional Kernel Herding (ACKH), a linear-time greedy algorithm that constructs a compressed set targeting the AMCMD. To better understand the advantages of directly compressing the conditional distribution rather than doing so via the joint distribution, we introduce Joint Kernel Herding (JKH), a straightforward adaptation of KH designed to compress the joint distribution of labelled data. While herding methods provide a simple and interpretable selection process, they rely on a greedy heuristic. To explore alternative optimisation strategies, we propose Joint Kernel Inducing Points (JKIP) and Average Conditional Kernel Inducing Points (ACKIP), which jointly optimise the compressed set while maintaining linear complexity. Experiments show that directly preserving conditional distributions with ACKIP outperforms both joint distribution compression (via JKH and JKIP) and the greedy selection used in ACKH. Moreover, we see that JKIP consistently outperforms JKH."
      },
      {
        "id": "oai:arXiv.org:2504.10150v1",
        "title": "HistLLM: A Unified Framework for LLM-Based Multimodal Recommendation with User History Encoding and Compression",
        "link": "https://arxiv.org/abs/2504.10150",
        "author": "Chen Zhang, Bo Hu, Weidong Chen, Zhendong Mao",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10150v1 Announce Type: cross \nAbstract: While large language models (LLMs) have proven effective in leveraging textual data for recommendations, their application to multimodal recommendation tasks remains relatively underexplored. Although LLMs can process multimodal information through projection functions that map visual features into their semantic space, recommendation tasks often require representing users' history interactions through lengthy prompts combining text and visual elements, which not only hampers training and inference efficiency but also makes it difficult for the model to accurately capture user preferences from complex and extended prompts, leading to reduced recommendation performance. To address this challenge, we introduce HistLLM, an innovative multimodal recommendation framework that integrates textual and visual features through a User History Encoding Module (UHEM), compressing multimodal user history interactions into a single token representation, effectively facilitating LLMs in processing user preferences. Extensive experiments demonstrate the effectiveness and efficiency of our proposed mechanism."
      },
      {
        "id": "oai:arXiv.org:2504.10179v1",
        "title": "The Future of MLLM Prompting is Adaptive: A Comprehensive Experimental Evaluation of Prompt Engineering Methods for Robust Multimodal Performance",
        "link": "https://arxiv.org/abs/2504.10179",
        "author": "Anwesha Mohanty, Venkatesh Balavadhani Parthasarathy, Arsalan Shahid",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10179v1 Announce Type: cross \nAbstract: Multimodal Large Language Models (MLLMs) are set to transform how machines process and generate human-like responses by integrating diverse modalities such as text, images, and code. Yet, effectively harnessing their capabilities hinges on optimal prompt engineering. We present a comprehensive experimental evaluation of seven prompt engineering methods applied to 13 open-source MLLMs over 24 tasks spanning Reasoning and Compositionality, Multimodal Understanding and Alignment, Complex Code Generation and Execution, and Knowledge Retrieval and Integration. Our approach stratifies models by parameter count into Small (<4B), Medium (4B-10B), and Large (>10B) categories and compares prompting techniques including Zero-Shot, One-Shot, Few-Shot, Chain-of-Thought, Analogical, Generated Knowledge, and Tree-of-Thought. While Large MLLMs excel in structured tasks such as code generation, achieving accuracies up to 96.88% under Few-Shot prompting, all models struggle with complex reasoning and abstract understanding, often yielding accuracies below 60% and high hallucination rates. Structured reasoning prompts frequently increased hallucination up to 75% in small models and led to longer response times (over 20 seconds in Large MLLMs), while simpler prompting methods provided more concise and efficient outputs. No single prompting method uniformly optimises all task types. Instead, adaptive strategies combining example-based guidance with selective structured reasoning are essential to enhance robustness, efficiency, and factual accuracy. Our findings offer practical recommendations for prompt engineering and support more reliable deployment of MLLMs across applications including AI-assisted coding, knowledge retrieval, and multimodal content understanding."
      },
      {
        "id": "oai:arXiv.org:2504.10208v1",
        "title": "From Prompting to Alignment: A Generative Framework for Query Recommendation",
        "link": "https://arxiv.org/abs/2504.10208",
        "author": "Erxue Min, Hsiu-Yuan Huang, Min Yang, Xihong Yang, Xin Jia, Yunfang Wu, Hengyi Cai, Shuaiqiang Wang, Dawei Yin",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10208v1 Announce Type: cross \nAbstract: In modern search systems, search engines often suggest relevant queries to users through various panels or components, helping refine their information needs. Traditionally, these recommendations heavily rely on historical search logs to build models, which suffer from cold-start or long-tail issues. Furthermore, tasks such as query suggestion, completion or clarification are studied separately by specific design, which lacks generalizability and hinders adaptation to novel applications. Despite recent attempts to explore the use of LLMs for query recommendation, these methods mainly rely on the inherent knowledge of LLMs or external sources like few-shot examples, retrieved documents, or knowledge bases, neglecting the importance of the calibration and alignment with user feedback, thus limiting their practical utility. To address these challenges, we first propose a general Generative Query Recommendation (GQR) framework that aligns LLM-based query generation with user preference. Specifically, we unify diverse query recommendation tasks by a universal prompt framework, leveraging the instruct-following capability of LLMs for effective generation. Secondly, we align LLMs with user feedback via presenting a CTR-alignment framework, which involves training a query-wise CTR predictor as a process reward model and employing list-wise preference alignment to maximize the click probability of the generated query list. Furthermore, recognizing the inconsistency between LLM knowledge and proactive search intents arising from the separation of user-initiated queries from models, we align LLMs with user initiative via retrieving co-occurrence queries as side information when historical logs are available."
      },
      {
        "id": "oai:arXiv.org:2504.10240v1",
        "title": "GNN-ACLP: Graph Neural Networks based Analog Circuit Link Prediction",
        "link": "https://arxiv.org/abs/2504.10240",
        "author": "Guanyuan Pan, Tiansheng Zhou, Bingtao Ma, Yaqi Wang, Jianxiang Zhao, Shuai Wang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10240v1 Announce Type: cross \nAbstract: Circuit link prediction identifying missing component connections from incomplete netlists is crucial in automating analog circuit design. However, existing methods face three main challenges: 1) Insufficient use of topological patterns in circuit graphs reduces prediction accuracy; 2) Data scarcity due to the complexity of annotations hinders model generalization; 3) Limited adaptability to various netlist formats. We propose GNN-ACLP, a Graph Neural Networks (GNNs) based framework featuring three innovations to tackle these challenges. First, we introduce the SEAL (Subgraphs, Embeddings, and Attributes for Link Prediction) framework and achieve port-level accuracy in circuit link prediction. Second, we propose Netlist Babel Fish, a netlist format conversion tool leveraging retrieval-augmented generation (RAG) with large language model (LLM) to enhance the compatibility of netlist formats. Finally, we construct SpiceNetlist, a comprehensive dataset that contains 775 annotated circuits across 10 different classes of components. The experimental results demonstrate an improvement of 15.05% on the SpiceNetlist dataset and 12.01% on the Image2Net dataset over the existing approach."
      },
      {
        "id": "oai:arXiv.org:2504.10244v1",
        "title": "Towards contrast- and pathology-agnostic clinical fetal brain MRI segmentation using SynthSeg",
        "link": "https://arxiv.org/abs/2504.10244",
        "author": "Ziyao Shang, Misha Kaandorp, Kelly Payette, Marina Fernandez Garcia, Roxane Licandro, Georg Langs, Jordina Aviles Verdera, Jana Hutter, Bjoern Menze, Gregor Kasprian, Meritxell Bach Cuadra, Andras Jakab",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10244v1 Announce Type: cross \nAbstract: Magnetic resonance imaging (MRI) has played a crucial role in fetal neurodevelopmental research. Structural annotations of MR images are an important step for quantitative analysis of the developing human brain, with Deep learning providing an automated alternative for this otherwise tedious manual process. However, segmentation performances of Convolutional Neural Networks often suffer from domain shift, where the network fails when applied to subjects that deviate from the distribution with which it is trained on. In this work, we aim to train networks capable of automatically segmenting fetal brain MRIs with a wide range of domain shifts pertaining to differences in subject physiology and acquisition environments, in particular shape-based differences commonly observed in pathological cases. We introduce a novel data-driven train-time sampling strategy that seeks to fully exploit the diversity of a given training dataset to enhance the domain generalizability of the trained networks. We adapted our sampler, together with other existing data augmentation techniques, to the SynthSeg framework, a generator that utilizes domain randomization to generate diverse training data, and ran thorough experimentations and ablation studies on a wide range of training/testing data to test the validity of the approaches. Our networks achieved notable improvements in the segmentation quality on testing subjects with intense anatomical abnormalities (p < 1e-4), though at the cost of a slighter decrease in performance in cases with fewer abnormalities. Our work also lays the foundation for future works on creating and adapting data-driven sampling strategies for other training pipelines."
      },
      {
        "id": "oai:arXiv.org:2504.10248v1",
        "title": "Adaptive Sensor Steering Strategy Using Deep Reinforcement Learning for Dynamic Data Acquisition in Digital Twins",
        "link": "https://arxiv.org/abs/2504.10248",
        "author": "Collins O. Ogbodo, Timothy J. Rogers, Mattia Dal Borgo, David J. Wagg",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10248v1 Announce Type: cross \nAbstract: This paper introduces a sensor steering methodology based on deep reinforcement learning to enhance the predictive accuracy and decision support capabilities of digital twins by optimising the data acquisition process. Traditional sensor placement techniques are often constrained by one-off optimisation strategies, which limit their applicability for online applications requiring continuous informative data assimilation. The proposed approach addresses this limitation by offering an adaptive framework for sensor placement within the digital twin paradigm. The sensor placement problem is formulated as a Markov decision process, enabling the training and deployment of an agent capable of dynamically repositioning sensors in response to the evolving conditions of the physical structure as represented by the digital twin. This ensures that the digital twin maintains a highly representative and reliable connection to its physical counterpart. The proposed framework is validated through a series of comprehensive case studies involving a cantilever plate structure subjected to diverse conditions, including healthy and damaged conditions. The results demonstrate the capability of the deep reinforcement learning agent to adaptively reposition sensors improving the quality of data acquisition and hence enhancing the overall accuracy of digital twins."
      },
      {
        "id": "oai:arXiv.org:2504.10250v1",
        "title": "MURR: Model Updating with Regularized Replay for Searching a Document Stream",
        "link": "https://arxiv.org/abs/2504.10250",
        "author": "Eugene Yang, Nicola Tonellotto, Dawn Lawrie, Sean MacAvaney, James Mayfield, Douglas W. Oard, Scott Miller",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10250v1 Announce Type: cross \nAbstract: The Internet produces a continuous stream of new documents and user-generated queries. These naturally change over time based on events in the world and the evolution of language. Neural retrieval models that were trained once on a fixed set of query-document pairs will quickly start misrepresenting newly-created content and queries, leading to less effective retrieval. Traditional statistical sparse retrieval can update collection statistics to reflect these changes in the use of language in documents and queries. In contrast, continued fine-tuning of the language model underlying neural retrieval approaches such as DPR and ColBERT creates incompatibility with previously-encoded documents. Re-encoding and re-indexing all previously-processed documents can be costly. In this work, we explore updating a neural dual encoder retrieval model without reprocessing past documents in the stream. We propose MURR, a model updating strategy with regularized replay, to ensure the model can still faithfully search existing documents without reprocessing, while continuing to update the model for the latest topics. In our simulated streaming environments, we show that fine-tuning models using MURR leads to more effective and more consistent retrieval results than other strategies as the stream of documents and queries progresses."
      },
      {
        "id": "oai:arXiv.org:2504.10253v1",
        "title": "TinyverseGP: Towards a Modular Cross-domain Benchmarking Framework for Genetic Programming",
        "link": "https://arxiv.org/abs/2504.10253",
        "author": "Roman Kalkreuth, Fabricio Olivetti de Fran\\c{c}a, Julian Dierkes, Marie Anastacio, Anja Jankovic, Zdenek Vasicek, Holger Hoos",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10253v1 Announce Type: cross \nAbstract: Over the years, genetic programming (GP) has evolved, with many proposed variations, especially in how they represent a solution. Being essentially a program synthesis algorithm, it is capable of tackling multiple problem domains. Current benchmarking initiatives are fragmented, as the different representations are not compared with each other and their performance is not measured across the different domains. In this work, we propose a unified framework, dubbed TinyverseGP (inspired by tinyGP), which provides support to multiple representations and problem domains, including symbolic regression, logic synthesis and policy search."
      },
      {
        "id": "oai:arXiv.org:2504.10266v1",
        "title": "Vision based driving agent for race car simulation environments",
        "link": "https://arxiv.org/abs/2504.10266",
        "author": "Gergely B\\'ari, L\\'aszl\\'o Palkovics",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10266v1 Announce Type: cross \nAbstract: In recent years, autonomous driving has become a popular field of study. As control at tire grip limit is essential during emergency situations, algorithms developed for racecars are useful for road cars too. This paper examines the use of Deep Reinforcement Learning (DRL) to solve the problem of grip limit driving in a simulated environment. Proximal Policy Optimization (PPO) method is used to train an agent to control the steering wheel and pedals of the vehicle, using only visual inputs to achieve professional human lap times. The paper outlines the formulation of the task of time optimal driving on a race track as a deep reinforcement learning problem, and explains the chosen observations, actions, and reward functions. The results demonstrate human-like learning and driving behavior that utilize maximum tire grip potential."
      },
      {
        "id": "oai:arXiv.org:2504.10277v1",
        "title": "RealHarm: A Collection of Real-World Language Model Application Failures",
        "link": "https://arxiv.org/abs/2504.10277",
        "author": "Pierre Le Jeune, Jiaen Liu, Luca Rossi, Matteo Dora",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10277v1 Announce Type: cross \nAbstract: Language model deployments in consumer-facing applications introduce numerous risks. While existing research on harms and hazards of such applications follows top-down approaches derived from regulatory frameworks and theoretical analyses, empirical evidence of real-world failure modes remains underexplored. In this work, we introduce RealHarm, a dataset of annotated problematic interactions with AI agents built from a systematic review of publicly reported incidents. Analyzing harms, causes, and hazards specifically from the deployer's perspective, we find that reputational damage constitutes the predominant organizational harm, while misinformation emerges as the most common hazard category. We empirically evaluate state-of-the-art guardrails and content moderation systems to probe whether such systems would have prevented the incidents, revealing a significant gap in the protection of AI applications."
      },
      {
        "id": "oai:arXiv.org:2504.10281v1",
        "title": "Zero-shot Autonomous Microscopy for Scalable and Intelligent Characterization of 2D Materials",
        "link": "https://arxiv.org/abs/2504.10281",
        "author": "Jingyun Yang, Ruoyan Avery Yin, Chi Jiang, Yuepeng Hu, Xiaokai Zhu, Xingjian Hu, Sutharsika Kumar, Xiao Wang, Xiaohua Zhai, Keran Rong, Yunyue Zhu, Tianyi Zhang, Zongyou Yin, Jing Kong, Neil Zhenqiang Gong, Zhichu Ren, Haozhe Wang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10281v1 Announce Type: cross \nAbstract: Characterization of atomic-scale materials traditionally requires human experts with months to years of specialized training. Even for trained human operators, accurate and reliable characterization remains challenging when examining newly discovered materials such as two-dimensional (2D) structures. This bottleneck drives demand for fully autonomous experimentation systems capable of comprehending research objectives without requiring large training datasets. In this work, we present ATOMIC (Autonomous Technology for Optical Microscopy & Intelligent Characterization), an end-to-end framework that integrates foundation models to enable fully autonomous, zero-shot characterization of 2D materials. Our system integrates the vision foundation model (i.e., Segment Anything Model), large language models (i.e., ChatGPT), unsupervised clustering, and topological analysis to automate microscope control, sample scanning, image segmentation, and intelligent analysis through prompt engineering, eliminating the need for additional training. When analyzing typical MoS2 samples, our approach achieves 99.7% segmentation accuracy for single layer identification, which is equivalent to that of human experts. In addition, the integrated model is able to detect grain boundary slits that are challenging to identify with human eyes. Furthermore, the system retains robust accuracy despite variable conditions including defocus, color temperature fluctuations, and exposure variations. It is applicable to a broad spectrum of common 2D materials-including graphene, MoS2, WSe2, SnSe-regardless of whether they were fabricated via chemical vapor deposition or mechanical exfoliation. This work represents the implementation of foundation models to achieve autonomous analysis, establishing a scalable and data-efficient characterization paradigm that fundamentally transforms the approach to nanoscale materials research."
      },
      {
        "id": "oai:arXiv.org:2504.10296v1",
        "title": "Siamese Network with Dual Attention for EEG-Driven Social Learning: Bridging the Human-Robot Gap in Long-Tail Autonomous Driving",
        "link": "https://arxiv.org/abs/2504.10296",
        "author": "Xiaoshan Zhou, Carol C. Menassa, Vineet R. Kamat",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10296v1 Announce Type: cross \nAbstract: Robots with wheeled, quadrupedal, or humanoid forms are increasingly integrated into built environments. However, unlike human social learning, they lack a critical pathway for intrinsic cognitive development, namely, learning from human feedback during interaction. To understand human ubiquitous observation, supervision, and shared control in dynamic and uncertain environments, this study presents a brain-computer interface (BCI) framework that enables classification of Electroencephalogram (EEG) signals to detect cognitively demanding and safety-critical events. As a timely and motivating co-robotic engineering application, we simulate a human-in-the-loop scenario to flag risky events in semi-autonomous robotic driving-representative of long-tail cases that pose persistent bottlenecks to the safety performance of smart mobility systems and robotic vehicles. Drawing on recent advances in few-shot learning, we propose a dual-attention Siamese convolutional network paired with Dynamic Time Warping Barycenter Averaging approach to generate robust EEG-encoded signal representations. Inverse source localization reveals activation in Broadman areas 4 and 9, indicating perception-action coupling during task-relevant mental imagery. The model achieves 80% classification accuracy under data-scarce conditions and exhibits a nearly 100% increase in the utility of salient features compared to state-of-the-art methods, as measured through integrated gradient attribution. Beyond performance, this study contributes to our understanding of the cognitive architecture required for BCI agents-particularly the role of attention and memory mechanisms-in categorizing diverse mental states and supporting both inter- and intra-subject adaptation. Overall, this research advances the development of cognitive robotics and socially guided learning for service robots in complex built environments."
      },
      {
        "id": "oai:arXiv.org:2504.10352v1",
        "title": "Pseudo-Autoregressive Neural Codec Language Models for Efficient Zero-Shot Text-to-Speech Synthesis",
        "link": "https://arxiv.org/abs/2504.10352",
        "author": "Yifan Yang, Shujie Liu, Jinyu Li, Yuxuan Hu, Haibin Wu, Hui Wang, Jianwei Yu, Lingwei Meng, Haiyang Sun, Yanqing Liu, Yan Lu, Kai Yu, Xie Chen",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10352v1 Announce Type: cross \nAbstract: Recent zero-shot text-to-speech (TTS) systems face a common dilemma: autoregressive (AR) models suffer from slow generation and lack duration controllability, while non-autoregressive (NAR) models lack temporal modeling and typically require complex designs. In this paper, we introduce a novel pseudo-autoregressive (PAR) codec language modeling approach that unifies AR and NAR modeling. Combining explicit temporal modeling from AR with parallel generation from NAR, PAR generates dynamic-length spans at fixed time steps. Building on PAR, we propose PALLE, a two-stage TTS system that leverages PAR for initial generation followed by NAR refinement. In the first stage, PAR progressively generates speech tokens along the time dimension, with each step predicting all positions in parallel but only retaining the left-most span. In the second stage, low-confidence tokens are iteratively refined in parallel, leveraging the global contextual information. Experiments demonstrate that PALLE, trained on LibriTTS, outperforms state-of-the-art systems trained on large-scale data, including F5-TTS, E2-TTS, and MaskGCT, on the LibriSpeech test-clean set in terms of speech quality, speaker similarity, and intelligibility, while achieving up to ten times faster inference speed. Audio samples are available at https://anonymous-palle.github.io."
      },
      {
        "id": "oai:arXiv.org:2504.10369v1",
        "title": "SymRTLO: Enhancing RTL Code Optimization with LLMs and Neuron-Inspired Symbolic Reasoning",
        "link": "https://arxiv.org/abs/2504.10369",
        "author": "Yiting Wang, Wanghao Ye, Ping Guo, Yexiao He, Ziyao Wang, Yexiao He, Bowei Tian, Shwai He, Guoheng Sun, Zheyu Shen, Sihan Chen, Ankur Srivastava, Qingfu Zhang, Gang Qu, Ang Li",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10369v1 Announce Type: cross \nAbstract: Optimizing Register Transfer Level (RTL) code is crucial for improving the power, performance, and area (PPA) of digital circuits in the early stages of synthesis. Manual rewriting, guided by synthesis feedback, can yield high-quality results but is time-consuming and error-prone. Most existing compiler-based approaches have difficulty handling complex design constraints. Large Language Model (LLM)-based methods have emerged as a promising alternative to address these challenges. However, LLM-based approaches often face difficulties in ensuring alignment between the generated code and the provided prompts. This paper presents SymRTLO, a novel neuron-symbolic RTL optimization framework that seamlessly integrates LLM-based code rewriting with symbolic reasoning techniques. Our method incorporates a retrieval-augmented generation (RAG) system of optimization rules and Abstract Syntax Tree (AST)-based templates, enabling LLM-based rewriting that maintains syntactic correctness while minimizing undesired circuit behaviors. A symbolic module is proposed for analyzing and optimizing finite state machine (FSM) logic, allowing fine-grained state merging and partial specification handling beyond the scope of pattern-based compilers. Furthermore, a fast verification pipeline, combining formal equivalence checks with test-driven validation, further reduces the complexity of verification. Experiments on the RTL-Rewriter benchmark with Synopsys Design Compiler and Yosys show that SymRTLO improves power, performance, and area (PPA) by up to 43.9%, 62.5%, and 51.1%, respectively, compared to the state-of-the-art methods."
      },
      {
        "id": "oai:arXiv.org:2504.10384v1",
        "title": "A 10.8mW Mixed-Signal Simulated Bifurcation Ising Solver using SRAM Compute-In-Memory with 0.6us Time-to-Solution",
        "link": "https://arxiv.org/abs/2504.10384",
        "author": "Alana Marie Dee, Sajjad Moazeni",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10384v1 Announce Type: cross \nAbstract: Combinatorial optimization problems are funda- mental for various fields ranging from finance to wireless net- works. This work presents a simulated bifurcation (SB) Ising solver in CMOS for NP-hard optimization problems. Analog domain computing led to a superior implementation of this algorithm as inherent and injected noise is required in SB Ising solvers. The architecture novelties include the use of SRAM compute-in-memory (CIM) to accelerate bifurcation as well as the generation and injection of optimal decaying noise in the analog domain. We propose a novel 10-T SRAM cell capable of performing ternary multiplication. When measured with 60- node, 50% density, random, binary MAXCUT graphs, this all- to-all connected Ising solver reliably achieves above 93% of the ground state solution in 0.6us with 10.8mW average power in TSMC 180nm CMOS. Our chip achieves an order of magnitude improvement in time-to-solution and power compared to previously proposed Ising solvers in CMOS and other platforms."
      },
      {
        "id": "oai:arXiv.org:2504.10388v1",
        "title": "Inferring genotype-phenotype maps using attention models",
        "link": "https://arxiv.org/abs/2504.10388",
        "author": "Krishna Rijal, Caroline M. Holmes, Samantha Petti, Gautam Reddy, Michael M. Desai, Pankaj Mehta",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10388v1 Announce Type: cross \nAbstract: Predicting phenotype from genotype is a central challenge in genetics. Traditional approaches in quantitative genetics typically analyze this problem using methods based on linear regression. These methods generally assume that the genetic architecture of complex traits can be parameterized in terms of an additive model, where the effects of loci are independent, plus (in some cases) pairwise epistatic interactions between loci. However, these models struggle to analyze more complex patterns of epistasis or subtle gene-environment interactions. Recent advances in machine learning, particularly attention-based models, offer a promising alternative. Initially developed for natural language processing, attention-based models excel at capturing context-dependent interactions and have shown exceptional performance in predicting protein structure and function. Here, we apply attention-based models to quantitative genetics. We analyze the performance of this attention-based approach in predicting phenotype from genotype using simulated data across a range of models with increasing epistatic complexity, and using experimental data from a recent quantitative trait locus mapping study in budding yeast. We find that our model demonstrates superior out-of-sample predictions in epistatic regimes compared to standard methods. We also explore a more general multi-environment attention-based model to jointly analyze genotype-phenotype maps across multiple environments and show that such architectures can be used for \"transfer learning\" - predicting phenotypes in novel environments with limited training data."
      },
      {
        "id": "oai:arXiv.org:2504.10397v1",
        "title": "Can LLMs Assist Expert Elicitation for Probabilistic Causal Modeling?",
        "link": "https://arxiv.org/abs/2504.10397",
        "author": "Olha Shaposhnyk, Daria Zahorska, Svetlana Yanushkevich",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10397v1 Announce Type: cross \nAbstract: Objective: This study investigates the potential of Large Language Models (LLMs) as an alternative to human expert elicitation for extracting structured causal knowledge and facilitating causal modeling in biometric and healthcare applications.\n  Material and Methods: LLM-generated causal structures, specifically Bayesian networks (BNs), were benchmarked against traditional statistical methods (e.g., Bayesian Information Criterion) using healthcare datasets. Validation techniques included structural equation modeling (SEM) to verifying relationships, and measures such as entropy, predictive accuracy, and robustness to compare network structures.\n  Results and Discussion: LLM-generated BNs demonstrated lower entropy than expert-elicited and statistically generated BNs, suggesting higher confidence and precision in predictions. However, limitations such as contextual constraints, hallucinated dependencies, and potential biases inherited from training data require further investigation.\n  Conclusion: LLMs represent a novel frontier in expert elicitation for probabilistic causal modeling, promising to improve transparency and reduce uncertainty in the decision-making using such models."
      },
      {
        "id": "oai:arXiv.org:2504.10412v1",
        "title": "AI-Driven Code Refactoring: Using Graph Neural Networks to Enhance Software Maintainability",
        "link": "https://arxiv.org/abs/2504.10412",
        "author": "Gopichand Bandarupalli",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10412v1 Announce Type: cross \nAbstract: This study explores Graph Neural Networks (GNNs) as a transformative tool for code refactoring, using abstract syntax trees (ASTs) to boost software maintainability. It analyzes a dataset of 2 million snippets from CodeSearchNet and a custom 75000-file GitHub Python corpus, comparing GNNs against rule-based SonarQube and decision trees. Metrics include cyclomatic complexity (target below 10), coupling (target below 5), and refactoring precision. GNNs achieve 92% accuracy, reducing complexity by 35% and coupling by 33%, outperforming SonarQube (78%, 16%) and decision trees (85%, 25%). Preprocessing fixed 60% of syntax errors. Bar graphs, tables, and AST visuals clarify results. This offers a scalable AI-driven path to cleaner codebases, which is crucial for software engineering."
      },
      {
        "id": "oai:arXiv.org:2504.10428v1",
        "title": "Learning with Positive and Imperfect Unlabeled Data",
        "link": "https://arxiv.org/abs/2504.10428",
        "author": "Jane H. Lee, Anay Mehrotra, Manolis Zampetakis",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10428v1 Announce Type: cross \nAbstract: We study the problem of learning binary classifiers from positive and unlabeled data when the unlabeled data distribution is shifted, which we call Positive and Imperfect Unlabeled (PIU) Learning. In the absence of covariate shifts, i.e., with perfect unlabeled data, Denis (1998) reduced this problem to learning under Massart noise; however, that reduction fails under even slight shifts.\n  Our main results on PIU learning are the characterizations of the sample complexity of PIU learning and a computationally and sample-efficient algorithm achieving a misclassification error $\\varepsilon$. We further show that our results lead to new algorithms for several related problems.\n  1. Learning from smooth distributions: We give algorithms that learn interesting concept classes from only positive samples under smooth feature distributions, bypassing known existing impossibility results and contributing to recent advances in smoothened learning (Haghtalab et al, J.ACM'24) (Chandrasekaran et al., COLT'24).\n  2. Learning with a list of unlabeled distributions: We design new algorithms that apply to a broad class of concept classes under the assumption that we are given a list of unlabeled distributions, one of which--unknown to the learner--is $O(1)$-close to the true feature distribution.\n  3. Estimation in the presence of unknown truncation: We give the first polynomial sample and time algorithm for estimating the parameters of an exponential family distribution from samples truncated to an unknown set approximable by polynomials in $L_1$-norm. This improves the algorithm by Lee et al. (FOCS'24) that requires approximation in $L_2$-norm.\n  4. Detecting truncation: We present new algorithms for detecting whether given samples have been truncated (or not) for a broad class of non-product distributions, including non-product distributions, improving the algorithm by De et al. (STOC'24)."
      },
      {
        "id": "oai:arXiv.org:2504.10445v1",
        "title": "RealWebAssist: A Benchmark for Long-Horizon Web Assistance with Real-World Users",
        "link": "https://arxiv.org/abs/2504.10445",
        "author": "Suyu Ye, Haojun Shi, Darren Shih, Hyokun Yun, Tanya Roosta, Tianmin Shu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10445v1 Announce Type: cross \nAbstract: To achieve successful assistance with long-horizon web-based tasks, AI agents must be able to sequentially follow real-world user instructions over a long period. Unlike existing web-based agent benchmarks, sequential instruction following in the real world poses significant challenges beyond performing a single, clearly defined task. For instance, real-world human instructions can be ambiguous, require different levels of AI assistance, and may evolve over time, reflecting changes in the user's mental state. To address this gap, we introduce RealWebAssist, a novel benchmark designed to evaluate sequential instruction-following in realistic scenarios involving long-horizon interactions with the web, visual GUI grounding, and understanding ambiguous real-world user instructions. RealWebAssist includes a dataset of sequential instructions collected from real-world human users. Each user instructs a web-based assistant to perform a series of tasks on multiple websites. A successful agent must reason about the true intent behind each instruction, keep track of the mental state of the user, understand user-specific routines, and ground the intended tasks to actions on the correct GUI elements. Our experimental results show that state-of-the-art models struggle to understand and ground user instructions, posing critical challenges in following real-world user instructions for long-horizon web assistance."
      },
      {
        "id": "oai:arXiv.org:2504.10453v1",
        "title": "Anchors no more: Using peculiar velocities to constrain $H_0$ and the primordial Universe without calibrators",
        "link": "https://arxiv.org/abs/2504.10453",
        "author": "Davide Piras, Francesco Sorrenti, Ruth Durrer, Martin Kunz",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10453v1 Announce Type: cross \nAbstract: We develop a novel approach to constrain the Hubble parameter $H_0$ and the primordial power spectrum amplitude $A_\\mathrm{s}$ using supernovae type Ia (SNIa) data. By considering SNIa as tracers of the peculiar velocity field, we can model their distance and their covariance as a function of cosmological parameters without the need of calibrators like Cepheids; this yields a new independent probe of the large-scale structure based on SNIa data without distance anchors. Crucially, we implement a differentiable pipeline in JAX, including efficient emulators and affine sampling, reducing inference time from years to hours on a single GPU. We first validate our method on mock datasets, demonstrating that we can constrain $H_0$ and $\\log 10^{10}A_\\mathrm{s}$ within $\\sim10\\%$ using $\\sim10^3$ SNIa. We then test our pipeline with SNIa from an $N$-body simulation, obtaining $7\\%$-level unbiased constraints on $H_0$ with a moderate noise level. We finally apply our method to Pantheon+ data, constraining $H_0$ at the $10\\%$ level without Cepheids when fixing $A_\\mathrm{s}$ to its $\\it{Planck}$ value. On the other hand, we obtain $15\\%$-level constraints on $\\log 10^{10}A_\\mathrm{s}$ in agreement with $\\it{Planck}$ when including Cepheids in the analysis. In light of upcoming observations of low redshift SNIa from the Zwicky Transient Facility and the Vera Rubin Legacy Survey of Space and Time, surveys for which our method will develop its full potential, we make our code publicly available."
      },
      {
        "id": "oai:arXiv.org:2004.13821v4",
        "title": "Fine-tuning Multi-hop Question Answering with Hierarchical Graph Network",
        "link": "https://arxiv.org/abs/2004.13821",
        "author": "Guanming Xiong",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2004.13821v4 Announce Type: replace \nAbstract: In this paper, we present a two stage model for multi-hop question answering. The first stage is a hierarchical graph network, which is used to reason over multi-hop question and is capable to capture different levels of granularity using the nature structure(i.e., paragraphs, questions, sentences and entities) of documents. The reasoning process is convert to node classify task(i.e., paragraph nodes and sentences nodes). The second stage is a language model fine-tuning task. In a word, stage one use graph neural network to select and concatenate support sentences as one paragraph, and stage two find the answer span in language model fine-tuning paradigm."
      },
      {
        "id": "oai:arXiv.org:2110.13694v5",
        "title": "A Fast Horizon Detector and a New Annotated Dataset for Maritime Video Processing",
        "link": "https://arxiv.org/abs/2110.13694",
        "author": "Yassir Zardoua, Boulaala Mohammed, Mhamed El Mrabet, Astito Abdelali",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2110.13694v5 Announce Type: replace \nAbstract: Accurate and fast sea horizon detection is vital for tasks in autonomous navigation and maritime security, such as video stabilization, target region reduction, precise tracking, and obstacle avoidance. This paper introduces a novel sea horizon detector from RGB videos, focusing on rapid and effective sea noise suppression while preserving weak horizon edges. Line fitting methods are subsequently employed on filtered edges for horizon detection. We address the filtering problem by extracting line segments with a very low edge threshold, ensuring the detection of line segments even in low-contrast horizon conditions. We show that horizon line segments have simple and relevant properties in RGB images, which we exploit to suppress noisy segments. Then we use the surviving segments to construct a filtered edge map and infer the horizon from the filtered edges. We propose a careful incorporation of temporal information for horizon inference and experimentally show its effectiveness. We address the computational constraint by providing a vectorized implementation for efficient CPU execution, and leveraging image downsizing with minimal loss of accuracy on the original size. Moreover, we contribute a public horizon line dataset to enrich existing data resources. Our algorithm's performance is rigorously evaluated against state-of-the-art methods, and its components are validated through ablation experiments. Source code and dataset files are available at: https://github.com/Zardoua-Yassir/A_fast_horizon_detector_and_a_new_annotated_dataset_for_maritime_video_processing"
      },
      {
        "id": "oai:arXiv.org:2111.12664v3",
        "title": "MIO : Mutual Information Optimization using Self-Supervised Binary Contrastive Learning",
        "link": "https://arxiv.org/abs/2111.12664",
        "author": "Siladittya Manna, Umapada Pal, Saumik Bhattacharya",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2111.12664v3 Announce Type: replace \nAbstract: Self-supervised contrastive learning frameworks have progressed rapidly over the last few years. In this paper, we propose a novel loss function for contrastive learning. We model our pre-training task as a binary classification problem to induce an implicit contrastive effect. We further improve the n\\\"aive loss function after removing the effect of the positive-positive repulsion and incorporating the upper bound of the negative pair repulsion. Unlike existing methods, the proposed loss function optimizes the mutual information in positive and negative pairs. We also present a closed-form expression for the parameter gradient flow and compare the behaviour of self-supervised contrastive frameworks using Hessian eigenspectrum to analytically study their convergence. The proposed method outperforms SOTA self-supervised contrastive frameworks on benchmark datasets such as CIFAR-10, CIFAR-100, STL-10, and Tiny-ImageNet. After 200 pretraining epochs with ResNet-18 as the backbone, the proposed model achieves an accuracy of 86.36%, 58.18%, 80.50%, and 30.87% on the CIFAR-10, CIFAR-100, STL-10, and Tiny-ImageNet datasets, respectively, and surpasses the SOTA contrastive baseline by 1.93%, 3.57%, 4.85%, and 0.33%, respectively. The proposed framework also achieves a state-of-the-art accuracy of 78.4% (200 epochs) and 65.22% (100 epochs) Top-1 Linear Evaluation accuracy on ImageNet100 and ImageNet1K datasets, respectively."
      },
      {
        "id": "oai:arXiv.org:2203.06663v2",
        "title": "Global2Local: A Joint-Hierarchical Attention for Video Captioning",
        "link": "https://arxiv.org/abs/2203.06663",
        "author": "Chengpeng Dai, Fuhai Chen, Xiaoshuai Sun, Rongrong Ji, Qixiang Ye, Yongjian Wu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2203.06663v2 Announce Type: replace \nAbstract: Recently, automatic video captioning has attracted increasing attention, where the core challenge lies in capturing the key semantic items, like objects and actions as well as their spatial-temporal correlations from the redundant frames and semantic content. To this end, existing works select either the key video clips in a global level~(across multi frames), or key regions within each frame, which, however, neglect the hierarchical order, i.e., key frames first and key regions latter. In this paper, we propose a novel joint-hierarchical attention model for video captioning, which embeds the key clips, the key frames and the key regions jointly into the captioning model in a hierarchical manner. Such a joint-hierarchical attention model first conducts a global selection to identify key frames, followed by a Gumbel sampling operation to identify further key regions based on the key frames, achieving an accurate global-to-local feature representation to guide the captioning. Extensive quantitative evaluations on two public benchmark datasets MSVD and MSR-VTT demonstrates the superiority of the proposed method over the state-of-the-art methods."
      },
      {
        "id": "oai:arXiv.org:2203.10651v3",
        "title": "Forecasting Sparse Movement Speed of Urban Road Networks with Nonstationary Temporal Matrix Factorization",
        "link": "https://arxiv.org/abs/2203.10651",
        "author": "Xinyu Chen, Chengyuan Zhang, Xi-Le Zhao, Nicolas Saunier, Lijun Sun",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2203.10651v3 Announce Type: replace \nAbstract: Movement speed data from urban road networks, computed from ridesharing vehicles or taxi trajectories, is often high-dimensional, sparse, and nonstationary (e.g., exhibiting seasonality). To address these challenges, we propose a Nonstationary Temporal Matrix Factorization (NoTMF) model that leverages matrix factorization to project high-dimensional and sparse movement speed data into low-dimensional latent spaces. This results in a concise formula with the multiplication between spatial and temporal factor matrices. To characterize the temporal correlations, NoTMF takes a latent equation on the seasonal differenced temporal factors using higher-order vector autoregression (VAR). This approach not only preserves the low-rank structure of sparse movement speed data but also maintains consistent temporal dynamics, including seasonality information. The learning process for NoTMF involves optimizing the spatial and temporal factor matrices along with a collection of VAR coefficient matrices. To solve this efficiently, we introduce an alternating minimization framework, which tackles a challenging procedure of estimating the temporal factor matrix using conjugate gradient method, as the subproblem involves both partially observed matrix factorization and seasonal differenced VAR. To evaluate the forecasting performance of NoTMF, we conduct extensive experiments on Uber movement speed datasets, which are estimated from ridesharing vehicle trajectories. These datasets contain a large proportion of missing values due to insufficient ridesharing vehicles on the urban road network. Despite the presence of missing data, NoTMF demonstrates superior forecasting accuracy and effectiveness compared to baseline models. Moreover, as the seasonality of movement speed data is of great concern, the experiment results highlight the significance of addressing the nonstationarity of movement speed data."
      },
      {
        "id": "oai:arXiv.org:2205.13283v4",
        "title": "Embedding Principle in Depth for the Loss Landscape Analysis of Deep Neural Networks",
        "link": "https://arxiv.org/abs/2205.13283",
        "author": "Zhiwei Bai, Tao Luo, Zhi-Qin John Xu, Yaoyu Zhang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2205.13283v4 Announce Type: replace \nAbstract: Understanding the relation between deep and shallow neural networks is extremely important for the theoretical study of deep learning. In this work, we discover an embedding principle in depth that loss landscape of an NN \"contains\" all critical points of the loss landscapes for shallower NNs. The key tool for our discovery is the critical lifting operator proposed in this work that maps any critical point of a network to critical manifolds of any deeper network while preserving the outputs. This principle provides new insights to many widely observed behaviors of DNNs. Regarding the easy training of deep networks, we show that local minimum of an NN can be lifted to strict saddle points of a deeper NN. Regarding the acceleration effect of batch normalization, we demonstrate that batch normalization helps avoid the critical manifolds lifted from shallower NNs by suppressing layer linearization. We also prove that increasing training data shrinks the lifted critical manifolds, which can result in acceleration of training as demonstrated in experiments. Overall, our discovery of the embedding principle in depth uncovers the depth-wise hierarchical structure of deep learning loss landscape, which serves as a solid foundation for the further study about the role of depth for DNNs."
      },
      {
        "id": "oai:arXiv.org:2206.05395v2",
        "title": "Why is constrained neural language generation particularly challenging?",
        "link": "https://arxiv.org/abs/2206.05395",
        "author": "Cristina Garbacea, Qiaozhu Mei",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2206.05395v2 Announce Type: replace \nAbstract: Recent advances in deep neural language models combined with the capacity of large scale datasets have accelerated the development of natural language generation systems that produce fluent and coherent texts (to various degrees of success) in a multitude of tasks and application contexts. However, controlling the output of these models for desired user and task needs is still an open challenge. This is crucial not only to customizing the content and style of the generated language, but also to their safe and reliable deployment in the real world. We present an extensive survey on the emerging topic of constrained neural language generation in which we formally define and categorize the problems of natural language generation by distinguishing between conditions and constraints (the latter being testable conditions on the output text instead of the input), present constrained text generation tasks, and review existing methods and evaluation metrics for constrained text generation. Our aim is to highlight recent progress and trends in this emerging field, informing on the most promising directions and limitations towards advancing the state-of-the-art of constrained neural language generation research."
      },
      {
        "id": "oai:arXiv.org:2206.05730v3",
        "title": "Adding New Categories in Object Detection Using Few-Shot Copy-Paste",
        "link": "https://arxiv.org/abs/2206.05730",
        "author": "Boyang Deng, Meiyan Lin, Shoulun Long",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2206.05730v3 Announce Type: replace \nAbstract: Developing data-efficient instance detection models that can handle rare object categories remains a key challenge in computer vision. However, existing research often overlooks data collection strategies and evaluation metrics tailored to real-world scenarios involving neural networks. In this study, we systematically investigate data collection and augmentation techniques focused on object occlusion, aiming to mimic occlusion relationships observed in practical applications. Surprisingly, we find that even a simple occlusion mechanism is sufficient to achieve strong performance when introducing new object categories. Notably, by adding just 15 images of a new category to a large-scale training dataset containing over half a million images across hundreds of categories, the model achieves 95\\% accuracy on an unseen test set with thousands of instances of the new category."
      },
      {
        "id": "oai:arXiv.org:2206.12680v5",
        "title": "Topology-aware Generalization of Decentralized SGD",
        "link": "https://arxiv.org/abs/2206.12680",
        "author": "Tongtian Zhu, Fengxiang He, Lan Zhang, Zhengyang Niu, Mingli Song, Dacheng Tao",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2206.12680v5 Announce Type: replace \nAbstract: This paper studies the algorithmic stability and generalizability of decentralized stochastic gradient descent (D-SGD). We prove that the consensus model learned by D-SGD is $\\mathcal{O}{(N^{-1}+m^{-1} +\\lambda^2)}$-stable in expectation in the non-convex non-smooth setting, where $N$ is the total sample size, $m$ is the worker number, and $1+\\lambda$ is the spectral gap that measures the connectivity of the communication topology. These results then deliver an $\\mathcal{O}{(N^{-(1+\\alpha)/2}+ m^{-(1+\\alpha)/2}+\\lambda^{1+\\alpha} + \\phi_{\\mathcal{S}})}$ in-average generalization bound, which is non-vacuous even when $\\lambda$ is closed to $1$, in contrast to vacuous as suggested by existing literature on the projected version of D-SGD. Our theory indicates that the generalizability of D-SGD is positively correlated with the spectral gap, and can explain why consensus control in initial training phase can ensure better generalization. Experiments of VGG-11 and ResNet-18 on CIFAR-10, CIFAR-100 and Tiny-ImageNet justify our theory. To our best knowledge, this is the first work on the topology-aware generalization of vanilla D-SGD. Code is available at https://github.com/Raiden-Zhu/Generalization-of-DSGD."
      },
      {
        "id": "oai:arXiv.org:2210.08714v3",
        "title": "Selective Query-guided Debiasing for Video Corpus Moment Retrieval",
        "link": "https://arxiv.org/abs/2210.08714",
        "author": "Sunjae Yoon, Ji Woo Hong, Eunseop Yoon, Dahyun Kim, Junyeong Kim, Hee Suk Yoon, Chang D. Yoo",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2210.08714v3 Announce Type: replace \nAbstract: Video moment retrieval (VMR) aims to localize target moments in untrimmed videos pertinent to a given textual query. Existing retrieval systems tend to rely on retrieval bias as a shortcut and thus, fail to sufficiently learn multi-modal interactions between query and video. This retrieval bias stems from learning frequent co-occurrence patterns between query and moments, which spuriously correlate objects (e.g., a pencil) referred in the query with moments (e.g., scene of writing with a pencil) where the objects frequently appear in the video, such that they converge into biased moment predictions. Although recent debiasing methods have focused on removing this retrieval bias, we argue that these biased predictions sometimes should be preserved because there are many queries where biased predictions are rather helpful. To conjugate this retrieval bias, we propose a Selective Query-guided Debiasing network (SQuiDNet), which incorporates the following two main properties: (1) Biased Moment Retrieval that intentionally uncovers the biased moments inherent in objects of the query and (2) Selective Query-guided Debiasing that performs selective debiasing guided by the meaning of the query. Our experimental results on three moment retrieval benchmarks (i.e., TVR, ActivityNet, DiDeMo) show the effectiveness of SQuiDNet and qualitative analysis shows improved interpretability."
      },
      {
        "id": "oai:arXiv.org:2211.13613v3",
        "title": "Ham2Pose: Animating Sign Language Notation into Pose Sequences",
        "link": "https://arxiv.org/abs/2211.13613",
        "author": "Rotem Shalev-Arkushin, Amit Moryossef, Ohad Fried",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2211.13613v3 Announce Type: replace \nAbstract: Translating spoken languages into Sign languages is necessary for open communication between the hearing and hearing-impaired communities. To achieve this goal, we propose the first method for animating a text written in HamNoSys, a lexical Sign language notation, into signed pose sequences. As HamNoSys is universal, our proposed method offers a generic solution invariant to the target Sign language. Our method gradually generates pose predictions using transformer encoders that create meaningful representations of the text and poses while considering their spatial and temporal information. We use weak supervision for the training process and show that our method succeeds in learning from partial and inaccurate data. Additionally, we offer a new distance measurement for pose sequences, normalized Dynamic Time Warping (nDTW), based on DTW over normalized keypoints trajectories, and validate its correctness using AUTSL, a large-scale Sign language dataset. We show that it measures the distance between pose sequences more accurately than existing measurements and use it to assess the quality of our generated pose sequences. Code for the data pre-processing, the model, and the distance measurement is publicly released for future research."
      },
      {
        "id": "oai:arXiv.org:2302.07224v2",
        "title": "Painting 3D Nature in 2D: View Synthesis of Natural Scenes from a Single Semantic Mask",
        "link": "https://arxiv.org/abs/2302.07224",
        "author": "Shangzan Zhang, Sida Peng, Tianrun Chen, Linzhan Mou, Haotong Lin, Kaicheng Yu, Yiyi Liao, Xiaowei Zhou",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2302.07224v2 Announce Type: replace \nAbstract: We introduce a novel approach that takes a single semantic mask as input to synthesize multi-view consistent color images of natural scenes, trained with a collection of single images from the Internet. Prior works on 3D-aware image synthesis either require multi-view supervision or learning category-level prior for specific classes of objects, which can hardly work for natural scenes. Our key idea to solve this challenging problem is to use a semantic field as the intermediate representation, which is easier to reconstruct from an input semantic mask and then translate to a radiance field with the assistance of off-the-shelf semantic image synthesis models. Experiments show that our method outperforms baseline methods and produces photorealistic, multi-view consistent videos of a variety of natural scenes."
      },
      {
        "id": "oai:arXiv.org:2306.00037v5",
        "title": "BotArtist: Generic approach for bot detection in Twitter via semi-automatic machine learning pipeline",
        "link": "https://arxiv.org/abs/2306.00037",
        "author": "Alexander Shevtsov, Despoina Antonakaki, Ioannis Lamprou, Polyvios Pratikakis, Sotiris Ioannidis",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2306.00037v5 Announce Type: replace \nAbstract: Twitter, as one of the most popular social networks, provides a platform for communication and online discourse. Unfortunately, it has also become a target for bots and fake accounts, resulting in the spread of false information and manipulation. This paper introduces a semi-automatic machine learning pipeline (SAMLP) designed to address the challenges associated with machine learning model development. Through this pipeline, we develop a comprehensive bot detection model named BotArtist, based on user profile features. SAMLP leverages nine distinct publicly available datasets to train the BotArtist model. To assess BotArtist's performance against current state-of-the-art solutions, we evaluate 35 existing Twitter bot detection methods, each utilizing a diverse range of features. Our comparative evaluation of BotArtist and these existing methods, conducted across nine public datasets under standardized conditions, reveals that the proposed model outperforms existing solutions by almost 10% in terms of F1-score, achieving an average score of 83.19% and 68.5% over specific and general approaches, respectively. As a result of this research, we provide one of the largest labeled Twitter bot datasets. The dataset contains extracted features combined with BotArtist predictions for 10,929,533 Twitter user profiles, collected via Twitter API during the 2022 Russo-Ukrainian War over a 16-month period. This dataset was created based on [Shevtsov et al., 2022a] where the original authors share anonymized tweets discussing the Russo-Ukrainian war, totaling 127,275,386 tweets. The combination of the existing textual dataset and the provided labeled bot and human profiles will enable future development of more advanced bot detection large language models in the post-Twitter API era."
      },
      {
        "id": "oai:arXiv.org:2306.00707v2",
        "title": "Renormalized Graph Representations for Node Classification",
        "link": "https://arxiv.org/abs/2306.00707",
        "author": "Francesco Caso, Giovanni Trappolini, Andrea Bacciu, Pietro Li\\`o, Fabrizio Silvestri",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2306.00707v2 Announce Type: replace \nAbstract: Graph neural networks process information on graphs represented at a given resolution scale. We analyze the effect of using different coarse-grained graph resolutions, obtained through the Laplacian renormalization group theory, on node classification tasks. At the theory's core is grouping nodes connected by significant information flow at a given time scale. Representations of the graph at different scales encode interaction information at different ranges. We specifically experiment using representations at the characteristic scale of the graph's mesoscopic structures. We provide the models with the original graph and the graph represented at the characteristic resolution scale and compare them to models that can only access the original graph. Our results showed that models with access to both the original graph and the characteristic scale graph can achieve statistically significant improvements in test accuracy."
      },
      {
        "id": "oai:arXiv.org:2306.02064v2",
        "title": "Unlearnable Examples Give a False Sense of Data Privacy: Understanding and Relearning",
        "link": "https://arxiv.org/abs/2306.02064",
        "author": "Pucheng Dang, Xing Hu, Kaidi Xu, Jinhao Duan, Di Huang, Husheng Han, Rui Zhang, Zidong Du",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2306.02064v2 Announce Type: replace \nAbstract: Unlearnable examples are proposed to prevent third parties from exploiting unauthorized data, which generates unlearnable examples by adding imperceptible perturbations to public publishing data. These unlearnable examples proficiently misdirect the model training process, leading it to focus on learning perturbation features while neglecting the semantic features of the image. In this paper, we make an in-depth analysis and observe that models can learn both image features and perturbation features of unlearnable examples at an early training stage, but are rapidly trapped in perturbation features learning since the shallow layers tend to learn on perturbation features and propagate harmful activations to deeper layers. Based on the observations, we propose Progressive Staged Training, a self-adaptive training framework specially designed to break unlearnable examples. The proposed framework effectively prevents models from becoming trapped in learning perturbation features. We evaluated our method on multiple model architectures over diverse datasets, e.g., CIFAR-10, CIFAR-100, and ImageNet-mini. Our method circumvents the unlearnability of all state-of-the-art methods in the literature, revealing that existing unlearnable examples give a false sense of privacy protection and provide a reliable baseline for further evaluation of unlearnable techniques."
      },
      {
        "id": "oai:arXiv.org:2307.02191v2",
        "title": "Evaluating AI systems under uncertain ground truth: a case study in dermatology",
        "link": "https://arxiv.org/abs/2307.02191",
        "author": "David Stutz, Ali Taylan Cemgil, Abhijit Guha Roy, Tatiana Matejovicova, Melih Barsbey, Patricia Strachan, Mike Schaekermann, Jan Freyberg, Rajeev Rikhye, Beverly Freeman, Javier Perez Matos, Umesh Telang, Dale R. Webster, Yuan Liu, Greg S. Corrado, Yossi Matias, Pushmeet Kohli, Yun Liu, Arnaud Doucet, Alan Karthikesalingam",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2307.02191v2 Announce Type: replace \nAbstract: For safety, medical AI systems undergo thorough evaluations before deployment, validating their predictions against a ground truth which is assumed to be fixed and certain. However, this ground truth is often curated in the form of differential diagnoses. While a single differential diagnosis reflects the uncertainty in one expert assessment, multiple experts introduce another layer of uncertainty through disagreement. Both forms of uncertainty are ignored in standard evaluation which aggregates these differential diagnoses to a single label. In this paper, we show that ignoring uncertainty leads to overly optimistic estimates of model performance, therefore underestimating risk associated with particular diagnostic decisions. To this end, we propose a statistical aggregation approach, where we infer a distribution on probabilities of underlying medical condition candidates themselves, based on observed annotations. This formulation naturally accounts for the potential disagreements between different experts, as well as uncertainty stemming from individual differential diagnoses, capturing the entire ground truth uncertainty. Our approach boils down to generating multiple samples of medical condition probabilities, then evaluating and averaging performance metrics based on these sampled probabilities. In skin condition classification, we find that a large portion of the dataset exhibits significant ground truth uncertainty and standard evaluation severely over-estimates performance without providing uncertainty estimates. In contrast, our framework provides uncertainty estimates on common metrics of interest such as top-k accuracy and average overlap, showing that performance can change multiple percentage points. We conclude that, while assuming a crisp ground truth can be acceptable for many AI applications, a more nuanced evaluation protocol should be utilized in medical diagnosis."
      },
      {
        "id": "oai:arXiv.org:2307.02694v5",
        "title": "Loss Functions and Metrics in Deep Learning",
        "link": "https://arxiv.org/abs/2307.02694",
        "author": "Juan Terven, Diana M. Cordova-Esparza, Alfonso Ramirez-Pedraza, Edgar A. Chavez-Urbiola, Julio A. Romero-Gonzalez",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2307.02694v5 Announce Type: replace \nAbstract: This paper presents a comprehensive review of loss functions and performance metrics in deep learning, highlighting key developments and practical insights across diverse application areas. We begin by outlining fundamental considerations in classic tasks such as regression and classification, then extend our analysis to specialized domains like computer vision and natural language processing including retrieval-augmented generation. In each setting, we systematically examine how different loss functions and evaluation metrics can be paired to address task-specific challenges such as class imbalance, outliers, and sequence-level optimization. Key contributions of this work include: (1) a unified framework for understanding how losses and metrics align with different learning objectives, (2) an in-depth discussion of multi-loss setups that balance competing goals, and (3) new insights into specialized metrics used to evaluate modern applications like retrieval-augmented generation, where faithfulness and context relevance are pivotal. Along the way, we highlight best practices for selecting or combining losses and metrics based on empirical behaviors and domain constraints. Finally, we identify open problems and promising directions, including the automation of loss-function search and the development of robust, interpretable evaluation measures for increasingly complex deep learning tasks. Our review aims to equip researchers and practitioners with clearer guidance in designing effective training pipelines and reliable model assessments for a wide spectrum of real-world applications."
      },
      {
        "id": "oai:arXiv.org:2307.03667v4",
        "title": "Testing the Predictions of Surprisal Theory in 11 Languages",
        "link": "https://arxiv.org/abs/2307.03667",
        "author": "Ethan Gotlieb Wilcox, Tiago Pimentel, Clara Meister, Ryan Cotterell, Roger P. Levy",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2307.03667v4 Announce Type: replace \nAbstract: A fundamental result in psycholinguistics is that less predictable words take a longer time to process. One theoretical explanation for this finding is Surprisal Theory (Hale, 2001; Levy, 2008), which quantifies a word's predictability as its surprisal, i.e. its negative log-probability given a context. While evidence supporting the predictions of Surprisal Theory have been replicated widely, most have focused on a very narrow slice of data: native English speakers reading English texts. Indeed, no comprehensive multilingual analysis exists. We address this gap in the current literature by investigating the relationship between surprisal and reading times in eleven different languages, distributed across five language families. Deriving estimates from language models trained on monolingual and multilingual corpora, we test three predictions associated with surprisal theory: (i) whether surprisal is predictive of reading times; (ii) whether expected surprisal, i.e. contextual entropy, is predictive of reading times; (iii) and whether the linking function between surprisal and reading times is linear. We find that all three predictions are borne out crosslinguistically. By focusing on a more diverse set of languages, we argue that these results offer the most robust link to-date between information theory and incremental language processing across languages."
      },
      {
        "id": "oai:arXiv.org:2307.12909v2",
        "title": "Dyn-E: Local Appearance Editing of Dynamic Neural Radiance Fields",
        "link": "https://arxiv.org/abs/2307.12909",
        "author": "Shangzan Zhang, Sida Peng, Yinji ShenTu, Qing Shuai, Tianrun Chen, Kaicheng Yu, Hujun Bao, Xiaowei Zhou",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2307.12909v2 Announce Type: replace \nAbstract: Recently, the editing of neural radiance fields (NeRFs) has gained considerable attention, but most prior works focus on static scenes while research on the appearance editing of dynamic scenes is relatively lacking. In this paper, we propose a novel framework to edit the local appearance of dynamic NeRFs by manipulating pixels in a single frame of training video. Specifically, to locally edit the appearance of dynamic NeRFs while preserving unedited regions, we introduce a local surface representation of the edited region, which can be inserted into and rendered along with the original NeRF and warped to arbitrary other frames through a learned invertible motion representation network. By employing our method, users without professional expertise can easily add desired content to the appearance of a dynamic scene. We extensively evaluate our approach on various scenes and show that our approach achieves spatially and temporally consistent editing results. Notably, our approach is versatile and applicable to different variants of dynamic NeRF representations."
      },
      {
        "id": "oai:arXiv.org:2308.06985v2",
        "title": "PatchContrast: Self-Supervised Pre-training for 3D Object Detection",
        "link": "https://arxiv.org/abs/2308.06985",
        "author": "Oren Shrout, Ori Nizan, Yizhak Ben-Shabat, Ayellet Tal",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2308.06985v2 Announce Type: replace \nAbstract: Accurately detecting objects in the environment is a key challenge for autonomous vehicles. However, obtaining annotated data for detection is expensive and time-consuming. We introduce PatchContrast, a novel self-supervised point cloud pre-training framework for 3D object detection. We propose to utilize two levels of abstraction to learn discriminative representation from unlabeled data: proposal-level and patch-level. The proposal-level aims at localizing objects in relation to their surroundings, whereas the patch-level adds information about the internal connections between the object's components, hence distinguishing between different objects based on their individual components. We demonstrate how these levels can be integrated into self-supervised pre-training for various backbones to enhance the downstream 3D detection task. We show that our method outperforms existing state-of-the-art models on three commonly-used 3D detection datasets."
      },
      {
        "id": "oai:arXiv.org:2309.15531v4",
        "title": "Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight Quantization of Large Language Models",
        "link": "https://arxiv.org/abs/2309.15531",
        "author": "Jung Hwan Heo, Jeonghoon Kim, Beomseok Kwon, Byeongwook Kim, Se Jung Kwon, Dongsoo Lee",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2309.15531v4 Announce Type: replace \nAbstract: Large Language Models (LLMs) have recently demonstrated remarkable success across various tasks. However, efficiently serving LLMs has been a challenge due to the large memory bottleneck, specifically in small batch inference settings (e.g. mobile devices). Weight-only quantization can be a promising approach, but sub-4 bit quantization remains a challenge due to large-magnitude activation outliers. To mitigate the undesirable outlier effect, we first propose per-IC quantization, a simple yet effective method that creates quantization groups within each input channel (IC) rather than the conventional per-output-channel (per-OC). Our method is motivated by the observation that activation outliers affect the input dimension of the weight matrix, so similarly grouping the weights in the IC direction can isolate outliers within a group. We also find that activation outliers do not dictate quantization difficulty, and inherent weight sensitivities also exist. With per-IC quantization as a new outlier-friendly scheme, we propose Adaptive Dimensions (AdaDim), a versatile quantization framework that can adapt to various weight sensitivity patterns. We demonstrate the effectiveness of AdaDim by augmenting prior methods such as Round-To-Nearest and GPTQ, showing significant improvements across various language modeling benchmarks for both base (up to +4.7% on MMLU) and instruction-tuned (up to +10% on HumanEval) LLMs. Code is available at https://github.com/johnheo/adadim-llm"
      },
      {
        "id": "oai:arXiv.org:2310.05241v2",
        "title": "SCANet: Scene Complexity Aware Network for Weakly-Supervised Video Moment Retrieval",
        "link": "https://arxiv.org/abs/2310.05241",
        "author": "Sunjae Yoon, Gwanhyeong Koo, Dahyun Kim, Chang D. Yoo",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2310.05241v2 Announce Type: replace \nAbstract: Video moment retrieval aims to localize moments in video corresponding to a given language query. To avoid the expensive cost of annotating the temporal moments, weakly-supervised VMR (wsVMR) systems have been studied. For such systems, generating a number of proposals as moment candidates and then selecting the most appropriate proposal has been a popular approach. These proposals are assumed to contain many distinguishable scenes in a video as candidates. However, existing proposals of wsVMR systems do not respect the varying numbers of scenes in each video, where the proposals are heuristically determined irrespective of the video. We argue that the retrieval system should be able to counter the complexities caused by varying numbers of scenes in each video. To this end, we present a novel concept of a retrieval system referred to as Scene Complexity Aware Network (SCANet), which measures the `scene complexity' of multiple scenes in each video and generates adaptive proposals responding to variable complexities of scenes in each video. Experimental results on three retrieval benchmarks (i.e., Charades-STA, ActivityNet, TVR) achieve state-of-the-art performances and demonstrate the effectiveness of incorporating the scene complexity."
      },
      {
        "id": "oai:arXiv.org:2310.13019v5",
        "title": "Tailoring Adversarial Attacks on Deep Neural Networks for Targeted Class Manipulation Using DeepFool Algorithm",
        "link": "https://arxiv.org/abs/2310.13019",
        "author": "S. M. Fazle Rabby Labib, Joyanta Jyoti Mondal, Meem Arafat Manab, Xi Xiao, Sarfaraz Newaz",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2310.13019v5 Announce Type: replace \nAbstract: The susceptibility of deep neural networks (DNNs) to adversarial attacks undermines their reliability across numerous applications, underscoring the necessity for an in-depth exploration of these vulnerabilities and the formulation of robust defense strategies. The DeepFool algorithm by Moosavi-Dezfooli et al. (2016) represents a pivotal step in identifying minimal perturbations required to induce misclassification of input images. Nonetheless, its generic methodology falls short in scenarios necessitating targeted interventions. Additionally, previous research studies have predominantly concentrated on the success rate of attacks without adequately addressing the consequential distortion of images, the maintenance of image quality, or the confidence threshold required for misclassification. To bridge these gaps, we introduce the Enhanced Targeted DeepFool (ET DeepFool) algorithm, an evolution of DeepFool that not only facilitates the specification of desired misclassification targets but also incorporates a configurable minimum confidence score. Our empirical investigations demonstrate the superiority of this refined approach in maintaining the integrity of images and minimizing perturbations across a variety of DNN architectures. Unlike previous iterations, such as the Targeted DeepFool by Gajjar et al. (2022), our method grants unparalleled control over the perturbation process, enabling precise manipulation of model responses. Preliminary outcomes reveal that certain models, including AlexNet and the advanced Vision Transformer, display commendable robustness to such manipulations. This discovery of varying levels of model robustness, as unveiled through our confidence level adjustments, could have far-reaching implications for the field of image recognition. Our code is available at https://github.com/FazleLabib/et_deepfool."
      },
      {
        "id": "oai:arXiv.org:2310.14778v5",
        "title": "Audio-Visual Speaker Tracking: Progress, Challenges, and Future Directions",
        "link": "https://arxiv.org/abs/2310.14778",
        "author": "Jinzheng Zhao, Yong Xu, Xinyuan Qian, Davide Berghi, Peipei Wu, Meng Cui, Jianyuan Sun, Philip J. B. Jackson, Wenwu Wang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2310.14778v5 Announce Type: replace \nAbstract: Audio-visual speaker tracking has drawn increasing attention over the past few years due to its academic values and wide applications. Audio and visual modalities can provide complementary information for localization and tracking. With audio and visual information, the Bayesian-based filter and deep learning-based methods can solve the problem of data association, audio-visual fusion and track management. In this paper, we conduct a comprehensive overview of audio-visual speaker tracking. To our knowledge, this is the first extensive survey over the past five years. We introduce the family of Bayesian filters and summarize the methods for obtaining audio-visual measurements. In addition, the existing trackers and their performance on the AV16.3 dataset are summarized. In the past few years, deep learning techniques have thrived, which also boost the development of audio-visual speaker tracking. The influence of deep learning techniques in terms of measurement extraction and state estimation is also discussed. Finally, we discuss the connections between audio-visual speaker tracking and other areas such as speech separation and distributed speaker tracking."
      },
      {
        "id": "oai:arXiv.org:2311.00990v2",
        "title": "VideoDreamer: Customized Multi-Subject Text-to-Video Generation with Disen-Mix Finetuning on Language-Video Foundation Models",
        "link": "https://arxiv.org/abs/2311.00990",
        "author": "Hong Chen, Xin Wang, Guanning Zeng, Yipeng Zhang, Yuwei Zhou, Feilin Han, Yaofei Wu, Wenwu Zhu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2311.00990v2 Announce Type: replace \nAbstract: Customized text-to-video generation aims to generate text-guided videos with user-given subjects, which has gained increasing attention. However, existing works are primarily limited to single-subject oriented text-to-video generation, leaving the more challenging problem of customized multi-subject generation unexplored. In this paper, we fill this gap and propose a novel VideoDreamer framework, which can generate temporally consistent text-guided videos that faithfully preserve the visual features of the given multiple subjects. Specifically, VideoDreamer adopts the pretrained Stable Diffusion with temporal modules as its base video generator, taking the power of the text-to-image model to generate diversified content. The video generator is further customized for multi-subjects, which leverages the proposed Disen-Mix Finetuning and Human-in-the-Loop Re-finetuning strategy, to tackle the attribute binding problem of multi-subject generation. Additionally, we present a disentangled motion customization strategy to finetune the temporal modules so that we can generate videos with both customized subjects and motions. To evaluate the performance of customized multi-subject text-to-video generation, we introduce the MultiStudioBench benchmark. Extensive experiments demonstrate the remarkable ability of VideoDreamer to generate videos with new content such as new events and backgrounds, tailored to the customized multiple subjects."
      },
      {
        "id": "oai:arXiv.org:2311.09245v2",
        "title": "Affine Invariance in Continuous-Domain Convolutional Neural Networks",
        "link": "https://arxiv.org/abs/2311.09245",
        "author": "Ali Mohaddes, Johannes Lederer",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2311.09245v2 Announce Type: replace \nAbstract: The notion of group invariance helps neural networks in recognizing patterns and features under geometric transformations. Group convolutional neural networks enhance traditional convolutional neural networks by incorporating group-based geometric structures into their design. This research studies affine invariance on continuous-domain convolutional neural networks. Despite other research considering isometric invariance or similarity invariance, we focus on the full structure of affine transforms generated by the group of all invertible $2 \\times 2$ real matrices (generalized linear group $\\mathrm{GL}_2(\\mathbb{R})$). We introduce a new criterion to assess the invariance of two signals under affine transformations. The input image is embedded into the affine Lie group $G_2 = \\mathbb{R}^2 \\ltimes \\mathrm{GL}_2(\\mathbb{R})$ to facilitate group convolution operations that respect affine invariance. Then, we analyze the convolution of embedded signals over $G_2$. In sum, our research could eventually extend the scope of geometrical transformations that usual deep-learning pipelines can handle."
      },
      {
        "id": "oai:arXiv.org:2312.06578v3",
        "title": "Multi-class Support Vector Machine with Maximizing Minimum Margin",
        "link": "https://arxiv.org/abs/2312.06578",
        "author": "Feiping Nie, Zhezheng Hao, Rong Wang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2312.06578v3 Announce Type: replace \nAbstract: Support Vector Machine (SVM) stands out as a prominent machine learning technique widely applied in practical pattern recognition tasks. It achieves binary classification by maximizing the \"margin\", which represents the minimum distance between instances and the decision boundary. Although many efforts have been dedicated to expanding SVM for multi-class case through strategies such as one versus one and one versus the rest, satisfactory solutions remain to be developed. In this paper, we propose a novel method for multi-class SVM that incorporates pairwise class loss considerations and maximizes the minimum margin. Adhering to this concept, we embrace a new formulation that imparts heightened flexibility to multi-class SVM. Furthermore, the correlations between the proposed method and multiple forms of multi-class SVM are analyzed. The proposed regularizer, akin to the concept of \"margin\", can serve as a seamless enhancement over the softmax in deep learning, providing guidance for network parameter learning. Empirical evaluations demonstrate the effectiveness and superiority of our proposed method over existing multi-classification methods.Code is available at https://github.com/zz-haooo/M3SVM."
      },
      {
        "id": "oai:arXiv.org:2312.09736v2",
        "title": "HEAR: Hearing Enhanced Audio Response for Video-grounded Dialogue",
        "link": "https://arxiv.org/abs/2312.09736",
        "author": "Sunjae Yoon, Dahyun Kim, Eunseop Yoon, Hee Suk Yoon, Junyeong Kim, Chnag D. Yoo",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2312.09736v2 Announce Type: replace \nAbstract: Video-grounded Dialogue (VGD) aims to answer questions regarding a given multi-modal input comprising video, audio, and dialogue history. Although there have been numerous efforts in developing VGD systems to improve the quality of their responses, existing systems are competent only to incorporate the information in the video and text and tend to struggle in extracting the necessary information from the audio when generating appropriate responses to the question. The VGD system seems to be deaf, and thus, we coin this symptom of current systems' ignoring audio data as a deaf response. To overcome the deaf response problem, Hearing Enhanced Audio Response (HEAR) framework is proposed to perform sensible listening by selectively attending to audio whenever the question requires it. The HEAR framework enhances the accuracy and audibility of VGD systems in a model-agnostic manner. HEAR is validated on VGD datasets (i.e., AVSD@DSTC7 and AVSD@DSTC8) and shows effectiveness with various VGD systems."
      },
      {
        "id": "oai:arXiv.org:2401.00448v3",
        "title": "Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws",
        "link": "https://arxiv.org/abs/2401.00448",
        "author": "Nikhil Sardana, Jacob Portes, Sasha Doubov, Jonathan Frankle",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2401.00448v3 Announce Type: replace \nAbstract: Large language model (LLM) scaling laws are empirical formulas that estimate changes in model quality as a result of increasing parameter count and training data. However, these formulas, including the popular Deepmind Chinchilla scaling laws, neglect to include the cost of inference. We modify the Chinchilla scaling laws to calculate the optimal LLM parameter count and pre-training data size to train and deploy a model of a given quality and inference demand. We conduct our analysis both in terms of a compute budget and real-world costs and find that LLM researchers expecting reasonably large inference demand (~1B requests) should train models smaller and longer than Chinchilla-optimal. Furthermore, we train 47 models of varying sizes and parameter counts to validate our formula and find that model quality continues to improve as we scale tokens per parameter to extreme ranges (up to 10,000). Finally, we ablate the procedure used to fit the Chinchilla scaling law coefficients and find that developing scaling laws only from data collected at typical token/parameter ratios overestimates the impact of additional tokens at these extreme ranges."
      },
      {
        "id": "oai:arXiv.org:2402.02379v2",
        "title": "Unveiling the Deficiencies of Pre-trained Text-and-Layout Models in Real-world Visually-rich Document Information Extraction",
        "link": "https://arxiv.org/abs/2402.02379",
        "author": "Chong Zhang, Yixi Zhao, Yulu Xie, Chenshu Yuan, Yi Tu, Ya Guo, Mingxu Chai, Ziyu Shen, Yue Zhang, Qi Zhang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2402.02379v2 Announce Type: replace \nAbstract: Recently developed pre-trained text-and-layout models (PTLMs) have shown remarkable success in multiple information extraction tasks on visually-rich documents (VrDs). However, despite achieving extremely high performance on benchmarks, their real-world performance falls short of expectations. Owing to this issue, we investigate the prevailing evaluation pipeline to reveal that: (1) The inadequate annotations within benchmark datasets introduce spurious correlations between task inputs and labels, which would lead to overly-optimistic estimation of model performance. (2) The evaluation solely relies on the performance on benchmarks and is insufficient to comprehensively explore the capabilities of methods in real-world scenarios. These problems impede the prevailing evaluation pipeline from reflecting the real-world performance of methods, misleading the design choices of method optimization. In this work, we introduce EC-FUNSD, an entity-centric dataset crafted for benchmarking information extraction from visually-rich documents. This dataset contains diverse layouts and high-quality annotations. Additionally, this dataset disentangles the falsely-coupled segment and entity annotations that arises from the block-level annotation of FUNSD. Using the proposed dataset, we evaluate the real-world information extraction capabilities of PTLMs from multiple aspects, including their absolute performance, as well as generalization, robustness and fairness. The results indicate that prevalent PTLMs do not perform as well as anticipated in real-world information extraction scenarios. We hope that our study can inspire reflection on the directions of PTLM development."
      },
      {
        "id": "oai:arXiv.org:2402.12280v2",
        "title": "Plato: Plan to Efficiently Decode for Large Language Model Inference",
        "link": "https://arxiv.org/abs/2402.12280",
        "author": "Shuowei Jin, Xueshen Liu, Yongji Wu, Haizhong Zheng, Qingzhao Zhang, Atul Prakash, Matthew Lentz, Danyang Zhuo, Feng Qian, Z. Morley Mao",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2402.12280v2 Announce Type: replace \nAbstract: Large language models (LLMs) have achieved remarkable success in natural language tasks, but their inference incurs substantial computational and memory overhead. To improve efficiency, parallel decoding methods like Skeleton-of-Thought (SoT) decompose prompts into sub-problems for concurrent processing. However, these methods significantly compromise answer quality by treating semantically linked sub-problems as independent. We propose Plato, a novel approach that co-designs algorithms and systems for semantic-aware parallel decoding. Plato leverages LLMs to organize sub-problems into a dependency graph based on logical and causal relationships, enabling concurrent decoding of non-dependent nodes while preserving answer coherence and quality. To further enhance efficiency, Plato pipelines planning and node decoding stages, implements a global context cache, and carefully structures node inference prompts to maximize key-value cache reuse and minimize overhead. Our evaluations show that Plato improves throughput by 68% over autoregressive decoding while achieving a 40% net win rate in answer quality. Compared to SoT, Plato demonstrates a remarkable 90% quality net-win rate. Ablation studies reveal that our pipeline design improves speedup by 29%, while our KV cache reuse optimization reduces overhead by 75%."
      },
      {
        "id": "oai:arXiv.org:2402.14701v3",
        "title": "COMPASS: Computational Mapping of Patient-Therapist Alliance Strategies with Language Modeling",
        "link": "https://arxiv.org/abs/2402.14701",
        "author": "Baihan Lin, Djallel Bouneffouf, Yulia Landa, Rachel Jespersen, Cheryl Corcoran, Guillermo Cecchi",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2402.14701v3 Announce Type: replace \nAbstract: The therapeutic working alliance is a critical predictor of psychotherapy success. Traditionally, working alliance assessment relies on questionnaires completed by both therapists and patients. In this paper, we present COMPASS, a novel framework to directly infer the therapeutic working alliance from the natural language used in psychotherapy sessions. Our approach leverages advanced large language models (LLMs) to analyze session transcripts and map them to distributed representations. These representations capture the semantic similarities between the dialogues and psychometric instruments, such as the Working Alliance Inventory. Analyzing a dataset of over 950 sessions spanning diverse psychiatric conditions -- including anxiety (N=498), depression (N=377), schizophrenia (N=71), and suicidal tendencies (N=12) -- collected between 1970 and 2012, we demonstrate the effectiveness of our method in providing fine-grained mapping of patient-therapist alignment trajectories, offering interpretable insights for clinical practice, and identifying emerging patterns related to the condition being treated. By employing various deep learning-based topic modeling techniques in combination with prompting generative language models, we analyze the topical characteristics of different psychiatric conditions and how these topics evolve during each turn of the conversation. This integrated framework enhances the understanding of therapeutic interactions, enables timely feedback for therapists on the quality of therapeutic relationships, and provides clear, actionable insights to improve the effectiveness of psychotherapy."
      },
      {
        "id": "oai:arXiv.org:2402.17097v3",
        "title": "Re-Ex: Revising after Explanation Reduces the Factual Errors in LLM Responses",
        "link": "https://arxiv.org/abs/2402.17097",
        "author": "Juyeon Kim, Jeongeun Lee, Yoonho Chang, Chanyeol Choi, Junseong Kim, Jy-yong Sohn",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2402.17097v3 Announce Type: replace \nAbstract: Mitigating hallucination issues is a key challenge that must be overcome to reliably deploy large language models (LLMs) in real-world scenarios. Recently, various methods have been proposed to detect and revise factual errors in LLM-generated texts, in order to reduce hallucination. In this paper, we propose Re-Ex, a method for post-editing LLM-generated responses. Re-Ex introduces a novel reasoning step dubbed as the factual error explanation step. Re-Ex revises the initial response of LLMs using 3-steps : first, external tools are used to retrieve the evidences of the factual errors in the initial LLM response; next, LLM is instructed to explain the problematic parts of the response based on the gathered evidence; finally, LLM revises the initial response using the explanations provided in the previous step. In addition to the explanation step, Re-Ex also incorporates new prompting techniques to reduce the token count and inference time required for the response revision process. Compared with existing methods including FacTool, CoVE, and RARR, Re-Ex provides better detection and revision performance with less inference time and fewer tokens in multiple benchmarks."
      },
      {
        "id": "oai:arXiv.org:2403.11116v4",
        "title": "PhD: A ChatGPT-Prompted Visual hallucination Evaluation Dataset",
        "link": "https://arxiv.org/abs/2403.11116",
        "author": "Jiazhen Liu, Yuhan Fu, Ruobing Xie, Runquan Xie, Xingwu Sun, Fengzong Lian, Zhanhui Kang, Xirong Li",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2403.11116v4 Announce Type: replace \nAbstract: Multimodal Large Language Models (MLLMs) hallucinate, resulting in an emerging topic of visual hallucination evaluation (VHE). This paper contributes a ChatGPT-Prompted visual hallucination evaluation Dataset (PhD) for objective VHE at a large scale. The essence of VHE is to ask an MLLM questions about specific images to assess its susceptibility to hallucination. Depending on what to ask (objects, attributes, sentiment, etc.) and how the questions are asked, we structure PhD along two dimensions, i.e. task and mode. Five visual recognition tasks, ranging from low-level (object / attribute recognition) to middle-level (sentiment / position recognition and counting), are considered. Besides a normal visual QA mode, which we term PhD-base, PhD also asks questions with specious context (PhD-sec) or with incorrect context ({PhD-icc), or with AI-generated counter common sense images (PhD-ccs). We construct PhD by a ChatGPT-assisted semi-automated pipeline, encompassing four pivotal modules: task-specific hallucinatory item (hitem) selection, hitem-embedded question generation, specious / incorrect context generation, and counter-common-sense (CCS) image generation. With over 14k daily images, 750 CCS images and 102k VQA triplets in total, PhD reveals considerable variability in MLLMs' performance across various modes and tasks, offering valuable insights into the nature of hallucination. As such, PhD stands as a potent tool not only for VHE but may also play a significant role in the refinement of MLLMs."
      },
      {
        "id": "oai:arXiv.org:2403.12938v2",
        "title": "Learning Neural Differential Algebraic Equations via Operator Splitting",
        "link": "https://arxiv.org/abs/2403.12938",
        "author": "James Koch, Madelyn Shapiro, Himanshu Sharma, Draguna Vrabie, Jan Drgona",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2403.12938v2 Announce Type: replace \nAbstract: Differential-Algebraic Equations (DAEs) describe the temporal evolution of systems that obey both differential and algebraic constraints. Of particular interest are systems that contain implicit relationships between their components, such as conservation relationships. Here, we present an Operator Splitting (OS) numerical integration scheme for learning unknown components of Differential-Algebraic Equations from time-series data. This methodology is built upon the concept of the Universal Differential Equation; that is, a model constructed as a system of Neural Ordinary Differential Equations informed by theory from particular science domains. In this work, we show that the proposed OS-based time-stepping scheme is suitable for relevant system-theoretic data-driven modeling tasks. Presented examples include (i) the inverse problem of tank-manifold dynamics and (ii) discrepancy modeling of a network of pumps, tanks, and pipes. Our experiments demonstrate the proposed method's robustness to noise and extrapolation ability to (i) learn the behaviors of the system components and their interaction physics and (ii) disambiguate between data trends and mechanistic relationships contained in the system."
      },
      {
        "id": "oai:arXiv.org:2403.13843v2",
        "title": "Machine Learning and Transformers for Thyroid Carcinoma Diagnosis: A Review",
        "link": "https://arxiv.org/abs/2403.13843",
        "author": "Yassine Habchi, Hamza Kheddar, Yassine Himeur, Mohamed Chahine Ghanem",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2403.13843v2 Announce Type: replace \nAbstract: The growing interest in developing smart diagnostic systems to help medical experts process extensive data for treating incurable diseases has been notable. In particular, the challenge of identifying thyroid cancer (TC) has seen progress with the use of machine learning (ML) and big data analysis, incorporating Transformers to evaluate TC prognosis and determine the risk of malignancy in individuals. This review article presents a summary of various studies on AI-based approaches, especially those employing Transformers, for diagnosing TC. It introduces a new categorization system for these methods based on artificial intelligence (AI) algorithms, the goals of the framework, and the computing environments used. Additionally, it scrutinizes and contrasts the available TC datasets by their features. The paper highlights the importance of AI instruments in aiding the diagnosis and treatment of TC through supervised, unsupervised, or mixed approaches, with a special focus on the ongoing importance of Transformers and large language models (LLMs) in medical diagnostics and disease management. It further discusses the progress made and the continuing obstacles in this area. Lastly, it explores future directions and focuses within this research field."
      },
      {
        "id": "oai:arXiv.org:2403.15449v3",
        "title": "Hatred Stems from Ignorance! Distillation of the Persuasion Modes in Countering Conversational Hate Speech",
        "link": "https://arxiv.org/abs/2403.15449",
        "author": "Ghadi Alyahya, Abeer Aldayel",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2403.15449v3 Announce Type: replace \nAbstract: Examining the factors that the counterspeech uses are at the core of understanding the optimal methods for confronting hate speech online. Various studies have assessed the emotional base factors used in counter speech, such as emotional empathy, offensiveness, and hostility. To better understand the counterspeech used in conversations, this study distills persuasion modes into reason, emotion, and credibility and evaluates their use in two types of conversation interactions: closed (multi-turn) and open (single-turn) concerning racism, sexism, and religious bigotry. The evaluation covers the distinct behaviors seen with human-sourced as opposed to machine-generated counterspeech. It also assesses the interplay between the stance taken and the mode of persuasion seen in the counterspeech.\n  Notably, we observe nuanced differences in the counterspeech persuasion modes used in open and closed interactions, especially in terms of the topic, with a general tendency to use reason as a persuasion mode to express the counterpoint to hate comments. The machine-generated counterspeech tends to exhibit an emotional persuasion mode, while human counters lean toward reason. Furthermore, our study shows that reason tends to obtain more supportive replies than other persuasion modes. The findings highlight the potential for incorporating persuasion modes into studies about countering hate speech, as they can serve as an optimal means of explainability and pave the way for the further adoption of the reply's stance and the role it plays in assessing what comprises the optimal counterspeech."
      },
      {
        "id": "oai:arXiv.org:2403.16450v3",
        "title": "Camera-aware Label Refinement for Unsupervised Person Re-identification",
        "link": "https://arxiv.org/abs/2403.16450",
        "author": "Pengna Li, Kangyi Wu, Wenli Huang, Sanping Zhou, Jinjun Wang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2403.16450v3 Announce Type: replace \nAbstract: Unsupervised person re-identification aims to retrieve images of a specified person without identity labels. Many recent unsupervised Re-ID approaches adopt clustering-based methods to measure cross-camera feature similarity to roughly divide images into clusters. They ignore the feature distribution discrepancy induced by camera domain gap, resulting in the unavoidable performance degradation. Camera information is usually available, and the feature distribution in the single camera usually focuses more on the appearance of the individual and has less intra-identity variance. Inspired by the observation, we introduce a \\textbf{C}amera-\\textbf{A}ware \\textbf{L}abel \\textbf{R}efinement~(CALR) framework that reduces camera discrepancy by clustering intra-camera similarity. Specifically, we employ intra-camera training to obtain reliable local pseudo labels within each camera, and then refine global labels generated by inter-camera clustering and train the discriminative model using more reliable global pseudo labels in a self-paced manner. Meanwhile, we develop a camera-alignment module to align feature distributions under different cameras, which could help deal with the camera variance further. Extensive experiments validate the superiority of our proposed method over state-of-the-art approaches. The code is accessible at https://github.com/leeBooMla/CALR."
      },
      {
        "id": "oai:arXiv.org:2403.18191v5",
        "title": "Measuring changes in polarisation using Singular Value Decomposition of network graphs",
        "link": "https://arxiv.org/abs/2403.18191",
        "author": "Sage Anastasi, Giulio Dalla Riva",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2403.18191v5 Announce Type: replace \nAbstract: In this paper we present new methods of measuring polarisation in social networks. We use Random Dot Product Graphs to embed social networks in metric spaces. Singular Value Decomposition of this social network then provider an embedded dimensionality which corresponds to the number of uncorrelated dimensions in the network. A decrease in the optimal dimensionality for the embedding of the network graph means that the dimensions in the network are becoming more correlated, and therefore the network is becoming more polarised.\n  We demonstrate this method by analysing social networks such as communication interactions among New Zealand Twitter users discussing climate change issues and international social media discussions of the COP conferences. In both cases, the decreasing embedded dimensionality indicates that these networks have become more polarised over time. We also use networks generated by stochastic block models to explore how an increase of the isolation between distinct communities, or the increase of the predominance of one community over the other, in the social networks decrease the embedded dimensionality and are therefore identifiable as polarisation processes."
      },
      {
        "id": "oai:arXiv.org:2404.00466v2",
        "title": "Computation and Communication Efficient Lightweighting Vertical Federated Learning for Smart Building IoT",
        "link": "https://arxiv.org/abs/2404.00466",
        "author": "Heqiang Wang, Xiang Liu, Yucheng Liu, Jia Zhou, Weihong Yang, Xiaoxiong Zhong",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2404.00466v2 Announce Type: replace \nAbstract: With the increasing number and enhanced capabilities of IoT devices in smart buildings, these devices are evolving beyond basic data collection and control to actively participate in deep learning tasks. Federated Learning (FL), as a decentralized learning paradigm, is well-suited for such scenarios. However, the limited computational and communication resources of IoT devices present significant challenges. While existing research has extensively explored efficiency improvements in Horizontal FL, these techniques cannot be directly applied to Vertical FL due to fundamental differences in data partitioning and model structure. To address this gap, we propose a Lightweight Vertical Federated Learning (LVFL) framework that jointly optimizes computational and communication efficiency. Our approach introduces two distinct lightweighting strategies: one for reducing the complexity of the feature model to improve local computation, and another for compressing feature embeddings to reduce communication overhead. Furthermore, we derive a convergence bound for the proposed LVFL algorithm that explicitly incorporates both computation and communication lightweighting ratios. Experimental results on an image classification task demonstrate that LVFL effectively mitigates resource demands while maintaining competitive learning performance."
      },
      {
        "id": "oai:arXiv.org:2404.07594v3",
        "title": "Weakly-Supervised Learning via Multi-Lateral Decoder Branching for Tool Segmentation in Robot-Assisted Cardiovascular Catheterization",
        "link": "https://arxiv.org/abs/2404.07594",
        "author": "Olatunji Mumini Omisore, Toluwanimi Akinyemi, Anh Nguyen, Lei Wang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2404.07594v3 Announce Type: replace \nAbstract: Robot-assisted catheterization has garnered a good attention for its potentials in treating cardiovascular diseases. However, advancing surgeon-robot collaboration still requires further research, particularly on task-specific automation. For instance, automated tool segmentation can assist surgeons in visualizing and tracking of endovascular tools during cardiac procedures. While learning-based models have demonstrated state-of-the-art segmentation performances, generating ground-truth labels for fully-supervised methods is both labor-intensive time consuming, and costly. In this study, we propose a weakly-supervised learning method with multi-lateral pseudo labeling for tool segmentation in cardiovascular angiogram datasets. The method utilizes a modified U-Net architecture featuring one encoder and multiple laterally branched decoders. The decoders generate diverse pseudo labels under different perturbations, augmenting available partial labels. The pseudo labels are self-generated using a mixed loss function with shared consistency across the decoders. The weakly-supervised model was trained end-to-end and validated using partially annotated angiogram data from three cardiovascular catheterization procedures. Validation results show that the model could perform closer to fully-supervised models. Also, the proposed weakly-supervised multi-lateral method outperforms three well known methods used for weakly-supervised learning, offering the highest segmentation performance across the three angiogram datasets. Furthermore, numerous ablation studies confirmed the model's consistent performance under different parameters. Finally, the model was applied for tool segmentation in a robot-assisted catheterization experiments. The model enhanced visualization with high connectivity indices for guidewire and catheter, and a mean processing time of 35 ms per frame."
      },
      {
        "id": "oai:arXiv.org:2404.13984v2",
        "title": "RHanDS: Refining Malformed Hands for Generated Images with Decoupled Structure and Style Guidance",
        "link": "https://arxiv.org/abs/2404.13984",
        "author": "Chengrui Wang, Pengfei Liu, Min Zhou, Ming Zeng, Xubin Li, Tiezheng Ge, Bo zheng",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2404.13984v2 Announce Type: replace \nAbstract: Although diffusion models can generate high-quality human images, their applications are limited by the instability in generating hands with correct structures. In this paper, we introduce RHanDS, a conditional diffusion-based framework designed to refine malformed hands by utilizing decoupled structure and style guidance. The hand mesh reconstructed from the malformed hand offers structure guidance for correcting the structure of the hand, while the malformed hand itself provides style guidance for preserving the style of the hand. To alleviate the mutual interference between style and structure guidance, we introduce a two-stage training strategy and build a series of multi-style hand datasets. In the first stage, we use paired hand images for training to ensure stylistic consistency in hand refining. In the second stage, various hand images generated based on human meshes are used for training, enabling the model to gain control over the hand structure. Experimental results demonstrate that RHanDS can effectively refine hand structure while preserving consistency in hand style."
      },
      {
        "id": "oai:arXiv.org:2404.17569v3",
        "title": "MaPa: Text-driven Photorealistic Material Painting for 3D Shapes",
        "link": "https://arxiv.org/abs/2404.17569",
        "author": "Shangzan Zhang, Sida Peng, Tao Xu, Yuanbo Yang, Tianrun Chen, Nan Xue, Yujun Shen, Hujun Bao, Ruizhen Hu, Xiaowei Zhou",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2404.17569v3 Announce Type: replace \nAbstract: This paper aims to generate materials for 3D meshes from text descriptions. Unlike existing methods that synthesize texture maps, we propose to generate segment-wise procedural material graphs as the appearance representation, which supports high-quality rendering and provides substantial flexibility in editing. Instead of relying on extensive paired data, i.e., 3D meshes with material graphs and corresponding text descriptions, to train a material graph generative model, we propose to leverage the pre-trained 2D diffusion model as a bridge to connect the text and material graphs. Specifically, our approach decomposes a shape into a set of segments and designs a segment-controlled diffusion model to synthesize 2D images that are aligned with mesh parts. Based on generated images, we initialize parameters of material graphs and fine-tune them through the differentiable rendering module to produce materials in accordance with the textual description. Extensive experiments demonstrate the superior performance of our framework in photorealism, resolution, and editability over existing methods. Project page: https://zju3dv.github.io/MaPa"
      },
      {
        "id": "oai:arXiv.org:2404.17662v4",
        "title": "PLAYER*: Enhancing LLM-based Multi-Agent Communication and Interaction in Murder Mystery Games",
        "link": "https://arxiv.org/abs/2404.17662",
        "author": "Qinglin Zhu, Runcong Zhao, Bin Liang, Jinhua Du, Lin Gui, Yulan He",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2404.17662v4 Announce Type: replace \nAbstract: We present PLAYER*, a novel framework for Large Language Model (LLM)-based agents in Murder Mystery Games (MMGs). MMGs pose unique challenges, including undefined state spaces, absent intermediate rewards, and the need for strategic interaction in a continuous language domain. PLAYER* addresses these complexities through a sensor-based representation of agent states, a question-targeting mechanism guided by information gain, and a pruning strategy to refine suspect lists and enhance decision-making efficiency. To enable systematic evaluation, we propose WellPlay, a dataset comprising 1,482 inferential questions across 12 games, categorized into objectives, reasoning, and relationships. Experiments demonstrate PLAYER*'s capacity to achieve superior performance in reasoning accuracy and efficiency compared to existing approaches, while also significantly improving the quality of agent-human interactions in MMGs. This study advances the development of reasoning agents for complex social and interactive scenarios."
      },
      {
        "id": "oai:arXiv.org:2404.18394v3",
        "title": "Reconstructing Satellites in 3D from Amateur Telescope Images",
        "link": "https://arxiv.org/abs/2404.18394",
        "author": "Zhiming Chang, Boyang Liu, Yifei Xia, Youming Guo, Boxin Shi, He Sun",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2404.18394v3 Announce Type: replace \nAbstract: Monitoring space objects is crucial for space situational awareness, yet reconstructing 3D satellite models from ground-based telescope images is challenging due to atmospheric turbulence, long observation distances, limited viewpoints, and low signal-to-noise ratios. In this paper, we propose a novel computational imaging framework that overcomes these obstacles by integrating a hybrid image pre-processing pipeline with a joint pose estimation and 3D reconstruction module based on controlled Gaussian Splatting (GS) and Branch-and-Bound (BnB) search. We validate our approach on both synthetic satellite datasets and on-sky observations of China's Tiangong Space Station and the International Space Station, achieving robust 3D reconstructions of low-Earth orbit satellites from ground-based data. Quantitative evaluations using SSIM, PSNR, LPIPS, and Chamfer Distance demonstrate that our method outperforms state-of-the-art NeRF-based approaches, and ablation studies confirm the critical role of each component. Our framework enables high-fidelity 3D satellite monitoring from Earth, offering a cost-effective alternative for space situational awareness. Project page: https://ai4scientificimaging.org/ReconstructingSatellites"
      },
      {
        "id": "oai:arXiv.org:2404.19656v2",
        "title": "Towards Scenario- and Capability-Driven Dataset Development and Evaluation: An Approach in the Context of Mapless Automated Driving",
        "link": "https://arxiv.org/abs/2404.19656",
        "author": "Felix Gr\\\"un, Marcus Nolte, Markus Maurer",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2404.19656v2 Announce Type: replace \nAbstract: The foundational role of datasets in defining the capabilities of deep learning models has led to their rapid proliferation. At the same time, published research focusing on the process of dataset development for environment perception in automated driving has been scarce, thereby reducing the applicability of openly available datasets and impeding the development of effective environment perception systems. Sensor-based, mapless automated driving is one of the contexts where this limitation is evident. While leveraging real-time sensor data, instead of pre-defined HD maps promises enhanced adaptability and safety by effectively navigating unexpected environmental changes, it also increases the demands on the scope and complexity of the information provided by the perception system.\n  To address these challenges, we propose a scenario- and capability-based approach to dataset development. Grounded in the principles of ISO 21448 (safety of the intended functionality, SOTIF), extended by ISO/TR 4804, our approach facilitates the structured derivation of dataset requirements. This not only aids in the development of meaningful new datasets but also enables the effective comparison of existing ones. Applying this methodology to a broad range of existing lane detection datasets, we identify significant limitations in current datasets, particularly in terms of real-world applicability, a lack of labeling of critical features, and an absence of comprehensive information for complex driving maneuvers."
      },
      {
        "id": "oai:arXiv.org:2405.01614v3",
        "title": "RULSurv: A probabilistic survival-based method for early censoring-aware prediction of remaining useful life in ball bearings",
        "link": "https://arxiv.org/abs/2405.01614",
        "author": "Christian Marius Lillelund, Fernando Pannullo, Morten Opprud Jakobsen, Manuel Morante, Christian Fischer Pedersen",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2405.01614v3 Announce Type: replace \nAbstract: Predicting the remaining useful life (RUL) of ball bearings is an active area of research, where novel machine learning techniques are continuously being applied to predict degradation trends and anticipate failures before they occur. However, few studies have explicitly addressed the challenge of handling censored data, where information about a specific event (\\eg mechanical failure) is incomplete or only partially observed. To address this issue, we introduce a novel and flexible method for early fault detection using Kullback-Leibler (KL) divergence and RUL estimation using survival analysis that naturally supports censored data. We demonstrate our approach in the XJTU-SY dataset using a 5-fold cross-validation strategy across three different operating conditions. When predicting the time to failure for bearings under the highest load (C1, 12.0 kN and 2100 RPM) with 25% random censoring, our approach achieves a mean absolute error (MAE) of 14.7 minutes (95% CI = 13.6-15.8) using a linear CoxPH model, and an MAE of 12.6 minutes (95% CI = 11.8-13.4) using a nonlinear Random Survival Forests model, compared to an MAE of 18.5 minutes (95% CI = 17.4-19.6) using a linear LASSO model that does not support censoring. Moreover, our approach achieves a mean cumulative relative accuracy (CRA) of 0.7586 over 5 bearings under the highest load, which improves over several state-of-the-art baselines. Our work highlights the importance of considering censored data as part of the model design when building predictive models for early fault detection and RUL estimation."
      },
      {
        "id": "oai:arXiv.org:2405.13637v5",
        "title": "Curriculum Direct Preference Optimization for Diffusion and Consistency Models",
        "link": "https://arxiv.org/abs/2405.13637",
        "author": "Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, Nicu Sebe, Mubarak Shah",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2405.13637v5 Announce Type: replace \nAbstract: Direct Preference Optimization (DPO) has been proposed as an effective and efficient alternative to reinforcement learning from human feedback (RLHF). In this paper, we propose a novel and enhanced version of DPO based on curriculum learning for text-to-image generation. Our method is divided into two training stages. First, a ranking of the examples generated for each prompt is obtained by employing a reward model. Then, increasingly difficult pairs of examples are sampled and provided to a text-to-image generative (diffusion or consistency) model. Generated samples that are far apart in the ranking are considered to form easy pairs, while those that are close in the ranking form hard pairs. In other words, we use the rank difference between samples as a measure of difficulty. The sampled pairs are split into batches according to their difficulty levels, which are gradually used to train the generative model. Our approach, Curriculum DPO, is compared against state-of-the-art fine-tuning approaches on nine benchmarks, outperforming the competing methods in terms of text alignment, aesthetics and human preference. Our code is available at https://github.com/CroitoruAlin/Curriculum-DPO."
      },
      {
        "id": "oai:arXiv.org:2405.13685v2",
        "title": "Financial Models in Generative Art: Black-Scholes-Inspired Concept Blending in Text-to-Image Diffusion",
        "link": "https://arxiv.org/abs/2405.13685",
        "author": "Divya Kothandaraman, Ming Lin, Dinesh Manocha",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2405.13685v2 Announce Type: replace \nAbstract: We introduce a novel approach for concept blending in pretrained text-to-image diffusion models, aiming to generate images at the intersection of multiple text prompts. At each time step during diffusion denoising, our algorithm forecasts predictions w.r.t. the generated image and makes informed text conditioning decisions. Central to our method is the unique analogy between diffusion models, which are rooted in non-equilibrium thermodynamics, and the Black-Scholes model for financial option pricing. By drawing parallels between key variables in both domains, we derive a robust algorithm for concept blending that capitalizes on the Markovian dynamics of the Black-Scholes framework. Our text-based concept blending algorithm is data-efficient, meaning it does not need additional training. Furthermore, it operates without human intervention or hyperparameter tuning. We highlight the benefits of our approach by comparing it qualitatively and quantitatively to other text based concept blending techniques, including linear interpolation, alternating prompts, step-wise prompt switching, and CLIP-guided prompt selection across various scenarios such as single object per text prompt, multiple objects per text prompt and objects against backgrounds. Our work shows that financially inspired techniques can enhance text-to-image concept blending in generative AI, paving the way for broader innovation. Code is available at https://github.com/divyakraman/BlackScholesDiffusion2024."
      },
      {
        "id": "oai:arXiv.org:2405.13721v2",
        "title": "Connectivity Shapes Implicit Regularization in Matrix Factorization Models for Matrix Completion",
        "link": "https://arxiv.org/abs/2405.13721",
        "author": "Zhiwei Bai, Jiajie Zhao, Yaoyu Zhang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2405.13721v2 Announce Type: replace \nAbstract: Matrix factorization models have been extensively studied as a valuable test-bed for understanding the implicit biases of overparameterized models. Although both low nuclear norm and low rank regularization have been studied for these models, a unified understanding of when, how, and why they achieve different implicit regularization effects remains elusive. In this work, we systematically investigate the implicit regularization of matrix factorization for solving matrix completion problems. We empirically discover that the connectivity of observed data plays a crucial role in the implicit bias, with a transition from low nuclear norm to low rank as data shifts from disconnected to connected with increased observations. We identify a hierarchy of intrinsic invariant manifolds in the loss landscape that guide the training trajectory to evolve from low-rank to higher-rank solutions. Based on this finding, we theoretically characterize the training trajectory as following the hierarchical invariant manifold traversal process, generalizing the characterization of Li et al. (2020) to include the disconnected case. Furthermore, we establish conditions that guarantee minimum nuclear norm, closely aligning with our experimental findings, and we provide a dynamics characterization condition for ensuring minimum rank. Our work reveals the intricate interplay between data connectivity, training dynamics, and implicit regularization in matrix factorization models."
      },
      {
        "id": "oai:arXiv.org:2405.15540v2",
        "title": "Bundle Neural Networks for message diffusion on graphs",
        "link": "https://arxiv.org/abs/2405.15540",
        "author": "Jacob Bamberger, Federico Barbero, Xiaowen Dong, Michael M. Bronstein",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2405.15540v2 Announce Type: replace \nAbstract: The dominant paradigm for learning on graph-structured data is message passing. Despite being a strong inductive bias, the local message passing mechanism suffers from pathological issues such as over-smoothing, over-squashing, and limited node-level expressivity. To address these limitations we propose Bundle Neural Networks (BuNN), a new type of GNN that operates via message diffusion over flat vector bundles - structures analogous to connections on Riemannian manifolds that augment the graph by assigning to each node a vector space and an orthogonal map. A BuNN layer evolves the features according to a diffusion-type partial differential equation. When discretized, BuNNs are a special case of Sheaf Neural Networks (SNNs), a recently proposed MPNN capable of mitigating over-smoothing. The continuous nature of message diffusion enables BuNNs to operate on larger scales of the graph and, therefore, to mitigate over-squashing. Finally, we prove that BuNN can approximate any feature transformation over nodes on any (potentially infinite) family of graphs given injective positional encodings, resulting in universal node-level expressivity. We support our theory via synthetic experiments and showcase the strong empirical performance of BuNNs over a range of real-world tasks, achieving state-of-the-art results on several standard benchmarks in transductive and inductive settings."
      },
      {
        "id": "oai:arXiv.org:2405.16213v2",
        "title": "Learning Visual-Semantic Subspace Representations",
        "link": "https://arxiv.org/abs/2405.16213",
        "author": "Gabriel Moreira, Manuel Marques, Jo\\~ao Paulo Costeira, Alexander Hauptmann",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2405.16213v2 Announce Type: replace \nAbstract: Learning image representations that capture rich semantic relationships remains a significant challenge. Existing approaches are either contrastive, lacking robust theoretical guarantees, or struggle to effectively represent the partial orders inherent to structured visual-semantic data. In this paper, we introduce a nuclear norm-based loss function, grounded in the same information theoretic principles that have proved effective in self-supervised learning. We present a theoretical characterization of this loss, demonstrating that, in addition to promoting class orthogonality, it encodes the spectral geometry of the data within a subspace lattice. This geometric representation allows us to associate logical propositions with subspaces, ensuring that our learned representations adhere to a predefined symbolic structure."
      },
      {
        "id": "oai:arXiv.org:2405.16519v3",
        "title": "Fourier Sliced-Wasserstein Embedding for Multisets and Measures",
        "link": "https://arxiv.org/abs/2405.16519",
        "author": "Tal Amir, Nadav Dym",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2405.16519v3 Announce Type: replace \nAbstract: We present the Fourier Sliced-Wasserstein (FSW) embedding - a novel method to embed multisets and measures over R^d into Euclidean space.\n  Our proposed embedding approximately preserves the sliced Wasserstein distance on distributions, thereby yielding geometrically meaningful representations that better capture the structure of the input. Moreover, it is injective on measures and bi-Lipschitz on multisets - a significant advantage over prevalent methods based on sum- or max-pooling, which are provably not bi-Lipschitz, and, in many cases, not even injective. The required output dimension for these guarantees is near-optimal: roughly 2Nd, where N is the maximal input multiset size.\n  Furthermore, we prove that it is impossible to embed distributions over R^d into Euclidean space in a bi-Lipschitz manner. Thus, the metric properties of our embedding are, in a sense, the best possible.\n  Through numerical experiments, we demonstrate that our method yields superior multiset representations that improve performance in practical learning tasks. Specifically, we show that (a) a simple combination of the FSW embedding with an MLP achieves state-of-the-art performance in learning the (non-sliced) Wasserstein distance; and (b) replacing max-pooling with the FSW embedding makes PointNet significantly more robust to parameter reduction, with only minor performance degradation even after a 40-fold reduction."
      },
      {
        "id": "oai:arXiv.org:2405.16555v2",
        "title": "Building Vision Models upon Heat Conduction",
        "link": "https://arxiv.org/abs/2405.16555",
        "author": "Zhaozhi Wang, Yue Liu, Yunjie Tian, Yunfan Liu, Yaowei Wang, Qixiang Ye",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2405.16555v2 Announce Type: replace \nAbstract: Visual representation models leveraging attention mechanisms are challenged by significant computational overhead, particularly when pursuing large receptive fields. In this study, we aim to mitigate this challenge by introducing the Heat Conduction Operator (HCO) built upon the physical heat conduction principle. HCO conceptualizes image patches as heat sources and models their correlations through adaptive thermal energy diffusion, enabling robust visual representations. HCO enjoys a computational complexity of O(N^1.5), as it can be implemented using discrete cosine transformation (DCT) operations. HCO is plug-and-play, combining with deep learning backbones produces visual representation models (termed vHeat) with global receptive fields. Experiments across vision tasks demonstrate that, beyond the stronger performance, vHeat achieves up to a 3x throughput, 80% less GPU memory allocation, and 35% fewer computational FLOPs compared to the Swin-Transformer. Code is available at https://github.com/MzeroMiko/vHeat."
      },
      {
        "id": "oai:arXiv.org:2405.20179v3",
        "title": "Robo-Instruct: Simulator-Augmented Instruction Alignment For Finetuning Code LLMs",
        "link": "https://arxiv.org/abs/2405.20179",
        "author": "Zichao Hu, Junyi Jessy Li, Arjun Guha, Joydeep Biswas",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2405.20179v3 Announce Type: replace \nAbstract: Code LLMs have shown promising results with converting tasks in natural language to programs that can be executed by service robots. We are interested in finetuning small, specialized LLMs for this purpose, but collecting datasets of task-program pairs specific to each robot is time-consuming and expensive. While approaches such as SELF-INSTRUCT and EVOL-INSTRUCT are capable of generating novel tasks given a few examples, they are unable to provide the corresponding programs that correctly abide by physical-world and robot-constraints using the provided programming interface. Using a simulator is a natural potential solution to checking for such constraints, but building simulation environments that can handle arbitrary tasks and their necessary objects and locations, is challenging. To address these challenges, we introduce ROBO-INSTRUCT, which synthesizes task-specific simulation environments on the fly during program execution, by opportunistically inferring entity properties and enforcing corresponding constraints based on how the entities are used in the task program. Additionally, ROBO-INSTRUCT integrates an LLM-aided post-processing procedure to refine instructions for better alignment with robot programs. We demonstrate the effectiveness of ROBO-INSTRUCT across multiple LLMs, showing that our fine-tuned models outperform all baseline methods and even match or surpass the performance of several larger and proprietary models."
      },
      {
        "id": "oai:arXiv.org:2406.00104v2",
        "title": "Scalable Bayesian Learning with posteriors",
        "link": "https://arxiv.org/abs/2406.00104",
        "author": "Samuel Duffield, Kaelan Donatella, Johnathan Chiu, Phoebe Klett, Daniel Simpson",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2406.00104v2 Announce Type: replace \nAbstract: Although theoretically compelling, Bayesian learning with modern machine learning models is computationally challenging since it requires approximating a high dimensional posterior distribution. In this work, we (i) introduce posteriors, an easily extensible PyTorch library hosting general-purpose implementations making Bayesian learning accessible and scalable to large data and parameter regimes; (ii) present a tempered framing of stochastic gradient Markov chain Monte Carlo, as implemented in posteriors, that transitions seamlessly into optimization and unveils a minor modification to deep ensembles to ensure they are asymptotically unbiased for the Bayesian posterior, and (iii) demonstrate and compare the utility of Bayesian approximations through experiments including an investigation into the cold posterior effect and applications with large language models."
      },
      {
        "id": "oai:arXiv.org:2406.01660v4",
        "title": "Self-Improving Robust Preference Optimization",
        "link": "https://arxiv.org/abs/2406.01660",
        "author": "Eugene Choi, Arash Ahmadian, Matthieu Geist, Oilvier Pietquin, Mohammad Gheshlaghi Azar",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2406.01660v4 Announce Type: replace \nAbstract: Online and offline RLHF methods, such as PPO and DPO, have been highly successful in aligning AI with human preferences. Despite their success, however, these methods suffer from fundamental limitations: (a) Models trained with RLHF can learn from mistakes or negative examples through RL mechanism or contrastive loss during training. However, at inference time, they lack an innate self-improvement mechanism for error corrections. (b) The optimal solution of existing methods is highly task-dependent, making it difficult for them to generalize to new tasks. To address these challenges, we propose Self-Improving Robust Preference Optimization (SRPO), a practical and mathematically principled offline RLHF framework. The key idea behind SRPO is to cast the problem of learning from human preferences as a self-improvement process, mathematically formulated as a min-max objective that jointly optimizes a self-improvement policy and a generative policy in an adversarial fashion. Crucially, the solution for this optimization problem is independent of the training task, which makes it robust to its changes. We then show that this objective can be reformulated as a non-adversarial offline loss, which can be efficiently optimized using standard supervised learning techniques at scale. To demonstrate SRPO's effectiveness, we evaluate it using AI Win-Rate (WR) against human (GOLD) completions. When tested on the XSum dataset, SRPO outperforms DPO by a margin of 15% after 5 self revisions, achieving an impressive 90% WR. Moreover, on the challenging Arena-Hard prompts, SRPO outperforms both DPO and IPO (by 4% without revision and 6% after a single revision), reaching a 56% WR against against Llama-3.1-8B-Instruct."
      },
      {
        "id": "oai:arXiv.org:2406.01867v4",
        "title": "MoLA: Motion Generation and Editing with Latent Diffusion Enhanced by Adversarial Training",
        "link": "https://arxiv.org/abs/2406.01867",
        "author": "Kengo Uchida, Takashi Shibuya, Yuhta Takida, Naoki Murata, Julian Tanke, Shusuke Takahashi, Yuki Mitsufuji",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2406.01867v4 Announce Type: replace \nAbstract: In text-to-motion generation, controllability as well as generation quality and speed has become increasingly critical. The controllability challenges include generating a motion of a length that matches the given textual description and editing the generated motions according to control signals, such as the start-end positions and the pelvis trajectory. In this paper, we propose MoLA, which provides fast, high-quality, variable-length motion generation and can also deal with multiple editing tasks in a single framework. Our approach revisits the motion representation used as inputs and outputs in the model, incorporating an activation variable to enable variable-length motion generation. Additionally, we integrate a variational autoencoder and a latent diffusion model, further enhanced through adversarial training, to achieve high-quality and fast generation. Moreover, we apply a training-free guided generation framework to achieve various editing tasks with motion control inputs. We quantitatively show the effectiveness of adversarial learning in text-to-motion generation, and demonstrate the applicability of our editing framework to multiple editing tasks in the motion domain."
      },
      {
        "id": "oai:arXiv.org:2406.04844v2",
        "title": "Multi-Granularity Language-Guided Training for Multi-Object Tracking",
        "link": "https://arxiv.org/abs/2406.04844",
        "author": "Yuhao Li, Jiale Cao, Muzammal Naseer, Yu Zhu, Jinqiu Sun, Yanning Zhang, Fahad Shahbaz Khan",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2406.04844v2 Announce Type: replace \nAbstract: Most existing multi-object tracking methods typically learn visual tracking features via maximizing dis-similarities of different instances and minimizing similarities of the same instance. While such a feature learning scheme achieves promising performance, learning discriminative features solely based on visual information is challenging especially in case of environmental interference such as occlusion, blur and domain variance. In this work, we argue that multi-modal language-driven features provide complementary information to classical visual features, thereby aiding in improving the robustness to such environmental interference. To this end, we propose a new multi-object tracking framework, named LG-MOT, that explicitly leverages language information at different levels of granularity (scene-and instance-level) and combines it with standard visual features to obtain discriminative representations. To develop LG-MOT, we annotate existing MOT datasets with scene-and instance-level language descriptions. We then encode both instance-and scene-level language information into high-dimensional embeddings, which are utilized to guide the visual features during training. At inference, our LG-MOT uses the standard visual features without relying on annotated language descriptions. Extensive experiments on three benchmarks, MOT17, DanceTrack and SportsMOT, reveal the merits of the proposed contributions leading to state-of-the-art performance. On the DanceTrack test set, our LG-MOT achieves an absolute gain of 2.2\\% in terms of target object association (IDF1 score), compared to the baseline using only visual features. Further, our LG-MOT exhibits strong cross-domain generalizability. The dataset and code will be available at https://github.com/WesLee88524/LG-MOT."
      },
      {
        "id": "oai:arXiv.org:2406.06044v2",
        "title": "FRAG: Frequency Adapting Group for Diffusion Video Editing",
        "link": "https://arxiv.org/abs/2406.06044",
        "author": "Sunjae Yoon, Gwanhyeong Koo, Geonwoo Kim, Chang D. Yoo",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2406.06044v2 Announce Type: replace \nAbstract: In video editing, the hallmark of a quality edit lies in its consistent and unobtrusive adjustment. Modification, when integrated, must be smooth and subtle, preserving the natural flow and aligning seamlessly with the original vision. Therefore, our primary focus is on overcoming the current challenges in high quality edit to ensure that each edit enhances the final product without disrupting its intended essence. However, quality deterioration such as blurring and flickering is routinely observed in recent diffusion video editing systems. We confirm that this deterioration often stems from high-frequency leak: the diffusion model fails to accurately synthesize high-frequency components during denoising process. To this end, we devise Frequency Adapting Group (FRAG) which enhances the video quality in terms of consistency and fidelity by introducing a novel receptive field branch to preserve high-frequency components during the denoising process. FRAG is performed in a model-agnostic manner without additional training and validates the effectiveness on video editing benchmarks (i.e., TGVE, DAVIS)."
      },
      {
        "id": "oai:arXiv.org:2406.10369v3",
        "title": "On the Preservation of Input/Output Directed Graph Informativeness under Crossover",
        "link": "https://arxiv.org/abs/2406.10369",
        "author": "Andreas Duus Pape, J. David Schaffer, Hiroki Sayama, Christopher Zosh",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2406.10369v3 Announce Type: replace \nAbstract: There is a broad class of networks which connect inputs to outputs. We provide a strong theoretical foundation for crossover across this class and connect it to informativeness, a measure of the connectedness of inputs to outputs. We define Input/Output Directed Graphs (or IOD Graphs) as graphs with nodes $N$ and directed edges $E$, where $N$ contains (a) a set of \"input nodes\" $I \\subset N$, where each $i \\in I$ has no incoming edges and any number of outgoing edges, and (b) a set of \"output nodes\" $O \\subset N$, where each $o \\in O$ has no outgoing edges and any number of incoming edges, and $I\\cap O = \\emptyset$. We define informativeness, which involves the connections via directed paths from the input nodes to the output nodes: A partially informative IOD Graph has at least one path from an input to an output, a very informative IOD Graph has a path from every input to some output, and a fully informative IOD Graph has a path from every input to every output.\n  A perceptron is an example of an IOD Graph. If it has non-zero weights and any number of layers, it is fully informative. As links are removed (assigned zero weight), the perceptron might become very, partially, or not informative.\n  We define a crossover operation on IOD Graphs in which we find subgraphs with matching sets of forward and backward directed links to \"swap.\" With this operation, IOD Graphs can be subject to evolutionary computation methods. We show that fully informative parents may yield a non-informative child. We also show that under conditions of contiguousness and the no dangling nodes condition, crossover compatible, partially informative parents yield partially informative children, and very informative input parents with partially informative output parents yield very informative children. However, even under these conditions, full informativeness may not be retained."
      },
      {
        "id": "oai:arXiv.org:2406.10999v5",
        "title": "Balancing Rigor and Utility: Mitigating Cognitive Biases in Large Language Models for Multiple-Choice Questions",
        "link": "https://arxiv.org/abs/2406.10999",
        "author": "Hanyang Zhong, Liman Wang, Wenting Cao, Zeyuan Sun",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2406.10999v5 Announce Type: replace \nAbstract: This paper examines the role of cognitive biases in the decision-making processes of large language models (LLMs), challenging the conventional goal of eliminating all biases. When properly balanced, we show that certain cognitive biases can enhance decision-making efficiency through rational deviations and heuristic shortcuts. By introducing heuristic moderation and an abstention option, which allows LLMs to withhold responses when uncertain, we reduce error rates, improve decision accuracy, and optimize decision rates. Using the Balance Rigor and Utility (BRU) dataset, developed through expert collaboration, our findings demonstrate that targeted inspection of cognitive biases aligns LLM decisions more closely with human reasoning, enhancing reliability and suggesting strategies for future improvements. This approach offers a novel way to leverage cognitive biases to improve the practical utility of LLMs across various applications."
      },
      {
        "id": "oai:arXiv.org:2406.13073v2",
        "title": "Let the Noise Speak: Harnessing Noise for a Unified Defense Against Adversarial and Backdoor Attacks",
        "link": "https://arxiv.org/abs/2406.13073",
        "author": "Md Hasan Shahriar, Ning Wang, Naren Ramakrishnan, Y. Thomas Hou, Wenjing Lou",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2406.13073v2 Announce Type: replace \nAbstract: The exponential adoption of machine learning (ML) is propelling the world into a future of distributed and intelligent automation and data-driven solutions. However, the proliferation of malicious data manipulation attacks against ML, namely adversarial and backdoor attacks, jeopardizes its reliability in safety-critical applications. The existing detection methods are attack-specific and built upon some strong assumptions, limiting them in diverse practical scenarios. Thus, motivated by the need for a more robust, unified, and attack-agnostic defense mechanism, we first investigate the shared traits of adversarial and backdoor attacks. Based on our observation, we propose NoiSec, a reconstruction-based intrusion detection system that brings a novel perspective by shifting focus from the reconstructed input to the reconstruction noise itself, which is the foundational root cause of such malicious data alterations. NoiSec disentangles the noise from the test input, extracts the underlying features from the noise, and leverages them to recognize systematic malicious manipulation. Our comprehensive evaluation of NoiSec demonstrates its high effectiveness across various datasets, including basic objects, natural scenes, traffic signs, medical images, spectrogram-based audio data, and wireless sensing against five state-of-the-art adversarial attacks and three backdoor attacks under challenging evaluation conditions. NoiSec demonstrates strong detection performance in both white-box and black-box adversarial attack scenarios, significantly outperforming the closest baseline models, particularly in an adaptive attack setting. We will provide the code for future baseline comparison. Our code and artifacts are publicly available at https://github.com/shahriar0651/NoiSec."
      },
      {
        "id": "oai:arXiv.org:2406.14510v2",
        "title": "V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data",
        "link": "https://arxiv.org/abs/2406.14510",
        "author": "Rotem Shalev-Arkushin, Aharon Azulay, Tavi Halperin, Eitan Richardson, Amit H. Bermano, Ohad Fried",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2406.14510v2 Announce Type: replace \nAbstract: Diffusion-based generative models have recently shown remarkable image and video editing capabilities. However, local video editing, particularly removal of small attributes like glasses, remains a challenge. Existing methods either alter the videos excessively, generate unrealistic artifacts, or fail to perform the requested edit consistently throughout the video. In this work, we focus on consistent and identity-preserving removal of glasses in videos, using it as a case study for consistent local attribute removal in videos. Due to the lack of paired data, we adopt a weakly supervised approach and generate synthetic imperfect data, using an adjusted pretrained diffusion model. We show that despite data imperfection, by learning from our generated data and leveraging the prior of pretrained diffusion models, our model is able to perform the desired edit consistently while preserving the original video content. Furthermore, we exemplify the generalization ability of our method to other local video editing tasks by applying it successfully to facial sticker-removal. Our approach demonstrates significant improvement over existing methods, showcasing the potential of leveraging synthetic data and strong video priors for local video editing tasks."
      },
      {
        "id": "oai:arXiv.org:2406.15583v2",
        "title": "Detecting AI-Generated Text: Factors Influencing Detectability with Current Methods",
        "link": "https://arxiv.org/abs/2406.15583",
        "author": "Kathleen C. Fraser, Hillary Dawkins, Svetlana Kiritchenko",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2406.15583v2 Announce Type: replace \nAbstract: Large language models (LLMs) have advanced to a point that even humans have difficulty discerning whether a text was generated by another human, or by a computer. However, knowing whether a text was produced by human or artificial intelligence (AI) is important to determining its trustworthiness, and has applications in many domains including detecting fraud and academic dishonesty, as well as combating the spread of misinformation and political propaganda. The task of AI-generated text (AIGT) detection is therefore both very challenging, and highly critical. In this survey, we summarize state-of-the art approaches to AIGT detection, including watermarking, statistical and stylistic analysis, and machine learning classification. We also provide information about existing datasets for this task. Synthesizing the research findings, we aim to provide insight into the salient factors that combine to determine how \"detectable\" AIGT text is under different scenarios, and to make practical recommendations for future work towards this significant technical and societal challenge."
      },
      {
        "id": "oai:arXiv.org:2406.18739v2",
        "title": "RetroGFN: Diverse and Feasible Retrosynthesis using GFlowNets",
        "link": "https://arxiv.org/abs/2406.18739",
        "author": "Piotr Gai\\'nski, Micha{\\l} Koziarski, Krzysztof Maziarz, Marwin Segler, Jacek Tabor, Marek \\'Smieja",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2406.18739v2 Announce Type: replace \nAbstract: Single-step retrosynthesis aims to predict a set of reactions that lead to the creation of a target molecule, which is a crucial task in molecular discovery. Although a target molecule can often be synthesized with multiple different reactions, it is not clear how to verify the feasibility of a reaction, because the available datasets cover only a tiny fraction of the possible solutions. Consequently, the existing models are not encouraged to explore the space of possible reactions sufficiently. In this paper, we propose a novel single-step retrosynthesis model, RetroGFN, that can explore outside the limited dataset and return a diverse set of feasible reactions by leveraging a feasibility proxy model during the training. We show that RetroGFN achieves competitive results on standard top-k accuracy while outperforming existing methods on round-trip accuracy. Moreover, we provide empirical arguments in favor of using round-trip accuracy, which expands the notion of feasibility with respect to the standard top-k accuracy metric."
      },
      {
        "id": "oai:arXiv.org:2407.06165v2",
        "title": "Tumor likelihood estimation on MRI prostate data by utilizing k-Space information",
        "link": "https://arxiv.org/abs/2407.06165",
        "author": "M. Rempe, F. H\\\"orst, C. Seibold, B. Hadaschik, M. Schlimbach, J. Egger, K. Kr\\\"oninger, F. Breuer, M. Blaimer, J. Kleesiek",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2407.06165v2 Announce Type: replace \nAbstract: We present a novel preprocessing and prediction pipeline for the classification of magnetic resonance imaging (MRI) that takes advantage of the information rich complex valued k-Space. Using a publicly available MRI raw dataset with 312 subject and a total of 9508 slices, we show the advantage of utilizing the k-Space for better prostate cancer likelihood estimation in comparison to just using the magnitudinal information in the image domain, with an AUROC of $86.1\\%\\pm1.8\\%$. Additionally, by using high undersampling rates and a simple principal component analysis (PCA) for coil compression, we reduce the time needed for reconstruction by avoiding the time intensive GRAPPA reconstruction algorithm. By using digital undersampling for our experiments, we show that scanning and reconstruction time could be reduced. Even with an undersampling factor of 16, our approach achieves meaningful results, with an AUROC of $71.4\\%\\pm2.9\\%$, using the PCA coil combination and taking into account the k-Space information. With this study, we were able to show the feasibility of preserving phase and k-Space information, with consistent results. Besides preserving valuable information for further diagnostics, this approach can work without the time intensive ADC and reconstruction calculations, greatly reducing the post processing, as well as potential scanning time, increasing patient comfort and allowing a close to real-time prediction."
      },
      {
        "id": "oai:arXiv.org:2407.07603v2",
        "title": "iiANET: Inception Inspired Attention Hybrid Network for efficient Long-Range Dependency",
        "link": "https://arxiv.org/abs/2407.07603",
        "author": "Haruna Yunusa, Qin Shiyin, Abdulrahman Hamman Adama Chukkol, Adamu Lawan, Abdulganiyu Abdu Yusuf, Isah Bello",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2407.07603v2 Announce Type: replace \nAbstract: The recent emergence of hybrid models has introduced a transformative approach to computer vision, gradually moving beyond conventional convolutional neural net-works and vision transformers. However, efficiently combining these two paradigms to better capture long-range dependencies in complex images remains a challenge. In this paper, we present iiANET (Inception Inspired Attention Network), an efficient hybrid visual backbone designed to improve the modeling of long-range dependen-cies. The core innovation of iiANET is the iiABlock, a unified building block that in-tegrates global r-MHSA (Multi-Head Self-Attention) and convolutional layers in paral-lel. This design enables iiABlock to simultaneously capture global context and local details, making it highly effective for extracting rich and diverse features. By effi-ciently fusing these complementary representations, iiABlock allows iiANET to achieve strong feature interaction while maintaining computational efficiency. Exten-sive qualitative and quantitative evaluations across various benchmarks show im-proved performance over several state-of-the-art models."
      },
      {
        "id": "oai:arXiv.org:2407.10989v2",
        "title": "Can Large Language Models Detect Verbal Indicators of Romantic Attraction?",
        "link": "https://arxiv.org/abs/2407.10989",
        "author": "Sandra C. Matz, Heinrich Peters, Moran Cerf, Eric Grunenberg, Paul W. Eastwick, Mitja D. Back, Eli J. Finkel",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2407.10989v2 Announce Type: replace \nAbstract: As artificial intelligence (AI) models become an integral part of everyday life, our interactions with them shift from purely functional exchanges to more relational experiences. For these experiences to be successful, artificial agents need to be able to detect and interpret social cues and interpersonal dynamics; both within and outside of their own human-agent relationships. In this paper, we explore whether AI models can accurately decode one of the arguably most important but complex social signals: romantic attraction. Specifically, we test whether Large Language Models can detect romantic attraction during brief getting-to-know-you interactions between humans. Examining data from 964 speed dates, we show that ChatGPT can predict both objective and subjective indicators of speed dating success (r=0.12-0.23). Although predictive performance remains relatively low, ChatGPT's predictions of actual matching (i.e., the exchange of contact information) were not only on par with those of human judges but incremental to speed daters' own predictions. In addition, ChatGPT's judgments showed substantial overlap with those made by human observers (r=0.21-0.35), highlighting similarities in their representation of romantic attraction that are independent of accuracy. Our findings also offer insights into how ChatGPT arrives at its predictions and the mistakes it makes. Specifically, we use a Brunswik lens approach to identify the linguistic and conversational cues utilized by ChatGPT (and human judges) vis-a-vis those that are predictive of actual matching."
      },
      {
        "id": "oai:arXiv.org:2407.12223v4",
        "title": "Conditional Quantile Estimation for Uncertain Watch Time in Short-Video Recommendation",
        "link": "https://arxiv.org/abs/2407.12223",
        "author": "Chengzhi Lin, Shuchang Liu, Chuyuan Wang, Yongqi Liu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2407.12223v4 Announce Type: replace \nAbstract: Accurately predicting watch time is crucial for optimizing recommendations and user experience in short video platforms. However, existing methods that estimate a single average watch time often fail to capture the inherent uncertainty in user engagement patterns. In this paper, we propose Conditional Quantile Estimation (CQE) to model the entire conditional distribution of watch time. Using quantile regression, CQE characterizes the complex watch-time distribution for each user-video pair, providing a flexible and comprehensive approach to understanding user behavior. We further design multiple strategies to combine the quantile estimates, adapting to different recommendation scenarios and user preferences. Extensive offline experiments and online A/B tests demonstrate the superiority of CQE in watch-time prediction and user engagement modeling. Specifically, deploying CQE online on a large-scale platform with hundreds of millions of daily active users has led to substantial gains in key evaluation metrics, including active days, engagement time, and video views. These results highlight the practical impact of our proposed approach in enhancing the user experience and overall performance of the short video recommendation system. The code will be released https://github.com/justopit/CQE."
      },
      {
        "id": "oai:arXiv.org:2407.16682v2",
        "title": "SAM-CP: Marrying SAM with Composable Prompts for Versatile Segmentation",
        "link": "https://arxiv.org/abs/2407.16682",
        "author": "Pengfei Chen, Lingxi Xie, Xinyue Huo, Xuehui Yu, Xiaopeng Zhang, Yingfei Sun, Zhenjun Han, Qi Tian",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2407.16682v2 Announce Type: replace \nAbstract: The Segment Anything model (SAM) has shown a generalized ability to group image pixels into patches, but applying it to semantic-aware segmentation still faces major challenges. This paper presents SAM-CP, a simple approach that establishes two types of composable prompts beyond SAM and composes them for versatile segmentation. Specifically, given a set of classes (in texts) and a set of SAM patches, the Type-I prompt judges whether a SAM patch aligns with a text label, and the Type-II prompt judges whether two SAM patches with the same text label also belong to the same instance. To decrease the complexity in dealing with a large number of semantic classes and patches, we establish a unified framework that calculates the affinity between (semantic and instance) queries and SAM patches and merges patches with high affinity to the query. Experiments show that SAM-CP achieves semantic, instance, and panoptic segmentation in both open and closed domains. In particular, it achieves state-of-the-art performance in open-vocabulary segmentation. Our research offers a novel and generalized methodology for equipping vision foundation models like SAM with multi-grained semantic perception abilities."
      },
      {
        "id": "oai:arXiv.org:2407.19546v3",
        "title": "MMCLIP: Cross-modal Attention Masked Modelling for Medical Language-Image Pre-Training",
        "link": "https://arxiv.org/abs/2407.19546",
        "author": "Biao Wu, Yutong Xie, Zeyu Zhang, Minh Hieu Phan, Qi Chen, Ling Chen, Qi Wu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2407.19546v3 Announce Type: replace \nAbstract: Vision-and-language pretraining (VLP) in the medical field utilizes contrastive learning on image-text pairs to achieve effective transfer across tasks. Yet, current VLP approaches with the masked modeling strategy face two challenges when applied to the medical domain. First, current models struggle to accurately reconstruct key pathological features due to the scarcity of medical data. Second, most methods only adopt either paired image-text or image-only data, failing to exploit the combination of both paired and unpaired data. To this end, this paper proposes the MMCLIP (Masked Medical Contrastive Language-Image Pre-Training) framework to enhance pathological learning and feature learning via unpaired data. First, we introduce the attention-masked image modeling (AttMIM) and entity-driven masked language modeling module (EntMLM), which learns to reconstruct pathological visual and textual tokens via multi-modal feature interaction, thus improving medical-enhanced features. The AttMIM module masks a portion of the image features that are highly responsive to textual features. This allows MMCLIP to improve the reconstruction of highly similar image data in medicine efficiency. Second, our MMCLIP capitalizes unpaired data to enhance multimodal learning by introducing disease-kind prompts. The experimental results show that MMCLIP achieves SOTA for zero-shot and fine-tuning classification performance on five datasets. Our code will be available at https://github.com/White65534/MMCLIP."
      },
      {
        "id": "oai:arXiv.org:2407.20021v4",
        "title": "MimiQ: Low-Bit Data-Free Quantization of Vision Transformers with Encouraging Inter-Head Attention Similarity",
        "link": "https://arxiv.org/abs/2407.20021",
        "author": "Kanghyun Choi, Hye Yoon Lee, Dain Kwon, SunJong Park, Kyuyeun Kim, Noseong Park, Jonghyun Choi, Jinho Lee",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2407.20021v4 Announce Type: replace \nAbstract: Data-free quantization (DFQ) is a technique that creates a lightweight network from its full-precision counterpart without the original training data, often through a synthetic dataset. Although several DFQ methods have been proposed for vision transformer (ViT) architectures, they fail to achieve efficacy in low-bit settings. Examining the existing methods, we observe that their synthetic data produce misaligned attention maps, while those of the real samples are highly aligned. From this observation, we find that aligning attention maps of synthetic data helps improve the overall performance of quantized ViTs. Motivated by this finding, we devise MimiQ, a novel DFQ method designed for ViTs that enhances inter-head attention similarity. First, we generate synthetic data by aligning head-wise attention outputs from each spatial query patch. Then, we align the attention maps of the quantized network to those of the full-precision teacher by applying head-wise structural attention distillation. The experimental results show that the proposed method significantly outperforms baselines, setting a new state-of-the-art for ViT-DFQ. This paper is an extended version of our work published in the proceedings of AAAI 2025, including additional supplementary material."
      },
      {
        "id": "oai:arXiv.org:2408.00672v3",
        "title": "ExpertAF: Expert Actionable Feedback from Video",
        "link": "https://arxiv.org/abs/2408.00672",
        "author": "Kumar Ashutosh, Tushar Nagarajan, Georgios Pavlakos, Kris Kitani, Kristen Grauman",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2408.00672v3 Announce Type: replace \nAbstract: Feedback is essential for learning a new skill or improving one's current skill-level. However, current methods for skill-assessment from video only provide scores or compare demonstrations, leaving the burden of knowing what to do differently on the user. We introduce a novel method to generate actionable feedback (AF) from video of a person doing a physical activity, such as basketball or soccer. Our method takes a video demonstration and its accompanying 3D body pose and generates (1) free-form expert commentary describing what the person is doing well and what they could improve, and (2) a visual expert demonstration that incorporates the required corrections. We show how to leverage Ego-Exo4D's [29] videos of skilled activity and expert commentary together with a strong language model to create a weakly-supervised training dataset for this task, and we devise a multimodal video-language model to infer coaching feedback. Our method is able to reason across multi-modal input combinations to output full spectrum, actionable coaching-expert commentary, expert video retrieval, and expert pose generation-outperforming strong vision-language models on both established metrics and human preference studies."
      },
      {
        "id": "oai:arXiv.org:2408.04917v2",
        "title": "Avoid Wasted Annotation Costs in Open-set Active Learning with Pre-trained Vision-Language Model",
        "link": "https://arxiv.org/abs/2408.04917",
        "author": "Jaehyuk Heo, Pilsung Kang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2408.04917v2 Announce Type: replace \nAbstract: Active learning (AL) aims to enhance model performance by selectively collecting highly informative data, thereby minimizing annotation costs. However, in practical scenarios, unlabeled data may contain out-of-distribution (OOD) samples, which are not used for training, leading to wasted annotation costs if data is incorrectly selected. Therefore, to make active learning feasible in real-world applications, it is crucial to consider not only the informativeness of unlabeled samples but also their purity to determine whether they belong to the in-distribution (ID). Recent studies have applied AL under these assumptions, but challenges remain due to the trade-off between informativeness and purity, as well as the heavy dependence on OOD samples. These issues lead to the collection of OOD samples, resulting in a significant waste of annotation costs. To address these challenges, we propose a novel query strategy, VLPure-AL, which minimizes cost losses while reducing dependence on OOD samples. VLPure-AL sequentially evaluates the purity and informativeness of data. First, it utilizes a pre-trained vision-language model to detect and exclude OOD data with high accuracy by leveraging linguistic and visual information of ID data. Second, it selects highly informative data from the remaining ID data, and then the selected samples are annotated by human experts. Experimental results on datasets with various open-set conditions demonstrate that VLPure-AL achieves the lowest cost loss and highest performance across all scenarios. Code is available at https://github.com/DSBA-Lab/OpenAL."
      },
      {
        "id": "oai:arXiv.org:2408.05886v3",
        "title": "Online-Score-Aided Federated Learning: Taming the Resource Constraints in Wireless Networks",
        "link": "https://arxiv.org/abs/2408.05886",
        "author": "Md-Ferdous Pervej, Minseok Choi, Andreas F. Molisch",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2408.05886v3 Announce Type: replace \nAbstract: While federated learning (FL) is a widely popular distributed machine learning (ML) strategy that protects data privacy, time-varying wireless network parameters and heterogeneous configurations of the wireless devices pose significant challenges. Although the limited radio and computational resources of the network and the clients, respectively, are widely acknowledged, two critical yet often ignored aspects are (a) wireless devices can only dedicate a small chunk of their limited storage for the FL task and (b) new training samples may arrive in an online manner in many practical wireless applications. Therefore, we propose a new FL algorithm called online-score-aided federated learning (OSAFL), specifically designed to learn tasks relevant to wireless applications under these practical considerations. Since clients' local training steps differ under resource constraints, which may lead to client drift under statistically heterogeneous data distributions, we leverage normalized gradient similarities and exploit weighting clients' updates based on optimized scores that facilitate the convergence rate of the proposed OSAFL algorithm without incurring any communication overheads to the clients or requiring any statistical data information from them. Our extensive simulation results on two different datasets with four popular ML models validate the effectiveness of OSAFL compared to five modified state-of-the-art FL baselines."
      },
      {
        "id": "oai:arXiv.org:2408.11816v3",
        "title": "Efficient Exploration and Discriminative World Model Learning with an Object-Centric Abstraction",
        "link": "https://arxiv.org/abs/2408.11816",
        "author": "Anthony GX-Chen, Kenneth Marino, Rob Fergus",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2408.11816v3 Announce Type: replace \nAbstract: In the face of difficult exploration problems in reinforcement learning, we study whether giving an agent an object-centric mapping (describing a set of items and their attributes) allow for more efficient learning. We found this problem is best solved hierarchically by modelling items at a higher level of state abstraction to pixels, and attribute change at a higher level of temporal abstraction to primitive actions. This abstraction simplifies the transition dynamic by making specific future states easier to predict. We make use of this to propose a fully model-based algorithm that learns a discriminative world model, plans to explore efficiently with only a count-based intrinsic reward, and can subsequently plan to reach any discovered (abstract) states.\n  We demonstrate the model's ability to (i) efficiently solve single tasks, (ii) transfer zero-shot and few-shot across item types and environments, and (iii) plan across long horizons. Across a suite of 2D crafting and MiniHack environments, we empirically show our model significantly out-performs state-of-the-art low-level methods (without abstraction), as well as performant model-free and model-based methods using the same abstraction. Finally, we show how to learn low level object-perturbing policies via reinforcement learning, and the object mapping itself by supervised learning."
      },
      {
        "id": "oai:arXiv.org:2408.14358v3",
        "title": "An Embedding is Worth a Thousand Noisy Labels",
        "link": "https://arxiv.org/abs/2408.14358",
        "author": "Francesco Di Salvo, Sebastian Doerrich, Ines Rieger, Christian Ledig",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2408.14358v3 Announce Type: replace \nAbstract: The performance of deep neural networks scales with dataset size and label quality, rendering the efficient mitigation of low-quality data annotations crucial for building robust and cost-effective systems. Existing strategies to address label noise exhibit severe limitations due to computational complexity and application dependency. In this work, we propose WANN, a Weighted Adaptive Nearest Neighbor approach that builds on self-supervised feature representations obtained from foundation models. To guide the weighted voting scheme, we introduce a reliability score $\\eta$, which measures the likelihood of a data label being correct. WANN outperforms reference methods, including a linear layer trained with robust loss functions, on diverse datasets of varying size and under various noise types and severities. WANN also exhibits superior generalization on imbalanced data compared to both Adaptive-NNs (ANN) and fixed k-NNs. Furthermore, the proposed weighting scheme enhances supervised dimensionality reduction under noisy labels. This yields a significant boost in classification performance with 10x and 100x smaller image embeddings, minimizing latency and storage requirements. Our approach, emphasizing efficiency and explainability, emerges as a simple, robust solution to overcome inherent limitations of deep neural network training. The code is available at https://github.com/francescodisalvo05/wann-noisy-labels ."
      },
      {
        "id": "oai:arXiv.org:2409.01115v3",
        "title": "Time series classification with random convolution kernels: pooling operators and input representations matter",
        "link": "https://arxiv.org/abs/2409.01115",
        "author": "Mouhamadou Mansour Lo, Gildas Morvan, Mathieu Rossi, Fabrice Morganti, David Mercier",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2409.01115v3 Announce Type: replace \nAbstract: This article presents a new approach based on MiniRocket, called SelF-Rocket, for fast time series classification (TSC). Unlike existing approaches based on random convolution kernels, it dynamically selects the best couple of input representations and pooling operator during the training process. SelF-Rocket achieves state-of-the-art accuracy on the University of California Riverside (UCR) TSC benchmark datasets."
      },
      {
        "id": "oai:arXiv.org:2409.09386v2",
        "title": "AMBER -- Advanced SegFormer for Multi-Band Image Segmentation: an application to Hyperspectral Imaging",
        "link": "https://arxiv.org/abs/2409.09386",
        "author": "Andrea Dosi, Massimo Brescia, Stefano Cavuoti, Mariarca D'Aniello, Michele Delli Veneri, Carlo Donadio, Adriano Ettari, Giuseppe Longo, Alvi Rownok, Luca Sannino, Maria Zampella",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2409.09386v2 Announce Type: replace \nAbstract: Deep learning has revolutionized the field of hyperspectral image (HSI) analysis, enabling the extraction of complex spectral and spatial features. While convolutional neural networks (CNNs) have been the backbone of HSI classification, their limitations in capturing global contextual features have led to the exploration of Vision Transformers (ViTs). This paper introduces AMBER, an advanced SegFormer specifically designed for multi-band image segmentation. AMBER enhances the original SegFormer by incorporating three-dimensional convolutions, custom kernel sizes, and a Funnelizer layer. This architecture enables processing hyperspectral data directly, without requiring spectral dimensionality reduction during preprocessing. Our experiments, conducted on three benchmark datasets (Salinas, Indian Pines, and Pavia University) and on a dataset from the PRISMA satellite, show that AMBER outperforms traditional CNN-based methods in terms of Overall Accuracy, Kappa coefficient, and Average Accuracy on the first three datasets, and achieves state-of-the-art performance on the PRISMA dataset. These findings highlight AMBER's robustness, adaptability to both airborne and spaceborne data, and its potential as a powerful solution for remote sensing and other domains requiring advanced analysis of high-dimensional data."
      },
      {
        "id": "oai:arXiv.org:2409.10589v3",
        "title": "Offline Reinforcement Learning for Learning to Dispatch for Job Shop Scheduling",
        "link": "https://arxiv.org/abs/2409.10589",
        "author": "Jesse van Remmerden, Zaharah Bukhsh, Yingqian Zhang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2409.10589v3 Announce Type: replace \nAbstract: The Job Shop Scheduling Problem (JSSP) is a complex combinatorial optimization problem. While online Reinforcement Learning (RL) has shown promise by quickly finding acceptable solutions for JSSP, it faces key limitations: it requires extensive training interactions from scratch leading to sample inefficiency, cannot leverage existing high-quality solutions, and often yields suboptimal results compared to traditional methods like Constraint Programming (CP). We introduce Offline Reinforcement Learning for Learning to Dispatch (Offline-LD), which addresses these limitations by learning from previously generated solutions. Our approach is motivated by scenarios where historical scheduling data and expert solutions are available, although our current evaluation focuses on benchmark problems. Offline-LD adapts two CQL-based Q-learning methods (mQRDQN and discrete mSAC) for maskable action spaces, introduces a novel entropy bonus modification for discrete SAC, and exploits reward normalization through preprocessing. Our experiments demonstrate that Offline-LD outperforms online RL on both generated and benchmark instances. Notably, by introducing noise into the expert dataset, we achieve similar or better results than those obtained from the expert dataset, suggesting that a more diverse training set is preferable because it contains counterfactual information."
      },
      {
        "id": "oai:arXiv.org:2409.12323v2",
        "title": "Depth Estimation Based on 3D Gaussian Splatting Siamese Defocus",
        "link": "https://arxiv.org/abs/2409.12323",
        "author": "Jinchang Zhang, Ningning Xu, Hao Zhang, Guoyu Lu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2409.12323v2 Announce Type: replace \nAbstract: Depth estimation is a fundamental task in 3D geometry. While stereo depth estimation can be achieved through triangulation methods, it is not as straightforward for monocular methods, which require the integration of global and local information. The Depth from Defocus (DFD) method utilizes camera lens models and parameters to recover depth information from blurred images and has been proven to perform well. However, these methods rely on All-In-Focus (AIF) images for depth estimation, which is nearly impossible to obtain in real-world applications. To address this issue, we propose a self-supervised framework based on 3D Gaussian splatting and Siamese networks. By learning the blur levels at different focal distances of the same scene in the focal stack, the framework predicts the defocus map and Circle of Confusion (CoC) from a single defocused image, using the defocus map as input to DepthNet for monocular depth estimation. The 3D Gaussian splatting model renders defocused images using the predicted CoC, and the differences between these and the real defocused images provide additional supervision signals for the Siamese Defocus self-supervised network. This framework has been validated on both artificially synthesized and real blurred datasets. Subsequent quantitative and visualization experiments demonstrate that our proposed framework is highly effective as a DFD method."
      },
      {
        "id": "oai:arXiv.org:2409.12499v2",
        "title": "End-to-end Open-vocabulary Video Visual Relationship Detection using Multi-modal Prompting",
        "link": "https://arxiv.org/abs/2409.12499",
        "author": "Yongqi Wang, Xinxiao Wu, Shuo Yang, Jiebo Luo",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2409.12499v2 Announce Type: replace \nAbstract: Open-vocabulary video visual relationship detection aims to expand video visual relationship detection beyond annotated categories by detecting unseen relationships between both seen and unseen objects in videos. Existing methods usually use trajectory detectors trained on closed datasets to detect object trajectories, and then feed these trajectories into large-scale pre-trained vision-language models to achieve open-vocabulary classification. Such heavy dependence on the pre-trained trajectory detectors limits their ability to generalize to novel object categories, leading to performance degradation. To address this challenge, we propose to unify object trajectory detection and relationship classification into an end-to-end open-vocabulary framework. Under this framework, we propose a relationship-aware open-vocabulary trajectory detector. It primarily consists of a query-based Transformer decoder, where the visual encoder of CLIP is distilled for frame-wise open-vocabulary object detection, and a trajectory associator. To exploit relationship context during trajectory detection, a relationship query is embedded into the Transformer decoder, and accordingly, an auxiliary relationship loss is designed to enable the decoder to perceive the relationships between objects explicitly. Moreover, we propose an open-vocabulary relationship classifier that leverages the rich semantic knowledge of CLIP to discover novel relationships. To adapt CLIP well to relationship classification, we design a multi-modal prompting method that employs spatio-temporal visual prompting for visual representation and vision-guided language prompting for language input. Extensive experiments on two public datasets, VidVRD and VidOR, demonstrate the effectiveness of our framework. Our framework is also applied to a more difficult cross-dataset scenario to further demonstrate its generalization ability."
      },
      {
        "id": "oai:arXiv.org:2409.15355v5",
        "title": "Block-Attention for Efficient Prefilling",
        "link": "https://arxiv.org/abs/2409.15355",
        "author": "Dongyang Ma, Yan Wang, Lan Tian",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2409.15355v5 Announce Type: replace \nAbstract: We introduce Block-attention, an attention mechanism designed to address the increased inference latency and cost in Retrieval-Augmented Generation (RAG) scenarios. Traditional approaches often encode the entire context in an auto-regressive manner. Instead, Block-attention divides retrieved documents into discrete blocks, with each block independently calculating key-value (KV) states except for the final block. In RAG scenarios, by defining each passage as a block, Block-attention enables us to reuse the KV states of passages that have been seen before, thereby significantly reducing the latency and the computation overhead during inference. The implementation of Block-attention involves block segmentation, position re-encoding, and fine-tuning the LLM to adapt to the Block-attention mechanism. Experiments on 11 diverse benchmarks, including RAG, ICL, and general domains, demonstrate that after block fine-tuning, the Block-attention model not only achieves performance comparable to that of full-attention models, but can also seamlessly switch between the block and full attention modes without any performance loss. Notably, Block-attention significantly reduces the time to first token (TTFT) and floating point operations (FLOPs) to a very low level. It only takes 45 ms to output the first token for an input sequence with a total length of 32K. Compared to the full-attention models, the TTFT and corresponding FLOPs are reduced by 98.7% and 99.8%, respectively. Additionally, in Appendix A, we elaborate on how Block-attention is applied in Game AI scenario and the substantial potential benefits it entails. We strongly suggest researchers in the gaming field not to overlook this section."
      },
      {
        "id": "oai:arXiv.org:2409.17091v2",
        "title": "Ctrl-GenAug: Controllable Generative Augmentation for Medical Sequence Classification",
        "link": "https://arxiv.org/abs/2409.17091",
        "author": "Xinrui Zhou, Yuhao Huang, Haoran Dou, Shijing Chen, Ao Chang, Jia Liu, Weiran Long, Jian Zheng, Erjiao Xu, Jie Ren, Ruobing Huang, Jun Cheng, Wufeng Xue, Dong Ni",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2409.17091v2 Announce Type: replace \nAbstract: In the medical field, the limited availability of large-scale datasets and labor-intensive annotation processes hinder the performance of deep models. Diffusion-based generative augmentation approaches present a promising solution to this issue, having been proven effective in advancing downstream medical recognition tasks. Nevertheless, existing works lack sufficient semantic and sequential steerability for challenging video/3D sequence generation, and neglect quality control of noisy synthesized samples, resulting in unreliable synthetic databases and severely limiting the performance of downstream tasks. In this work, we present Ctrl-GenAug, a novel and general generative augmentation framework that enables highly semantic- and sequential-customized sequence synthesis and suppresses incorrectly synthesized samples, to aid medical sequence classification. Specifically, we first design a multimodal conditions-guided sequence generator for controllably synthesizing diagnosis-promotive samples. A sequential augmentation module is integrated to enhance the temporal/stereoscopic coherence of generated samples. Then, we propose a noisy synthetic data filter to suppress unreliable cases at semantic and sequential levels. Extensive experiments on 3 medical datasets, using 11 networks trained on 3 paradigms, comprehensively analyze the effectiveness and generality of Ctrl-GenAug, particularly in underrepresented high-risk populations and out-domain conditions."
      },
      {
        "id": "oai:arXiv.org:2409.17671v4",
        "title": "Leveraging Anthropometric Measurements to Improve Human Mesh Estimation and Ensure Consistent Body Shapes",
        "link": "https://arxiv.org/abs/2409.17671",
        "author": "Katja Ludwig, Julian Lorenz, Daniel Kienzle, Tuan Bui, Rainer Lienhart",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2409.17671v4 Announce Type: replace \nAbstract: The basic body shape (i.e., the body shape in T-pose) of a person does not change within a single video. However, most SOTA human mesh estimation (HME) models output a slightly different, thus inconsistent basic body shape for each video frame. Furthermore, we find that SOTA 3D human pose estimation (HPE) models outperform HME models regarding the precision of the estimated 3D keypoint positions. We solve the problem of inconsistent body shapes by leveraging anthropometric measurements like taken by tailors from humans. We create a model called A2B that converts given anthropometric measurements to basic body shape parameters of human mesh models. We obtain superior and consistent human meshes by combining the A2B model results with the keypoints of 3D HPE models using inverse kinematics. We evaluate our approach on challenging datasets like ASPset or fit3D, where we can lower the MPJPE by over 30 mm compared to SOTA HME models. Further, replacing estimates of the body shape parameters from existing HME models with A2B results not only increases the performance of these HME models, but also guarantees consistent body shapes."
      },
      {
        "id": "oai:arXiv.org:2410.01432v2",
        "title": "Adaptive teachers for amortized samplers",
        "link": "https://arxiv.org/abs/2410.01432",
        "author": "Minsu Kim, Sanghyeok Choi, Taeyoung Yun, Emmanuel Bengio, Leo Feng, Jarrid Rector-Brooks, Sungsoo Ahn, Jinkyoo Park, Nikolay Malkin, Yoshua Bengio",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.01432v2 Announce Type: replace \nAbstract: Amortized inference is the task of training a parametric model, such as a neural network, to approximate a distribution with a given unnormalized density where exact sampling is intractable. When sampling is implemented as a sequential decision-making process, reinforcement learning (RL) methods, such as generative flow networks, can be used to train the sampling policy. Off-policy RL training facilitates the discovery of diverse, high-reward candidates, but existing methods still face challenges in efficient exploration. We propose to use an adaptive training distribution (the \\teacher) to guide the training of the primary amortized sampler (the \\student). The \\teacher, an auxiliary behavior model, is trained to sample high-loss regions of the \\student and can generalize across unexplored modes, thereby enhancing mode coverage by providing an efficient training curriculum. We validate the effectiveness of this approach in a synthetic environment designed to present an exploration challenge, two diffusion-based sampling tasks, and four biochemical discovery tasks demonstrating its ability to improve sample efficiency and mode coverage. Source code is available at https://github.com/alstn12088/adaptive-teacher."
      },
      {
        "id": "oai:arXiv.org:2410.02321v2",
        "title": "Convergence of Score-Based Discrete Diffusion Models: A Discrete-Time Analysis",
        "link": "https://arxiv.org/abs/2410.02321",
        "author": "Zikun Zhang, Zixiang Chen, Quanquan Gu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.02321v2 Announce Type: replace \nAbstract: Diffusion models have achieved great success in generating high-dimensional samples across various applications. While the theoretical guarantees for continuous-state diffusion models have been extensively studied, the convergence analysis of the discrete-state counterparts remains under-explored. In this paper, we study the theoretical aspects of score-based discrete diffusion models under the Continuous Time Markov Chain (CTMC) framework. We introduce a discrete-time sampling algorithm in the general state space $[S]^d$ that utilizes score estimators at predefined time points. We derive convergence bounds for the Kullback-Leibler (KL) divergence and total variation (TV) distance between the generated sample distribution and the data distribution, considering both scenarios with and without early stopping under reasonable assumptions. Notably, our KL divergence bounds are nearly linear in the dimension $d$, aligning with state-of-the-art results for diffusion models. Our convergence analysis employs a Girsanov-based method and establishes key properties of the discrete score function, which are essential for characterizing the discrete-time sampling process."
      },
      {
        "id": "oai:arXiv.org:2410.02761v4",
        "title": "FakeShield: Explainable Image Forgery Detection and Localization via Multi-modal Large Language Models",
        "link": "https://arxiv.org/abs/2410.02761",
        "author": "Zhipei Xu, Xuanyu Zhang, Runyi Li, Zecheng Tang, Qing Huang, Jian Zhang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.02761v4 Announce Type: replace \nAbstract: The rapid development of generative AI is a double-edged sword, which not only facilitates content creation but also makes image manipulation easier and more difficult to detect. Although current image forgery detection and localization (IFDL) methods are generally effective, they tend to face two challenges: \\textbf{1)} black-box nature with unknown detection principle, \\textbf{2)} limited generalization across diverse tampering methods (e.g., Photoshop, DeepFake, AIGC-Editing). To address these issues, we propose the explainable IFDL task and design FakeShield, a multi-modal framework capable of evaluating image authenticity, generating tampered region masks, and providing a judgment basis based on pixel-level and image-level tampering clues. Additionally, we leverage GPT-4o to enhance existing IFDL datasets, creating the Multi-Modal Tamper Description dataSet (MMTD-Set) for training FakeShield's tampering analysis capabilities. Meanwhile, we incorporate a Domain Tag-guided Explainable Forgery Detection Module (DTE-FDM) and a Multi-modal Forgery Localization Module (MFLM) to address various types of tamper detection interpretation and achieve forgery localization guided by detailed textual descriptions. Extensive experiments demonstrate that FakeShield effectively detects and localizes various tampering techniques, offering an explainable and superior solution compared to previous IFDL methods. The code is available at https://github.com/zhipeixu/FakeShield."
      },
      {
        "id": "oai:arXiv.org:2410.04601v2",
        "title": "ProtoMed-LLM: An Automatic Evaluation Framework for Large Language Models in Medical Protocol Formulation",
        "link": "https://arxiv.org/abs/2410.04601",
        "author": "Seungjun Yi, Jaeyoung Lim, Juyong Yoon",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.04601v2 Announce Type: replace \nAbstract: Automated generation of scientific protocols executable by robots can significantly accelerate scientific research processes. Large Language Models (LLMs) excel at Scientific Protocol Formulation Tasks (SPFT), but the evaluation of their capabilities rely on human evaluation. Here, we propose a flexible, automatic framework to evaluate LLMs' capability on SPFT: ProtoMed-LLM. This framework prompts the target model and GPT-4 to extract pseudocode from biology protocols using only predefined lab actions and evaluates the output of the target model using LLAM-EVAL, the pseudocode generated by GPT-4 serving as a baseline and Llama-3 acting as the evaluator. Our adaptable prompt-based evaluation method, LLAM-EVAL, offers significant flexibility in terms of evaluation model, material, criteria, and is free of cost. We evaluate GPT variations, Llama, Mixtral, Gemma, Cohere, and Gemini. Overall, we find that GPT and Cohere are powerful scientific protocol formulators. We also introduce BIOPROT 2.0, a dataset with biology protocols and corresponding pseudocodes, which can aid LLMs in formulation and evaluation of SPFT. Our work is extensible to assess LLMs on SPFT across various domains and other fields that require protocol generation for specific goals."
      },
      {
        "id": "oai:arXiv.org:2410.05168v4",
        "title": "ReasoningRank: Teaching Student Models to Rank through Reasoning-Based Knowledge Distillation",
        "link": "https://arxiv.org/abs/2410.05168",
        "author": "Yuelyu Ji, Zhuochun Li, Rui Meng, Daqing He",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.05168v4 Announce Type: replace \nAbstract: Reranking documents based on their relevance to a given query is a critical task in information retrieval. Traditional reranking methods often lack transparency and rely on proprietary models, hindering reproducibility and interpretability. We propose Reason-to-Rank (R2R), a novel open-source reranking approach that enhances transparency by generating two types of reasoning: direct relevance reasoning, which explains how a document addresses the query, and comparison reasoning, which justifies the relevance of one document over another. We leverage large language models (LLMs) as teacher models to generate these explanations and distill this knowledge into smaller, openly available student models. Our student models are trained to generate meaningful reasoning and rerank documents, achieving competitive performance across multiple datasets, including MSMARCO and BRIGHT. Experiments demonstrate that R2R not only improves reranking accuracy but also provides valuable insights into the decision-making process. By offering a structured and interpretable solution with openly accessible resources, R2R aims to bridge the gap between effectiveness and transparency in information retrieval, fostering reproducibility and further research in the field."
      },
      {
        "id": "oai:arXiv.org:2410.05260v3",
        "title": "DartControl: A Diffusion-Based Autoregressive Motion Model for Real-Time Text-Driven Motion Control",
        "link": "https://arxiv.org/abs/2410.05260",
        "author": "Kaifeng Zhao, Gen Li, Siyu Tang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.05260v3 Announce Type: replace \nAbstract: Text-conditioned human motion generation, which allows for user interaction through natural language, has become increasingly popular. Existing methods typically generate short, isolated motions based on a single input sentence. However, human motions are continuous and can extend over long periods, carrying rich semantics. Creating long, complex motions that precisely respond to streams of text descriptions, particularly in an online and real-time setting, remains a significant challenge. Furthermore, incorporating spatial constraints into text-conditioned motion generation presents additional challenges, as it requires aligning the motion semantics specified by text descriptions with geometric information, such as goal locations and 3D scene geometry. To address these limitations, we propose DartControl, in short DART, a Diffusion-based Autoregressive motion primitive model for Real-time Text-driven motion control. Our model effectively learns a compact motion primitive space jointly conditioned on motion history and text inputs using latent diffusion models. By autoregressively generating motion primitives based on the preceding history and current text input, DART enables real-time, sequential motion generation driven by natural language descriptions. Additionally, the learned motion primitive space allows for precise spatial motion control, which we formulate either as a latent noise optimization problem or as a Markov decision process addressed through reinforcement learning. We present effective algorithms for both approaches, demonstrating our model's versatility and superior performance in various motion synthesis tasks. Experiments show our method outperforms existing baselines in motion realism, efficiency, and controllability. Video results are available on the project page: https://zkf1997.github.io/DART/."
      },
      {
        "id": "oai:arXiv.org:2410.05431v2",
        "title": "Continuous Ensemble Weather Forecasting with Diffusion models",
        "link": "https://arxiv.org/abs/2410.05431",
        "author": "Martin Andrae, Tomas Landelius, Joel Oskarsson, Fredrik Lindsten",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.05431v2 Announce Type: replace \nAbstract: Weather forecasting has seen a shift in methods from numerical simulations to data-driven systems. While initial research in the area focused on deterministic forecasting, recent works have used diffusion models to produce skillful ensemble forecasts. These models are trained on a single forecasting step and rolled out autoregressively. However, they are computationally expensive and accumulate errors for high temporal resolution due to the many rollout steps. We address these limitations with Continuous Ensemble Forecasting, a novel and flexible method for sampling ensemble forecasts in diffusion models. The method can generate temporally consistent ensemble trajectories completely in parallel, with no autoregressive steps. Continuous Ensemble Forecasting can also be combined with autoregressive rollouts to yield forecasts at an arbitrary fine temporal resolution without sacrificing accuracy. We demonstrate that the method achieves competitive results for global weather forecasting with good probabilistic properties."
      },
      {
        "id": "oai:arXiv.org:2410.07177v2",
        "title": "MM-Ego: Towards Building Egocentric Multimodal LLMs for Video QA",
        "link": "https://arxiv.org/abs/2410.07177",
        "author": "Hanrong Ye, Haotian Zhang, Erik Daxberger, Lin Chen, Zongyu Lin, Yanghao Li, Bowen Zhang, Haoxuan You, Dan Xu, Zhe Gan, Jiasen Lu, Yinfei Yang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.07177v2 Announce Type: replace \nAbstract: This research aims to comprehensively explore building a multimodal foundation model for egocentric video understanding. To achieve this goal, we work on three fronts. First, as there is a lack of QA data for egocentric video understanding, we automatically generate 7M high-quality QA samples for egocentric videos ranging from 30 seconds to one hour long in Ego4D based on human-annotated data. This is one of the largest egocentric QA datasets. Second, we contribute a challenging egocentric QA benchmark with 629 videos and 7,026 questions to evaluate the models' ability in recognizing and memorizing visual details across videos of varying lengths. We introduce a new de-biasing evaluation method to help mitigate the unavoidable language bias present in the models being evaluated. Third, we propose a specialized multimodal architecture featuring a novel \"Memory Pointer Prompting\" mechanism. This design includes a \\textit{global glimpse} step to gain an overarching understanding of the entire video and identify key visual information, followed by a fallback step that utilizes the key visual information to generate responses. This enables the model to more effectively comprehend extended video content. With the data, benchmark, and model, we build MM-Ego, an egocentric multimodal LLM that shows powerful performance on egocentric video understanding."
      },
      {
        "id": "oai:arXiv.org:2410.09303v2",
        "title": "Exact Byte-Level Probabilities from Tokenized Language Models for FIM-Tasks and Model Ensembles",
        "link": "https://arxiv.org/abs/2410.09303",
        "author": "Buu Phan, Brandon Amos, Itai Gat, Marton Havasi, Matthew Muckley, Karen Ullrich",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.09303v2 Announce Type: replace \nAbstract: Tokenization is associated with many poorly understood shortcomings in language models (LMs), yet remains an important component for long sequence scaling purposes. This work studies how tokenization impacts model performance by analyzing and comparing the stochastic behavior of tokenized models with their byte-level, or token-free, counterparts. We discover that, even when the two models are statistically equivalent, their predictive distributions over the next byte can be substantially different, a phenomenon we term as ``tokenization bias''. To fully characterize this phenomenon, we introduce the Byte-Token Representation Lemma, a framework that establishes a mapping between the learned token distribution and its equivalent byte-level distribution. From this result, we develop a next-byte sampling algorithm that eliminates tokenization bias without requiring further training or optimization. In other words, this enables zero-shot conversion of tokenized LMs into statistically equivalent token-free ones. We demonstrate its broad applicability with two use cases: fill-in-the-middle (FIM) tasks and model ensembles. In FIM tasks where input prompts may terminate mid-token, leading to out-of-distribution tokenization, our method mitigates performance degradation and achieves 18% improvement in FIM coding benchmarks, while consistently outperforming the standard token healing fix. For model ensembles where each model employs a distinct vocabulary, our approach enables seamless integration, resulting in improved performance up to 3.7% over individual models across various standard baselines in reasoning, knowledge, and coding. Code is available at: https://github.com/facebookresearch/Exact-Byte-Level-Probabilities-from-Tokenized-LMs"
      },
      {
        "id": "oai:arXiv.org:2410.09505v2",
        "title": "HG2P: Hippocampus-inspired High-reward Graph and Model-Free Q-Gradient Penalty for Path Planning and Motion Control",
        "link": "https://arxiv.org/abs/2410.09505",
        "author": "Haoran Wang, Yaoru Sun, Zeshen Tang, Haibo Shi, Chenyuan Jiao",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.09505v2 Announce Type: replace \nAbstract: Goal-conditioned hierarchical reinforcement learning (HRL) decomposes complex reaching tasks into a sequence of simple subgoal-conditioned tasks, showing significant promise for addressing long-horizon planning in large-scale environments. This paper bridges the goal-conditioned HRL based on graph-based planning to brain mechanisms, proposing a hippocampus-striatum-like dual-controller hypothesis. Inspired by the brain mechanisms of organisms (i.e., the high-reward preferences observed in hippocampal replay) and instance-based theory, we propose a high-return sampling strategy for constructing memory graphs, improving sample efficiency. Additionally, we derive a model-free lower-level Q-function gradient penalty to resolve the model dependency issues present in prior work, improving the generalization of Lipschitz constraints in applications. Finally, we integrate these two extensions, High-reward Graph and model-free Gradient Penalty (HG2P), into the state-of-the-art framework ACLG, proposing a novel goal-conditioned HRL framework, HG2P+ACLG. Experimentally, the results demonstrate that our method outperforms state-of-the-art goal-conditioned HRL algorithms on a variety of long-horizon navigation tasks and robotic manipulation tasks."
      },
      {
        "id": "oai:arXiv.org:2410.09880v2",
        "title": "Improving Colorectal Cancer Screening and Risk Assessment through Predictive Modeling on Medical Images and Records",
        "link": "https://arxiv.org/abs/2410.09880",
        "author": "Shuai Jiang, Christina Robinson, Joseph Anderson, William Hisey, Lynn Butterly, Arief Suriawinata, Saeed Hassanpour",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.09880v2 Announce Type: replace \nAbstract: Colonoscopy screening effectively identifies and removes polyps before they progress to colorectal cancer (CRC), but current follow-up guidelines rely primarily on histopathological features, overlooking other important CRC risk factors. Variability in polyp characterization among pathologists also hinders consistent surveillance decisions. Advances in digital pathology and deep learning enable the integration of pathology slides and medical records for more accurate CRC risk prediction. Using data from the New Hampshire Colonoscopy Registry, including longitudinal follow-up, we adapted a transformer-based model for histopathology image analysis to predict 5-year CRC risk. We further explored multi-modal fusion strategies to combine clinical records with deep learning-derived image features. Training the model to predict intermediate clinical variables improved 5-year CRC risk prediction (AUC = 0.630) compared to direct prediction (AUC = 0.615, p = 0.013). Incorporating both imaging and non-imaging data, without requiring manual slide review, further improved performance (AUC = 0.674) compared to traditional features from colonoscopy and microscopy reports (AUC = 0.655, p = 0.001). These results highlight the value of integrating diverse data modalities with computational methods to enhance CRC risk stratification."
      },
      {
        "id": "oai:arXiv.org:2410.11709v3",
        "title": "GeOT: A spatially explicit framework for evaluating spatio-temporal predictions",
        "link": "https://arxiv.org/abs/2410.11709",
        "author": "Nina Wiedemann, Th\\'eo Uscidda, Martin Raubal",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.11709v3 Announce Type: replace \nAbstract: When predicting observations across space and time, the spatial layout of errors impacts a model's real-world utility. For instance, in bike sharing demand prediction, error patterns translate to relocation costs. However, commonly used error metrics in GeoAI evaluate predictions point-wise, neglecting effects such as spatial heterogeneity, autocorrelation, and the Modifiable Areal Unit Problem. We put forward Optimal Transport (OT) as a spatial evaluation metric and loss function. The proposed framework, called GeOT, assesses the performance of prediction models by quantifying the transport costs associated with their prediction errors. Through experiments on real and synthetic data, we demonstrate that 1) the spatial distribution of prediction errors relates to real-world costs in many applications, 2) OT captures these spatial costs more accurately than existing metrics, and 3) OT enhances comparability across spatial and temporal scales. Finally, we advocate for leveraging OT as a loss function in neural networks to improve the spatial accuracy of predictions. Experiments with bike sharing, charging station, and traffic datasets show that spatial costs are significantly reduced with only marginal changes to non-spatial error metrics. Thus, this approach not only offers a spatially explicit tool for model evaluation and selection, but also integrates spatial considerations into model training. All code is available at https://github.com/mie-lab/geospatialOT."
      },
      {
        "id": "oai:arXiv.org:2410.12877v2",
        "title": "Improving Instruction-Following in Language Models through Activation Steering",
        "link": "https://arxiv.org/abs/2410.12877",
        "author": "Alessandro Stolfo, Vidhisha Balachandran, Safoora Yousefi, Eric Horvitz, Besmira Nushi",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.12877v2 Announce Type: replace \nAbstract: The ability to follow instructions is crucial for numerous real-world applications of language models. In pursuit of deeper insights and more powerful capabilities, we derive instruction-specific vector representations from language models and use them to steer models accordingly. These vectors are computed as the difference in activations between inputs with and without instructions, enabling a modular approach to activation steering. We demonstrate how this method can enhance model adherence to constraints such as output format, length, and word inclusion, providing inference-time control over instruction following. Our experiments across four models demonstrate how we can use the activation vectors to guide models to follow constraints even without explicit instructions and to enhance performance when instructions are present. Additionally, we explore the compositionality of activation steering, successfully applying multiple instructions simultaneously. Finally, we demonstrate that steering vectors computed on instruction-tuned models can transfer to improve base models. Our findings demonstrate that activation steering offers a practical and scalable approach for fine-grained control in language generation. Our code and data are available at https://github.com/microsoft/llm-steer-instruct."
      },
      {
        "id": "oai:arXiv.org:2410.14763v2",
        "title": "Enabling Scalable Evaluation of Bias Patterns in Medical LLMs",
        "link": "https://arxiv.org/abs/2410.14763",
        "author": "Hamed Fayyaz, Raphael Poulain, Rahmatollah Beheshti",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.14763v2 Announce Type: replace \nAbstract: Large language models (LLMs) have shown impressive potential in helping with numerous medical challenges. Deploying LLMs in high-stakes applications such as medicine, however, brings in many concerns. One major area of concern relates to biased behaviors of LLMs in medical applications, leading to unfair treatment of individuals. To pave the way for the responsible and impactful deployment of Med LLMs, rigorous evaluation is a key prerequisite. Due to the huge complexity and variability of different medical scenarios, existing work in this domain has primarily relied on using manually crafted datasets for bias evaluation. In this study, we present a new method to scale up such bias evaluations by automatically generating test cases based on rigorous medical evidence. We specifically target the challenges of a) domain-specificity of bias characterization, b) hallucinating while generating the test cases, and c) various dependencies between the health outcomes and sensitive attributes. To that end, we offer new methods to address these challenges integrated with our generative pipeline, using medical knowledge graphs, medical ontologies, and customized general LLM evaluation frameworks in our method. Through a series of extensive experiments, we show that the test cases generated by our proposed method can effectively reveal bias patterns in Med LLMs at larger and more flexible scales than human-crafted datasets. We publish a large bias evaluation dataset using our pipeline, which is dedicated to a few medical case studies. A live demo of our application for vignette generation is available at https://vignette.streamlit.app. Our code is also available at https://github.com/healthylaife/autofair."
      },
      {
        "id": "oai:arXiv.org:2410.15038v3",
        "title": "A Multimodal Vision Foundation Model for Clinical Dermatology",
        "link": "https://arxiv.org/abs/2410.15038",
        "author": "Siyuan Yan, Zhen Yu, Clare Primiero, Cristina Vico-Alonso, Zhonghua Wang, Litao Yang, Philipp Tschandl, Ming Hu, Lie Ju, Gin Tan, Vincent Tang, Aik Beng Ng, David Powell, Paul Bonnington, Simon See, Elisabetta Magnaterra, Peter Ferguson, Jennifer Nguyen, Pascale Guitera, Jose Banuls, Monika Janda, Victoria Mar, Harald Kittler, H. Peter Soyer, Zongyuan Ge",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.15038v3 Announce Type: replace \nAbstract: Diagnosing and treating skin diseases require advanced visual skills across domains and the ability to synthesize information from multiple imaging modalities. While current deep learning models excel at specific tasks like skin cancer diagnosis from dermoscopic images, they struggle to meet the complex, multimodal requirements of clinical practice. Here, we introduce PanDerm, a multimodal dermatology foundation model pretrained through self-supervised learning on over 2 million real-world skin disease images from 11 clinical institutions across 4 imaging modalities. We evaluated PanDerm on 28 diverse benchmarks, including skin cancer screening, risk stratification, differential diagnosis of common and rare skin conditions, lesion segmentation, longitudinal monitoring, and metastasis prediction and prognosis. PanDerm achieved state-of-the-art performance across all evaluated tasks, often outperforming existing models when using only 10% of labeled data. We conducted three reader studies to assess PanDerm's potential clinical utility. PanDerm outperformed clinicians by 10.2% in early-stage melanoma detection through longitudinal analysis, improved clinicians' skin cancer diagnostic accuracy by 11% on dermoscopy images, and enhanced non-dermatologist healthcare providers' differential diagnosis by 16.5% across 128 skin conditions on clinical photographs. These results demonstrate PanDerm's potential to improve patient care across diverse clinical scenarios and serve as a model for developing multimodal foundation models in other medical specialties, potentially accelerating the integration of AI support in healthcare. The code can be found at https://github.com/SiyuanYan1/PanDerm."
      },
      {
        "id": "oai:arXiv.org:2410.17509v2",
        "title": "WAGLE: Strategic Weight Attribution for Effective and Modular Unlearning in Large Language Models",
        "link": "https://arxiv.org/abs/2410.17509",
        "author": "Jinghan Jia, Jiancheng Liu, Yihua Zhang, Parikshit Ram, Nathalie Baracaldo, Sijia Liu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.17509v2 Announce Type: replace \nAbstract: The need for effective unlearning mechanisms in large language models (LLMs) is increasingly urgent, driven by the necessity to adhere to data regulations and foster ethical generative AI practices. Despite growing interest of LLM unlearning, much of the existing research has focused on varied unlearning method designs to boost effectiveness and efficiency. However, the inherent relationship between model weights and LLM unlearning has not been extensively examined. In this paper, we systematically explore how model weights interact with unlearning processes in LLMs and we design the weight attribution-guided LLM unlearning method, WAGLE, which unveils the interconnections between 'influence' of weights and 'influence' of data to forget and retain in LLM generation. By strategically guiding the LLM unlearning across different types of unlearning methods and tasks, WAGLE can erase the undesired content, while maintaining the performance of the original tasks. We refer to the weight attribution-guided LLM unlearning method as WAGLE, which unveils the interconnections between 'influence' of weights and 'influence' of data to forget and retain in LLM generation. Our extensive experiments show that WAGLE boosts unlearning performance across a range of LLM unlearning methods such as gradient difference and (negative) preference optimization, applications such as fictitious unlearning, malicious use prevention, and copyrighted information removal, and models including Zephyr-7b-beta and Llama2-7b. To the best of our knowledge, our work offers the first principled method for attributing and pinpointing the influential weights in enhancing LLM unlearning. It stands in contrast to previous methods that lack weight attribution and simpler weight attribution techniques."
      },
      {
        "id": "oai:arXiv.org:2410.18194v2",
        "title": "ZIP-FIT: Embedding-Free Data Selection via Compression-Based Alignment",
        "link": "https://arxiv.org/abs/2410.18194",
        "author": "Elyas Obbad, Iddah Mlauzi, Brando Miranda, Rylan Schaeffer, Kamal Obbad, Suhana Bedi, Sanmi Koyejo",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.18194v2 Announce Type: replace \nAbstract: Data selection is crucial for optimizing language model (LM) performance on specific tasks, yet most existing methods fail to effectively consider the target task distribution.\n  Current approaches either ignore task-specific requirements entirely or rely on approximations that fail to capture the nuanced patterns needed for tasks like Autoformalization or code generation.\n  Methods that do consider the target distribution often rely on simplistic, sometimes noisy, representations, like hashed n-gram features, which can lead to collisions and introduce noise.\n  We introduce ZIP-FIT, a data selection framework that uses gzip compression to directly measure alignment between potential training data and the target task distribution.\n  In extensive evaluations on Autoformalization and Python code generation, ZIP-FIT significantly outperforms leading baselines like DSIR and D4.\n  Models trained on ZIP-FIT-selected data achieve their lowest cross-entropy loss up to 85.1\\% faster than baselines, demonstrating that better task alignment leads to more efficient learning.\n  In addition, ZIP-FIT performs selection up to 65.8\\% faster than DSIR and two orders of magnitude faster than D4.\n  Notably, ZIP-FIT shows that smaller, well-aligned datasets often outperform larger but less targeted ones, demonstrating that a small amount of higher quality data is superior to a large amount of lower quality data.\n  Our results imply that task-aware data selection is crucial for efficient domain adaptation, and that compression offers a principled way to measure task alignment.\n  By showing that targeted data selection can dramatically improve task-specific performance, our work provides new insights into the relationship between data quality, task alignment, and model learning efficiency."
      },
      {
        "id": "oai:arXiv.org:2410.18491v2",
        "title": "ChineseSafe: A Chinese Benchmark for Evaluating Safety in Large Language Models",
        "link": "https://arxiv.org/abs/2410.18491",
        "author": "Hengxiang Zhang, Hongfu Gao, Qiang Hu, Guanhua Chen, Lili Yang, Bingyi Jing, Hongxin Wei, Bing Wang, Haifeng Bai, Lei Yang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.18491v2 Announce Type: replace \nAbstract: With the rapid development of Large language models (LLMs), understanding the capabilities of LLMs in identifying unsafe content has become increasingly important. While previous works have introduced several benchmarks to evaluate the safety risk of LLMs, the community still has a limited understanding of current LLMs' capability to recognize illegal and unsafe content in Chinese contexts. In this work, we present a Chinese safety benchmark (ChineseSafe) to facilitate research on the content safety of large language models. To align with the regulations for Chinese Internet content moderation, our ChineseSafe contains 205,034 examples across 4 classes and 10 sub-classes of safety issues. For Chinese contexts, we add several special types of illegal content: political sensitivity, pornography, and variant/homophonic words. Moreover, we employ two methods to evaluate the legal risks of popular LLMs, including open-sourced models and APIs. The results reveal that many LLMs exhibit vulnerability to certain types of safety issues, leading to legal risks in China. Our work provides a guideline for developers and researchers to facilitate the safety of LLMs. Our results are also available at https://huggingface.co/spaces/SUSTech/ChineseSafe-Benchmark. Additionally, we release a test set comprising 200,000 examples, which is publicly accessible at https://huggingface.co/datasets/SUSTech/ChineseSafe."
      },
      {
        "id": "oai:arXiv.org:2410.19631v4",
        "title": "Efficient Biological Data Acquisition through Inference Set Design",
        "link": "https://arxiv.org/abs/2410.19631",
        "author": "Ihor Neporozhnii, Julien Roy, Emmanuel Bengio, Jason Hartford",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.19631v4 Announce Type: replace \nAbstract: In drug discovery, highly automated high-throughput laboratories are used to screen a large number of compounds in search of effective drugs. These experiments are expensive, so one might hope to reduce their cost by only experimenting on a subset of the compounds, and predicting the outcomes of the remaining experiments. In this work, we model this scenario as a sequential subset selection problem: we aim to select the smallest set of candidates in order to achieve some desired level of accuracy for the system as a whole. Our key observation is that, if there is heterogeneity in the difficulty of the prediction problem across the input space, selectively obtaining the labels for the hardest examples in the acquisition pool will leave only the relatively easy examples to remain in the inference set, leading to better overall system performance. We call this mechanism inference set design, and propose the use of a confidence-based active learning solution to prune out these challenging examples. Our algorithm includes an explicit stopping criterion that interrupts the acquisition loop when it is sufficiently confident that the system has reached the target performance. Our empirical studies on image and molecular datasets, as well as a real-world large-scale biological assay, show that active learning for inference set design leads to significant reduction in experimental cost while retaining high system performance."
      },
      {
        "id": "oai:arXiv.org:2410.21491v2",
        "title": "Trustworthiness of Stochastic Gradient Descent in Distributed Learning",
        "link": "https://arxiv.org/abs/2410.21491",
        "author": "Hongyang Li, Caesar Wu, Mohammed Chadli, Said Mammar, Pascal Bouvry",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.21491v2 Announce Type: replace \nAbstract: Distributed learning (DL) uses multiple nodes to accelerate training, enabling efficient optimization of large-scale models. Stochastic Gradient Descent (SGD), a key optimization algorithm, plays a central role in this process. However, communication bottlenecks often limit scalability and efficiency, leading to increasing adoption of compressed SGD techniques to alleviate these challenges. Despite addressing communication overheads, compressed SGD introduces trustworthiness concerns, as gradient exchanges among nodes are vulnerable to attacks like gradient inversion (GradInv) and membership inference attacks (MIA). The trustworthiness of compressed SGD remains unexplored, leaving important questions about its reliability unanswered.\n  In this paper, we provide a trustworthiness evaluation of compressed versus uncompressed SGD. Specifically, we conducted empirical studies using GradInv attacks, revealing that compressed SGD demonstrates significantly higher resistance to privacy leakage compared to uncompressed SGD. In addition, our findings suggest that MIA may not be a reliable metric for assessing privacy risks in distributed learning."
      },
      {
        "id": "oai:arXiv.org:2410.22897v2",
        "title": "A Graph-Based Model for Vehicle-Centric Data Sharing Ecosystem",
        "link": "https://arxiv.org/abs/2410.22897",
        "author": "Haiyue Yuan, Ali Raza, Nikolay Matyunin, Jibesh Patra, Shujun Li",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.22897v2 Announce Type: replace \nAbstract: The development of technologies has prompted a paradigm shift in the automotive industry, with an increasing focus on connected services and autonomous driving capabilities. This transformation allows vehicles to collect and share vast amounts of vehicle-specific and personal data. While these technological advancements offer enhanced user experiences, they also raise privacy concerns. To understand the ecosystem of data collection and sharing in modern vehicles, we adopted the ontology 101 methodology to incorporate information extracted from different sources, including analysis of privacy policies using GPT-4, a small-scale systematic literature review, and an existing ontology, to develop a high-level conceptual graph-based model, aiming to get insights into how modern vehicles handle data exchange among different parties. This serves as a foundational model with the flexibility and scalability to further expand for modelling and analysing data sharing practices across diverse contexts. Two realistic examples were developed to demonstrate the usefulness and effectiveness of discovering insights into privacy regarding vehicle-related data sharing. We also recommend several future research directions, such as exploring advanced ontology languages for reasoning tasks, supporting topological analysis for discovering data privacy risks/concerns, and developing useful tools for comparative analysis, to strengthen the understanding of the vehicle-centric data sharing ecosystem."
      },
      {
        "id": "oai:arXiv.org:2410.24037v2",
        "title": "TPC: Test-time Procrustes Calibration for Diffusion-based Human Image Animation",
        "link": "https://arxiv.org/abs/2410.24037",
        "author": "Sunjae Yoon, Gwanhyeong Koo, Younghwan Lee, Chang D. Yoo",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.24037v2 Announce Type: replace \nAbstract: Human image animation aims to generate a human motion video from the inputs of a reference human image and a target motion video. Current diffusion-based image animation systems exhibit high precision in transferring human identity into targeted motion, yet they still exhibit irregular quality in their outputs. Their optimal precision is achieved only when the physical compositions (i.e., scale and rotation) of the human shapes in the reference image and target pose frame are aligned. In the absence of such alignment, there is a noticeable decline in fidelity and consistency. Especially, in real-world environments, this compositional misalignment commonly occurs, posing significant challenges to the practical usage of current systems. To this end, we propose Test-time Procrustes Calibration (TPC), which enhances the robustness of diffusion-based image animation systems by maintaining optimal performance even when faced with compositional misalignment, effectively addressing real-world scenarios. The TPC provides a calibrated reference image for the diffusion model, enhancing its capability to understand the correspondence between human shapes in the reference and target images. Our method is simple and can be applied to any diffusion-based image animation system in a model-agnostic manner, improving the effectiveness at test time without additional training."
      },
      {
        "id": "oai:arXiv.org:2411.00246v2",
        "title": "ResiDual Transformer Alignment with Spectral Decomposition",
        "link": "https://arxiv.org/abs/2411.00246",
        "author": "Lorenzo Basile, Valentino Maiorca, Luca Bortolussi, Emanuele Rodol\\`a, Francesco Locatello",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.00246v2 Announce Type: replace \nAbstract: When examined through the lens of their residual streams, a puzzling property emerges in transformer networks: residual contributions (e.g., attention heads) sometimes specialize in specific tasks or input attributes. In this paper, we analyze this phenomenon in vision transformers, focusing on the spectral geometry of residuals, and explore its implications for modality alignment in vision-language models. First, we link it to the intrinsically low-dimensional structure of visual head representations, zooming into their principal components and showing that they encode specialized roles across a wide variety of input data distributions. Then, we analyze the effect of head specialization in multimodal models, focusing on how improved alignment between text and specialized heads impacts zero-shot classification performance. This specialization-performance link consistently holds across diverse pre-training data, network sizes, and objectives, demonstrating a powerful new mechanism for boosting zero-shot classification through targeted alignment. Ultimately, we translate these insights into actionable terms by introducing ResiDual, a technique for spectral alignment of the residual stream. Much like panning for gold, it lets the noise from irrelevant unit principal components (i.e., attributes) wash away to amplify task-relevant ones. Remarkably, this dual perspective on modality alignment yields fine-tuning level performance on different data distributions while modelling an extremely interpretable and parameter-efficient transformation, as we extensively show on 70 pre-trained network-dataset combinations (7 models, 10 datasets)."
      },
      {
        "id": "oai:arXiv.org:2411.01894v2",
        "title": "Efficient Active Imitation Learning with Random Network Distillation",
        "link": "https://arxiv.org/abs/2411.01894",
        "author": "Emilien Bir\\'e, Anthony Kobanda, Ludovic Denoyer, R\\'emy Portelas",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.01894v2 Announce Type: replace \nAbstract: Developing agents for complex and underspecified tasks, where no clear objective exists, remains challenging but offers many opportunities. This is especially true in video games, where simulated players (bots) need to play realistically, and there is no clear reward to evaluate them. While imitation learning has shown promise in such domains, these methods often fail when agents encounter out-of-distribution scenarios during deployment. Expanding the training dataset is a common solution, but it becomes impractical or costly when relying on human demonstrations. This article addresses active imitation learning, aiming to trigger expert intervention only when necessary, reducing the need for constant expert input along training. We introduce Random Network Distillation DAgger (RND-DAgger), a new active imitation learning method that limits expert querying by using a learned state-based out-of-distribution measure to trigger interventions. This approach avoids frequent expert-agent action comparisons, thus making the expert intervene only when it is useful. We evaluate RND-DAgger against traditional imitation learning and other active approaches in 3D video games (racing and third-person navigation) and in a robotic locomotion task and show that RND-DAgger surpasses previous methods by reducing expert queries. https://sites.google.com/view/rnd-dagger"
      },
      {
        "id": "oai:arXiv.org:2411.06770v3",
        "title": "Sketched Adaptive Federated Deep Learning: A Sharp Convergence Analysis",
        "link": "https://arxiv.org/abs/2411.06770",
        "author": "Zhijie Chen, Qiaobo Li, Arindam Banerjee",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.06770v3 Announce Type: replace \nAbstract: Combining gradient compression methods (e.g., CountSketch, quantization) and adaptive optimizers (e.g., Adam, AMSGrad) is a desirable goal in federated learning (FL), with potential benefits on both fewer communication rounds and less per-round communication. In spite of the preliminary empirical success of sketched adaptive methods, existing convergence analyses show the communication cost to have a linear dependence on the ambient dimension, i.e., number of parameters, which is prohibitively high for modern deep learning models. In this work, we introduce specific sketched adaptive federated learning (SAFL) algorithms and, as our main contribution, provide theoretical convergence analyses in different FL settings with guarantees on communication cost depending only logarithmically (instead of linearly) on the ambient dimension. Unlike existing analyses, we show that the entry-wise sketching noise existent in the preconditioners and the first moments of SAFL can be implicitly addressed by leveraging the recently-popularized anisotropic curvatures in deep learning losses, e.g., fast decaying loss Hessian eigen-values. In the i.i.d. client setting of FL, we show that SAFL achieves asymptotic $O(1/\\sqrt{T})$ convergence, and converges faster in the initial epochs. In the non-i.i.d. client setting, where non-adaptive methods lack convergence guarantees, we show that SACFL (SAFL with clipping) algorithms can provably converge in spite of the additional heavy-tailed noise. Our theoretical claims are supported by empirical studies on vision and language tasks, and in both fine-tuning and training-from-scratch regimes. Surprisingly, as a by-product of our analysis, the proposed SAFL methods are competitive with the state-of-the-art communication-efficient federated learning algorithms based on error feedback."
      },
      {
        "id": "oai:arXiv.org:2411.07107v3",
        "title": "Training Neural Networks as Recognizers of Formal Languages",
        "link": "https://arxiv.org/abs/2411.07107",
        "author": "Alexandra Butoi, Ghazal Khalighinejad, Anej Svete, Josef Valvoda, Ryan Cotterell, Brian DuSell",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.07107v3 Announce Type: replace \nAbstract: Characterizing the computational power of neural network architectures in terms of formal language theory remains a crucial line of research, as it describes lower and upper bounds on the reasoning capabilities of modern AI. However, when empirically testing these bounds, existing work often leaves a discrepancy between experiments and the formal claims they are meant to support. The problem is that formal language theory pertains specifically to recognizers: machines that receive a string as input and classify whether it belongs to a language. On the other hand, it is common instead to evaluate language models on proxy tasks, e.g., language modeling or sequence-to-sequence transduction, that are similar in only an informal sense to the underlying theory. We correct this mismatch by training and evaluating neural networks directly as binary classifiers of strings, using a general method that can be applied to a wide variety of languages. As part of this, we extend an algorithm recently proposed by Sn{\\ae}bjarnarson et al. (2025) for efficient length-controlled sampling of strings from regular languages. We provide results on a variety of languages across the Chomsky hierarchy for three neural architectures: a simple RNN, an LSTM, and a causally-masked transformer. We find that the RNN and LSTM often outperform the transformer, and that auxiliary training objectives such as language modeling can help, although no single objective uniformly improves performance across languages and architectures. Our contributions will facilitate theoretically sound empirical testing of language recognition claims in future work. We have released our datasets as a benchmark called FLaRe (Formal Language Recognition), along with our code."
      },
      {
        "id": "oai:arXiv.org:2411.08395v2",
        "title": "MambaXCTrack: Mamba-based Tracker with SSM Cross-correlation and Motion Prompt for Ultrasound Needle Tracking",
        "link": "https://arxiv.org/abs/2411.08395",
        "author": "Yuelin Zhang, Long Lei, Wanquan Yan, Tianyi Zhang, Raymond Shing-Yan Tang, Shing Shin Cheng",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.08395v2 Announce Type: replace \nAbstract: Ultrasound (US)-guided needle insertion is widely employed in percutaneous interventions. However, providing feedback on the needle tip position via US imaging presents challenges due to noise, artifacts, and the thin imaging plane of US, which degrades needle features and leads to intermittent tip visibility. In this paper, a Mamba-based US needle tracker MambaXCTrack utilizing structured state space models cross-correlation (SSMX-Corr) and implicit motion prompt is proposed, which is the first application of Mamba in US needle tracking. The SSMX-Corr enhances cross-correlation by long-range modeling and global searching of distant semantic features between template and search maps, benefiting the tracking under noise and artifacts by implicitly learning potential distant semantic cues. By combining with cross-map interleaved scan (CIS), local pixel-wise interaction with positional inductive bias can also be introduced to SSMX-Corr. The implicit low-level motion descriptor is proposed as a non-visual prompt to enhance tracking robustness, addressing the intermittent tip visibility problem. Extensive experiments on a dataset with motorized needle insertion in both phantom and tissue samples demonstrate that the proposed tracker outperforms other state-of-the-art trackers while ablation studies further highlight the effectiveness of each proposed tracking module."
      },
      {
        "id": "oai:arXiv.org:2411.12044v2",
        "title": "ITACLIP: Boosting Training-Free Semantic Segmentation with Image, Text, and Architectural Enhancements",
        "link": "https://arxiv.org/abs/2411.12044",
        "author": "M. Arda Ayd{\\i}n, Efe Mert \\c{C}{\\i}rpar, Elvin Abdinli, Gozde Unal, Yusuf H. Sahin",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.12044v2 Announce Type: replace \nAbstract: Recent advances in foundational Vision Language Models (VLMs) have reshaped the evaluation paradigm in computer vision tasks. These foundational models, especially CLIP, have accelerated research in open-vocabulary computer vision tasks, including Open-Vocabulary Semantic Segmentation (OVSS). Although the initial results are promising, the dense prediction capabilities of VLMs still require further improvement. In this study, we enhance the semantic segmentation performance of CLIP by introducing new modules and modifications: 1) architectural changes in the last layer of ViT and the incorporation of attention maps from the middle layers with the last layer, 2) Image Engineering: applying data augmentations to enrich input image representations, and 3) using Large Language Models (LLMs) to generate definitions and synonyms for each class name to leverage CLIP's open-vocabulary capabilities. Our training-free method, ITACLIP, outperforms current state-of-the-art approaches on segmentation benchmarks such as COCO-Stuff, COCO-Object, Pascal Context, and Pascal VOC. Our code is available at https://github.com/m-arda-aydn/ITACLIP."
      },
      {
        "id": "oai:arXiv.org:2411.13055v2",
        "title": "Hardware Scaling Trends and Diminishing Returns in Large-Scale Distributed Training",
        "link": "https://arxiv.org/abs/2411.13055",
        "author": "Jared Fernandez, Luca Wehrstedt, Leonid Shamis, Mostafa Elhoushi, Kalyan Saladi, Yonatan Bisk, Emma Strubell, Jacob Kahn",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.13055v2 Announce Type: replace \nAbstract: Dramatic increases in the capabilities of neural network models in recent years are driven by scaling model size, training data, and corresponding computational resources. To develop the exceedingly large networks required in modern applications, such as large language models (LLMs), model training is distributed across tens of thousands of hardware accelerators (e.g. GPUs), requiring orchestration of computation and communication across large computing clusters. In this work, we demonstrate that careful consideration of hardware configuration and parallelization strategy is critical for effective (i.e. compute- and cost-efficient) scaling of model size, training data, and total computation. We conduct an extensive empirical study of the performance of large-scale LLM training workloads across model size, hardware configurations, and distributed parallelization strategies. We demonstrate that: (1) beyond certain scales, overhead incurred from certain distributed communication strategies leads parallelization strategies previously thought to be sub-optimal in fact become preferable; and (2) scaling the total number of accelerators for large model training quickly yields diminishing returns even when hardware and parallelization strategies are properly optimized, implying poor marginal performance per additional unit of power or GPU-hour."
      },
      {
        "id": "oai:arXiv.org:2411.14585v2",
        "title": "Efficient Spatio-Temporal Signal Recognition on Edge Devices Using PointLCA-Net",
        "link": "https://arxiv.org/abs/2411.14585",
        "author": "Sanaz Mahmoodi Takaghaj, Jack Sampson",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.14585v2 Announce Type: replace \nAbstract: Recent advancements in machine learning, particularly through deep learning architectures like PointNet, have transformed the processing of three-dimensional (3D) point clouds, significantly improving 3D object classification and segmentation tasks. While 3D point clouds provide detailed spatial information, spatio-temporal signals introduce a dynamic element that accounts for changes over time. However, applying deep learning techniques to spatio-temporal signals and deploying them on edge devices presents challenges, including real-time processing, memory capacity, and power consumption. To address these issues, this paper presents a novel approach that combines PointNet's feature extraction with the in-memory computing capabilities and energy efficiency of neuromorphic systems for spatio-temporal signal recognition. The proposed method consists of a two-stage process: in the first stage, PointNet extracts features from the spatio-temporal signals, which are then stored in non-volatile memristor crossbar arrays. In the second stage, these features are processed by a single-layer spiking neural encoder-decoder that employs the Locally Competitive Algorithm (LCA) for efficient encoding and classification. This work integrates the strengths of both PointNet and LCA, enhancing computational efficiency and energy performance on edge devices. PointLCA-Net achieves high recognition accuracy for spatio-temporal data with substantially lower energy burden during both inference and training than comparable approaches, thus advancing the deployment of advanced neural architectures in energy-constrained environments."
      },
      {
        "id": "oai:arXiv.org:2411.15806v2",
        "title": "Broad Critic Deep Actor Reinforcement Learning for Continuous Control",
        "link": "https://arxiv.org/abs/2411.15806",
        "author": "Shiron Thalagala, Pak Kin Wong, Xiaozheng Wang, Tianang Sun",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.15806v2 Announce Type: replace \nAbstract: In the domain of continuous control, deep reinforcement learning (DRL) demonstrates promising results. However, the dependence of DRL on deep neural networks (DNNs) results in the demand for extensive data and increased computational cost. To address this issue, a novel hybrid actor-critic reinforcement learning (RL) framework is introduced. The proposed framework integrates the broad learning system (BLS) with DNN, aiming to merge the strengths of both distinct architectural paradigms. Specifically, the critic network employs BLS for rapid value estimation via ridge regression, while the actor network retains the DNN structure to optimize policy gradients. This hybrid design is generalizable and can enhance existing actor-critic algorithms. To demonstrate its versatility, the proposed framework is integrated into three widely used actor-critic algorithms -- deep deterministic policy gradient (DDPG), soft actor-critic (SAC), and twin delayed DDPG (TD3), resulting in BLS-augmented variants. Experimental results reveal that all BLS-enhanced versions surpass their original counterparts in terms of training efficiency and accuracy. These improvements highlight the suitability of the proposed framework for real-time control scenarios, where computational efficiency and rapid adaptation are critical."
      },
      {
        "id": "oai:arXiv.org:2411.16219v2",
        "title": "Weakly Supervised Panoptic Segmentation for Defect-Based Grading of Fresh Produce",
        "link": "https://arxiv.org/abs/2411.16219",
        "author": "Manuel Knott, Divinefavour Odion, Sameer Sontakke, Anup Karwa, Thijs Defraeye",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.16219v2 Announce Type: replace \nAbstract: Visual inspection for defect grading in agricultural supply chains is crucial but traditionally labor-intensive and error-prone. Automated computer vision methods typically require extensively annotated datasets, which are often unavailable in decentralized supply chains. We address this challenge by evaluating the Segment Anything Model (SAM) to generate dense panoptic segmentation masks from sparse annotations. These dense predictions are then used to train a supervised panoptic segmentation model. Focusing on banana surface defects (bruises and scars), we validate our approach using 476 field images annotated with 1440 defects. While SAM-generated masks generally align with human annotations, substantially reducing annotation effort, we explicitly identify failure cases associated with specific defect sizes and shapes. Despite these limitations, our approach offers practical estimates of defect number and relative size from panoptic masks, underscoring the potential and current boundaries of foundation models for defect quantification in low-data agricultural scenarios. GitHub: https://github.com/manuelknott/banana-defect-segmentation"
      },
      {
        "id": "oai:arXiv.org:2411.16549v2",
        "title": "In-Context Deep Learning via Transformer Models",
        "link": "https://arxiv.org/abs/2411.16549",
        "author": "Weimin Wu, Maojiang Su, Jerry Yao-Chieh Hu, Zhao Song, Han Liu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.16549v2 Announce Type: replace \nAbstract: We investigate the transformer's capability to simulate the training process of deep models via in-context learning (ICL), i.e., in-context deep learning. Our key contribution is providing a positive example of using a transformer to train a deep neural network by gradient descent in an implicit fashion via ICL. Specifically, we provide an explicit construction of a $(2N+4)L$-layer transformer capable of simulating $L$ gradient descent steps of an $N$-layer ReLU network through ICL. We also give the theoretical guarantees for the approximation within any given error and the convergence of the ICL gradient descent. Additionally, we extend our analysis to the more practical setting using Softmax-based transformers. We validate our findings on synthetic datasets for 3-layer, 4-layer, and 6-layer neural networks. The results show that ICL performance matches that of direct training."
      },
      {
        "id": "oai:arXiv.org:2411.16800v4",
        "title": "Phys4DGen: Physics-Compliant 4D Generation with Multi-Material Composition Perception",
        "link": "https://arxiv.org/abs/2411.16800",
        "author": "Jiajing Lin, Zhenzhong Wang, Dejun Xu, Shu Jiang, YunPeng Gong, Min Jiang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.16800v4 Announce Type: replace \nAbstract: 4D content generation aims to create dynamically evolving 3D content that responds to specific input objects such as images or 3D representations. Current approaches typically incorporate physical priors to animate 3D representations, but these methods suffer from significant limitations: they not only require users lacking physics expertise to manually specify material properties but also struggle to effectively handle the generation of multi-material composite objects. To address these challenges, we propose Phys4DGen, a novel 4D generation framework that integrates multi-material composition perception with physical simulation. The framework achieves automated, physically plausible 4D generation through three innovative modules: first, the 3D Material Grouping module partitions heterogeneous material regions on 3D representation surfaces via semantic segmentation; second, the Internal Physical Structure Discovery module constructs the mechanical structure of object interiors; finally, we distill physical prior knowledge from multimodal large language models to enable rapid and automatic material properties identification for both objects' surfaces and interiors. Experiments on both synthetic and real-world datasets demonstrate that Phys4DGen can generate high-fidelity 4D content with physical realism in open-world scenarios, significantly outperforming state-of-the-art methods."
      },
      {
        "id": "oai:arXiv.org:2412.00664v2",
        "title": "Improving Decoupled Posterior Sampling for Inverse Problems using Data Consistency Constraint",
        "link": "https://arxiv.org/abs/2412.00664",
        "author": "Zhi Qi, Shihong Yuan, Yulin Yuan, Linling Kuang, Yoshiyuki Kabashima, Xiangming Meng",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.00664v2 Announce Type: replace \nAbstract: Diffusion models have shown strong performances in solving inverse problems through posterior sampling while they suffer from errors during earlier steps. To mitigate this issue, several Decoupled Posterior Sampling methods have been recently proposed. However, the reverse process in these methods ignores measurement information, leading to errors that impede effective optimization in subsequent steps. To solve this problem, we propose Guided Decoupled Posterior Sampling (GDPS) by integrating a data consistency constraint in the reverse process. The constraint performs a smoother transition within the optimization process, facilitating a more effective convergence toward the target distribution. Furthermore, we extend our method to latent diffusion models and Tweedie's formula, demonstrating its scalability. We evaluate GDPS on the FFHQ and ImageNet datasets across various linear and nonlinear tasks under both standard and challenging conditions. Experimental results demonstrate that GDPS achieves state-of-the-art performance, improving accuracy over existing methods."
      },
      {
        "id": "oai:arXiv.org:2412.00905v2",
        "title": "Ref-GS: Directional Factorization for 2D Gaussian Splatting",
        "link": "https://arxiv.org/abs/2412.00905",
        "author": "Youjia Zhang, Anpei Chen, Yumin Wan, Zikai Song, Junqing Yu, Yawei Luo, Wei Yang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.00905v2 Announce Type: replace \nAbstract: In this paper, we introduce Ref-GS, a novel approach for directional light factorization in 2D Gaussian splatting, which enables photorealistic view-dependent appearance rendering and precise geometry recovery. Ref-GS builds upon the deferred rendering of Gaussian splatting and applies directional encoding to the deferred-rendered surface, effectively reducing the ambiguity between orientation and viewing angle. Next, we introduce a spherical Mip-grid to capture varying levels of surface roughness, enabling roughness-aware Gaussian shading. Additionally, we propose a simple yet efficient geometry-lighting factorization that connects geometry and lighting via the vector outer product, significantly reducing renderer overhead when integrating volumetric attributes. Our method achieves superior photorealistic rendering for a range of open-world scenes while also accurately recovering geometry."
      },
      {
        "id": "oai:arXiv.org:2412.00932v2",
        "title": "FIction: 4D Future Interaction Prediction from Video",
        "link": "https://arxiv.org/abs/2412.00932",
        "author": "Kumar Ashutosh, Georgios Pavlakos, Kristen Grauman",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.00932v2 Announce Type: replace \nAbstract: Anticipating how a person will interact with objects in an environment is essential for activity understanding, but existing methods are limited to the 2D space of video frames-capturing physically ungrounded predictions of \"what\" and ignoring the \"where\" and \"how\". We introduce FIction for 4D future interaction prediction from videos. Given an input video of a human activity, the goal is to predict which objects at what 3D locations the person will interact with in the next time period (e.g., cabinet, fridge), and how they will execute that interaction (e.g., poses for bending, reaching, pulling). Our novel model FIction fuses the past video observation of the person's actions and their environment to predict both the \"where\" and \"how\" of future interactions. Through comprehensive experiments on a variety of activities and real-world environments in EgoExo4D, we show that our proposed approach outperforms prior autoregressive and (lifted) 2D video models substantially, with more than 30% relative gains."
      },
      {
        "id": "oai:arXiv.org:2412.02637v2",
        "title": "Words and Action: Modeling Linguistic Leadership in #BlackLivesMatter Communities",
        "link": "https://arxiv.org/abs/2412.02637",
        "author": "Dani Roytburg, Deborah Olorunisola, Sandeep Soni, Lauren Klein",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.02637v2 Announce Type: replace \nAbstract: In this project, we describe a method of modeling semantic leadership across a set of communities associated with the #BlackLivesMatter movement, which has been informed by qualitative research on the structure of social media and Black Twitter in particular. We describe our bespoke approaches to time-binning, community clustering, and connecting communities over time, as well as our adaptation of state-of-the-art approaches to semantic change detection and semantic leadership induction. We find substantial evidence of the leadership role of BLM activists and progressives, as well as Black celebrities. We also find evidence of the sustained engagement of the conservative community with this discourse, suggesting an alternative explanation for how we arrived at the present moment, in which \"anti-woke\" and \"anti-CRT\" bills are being enacted nationwide."
      },
      {
        "id": "oai:arXiv.org:2412.03539v2",
        "title": "NODE-AdvGAN: Improving the transferability and perceptual similarity of adversarial examples by dynamic-system-driven adversarial generative model",
        "link": "https://arxiv.org/abs/2412.03539",
        "author": "Xinheng Xie, Yue Wu, Cuiyu He",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.03539v2 Announce Type: replace \nAbstract: Understanding adversarial examples is crucial for improving model robustness, as they introduce imperceptible perturbations to deceive models. Effective adversarial examples, therefore, offer the potential to train more robust models by eliminating model singularities. We propose NODE-AdvGAN, a novel approach that treats adversarial generation as a continuous process and employs a Neural Ordinary Differential Equation (NODE) to simulate generator dynamics. By mimicking the iterative nature of traditional gradient-based methods, NODE-AdvGAN generates smoother and more precise perturbations that preserve high perceptual similarity when added to benign images. We also propose a new training strategy, NODE-AdvGAN-T, which enhances transferability in black-box attacks by tuning the noise parameters during training. Experiments demonstrate that NODE-AdvGAN and NODE-AdvGAN-T generate more effective adversarial examples that achieve higher attack success rates while preserving better perceptual quality than baseline models."
      },
      {
        "id": "oai:arXiv.org:2412.03572v2",
        "title": "Navigation World Models",
        "link": "https://arxiv.org/abs/2412.03572",
        "author": "Amir Bar, Gaoyue Zhou, Danny Tran, Trevor Darrell, Yann LeCun",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.03572v2 Announce Type: replace \nAbstract: Navigation is a fundamental skill of agents with visual-motor capabilities. We introduce a Navigation World Model (NWM), a controllable video generation model that predicts future visual observations based on past observations and navigation actions. To capture complex environment dynamics, NWM employs a Conditional Diffusion Transformer (CDiT), trained on a diverse collection of egocentric videos of both human and robotic agents, and scaled up to 1 billion parameters. In familiar environments, NWM can plan navigation trajectories by simulating them and evaluating whether they achieve the desired goal. Unlike supervised navigation policies with fixed behavior, NWM can dynamically incorporate constraints during planning. Experiments demonstrate its effectiveness in planning trajectories from scratch or by ranking trajectories sampled from an external policy. Furthermore, NWM leverages its learned visual priors to imagine trajectories in unfamiliar environments from a single input image, making it a flexible and powerful tool for next-generation navigation systems."
      },
      {
        "id": "oai:arXiv.org:2412.03783v2",
        "title": "Expressivity of Representation Learning on Continuous-Time Dynamic Graphs: An Information-Flow Centric Review",
        "link": "https://arxiv.org/abs/2412.03783",
        "author": "Sofiane Ennadir, Gabriela Zarzar Gandler, Filip Cornell, Lele Cao, Oleg Smirnov, Tianze Wang, Levente Z\\'olyomi, Bj\\\"orn Brinne, Sahar Asadi",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.03783v2 Announce Type: replace \nAbstract: Graphs are ubiquitous in real-world applications, ranging from social networks to biological systems, and have inspired the development of Graph Neural Networks (GNNs) for learning expressive representations. While most research has centered on static graphs, many real-world scenarios involve dynamic, temporally evolving graphs, motivating the need for Continuous-Time Dynamic Graph (CTDG) models. This paper provides a comprehensive review of Graph Representation Learning (GRL) on CTDGs with a focus on Self-Supervised Representation Learning (SSRL). We introduce a novel theoretical framework that analyzes the expressivity of CTDG models through an Information-Flow (IF) lens, quantifying their ability to propagate and encode temporal and structural information. Leveraging this framework, we categorize existing CTDG methods based on their suitability for different graph types and application scenarios. Within the same scope, we examine the design of SSRL methods tailored to CTDGs, such as predictive and contrastive approaches, highlighting their potential to mitigate the reliance on labeled data. Empirical evaluations on synthetic and real-world datasets validate our theoretical insights, demonstrating the strengths and limitations of various methods across long-range, bi-partite and community-based graphs. This work offers both a theoretical foundation and practical guidance for selecting and developing CTDG models, advancing the understanding of GRL in dynamic settings."
      },
      {
        "id": "oai:arXiv.org:2412.05074v3",
        "title": "LoFi: Vision-Aided Label Generator for Wi-Fi Localization and Tracking",
        "link": "https://arxiv.org/abs/2412.05074",
        "author": "Zijian Zhao, Tingwei Chen, Fanyi Meng, Zhijie Cai, Hang Li, Xiaoyang Li, Guangxu Zhu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.05074v3 Announce Type: replace \nAbstract: Data-driven Wi-Fi localization and tracking have shown great promise due to their lower reliance on specialized hardware compared to model-based methods. However, most existing data collection techniques provide only coarse-grained ground truth or a limited number of labeled points, significantly hindering the advancement of data-driven approaches. While systems like lidar can deliver precise ground truth, their high costs make them inaccessible to many users. To address these challenges, we propose LoFi, a vision-aided label generator for Wi-Fi localization and tracking. LoFi can generate ground truth position coordinates solely from 2D images, offering high precision, low cost, and ease of use. Utilizing our method, we have compiled a Wi-Fi tracking and localization dataset using the ESP32-S3 and a webcam, which will be open-sourced along with the code upon publication."
      },
      {
        "id": "oai:arXiv.org:2412.10061v2",
        "title": "Quaffure: Real-Time Quasi-Static Neural Hair Simulation",
        "link": "https://arxiv.org/abs/2412.10061",
        "author": "Tuur Stuyck, Gene Wei-Chin Lin, Egor Larionov, Hsiao-yu Chen, Aljaz Bozic, Nikolaos Sarafianos, Doug Roble",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.10061v2 Announce Type: replace \nAbstract: Realistic hair motion is crucial for high-quality avatars, but it is often limited by the computational resources available for real-time applications. To address this challenge, we propose a novel neural approach to predict physically plausible hair deformations that generalizes to various body poses, shapes, and hairstyles. Our model is trained using a self-supervised loss, eliminating the need for expensive data generation and storage. We demonstrate our method's effectiveness through numerous results across a wide range of pose and shape variations, showcasing its robust generalization capabilities and temporally smooth results. Our approach is highly suitable for real-time applications with an inference time of only a few milliseconds on consumer hardware and its ability to scale to predicting the drape of 1000 grooms in 0.3 seconds.\n  Please see our project page here following https://tuurstuyck.github.io/quaffure/quaffure.html"
      },
      {
        "id": "oai:arXiv.org:2412.10209v2",
        "title": "GAF: Gaussian Avatar Reconstruction from Monocular Videos via Multi-view Diffusion",
        "link": "https://arxiv.org/abs/2412.10209",
        "author": "Jiapeng Tang, Davide Davoli, Tobias Kirschstein, Liam Schoneveld, Matthias Niessner",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.10209v2 Announce Type: replace \nAbstract: We propose a novel approach for reconstructing animatable 3D Gaussian avatars from monocular videos captured by commodity devices like smartphones. Photorealistic 3D head avatar reconstruction from such recordings is challenging due to limited observations, which leaves unobserved regions under-constrained and can lead to artifacts in novel views. To address this problem, we introduce a multi-view head diffusion model, leveraging its priors to fill in missing regions and ensure view consistency in Gaussian splatting renderings. To enable precise viewpoint control, we use normal maps rendered from FLAME-based head reconstruction, which provides pixel-aligned inductive biases. We also condition the diffusion model on VAE features extracted from the input image to preserve facial identity and appearance details. For Gaussian avatar reconstruction, we distill multi-view diffusion priors by using iteratively denoised images as pseudo-ground truths, effectively mitigating over-saturation issues. To further improve photorealism, we apply latent upsampling priors to refine the denoised latent before decoding it into an image. We evaluate our method on the NeRSemble dataset, showing that GAF outperforms previous state-of-the-art methods in novel view synthesis. Furthermore, we demonstrate higher-fidelity avatar reconstructions from monocular videos captured on commodity devices."
      },
      {
        "id": "oai:arXiv.org:2412.10423v2",
        "title": "Look Before You Leap: Enhancing Attention and Vigilance Regarding Harmful Content with GuidelineLLM",
        "link": "https://arxiv.org/abs/2412.10423",
        "author": "Shaoqing Zhang, Zhuosheng Zhang, Kehai Chen, Rongxiang Weng, Muyun Yang, Tiejun Zhao, Min Zhang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.10423v2 Announce Type: replace \nAbstract: Despite being empowered with alignment mechanisms, large language models (LLMs) are increasingly vulnerable to emerging jailbreak attacks that can compromise their alignment mechanisms. This vulnerability poses significant risks to real-world applications. Existing work faces challenges in both training efficiency and generalization capabilities (i.e., Reinforcement Learning from Human Feedback and Red-Teaming). Developing effective strategies to enable LLMs to resist continuously evolving jailbreak attempts represents a significant challenge. To address this challenge, we propose a novel defensive paradigm called GuidelineLLM, which assists LLMs in recognizing queries that may have harmful content. Before LLMs respond to a query, GuidelineLLM first identifies potential risks associated with the query, summarizes these risks into guideline suggestions, and then feeds these guidelines to the responding LLMs. Importantly, our approach eliminates the necessity for additional safety fine-tuning of the LLMs themselves; only the GuidelineLLM requires fine-tuning. This characteristic enhances the general applicability of GuidelineLLM across various LLMs. Experimental results demonstrate that GuidelineLLM can significantly reduce the attack success rate (ASR) against LLM (an average reduction of 34.17\\% ASR) while maintaining the usefulness of LLM in handling benign queries. The code is available at https://github.com/sqzhang-lazy/GuidelineLLM."
      },
      {
        "id": "oai:arXiv.org:2412.10834v2",
        "title": "CFSSeg: Closed-Form Solution for Class-Incremental Semantic Segmentation of 2D Images and 3D Point Clouds",
        "link": "https://arxiv.org/abs/2412.10834",
        "author": "Jiaxu Li, Rui Li, Jianyu Qi, Songning Lai, Linpu Lv, Kejia Fan, Jianheng Tang, Yutao Yue, Dongzhan Zhou, Yuanhuai Liu, Huiping Zhuang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.10834v2 Announce Type: replace \nAbstract: 2D images and 3D point clouds are foundational data types for multimedia applications, including real-time video analysis, augmented reality (AR), and 3D scene understanding. Class-incremental semantic segmentation (CSS) requires incrementally learning new semantic categories while retaining prior knowledge. Existing methods typically rely on computationally expensive training based on stochastic gradient descent, employing complex regularization or exemplar replay. However, stochastic gradient descent-based approaches inevitably update the model's weights for past knowledge, leading to catastrophic forgetting, a problem exacerbated by pixel/point-level granularity. To address these challenges, we propose CFSSeg, a novel exemplar-free approach that leverages a closed-form solution, offering a practical and theoretically grounded solution for continual semantic segmentation tasks. This eliminates the need for iterative gradient-based optimization and storage of past data, requiring only a single pass through new samples per step. It not only enhances computational efficiency but also provides a practical solution for dynamic, privacy-sensitive multimedia environments. Extensive experiments on 2D and 3D benchmark datasets such as Pascal VOC2012, S3DIS, and ScanNet demonstrate CFSSeg's superior performance."
      },
      {
        "id": "oai:arXiv.org:2412.10924v4",
        "title": "Tokens, the oft-overlooked appetizer: Large language models, the distributional hypothesis, and meaning",
        "link": "https://arxiv.org/abs/2412.10924",
        "author": "Julia Witte Zimmerman, Denis Hudon, Kathryn Cramer, Alejandro J. Ruiz, Calla Beauregard, Ashley Fehr, Mikaela Irene Fudolig, Bradford Demarest, Yoshi Meke Bird, Milo Z. Trujillo, Christopher M. Danforth, Peter Sheridan Dodds",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.10924v4 Announce Type: replace \nAbstract: Tokenization is a necessary component within the current architecture of many language models, including the transformer-based large language models (LLMs) of Generative AI, yet its impact on the model's cognition is often overlooked. We argue that LLMs demonstrate that the Distributional Hypothesis (DH) is sufficient for reasonably human-like language performance, and that the emergence of human-meaningful linguistic units among tokens and current structural constraints motivate changes to existing, linguistically-agnostic tokenization techniques, particularly with respect to their roles as (1) semantic primitives and as (2) vehicles for conveying salient distributional patterns from human language to the model. We explore tokenizations from a BPE tokenizer; extant model vocabularies obtained from Hugging Face and tiktoken; and the information in exemplar token vectors as they move through the layers of a RoBERTa (large) model. Besides creating sub-optimal semantic building blocks and obscuring the model's access to the necessary distributional patterns, we describe how tokens and pretraining can act as a backdoor for bias and other unwanted content, which current alignment practices may not remediate. Additionally, we relay evidence that the tokenization algorithm's objective function impacts the LLM's cognition, despite being arguably meaningfully insulated from the main system intelligence. [First uploaded to arXiv in December, 2024.]"
      },
      {
        "id": "oai:arXiv.org:2412.11050v2",
        "title": "RAC3: Retrieval-Augmented Corner Case Comprehension for Autonomous Driving with Vision-Language Models",
        "link": "https://arxiv.org/abs/2412.11050",
        "author": "Yujin Wang, Quanfeng Liu, Jiaqi Fan, Jinlong Hong, Hongqing Chu, Mengjian Tian, Bingzhao Gao, Hong Chen",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.11050v2 Announce Type: replace \nAbstract: Understanding and addressing corner cases is essential for ensuring the safety and reliability of autonomous driving systems. Vision-language models (VLMs) play a crucial role in enhancing scenario comprehension, yet they face significant challenges, such as hallucination and insufficient real-world grounding, which compromise their performance in critical driving scenarios. In this work, RAC3, a novel framework designed to enhance the performance of VLMs in corner case comprehension, is proposed. RAC3 integrates a frequency-spatial fusion (FSF) image encoder, cross-modal alignment fine-tuning with hard and semi-hard negative mining, and a fast querying pipeline based on KMeans clustering and hierarchical navigable small world (HNSW) indexing. A multimodal chain-of-thought (CoT) prompting strategy to guide analogical reasoning and reduce hallucinations during inference is introduced. Moreover, an update mechanism is integrated into RAC3 to ensure continual learning within the framework. Extensive experiments on the CODA and NuScenes datasets demonstrate that RAC3 significantly improves corner case comprehension across multiple downstream tasks. Compared to prior state-of-the-art methods, RAC3 achieves the highest final score of 74.46 on the CODA-LM benchmark and shows consistent performance gains when integrated with end-to-end frameworks like DriveLM. These results demonstrate the effectiveness of retrieval-augmented strategies and cross-modal alignment for safer and more interpretable autonomous driving."
      },
      {
        "id": "oai:arXiv.org:2412.11423v2",
        "title": "Nearly Zero-Cost Protection Against Mimicry by Personalized Diffusion Models",
        "link": "https://arxiv.org/abs/2412.11423",
        "author": "Namhyuk Ahn, KiYoon Yoo, Wonhyuk Ahn, Daesik Kim, Seung-Hun Nam",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.11423v2 Announce Type: replace \nAbstract: Recent advancements in diffusion models revolutionize image generation but pose risks of misuse, such as replicating artworks or generating deepfakes. Existing image protection methods, though effective, struggle to balance protection efficacy, invisibility, and latency, thus limiting practical use. We introduce perturbation pre-training to reduce latency and propose a mixture-of-perturbations approach that dynamically adapts to input images to minimize performance degradation. Our novel training strategy computes protection loss across multiple VAE feature spaces, while adaptive targeted protection at inference enhances robustness and invisibility. Experiments show comparable protection performance with improved invisibility and drastically reduced inference time. The code and demo are available at https://webtoon.github.io/impasto"
      },
      {
        "id": "oai:arXiv.org:2412.11609v3",
        "title": "CLIP-SR: Collaborative Linguistic and Image Processing for Super-Resolution",
        "link": "https://arxiv.org/abs/2412.11609",
        "author": "Bingwen Hu, Heng Liu, Zhedong Zheng, Ping Liu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.11609v3 Announce Type: replace \nAbstract: Convolutional Neural Networks (CNNs) have significantly advanced Image Super-Resolution (SR), yet most CNN-based methods rely solely on pixel-based transformations, often leading to artifacts and blurring, particularly under severe downsampling rates (\\eg, 8$\\times$ or 16$\\times$). The recently developed text-guided SR approaches leverage textual descriptions to enhance their detail restoration capabilities but frequently struggle with effectively performing alignment, resulting in semantic inconsistencies. To address these challenges, we propose a multi-modal semantic enhancement framework that integrates textual semantics with visual features, effectively mitigating semantic mismatches and detail losses in highly degraded low-resolution (LR) images. Our method enables realistic, high-quality SR to be performed at large upscaling factors, with a maximum scaling ratio of 16$\\times$. The framework integrates both text and image inputs using the prompt predictor, the Text-Image Fusion Block (TIFBlock), and the Iterative Refinement Module, leveraging Contrastive Language-Image Pretraining (CLIP) features to guide a progressive enhancement process with fine-grained alignment. This synergy produces high-resolution outputs with sharp textures and strong semantic coherence, even at substantial scaling factors. Extensive comparative experiments and ablation studies validate the effectiveness of our approach. Furthermore, by leveraging textual semantics, our method offers a degree of super-resolution editability, allowing for controlled enhancements while preserving semantic consistency. The code is available at https://github.com/hengliusky/CLIP-SR."
      },
      {
        "id": "oai:arXiv.org:2412.12496v4",
        "title": "Faster Vision Mamba is Rebuilt in Minutes via Merged Token Re-training",
        "link": "https://arxiv.org/abs/2412.12496",
        "author": "Mingjia Shi, Yuhao Zhou, Ruiji Yu, Zekai Li, Zhiyuan Liang, Xuanlei Zhao, Xiaojiang Peng, Shanmukha Ramakrishna Vedantam, Wangbo Zhao, Kai Wang, Yang You",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.12496v4 Announce Type: replace \nAbstract: Vision Mamba has shown close to state of the art performance on computer vision tasks, drawing much interest in increasing it's efficiency. A promising approach is token reduction (that has been successfully implemented in ViTs). Pruning informative tokens in Mamba leads to a high loss of key knowledge and degraded performance. An alternative, of merging tokens preserves more information than pruning, also suffers for large compression ratios. Our key insight is that a quick round of retraining after token merging yeilds robust results across various compression ratios. Empirically, pruned Vims only drop up to 0.9% accuracy on ImageNet-1K, recovered by our proposed framework R-MeeTo in our main evaluation. We show how simple and effective the fast recovery can be achieved at minute-level, in particular, a 35.9% accuracy spike over 3 epochs of training on Vim-Ti. Moreover, Vim-Ti/S/B are re-trained within 5/7/17 minutes, and Vim-S only drops 1.3% with 1.2x (up to 1.5x) speed up in inference."
      },
      {
        "id": "oai:arXiv.org:2412.13183v2",
        "title": "Real-time Free-view Human Rendering from Sparse-view RGB Videos using Double Unprojected Textures",
        "link": "https://arxiv.org/abs/2412.13183",
        "author": "Guoxing Sun, Rishabh Dabral, Heming Zhu, Pascal Fua, Christian Theobalt, Marc Habermann",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.13183v2 Announce Type: replace \nAbstract: Real-time free-view human rendering from sparse-view RGB inputs is a challenging task due to the sensor scarcity and the tight time budget. To ensure efficiency, recent methods leverage 2D CNNs operating in texture space to learn rendering primitives. However, they either jointly learn geometry and appearance, or completely ignore sparse image information for geometry estimation, significantly harming visual quality and robustness to unseen body poses. To address these issues, we present Double Unprojected Textures, which at the core disentangles coarse geometric deformation estimation from appearance synthesis, enabling robust and photorealistic 4K rendering in real-time. Specifically, we first introduce a novel image-conditioned template deformation network, which estimates the coarse deformation of the human template from a first unprojected texture. This updated geometry is then used to apply a second and more accurate texture unprojection. The resulting texture map has fewer artifacts and better alignment with input views, which benefits our learning of finer-level geometry and appearance represented by Gaussian splats. We validate the effectiveness and efficiency of the proposed method in quantitative and qualitative experiments, which significantly surpasses other state-of-the-art methods. Project page: https://vcai.mpi-inf.mpg.de/projects/DUT/"
      },
      {
        "id": "oai:arXiv.org:2412.13573v2",
        "title": "Seeking Consistent Flat Minima for Better Domain Generalization via Refining Loss Landscapes",
        "link": "https://arxiv.org/abs/2412.13573",
        "author": "Aodi Li, Liansheng Zhuang, Xiao Long, Minghong Yao, Shafei Wang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.13573v2 Announce Type: replace \nAbstract: Domain generalization aims to learn a model from multiple training domains and generalize it to unseen test domains. Recent theory has shown that seeking the deep models, whose parameters lie in the flat minima of the loss landscape, can significantly reduce the out-of-domain generalization error. However, existing methods often neglect the consistency of loss landscapes in different domains, resulting in models that are not simultaneously in the optimal flat minima in all domains, which limits their generalization ability. To address this issue, this paper proposes an iterative Self-Feedback Training (SFT) framework to seek consistent flat minima that are shared across different domains by progressively refining loss landscapes during training. It alternatively generates a feedback signal by measuring the inconsistency of loss landscapes in different domains and refines these loss landscapes for greater consistency using this feedback signal. Benefiting from the consistency of the flat minima within these refined loss landscapes, our SFT helps achieve better out-of-domain generalization. Extensive experiments on DomainBed demonstrate superior performances of SFT when compared to state-of-the-art sharpness-aware methods and other prevalent DG baselines. On average across five DG benchmarks, SFT surpasses the sharpness-aware minimization by 2.6% with ResNet-50 and 1.5% with ViT-B/16, respectively. The code will be available soon."
      },
      {
        "id": "oai:arXiv.org:2412.14496v2",
        "title": "WikiStyle+: A Multimodal Approach to Content-Style Representation Disentanglement for Artistic Image Stylization",
        "link": "https://arxiv.org/abs/2412.14496",
        "author": "Ma Zhuoqi, Zhang Yixuan, You Zejun, Tian Long, Liu Xiyang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.14496v2 Announce Type: replace \nAbstract: Artistic image stylization aims to render the content provided by text or image with the target style, where content and style decoupling is the key to achieve satisfactory results. However, current methods for content and style disentanglement primarily rely on image supervision, which leads to two problems: 1) models can only support one modality for style or content input;2) incomplete disentanglement resulting in content leakage from the reference image. To address the above issues, this paper proposes a multimodal approach to content-style disentanglement for artistic image stylization. We construct a \\textit{WikiStyle+} dataset consists of artworks with corresponding textual descriptions for style and content. Based on the multimodal dataset, we propose a disentangled representations-guided diffusion model. The disentangled representations are first learned by Q-Formers and then injected into a pre-trained diffusion model using learnable multi-step cross-attention layers. Experimental results show that our method achieves a thorough disentanglement of content and style in reference images under multimodal supervision, thereby enabling more refined stylization that aligns with the artistic characteristics of the reference style. The code of our method will be available upon acceptance."
      },
      {
        "id": "oai:arXiv.org:2412.14819v3",
        "title": "Multi-Level Embedding and Alignment Network with Consistency and Invariance Learning for Cross-View Geo-Localization",
        "link": "https://arxiv.org/abs/2412.14819",
        "author": "Zhongwei Chen, Zhao-Xu Yang, Hai-Jun Rong",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.14819v3 Announce Type: replace \nAbstract: Cross-View Geo-Localization (CVGL) involves determining the localization of drone images by retrieving the most similar GPS-tagged satellite images. However, the imaging gaps between platforms are often significant and the variations in viewpoints are substantial, which limits the ability of existing methods to effectively associate cross-view features and extract consistent and invariant characteristics. Moreover, existing methods often overlook the problem of increased computational and storage requirements when improving model performance. To handle these limitations, we propose a lightweight enhanced alignment network, called the Multi-Level Embedding and Alignment Network (MEAN). The MEAN network uses a progressive multi-level enhancement strategy, global-to-local associations, and cross-domain alignment, enabling feature communication across levels. This allows MEAN to effectively connect features at different levels and learn robust cross-view consistent mappings and modality-invariant features. Moreover, MEAN adopts a shallow backbone network combined with a lightweight branch design, effectively reducing parameter count and computational complexity. Experimental results on the University-1652 and SUES-200 datasets demonstrate that MEAN reduces parameter count by 62.17% and computational complexity by 70.99% compared to state-of-the-art models, while maintaining competitive or even superior performance. Our code and models will be released on https://github.com/ISChenawei/MEAN."
      },
      {
        "id": "oai:arXiv.org:2412.15429v4",
        "title": "Offline Safe Reinforcement Learning Using Trajectory Classification",
        "link": "https://arxiv.org/abs/2412.15429",
        "author": "Ze Gong, Akshat Kumar, Pradeep Varakantham",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.15429v4 Announce Type: replace \nAbstract: Offline safe reinforcement learning (RL) has emerged as a promising approach for learning safe behaviors without engaging in risky online interactions with the environment. Most existing methods in offline safe RL rely on cost constraints at each time step (derived from global cost constraints) and this can result in either overly conservative policies or violation of safety constraints. In this paper, we propose to learn a policy that generates desirable trajectories and avoids undesirable trajectories. To be specific, we first partition the pre-collected dataset of state-action trajectories into desirable and undesirable subsets. Intuitively, the desirable set contains high reward and safe trajectories, and undesirable set contains unsafe trajectories and low-reward safe trajectories. Second, we learn a policy that generates desirable trajectories and avoids undesirable trajectories, where (un)desirability scores are provided by a classifier learnt from the dataset of desirable and undesirable trajectories. This approach bypasses the computational complexity and stability issues of a min-max objective that is employed in existing methods. Theoretically, we also show our approach's strong connections to existing learning paradigms involving human feedback. Finally, we extensively evaluate our method using the DSRL benchmark for offline safe RL. Empirically, our method outperforms competitive baselines, achieving higher rewards and better constraint satisfaction across a wide variety of benchmark tasks."
      },
      {
        "id": "oai:arXiv.org:2412.16003v2",
        "title": "Choose Your Explanation: A Comparison of SHAP and GradCAM in Human Activity Recognition",
        "link": "https://arxiv.org/abs/2412.16003",
        "author": "Felix Tempel, Daniel Groos, Espen Alexander F. Ihlen, Lars Adde, Inga Str\\\"umke",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.16003v2 Announce Type: replace \nAbstract: Explaining machine learning (ML) models using eXplainable AI (XAI) techniques has become essential to make them more transparent and trustworthy. This is especially important in high-stakes domains like healthcare, where understanding model decisions is critical to ensure ethical, sound, and trustworthy outcome predictions. However, users are often confused about which explanability method to choose for their specific use case. We present a comparative analysis of widely used explainability methods, Shapley Additive Explanations (SHAP) and Gradient-weighted Class Activation Mapping (Grad-CAM), within the domain of human activity recognition (HAR) utilizing graph convolutional networks (GCNs). By evaluating these methods on skeleton-based data from two real-world datasets, including a healthcare-critical cerebral palsy (CP) case, this study provides vital insights into both approaches' strengths, limitations, and differences, offering a roadmap for selecting the most appropriate explanation method based on specific models and applications. We quantitatively and quantitatively compare these methods, focusing on feature importance ranking, interpretability, and model sensitivity through perturbation experiments. While SHAP provides detailed input feature attribution, Grad-CAM delivers faster, spatially oriented explanations, making both methods complementary depending on the application's requirements. Given the importance of XAI in enhancing trust and transparency in ML models, particularly in sensitive environments like healthcare, our research demonstrates how SHAP and Grad-CAM could complement each other to provide more interpretable and actionable model explanations."
      },
      {
        "id": "oai:arXiv.org:2412.16148v2",
        "title": "Frequency Is What You Need: Word-frequency Masking Benefits Vision-Language Model Pre-training",
        "link": "https://arxiv.org/abs/2412.16148",
        "author": "Mingliang Liang, Martha Larson",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.16148v2 Announce Type: replace \nAbstract: Vision Language Models (VLMs) can be trained more efficiently if training sets can be reduced in size. Recent work has shown the benefits of masking text during VLM training using a variety of approaches: truncation, random masking, block masking and syntax masking. In this paper, we show that the best masking strategy changes over training epochs and that, given sufficient training epochs. We analyze existing text masking approaches including syntax masking, which is currently the state of the art, and identify the word frequency distribution as important in determining their success. Experiments on a large range of data sets demonstrate that syntax masking is outperformed by other approaches, given sufficient epochs, and that our proposed frequency-based approach, called Contrastive Language-Image Pre-training with Word Frequency Masking (CLIPF) has numerous advantages. The benefits are particularly evident as the number of input tokens decreases."
      },
      {
        "id": "oai:arXiv.org:2412.20211v3",
        "title": "Generative Regression Based Watch Time Prediction for Short-Video Recommendation",
        "link": "https://arxiv.org/abs/2412.20211",
        "author": "Hongxu Ma, Kai Tian, Tao Zhang, Xuefeng Zhang, Han Zhou, Chunjie Chen, Han Li, Jihong Guan, Shuigeng Zhou",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.20211v3 Announce Type: replace \nAbstract: Watch time prediction (WTP) has emerged as a pivotal task in short video recommendation systems, designed to quantify user engagement through continuous interaction modeling. Predicting users' watch times on videos often encounters fundamental challenges, including wide value ranges and imbalanced data distributions, which can lead to significant estimation bias when directly applying regression techniques. Recent studies have attempted to address these issues by converting the continuous watch time estimation into an ordinal regression task. While these methods demonstrate partial effectiveness, they exhibit notable limitations: (1) the discretization process frequently relies on bucket partitioning, inherently reducing prediction flexibility and accuracy and (2) the interdependencies among different partition intervals remain underutilized, missing opportunities for effective error correction.\n  Inspired by language modeling paradigms, we propose a novel Generative Regression (GR) framework that reformulates WTP as a sequence generation task. Our approach employs \\textit{structural discretization} to enable nearly lossless value reconstruction while maintaining prediction fidelity. Through carefully designed vocabulary construction and label encoding schemes, each watch time is bijectively mapped to a token sequence. To mitigate the training-inference discrepancy caused by teacher-forcing, we introduce a \\textit{curriculum learning with embedding mixup} strategy that gradually transitions from guided to free-generation modes. We evaluate our method against state-of-the-art approaches on two public datasets and one industrial dataset. We also perform online A/B testing on the Kuaishou App to confirm the real-world effectiveness. The results conclusively show that GR outperforms existing techniques significantly."
      },
      {
        "id": "oai:arXiv.org:2412.21063v2",
        "title": "Navigating Image Restoration with VAR's Distribution Alignment Prior",
        "link": "https://arxiv.org/abs/2412.21063",
        "author": "Siyang Wang, Feng Zhao",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.21063v2 Announce Type: replace \nAbstract: Generative models trained on extensive high-quality datasets effectively capture the structural and statistical properties of clean images, rendering them powerful priors for transforming degraded features into clean ones in image restoration. VAR, a novel image generative paradigm, surpasses diffusion models in generation quality by applying a next-scale prediction approach. It progressively captures both global structures and fine-grained details through the autoregressive process, consistent with the multi-scale restoration principle widely acknowledged in the restoration community. Furthermore, we observe that during the image reconstruction process utilizing VAR, scale predictions automatically modulate the input, facilitating the alignment of representations at subsequent scales with the distribution of clean images. To harness VAR's adaptive distribution alignment capability in image restoration tasks, we formulate the multi-scale latent representations within VAR as the restoration prior, thus advancing our delicately designed VarFormer framework. The strategic application of these priors enables our VarFormer to achieve remarkable generalization on unseen tasks while also reducing training computational costs. Extensive experiments underscores that our VarFormer outperforms existing multi-task image restoration methods across various restoration tasks."
      },
      {
        "id": "oai:arXiv.org:2501.00569v4",
        "title": "Probing Visual Language Priors in VLMs",
        "link": "https://arxiv.org/abs/2501.00569",
        "author": "Tiange Luo, Ang Cao, Gunhee Lee, Justin Johnson, Honglak Lee",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.00569v4 Announce Type: replace \nAbstract: Despite recent advances in Vision-Language Models (VLMs), they may over-rely on visual language priors existing in their training data rather than true visual reasoning. To investigate this, we introduce ViLP, a benchmark featuring deliberately out-of-distribution images synthesized via image generation models and out-of-distribution Q&amp;A pairs. Each question in ViLP is coupled with three potential answers and three corresponding images: one that can be resolved by text priors alone and two that demand visual reasoning. Although, humans achieve near-perfect accuracy, modern VLMs falter; for instance, GPT-4 achieves only 66.17% on ViLP. To alleviate this, we propose a self-improving framework in which models generate new VQA data, then apply pixel-level and semantic corruptions to form \"good-bad\" image pairs for self-training. Our training objectives compel VLMs to focus more on the actual visual inputs, and we demonstrate their effectiveness in boosting the performance of open-source VLMs, including LLaVA-v1.5 and Cambrian."
      },
      {
        "id": "oai:arXiv.org:2501.00765v2",
        "title": "Beyond Words: AuralLLM and SignMST-C for Sign Language Production and Bidirectional Accessibility",
        "link": "https://arxiv.org/abs/2501.00765",
        "author": "Yulong Li, Yuxuan Zhang, Feilong Tang, Ming Hu, Zhixiang Lu, Haochen Xue, Jianghao Wu, Mian Zhou, Kang Dang, Chong Li, Yifang Wang, Imran Razzak, Jionglong Su",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.00765v2 Announce Type: replace \nAbstract: Sign language is the primary communication mode for 72 million hearing-impaired individuals worldwide, necessitating effective bidirectional Sign Language Production and Sign Language Translation systems. However, functional bidirectional systems require a unified linguistic environment, hindered by the lack of suitable unified datasets, particularly those providing the necessary pose information for accurate Sign Language Production (SLP) evaluation. Concurrently, current SLP evaluation methods like back-translation ignore pose accuracy, and high-quality coordinated generation remains challenging. To create this crucial environment and overcome these challenges, we introduce CNText2Sign and CNSign, which together constitute the first unified dataset aimed at supporting bidirectional accessibility systems for Chinese sign language; CNText2Sign provides 15,000 natural language-to-sign mappings and standardized skeletal keypoints for 8,643 vocabulary items supporting pose assessment. Building upon this foundation, we propose the AuraLLM model, which leverages a decoupled architecture with CNText2Sign's pose data for novel direct gesture accuracy assessment. The model employs retrieval augmentation and Cascading Vocabulary Resolution to handle semantic mapping and out-of-vocabulary words and achieves all-scenario production with controllable coordination of gestures and facial expressions via pose-conditioned video synthesis. Concurrently, our Sign Language Translation model SignMST-C employs targeted self-supervised pretraining for dynamic feature capture, achieving new SOTA results on PHOENIX2014-T with BLEU-4 scores up to 32.08. AuraLLM establishes a strong performance baseline on CNText2Sign with a BLEU-4 score of 50.41 under direct evaluation."
      },
      {
        "id": "oai:arXiv.org:2501.01874v2",
        "title": "DFF: Decision-Focused Fine-tuning for Smarter Predict-then-Optimize with Limited Data",
        "link": "https://arxiv.org/abs/2501.01874",
        "author": "Jiaqi Yang, Enming Liang, Zicheng Su, Zhichao Zou, Peng Zhen, Jiecheng Guo, Wanjing Ma, Kun An",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.01874v2 Announce Type: replace \nAbstract: Decision-focused learning (DFL) offers an end-to-end approach to the predict-then-optimize (PO) framework by training predictive models directly on decision loss (DL), enhancing decision-making performance within PO contexts. However, the implementation of DFL poses distinct challenges. Primarily, DL can result in deviation from the physical significance of the predictions under limited data. Additionally, some predictive models are non-differentiable or black-box, which cannot be adjusted using gradient-based methods. To tackle the above challenges, we propose a novel framework, Decision-Focused Fine-tuning (DFF), which embeds the DFL module into the PO pipeline via a novel bias correction module. DFF is formulated as a constrained optimization problem that maintains the proximity of the DL-enhanced model to the original predictive model within a defined trust region. We theoretically prove that DFF strictly confines prediction bias within a predetermined upper bound, even with limited datasets, thereby substantially reducing prediction shifts caused by DL under limited data. Furthermore, the bias correction module can be integrated into diverse predictive models, enhancing adaptability to a broad range of PO tasks. Extensive evaluations on synthetic and real-world datasets, including network flow, portfolio optimization, and resource allocation problems with different predictive models, demonstrate that DFF not only improves decision performance but also adheres to fine-tuning constraints, showcasing robust adaptability across various scenarios."
      },
      {
        "id": "oai:arXiv.org:2501.04486v2",
        "title": "MB-TaylorFormer V2: Improved Multi-branch Linear Transformer Expanded by Taylor Formula for Image Restoration",
        "link": "https://arxiv.org/abs/2501.04486",
        "author": "Zhi Jin, Yuwei Qiu, Kaihao Zhang, Hongdong Li, Wenhan Luo",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.04486v2 Announce Type: replace \nAbstract: Recently, Transformer networks have demonstrated outstanding performance in the field of image restoration due to the global receptive field and adaptability to input. However, the quadratic computational complexity of Softmax-attention poses a significant limitation on its extensive application in image restoration tasks, particularly for high-resolution images. To tackle this challenge, we propose a novel variant of the Transformer. This variant leverages the Taylor expansion to approximate the Softmax-attention and utilizes the concept of norm-preserving mapping to approximate the remainder of the first-order Taylor expansion, resulting in a linear computational complexity. Moreover, we introduce a multi-branch architecture featuring multi-scale patch embedding into the proposed Transformer, which has four distinct advantages: 1) various sizes of the receptive field; 2) multi-level semantic information; 3) flexible shapes of the receptive field; 4) accelerated training and inference speed. Hence, the proposed model, named the second version of Taylor formula expansion-based Transformer (for short MB-TaylorFormer V2) has the capability to concurrently process coarse-to-fine features, capture long-distance pixel interactions with limited computational cost, and improve the approximation of the Taylor expansion remainder. Experimental results across diverse image restoration benchmarks demonstrate that MB-TaylorFormer V2 achieves state-of-the-art performance in multiple image restoration tasks, such as image dehazing, deraining, desnowing, motion deblurring, and denoising, with very little computational overhead. The source code is available at https://github.com/FVL2020/MB-TaylorFormerV2."
      },
      {
        "id": "oai:arXiv.org:2501.05763v4",
        "title": "StarGen: A Spatiotemporal Autoregression Framework with Video Diffusion Model for Scalable and Controllable Scene Generation",
        "link": "https://arxiv.org/abs/2501.05763",
        "author": "Shangjin Zhai, Zhichao Ye, Jialin Liu, Weijian Xie, Jiaqi Hu, Zhen Peng, Hua Xue, Danpeng Chen, Xiaomeng Wang, Lei Yang, Nan Wang, Haomin Liu, Guofeng Zhang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.05763v4 Announce Type: replace \nAbstract: Recent advances in large reconstruction and generative models have significantly improved scene reconstruction and novel view generation. However, due to compute limitations, each inference with these large models is confined to a small area, making long-range consistent scene generation challenging. To address this, we propose StarGen, a novel framework that employs a pre-trained video diffusion model in an autoregressive manner for long-range scene generation. The generation of each video clip is conditioned on the 3D warping of spatially adjacent images and the temporally overlapping image from previously generated clips, improving spatiotemporal consistency in long-range scene generation with precise pose control. The spatiotemporal condition is compatible with various input conditions, facilitating diverse tasks, including sparse view interpolation, perpetual view generation, and layout-conditioned city generation. Quantitative and qualitative evaluations demonstrate StarGen's superior scalability, fidelity, and pose accuracy compared to state-of-the-art methods. Project page: https://zju3dv.github.io/StarGen."
      },
      {
        "id": "oai:arXiv.org:2501.06019v2",
        "title": "BRIGHT: A globally distributed multimodal building damage assessment dataset with very-high-resolution for all-weather disaster response",
        "link": "https://arxiv.org/abs/2501.06019",
        "author": "Hongruixuan Chen, Jian Song, Olivier Dietrich, Clifford Broni-Bediako, Weihao Xuan, Junjue Wang, Xinlei Shao, Yimin Wei, Junshi Xia, Cuiling Lan, Konrad Schindler, Naoto Yokoya",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.06019v2 Announce Type: replace \nAbstract: Disaster events occur around the world and cause significant damage to human life and property. Earth observation (EO) data enables rapid and comprehensive building damage assessment (BDA), an essential capability in the aftermath of a disaster to reduce human casualties and to inform disaster relief efforts. Recent research focuses on the development of AI models to achieve accurate mapping of unseen disaster events, mostly using optical EO data. However, solutions based on optical data are limited to clear skies and daylight hours, preventing a prompt response to disasters. Integrating multimodal (MM) EO data, particularly the combination of optical and SAR imagery, makes it possible to provide all-weather, day-and-night disaster responses. Despite this potential, the development of robust multimodal AI models has been constrained by the lack of suitable benchmark datasets. In this paper, we present a BDA dataset using veRy-hIGH-resoluTion optical and SAR imagery (BRIGHT) to support AI-based all-weather disaster response. To the best of our knowledge, BRIGHT is the first open-access, globally distributed, event-diverse MM dataset specifically curated to support AI-based disaster response. It covers five types of natural disasters and two types of man-made disasters across 14 regions worldwide, with a particular focus on developing countries where external assistance is most needed. The optical and SAR imagery in BRIGHT, with a spatial resolution between 0.3-1 meters, provides detailed representations of individual buildings, making it ideal for precise BDA. In our experiments, we have tested seven advanced AI models trained with our BRIGHT to validate the transferability and robustness. The dataset and code are available at https://github.com/ChenHongruixuan/BRIGHT. BRIGHT also serves as the official dataset for the 2025 IEEE GRSS Data Fusion Contest."
      },
      {
        "id": "oai:arXiv.org:2501.07015v3",
        "title": "SplatMAP: Online Dense Monocular SLAM with 3D Gaussian Splatting",
        "link": "https://arxiv.org/abs/2501.07015",
        "author": "Yue Hu, Rong Liu, Meida Chen, Peter Beerel, Andrew Feng",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.07015v3 Announce Type: replace \nAbstract: Achieving high-fidelity 3D reconstruction from monocular video remains challenging due to the inherent limitations of traditional methods like Structure-from-Motion (SfM) and monocular SLAM in accurately capturing scene details. While differentiable rendering techniques such as Neural Radiance Fields (NeRF) address some of these challenges, their high computational costs make them unsuitable for real-time applications. Additionally, existing 3D Gaussian Splatting (3DGS) methods often focus on photometric consistency, neglecting geometric accuracy and failing to exploit SLAM's dynamic depth and pose updates for scene refinement. We propose a framework integrating dense SLAM with 3DGS for real-time, high-fidelity dense reconstruction. Our approach introduces SLAM-Informed Adaptive Densification, which dynamically updates and densifies the Gaussian model by leveraging dense point clouds from SLAM. Additionally, we incorporate Geometry-Guided Optimization, which combines edge-aware geometric constraints and photometric consistency to jointly optimize the appearance and geometry of the 3DGS scene representation, enabling detailed and accurate SLAM mapping reconstruction. Experiments on the Replica and TUM-RGBD datasets demonstrate the effectiveness of our approach, achieving state-of-the-art results among monocular systems. Specifically, our method achieves a PSNR of 36.864, SSIM of 0.985, and LPIPS of 0.040 on Replica, representing improvements of 10.7%, 6.4%, and 49.4%, respectively, over the previous SOTA. On TUM-RGBD, our method outperforms the closest baseline by 10.2%, 6.6%, and 34.7% in the same metrics. These results highlight the potential of our framework in bridging the gap between photometric and geometric dense 3D scene representations, paving the way for practical and efficient monocular dense reconstruction."
      },
      {
        "id": "oai:arXiv.org:2501.07824v4",
        "title": "Real-time Verification and Refinement of Language Model Text Generation",
        "link": "https://arxiv.org/abs/2501.07824",
        "author": "Joonho Ko, Jinheon Baek, Sung Ju Hwang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.07824v4 Announce Type: replace \nAbstract: Large language models (LLMs) have shown remarkable performance across a wide range of natural language tasks. However, a critical challenge remains in that they sometimes generate factually incorrect answers. To address this, while many previous work has focused on identifying errors in their generation and further refining them, they are slow in deployment since they are designed to verify the response from LLMs only after their entire generation (from the first to last tokens) is done. Further, we observe that once LLMs generate incorrect tokens early on, there is a higher likelihood that subsequent tokens will also be factually incorrect. To this end, in this work, we propose Streaming-VR (Streaming Verification and Refinement), a novel approach designed to enhance the efficiency of verification and refinement of LLM outputs. Specifically, the proposed Streaming-VR enables on-the-fly verification and correction of tokens as they are being generated, similar to a streaming process, ensuring that each subset of tokens is checked and refined in real-time by another LLM as the LLM constructs its response. Through comprehensive evaluations on multiple datasets, we demonstrate that our approach not only enhances the factual accuracy of LLMs, but also offers a more efficient solution compared to prior refinement methods."
      },
      {
        "id": "oai:arXiv.org:2501.09267v2",
        "title": "Are Open-Vocabulary Models Ready for Detection of MEP Elements on Construction Sites",
        "link": "https://arxiv.org/abs/2501.09267",
        "author": "Abdalwhab Abdalwhab, Ali Imran, Sina Heydarian, Ivanka Iordanova, David St-Onge",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.09267v2 Announce Type: replace \nAbstract: The construction industry has long explored robotics and computer vision, yet their deployment on construction sites remains very limited. These technologies have the potential to revolutionize traditional workflows by enhancing accuracy, efficiency, and safety in construction management. Ground robots equipped with advanced vision systems could automate tasks such as monitoring mechanical, electrical, and plumbing (MEP) systems. The present research evaluates the applicability of open-vocabulary vision-language models compared to fine-tuned, lightweight, closed-set object detectors for detecting MEP components using a mobile ground robotic platform. A dataset collected with cameras mounted on a ground robot was manually annotated and analyzed to compare model performance. The results demonstrate that, despite the versatility of vision-language models, fine-tuned lightweight models still largely outperform them in specialized environments and for domain-specific tasks."
      },
      {
        "id": "oai:arXiv.org:2501.11515v3",
        "title": "UltraFusion: Ultra High Dynamic Imaging using Exposure Fusion",
        "link": "https://arxiv.org/abs/2501.11515",
        "author": "Zixuan Chen, Yujin Wang, Xin Cai, Zhiyuan You, Zheming Lu, Fan Zhang, Shi Guo, Tianfan Xue",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.11515v3 Announce Type: replace \nAbstract: Capturing high dynamic range (HDR) scenes is one of the most important issues in camera design. Majority of cameras use exposure fusion, which fuses images captured by different exposure levels, to increase dynamic range. However, this approach can only handle images with limited exposure difference, normally 3-4 stops. When applying to very high dynamic range scenes where a large exposure difference is required, this approach often fails due to incorrect alignment or inconsistent lighting between inputs, or tone mapping artifacts. In this work, we propose \\model, the first exposure fusion technique that can merge inputs with 9 stops differences. The key idea is that we model exposure fusion as a guided inpainting problem, where the under-exposed image is used as a guidance to fill the missing information of over-exposed highlights in the over-exposed region. Using an under-exposed image as a soft guidance, instead of a hard constraint, our model is robust to potential alignment issue or lighting variations. Moreover, by utilizing the image prior of the generative model, our model also generates natural tone mapping, even for very high-dynamic range scenes. Our approach outperforms HDR-Transformer on latest HDR benchmarks. Moreover, to test its performance in ultra high dynamic range scenes, we capture a new real-world exposure fusion benchmark, UltraFusion dataset, with exposure differences up to 9 stops, and experiments show that UltraFusion can generate beautiful and high-quality fusion results under various scenarios. Code and data will be available at https://openimaginglab.github.io/UltraFusion."
      },
      {
        "id": "oai:arXiv.org:2501.12121v4",
        "title": "Learning Dynamic Representations via An Optimally-Weighted Maximum Mean Discrepancy Optimization Framework for Continual Learning",
        "link": "https://arxiv.org/abs/2501.12121",
        "author": "KaiHui Huang, RunQing Wu, JinHui Shen, HanYi Zhang, Ling Ge, JiGuo Yu, Fei Ye",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.12121v4 Announce Type: replace \nAbstract: Continual learning has emerged as a pivotal area of research, primarily due to its advantageous characteristic that allows models to persistently acquire and retain information. However, catastrophic forgetting can severely impair model performance. In this study, we address network forgetting by introducing a novel framework termed Optimally-Weighted Maximum Mean Discrepancy (OWMMD), which imposes penalties on representation alterations via a Multi-Level Feature Matching Mechanism (MLFMM). Furthermore, we propose an Adaptive Regularization Optimization (ARO) strategy to refine the adaptive weight vectors, which autonomously assess the significance of each feature layer throughout the optimization process, The proposed ARO approach can relieve the over-regularization problem and promote the future task learning. We conduct a comprehensive series of experiments, benchmarking our proposed method against several established baselines. The empirical findings indicate that our approach achieves state-of-the-art performance."
      },
      {
        "id": "oai:arXiv.org:2501.13341v4",
        "title": "Multi-aspect Knowledge Distillation with Large Language Model",
        "link": "https://arxiv.org/abs/2501.13341",
        "author": "Taegyeong Lee, Jinsik Bang, Soyeong Kwon, Taehwan Kim",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.13341v4 Announce Type: replace \nAbstract: Recent advancements in deep learning have significantly improved performance on computer vision tasks. Previous image classification methods primarily modify model architectures or add features, and they optimize models using cross-entropy loss on class logits. Since they focus on classifying images with considering class labels, these methods may struggle to learn various \\emph{aspects} of classes (e.g., natural positions and shape changes). Rethinking the previous approach from a novel view, we propose a multi-aspect knowledge distillation method using Multimodal Large Language Models (MLLMs). Our approach involves: 1) querying Large Language Model with multi-aspect questions relevant to the knowledge we want to transfer to the model, 2) extracting corresponding logits from MLLM, and 3) expanding the model's output dimensions to distill these multi-aspect logits. We then apply cross-entropy loss to class logits and binary cross-entropy loss to multi-aspect logits. Through our method, the model can learn not only the knowledge about visual aspects but also the abstract and complex aspects that require a deeper understanding. We primarily apply our method to image classification, and to explore the potential for extending our model, such as object detection. In all experimental results, our method improves the performance of the baselines. Additionally, we analyze the effect of multi-aspect knowledge distillation. These results demonstrate that our method can transfer knowledge about various aspects to the model and the aspect knowledge can enhance model performance in computer vision tasks."
      },
      {
        "id": "oai:arXiv.org:2501.14846v5",
        "title": "Wormhole Memory: A Rubik's Cube for Cross-Dialogue Retrieval",
        "link": "https://arxiv.org/abs/2501.14846",
        "author": "Libo Wang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.14846v5 Announce Type: replace \nAbstract: In view of the gap in the current large language model in sharing memory across dialogues, this research proposes a wormhole memory module (WMM) to realize memory as a Rubik's cube that can be arbitrarily retrieved between different dialogues. Through simulation experiments, the researcher built an experimental framework based on the Python environment and used setting memory barriers to simulate the current situation where memories between LLMs dialogues are difficult to share. The CoQA development data set was imported into the experiment, and the feasibility of its cross-dialogue memory retrieval function was verified for WMM's nonlinear indexing and dynamic retrieval, and a comparative analysis was conducted with the capabilities of Titans and MemGPT memory modules. Experimental results show that WMM demonstrated the ability to retrieve memory across dialogues and the stability of quantitative indicators in eight experiments. It contributes new technical approaches to the optimization of memory management of LLMs and provides experience for the practical application in the future."
      },
      {
        "id": "oai:arXiv.org:2501.15503v2",
        "title": "Domain Adaptation from Generated Multi-Weather Images for Unsupervised Maritime Object Classification",
        "link": "https://arxiv.org/abs/2501.15503",
        "author": "Dan Song, Shumeng Huo, Wenhui Li, Lanjun Wang, Chao Xue, An-An Liu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.15503v2 Announce Type: replace \nAbstract: The classification and recognition of maritime objects are crucial for enhancing maritime safety, monitoring, and intelligent sea environment prediction. However, existing unsupervised methods for maritime object classification often struggle with the long-tail data distributions in both object categories and weather conditions. In this paper, we construct a dataset named AIMO produced by large-scale generative models with diverse weather conditions and balanced object categories, and collect a dataset named RMO with real-world images where long-tail issue exists. We propose a novel domain adaptation approach that leverages AIMO (source domain) to address the problem of limited labeled data, unbalanced distribution and domain shift in RMO (target domain), enhance the generalization of source features with the Vision-Language Models such as CLIP, and propose a difficulty score for curriculum learning to optimize training process. Experimental results shows that the proposed method significantly improves the classification accuracy, particularly for samples within rare object categories and weather conditions. Datasets and codes will be publicly available at https://github.com/honoria0204/AIMO."
      },
      {
        "id": "oai:arXiv.org:2501.16003v4",
        "title": "Improving Tropical Cyclone Forecasting With Video Diffusion Models",
        "link": "https://arxiv.org/abs/2501.16003",
        "author": "Zhibo Ren, Pritthijit Nath, Pancham Shukla",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.16003v4 Announce Type: replace \nAbstract: Tropical cyclone (TC) forecasting is crucial for disaster preparedness and mitigation. While recent deep learning approaches have shown promise, existing methods often treat TC evolution as a series of independent frame-to-frame predictions, limiting their ability to capture long-term dynamics. We present a novel application of video diffusion models for TC forecasting that explicitly models temporal dependencies through additional temporal layers. Our approach enables the model to generate multiple frames simultaneously, better capturing cyclone evolution patterns. We introduce a two-stage training strategy that significantly improves individual-frame quality and performance in low-data regimes. Experimental results show our method outperforms the previous approach of Nath et al. by 19.3% in MAE, 16.2% in PSNR, and 36.1% in SSIM. Most notably, we extend the reliable forecasting horizon from 36 to 50 hours. Through comprehensive evaluation using both traditional metrics and Fr\\'echet Video Distance (FVD), we demonstrate that our approach produces more temporally coherent forecasts while maintaining competitive single-frame quality. Code accessible at https://github.com/Ren-creater/forecast-video-diffmodels."
      },
      {
        "id": "oai:arXiv.org:2501.17391v2",
        "title": "Learning Free Token Reduction for Multi-Modal Large Language Models",
        "link": "https://arxiv.org/abs/2501.17391",
        "author": "Zihui Zhao, Yingxin Li, Yang Li",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.17391v2 Announce Type: replace \nAbstract: Vision-Language Models (VLMs) have achieved remarkable success across a range of multimodal tasks; however, their practical deployment is often constrained by high computational costs and prolonged inference times. Since the vision modality typically carries more information than the text modality, compressing visual prompts offers a promising solution to alleviate these challenges. Existing approaches predominantly focus on refining model architectures or directly reducing the number of visual tokens. However, these methods often compromise inference performance due to a lack of consideration for the unique spatial and temporal characteristics of visual data. In this work, we propose a token compression paradigm that operates on both spatial and temporal dimensions. Our approach includes a learning-free, plug-and-play compression pipeline that can be seamlessly integrated into most Multimodal Large Language Model (MLLM) frameworks. By leveraging this method, we enhance the model inference capability while simultaneously reducing its computational cost. Experimental results on the Video-QA task demonstrate the effectiveness of the proposed approach, showcasing significant improvements in efficiency without sacrificing performance."
      },
      {
        "id": "oai:arXiv.org:2501.17468v2",
        "title": "Solving Inverse Problems using Diffusion with Iterative Colored Renoising",
        "link": "https://arxiv.org/abs/2501.17468",
        "author": "Matt C. Bendel, Saurav K. Shastri, Rizwan Ahmad, Philip Schniter",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.17468v2 Announce Type: replace \nAbstract: Imaging inverse problems can be solved in an unsupervised manner using pre-trained diffusion models, but doing so requires approximating the gradient of the measurement-conditional score function in the diffusion reverse process. We show that the approximations produced by existing methods are relatively poor, especially early in the reverse process, and so we propose a new approach that iteratively reestimates and \"renoises\" the estimate several times per diffusion step. This iterative approach, which we call Fast Iterative REnoising (FIRE), injects colored noise that is shaped to ensure that the pre-trained diffusion model always sees white noise, in accordance with how it was trained. We then embed FIRE into the DDIM reverse process and show that the resulting \"DDfire\" offers state-of-the-art accuracy and runtime on several linear inverse problems, as well as phase retrieval. Our implementation is at https://github.com/matt-bendel/DDfire"
      },
      {
        "id": "oai:arXiv.org:2501.17636v3",
        "title": "HOMER: Homography-Based Efficient Multi-view 3D Object Removal",
        "link": "https://arxiv.org/abs/2501.17636",
        "author": "Jingcheng Ni, Weiguang Zhao, Daniel Wang, Ziyao Zeng, Chenyu You, Alex Wong, Kaizhu Huang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.17636v3 Announce Type: replace \nAbstract: 3D object removal is an important sub-task in 3D scene editing, with broad applications in scene understanding, augmented reality, and robotics. However, existing methods struggle to achieve a desirable balance among consistency, usability, and computational efficiency in multi-view settings. These limitations are primarily due to unintuitive user interaction in the source view, inefficient multi-view object mask generation, computationally expensive inpainting procedures, and a lack of applicability across different radiance field representations. To address these challenges, we propose a novel pipeline that improves the quality and efficiency of multi-view object mask generation and inpainting. Our method introduces an intuitive region-based interaction mechanism in the source view and eliminates the need for camera poses or extra model training. Our lightweight HoMM module is employed to achieve high-quality multi-view mask propagation with enhanced efficiency. In the inpainting stage, we further reduce computational costs by performing inpainting only on selected key views and propagating the results to other views via homography-based mapping. Our pipeline is compatible with a variety of radiance field frameworks, including NeRF and 3D Gaussian Splatting, demonstrating improved generalizability and practicality in real-world scenarios. Additionally, we present a new 3D multi-object removal dataset with greater object diversity and viewpoint variation than existing datasets. Experiments on public benchmarks and our proposed dataset show that our method achieves state-of-the-art performance while reducing runtime to one-fifth of that required by leading baselines."
      },
      {
        "id": "oai:arXiv.org:2502.00601v2",
        "title": "Enhancing Offline Reinforcement Learning with Curriculum Learning-Based Trajectory Valuation",
        "link": "https://arxiv.org/abs/2502.00601",
        "author": "Amir Abolfazli, Zekun Song, Avishek Anand, Wolfgang Nejdl",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.00601v2 Announce Type: replace \nAbstract: The success of deep reinforcement learning (DRL) relies on the availability and quality of training data, often requiring extensive interactions with specific environments. In many real-world scenarios, where data collection is costly and risky, offline reinforcement learning (RL) offers a solution by utilizing data collected by domain experts and searching for a batch-constrained optimal policy. This approach is further augmented by incorporating external data sources, expanding the range and diversity of data collection possibilities. However, existing offline RL methods often struggle with challenges posed by non-matching data from these external sources. In this work, we specifically address the problem of source-target domain mismatch in scenarios involving mixed datasets, characterized by a predominance of source data generated from random or suboptimal policies and a limited amount of target data generated from higher-quality policies. To tackle this problem, we introduce Transition Scoring (TS), a novel method that assigns scores to transitions based on their similarity to the target domain, and propose Curriculum Learning-Based Trajectory Valuation (CLTV), which effectively leverages these transition scores to identify and prioritize high-quality trajectories through a curriculum learning approach. Our extensive experiments across various offline RL methods and MuJoCo environments, complemented by rigorous theoretical analysis, demonstrate that CLTV enhances the overall performance and transferability of policies learned by offline RL algorithms."
      },
      {
        "id": "oai:arXiv.org:2502.00675v3",
        "title": "ReFoRCE: A Text-to-SQL Agent with Self-Refinement, Format Restriction, and Column Exploration",
        "link": "https://arxiv.org/abs/2502.00675",
        "author": "Minghang Deng, Ashwin Ramachandran, Canwen Xu, Lanxiang Hu, Zhewei Yao, Anupam Datta, Hao Zhang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.00675v3 Announce Type: replace \nAbstract: Text-to-SQL systems have unlocked easier access to critical data insights by enabling natural language queries over structured databases. However, deploying such systems in enterprise environments remains challenging due to factors such as large, complex schemas (> 3000 columns), diverse SQL dialects (e.g., BigQuery, Snowflake) and sophisticated query requirements (e.g., transformation, analytics). Current state-of-the-art performance on the Spider 2.0 dataset -- a benchmark built to mimic such complex environments -- remains limited at 20%. Key limitations include inadequate instruction-following, poor long-context comprehension, weak self-refinement, and insufficient dialect-specific knowledge. To address these gaps, we propose ReFoRCE (Self-Refinement Agent with Format Restriction and Column Exploration) which introduces (1) table compression to mitigate long-context limitations (2) format restriction to ensure accurate answer format, and (3) iterative column exploration for enhanced schema understanding. Additionally, it employs self-refinement pipeline consisting of (1) parallelized workflows with voting mechanisms and (2) a Common Table Expression (CTE) based refinement approach to handle unresolved cases. ReFoRCE achieves state-of-the-art results scoring 31.26 on the Spider 2.0-Snow and scoring 30.35 on the Spider 2.0-Lite tasks. Our code is available at https://github.com/hao-ai-lab/ReFoRCE."
      },
      {
        "id": "oai:arXiv.org:2502.00859v2",
        "title": "FedRIR: Rethinking Information Representation in Federated Learning",
        "link": "https://arxiv.org/abs/2502.00859",
        "author": "Yongqiang Huang, Zerui Shao, Ziyuan Yang, Zexin Lu, Yi Zhang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.00859v2 Announce Type: replace \nAbstract: Mobile and Web-of-Things (WoT) devices at the network edge generate vast amounts of data for machine learning applications, yet privacy concerns hinder centralized model training. Federated Learning (FL) allows clients (devices) to collaboratively train a shared model coordinated by a central server without transfer private data, but inherent statistical heterogeneity among clients presents challenges, often leading to a dilemma between clients' needs for personalized local models and the server's goal of building a generalized global model. Existing FL methods typically prioritize either global generalization or local personalization, resulting in a trade-off between these two objectives and limiting the full potential of diverse client data. To address this challenge, we propose a novel framework that simultaneously enhances global generalization and local personalization by Rethinking Information Representation in the Federated learning process (FedRIR). Specifically, we introduce Masked Client-Specific Learning (MCSL), which isolates and extracts fine-grained client-specific features tailored to each client's unique data characteristics, thereby enhancing personalization. Concurrently, the Information Distillation Module (IDM) refines the global shared features by filtering out redundant client-specific information, resulting in a purer and more robust global representation that enhances generalization. By integrating the refined global features with the isolated client-specific features, we construct enriched representations that effectively capture both global patterns and local nuances, thereby improving the performance of downstream tasks on the client. The code is available at https://github.com/Deep-Imaging-Group/FedRIR."
      },
      {
        "id": "oai:arXiv.org:2502.00896v3",
        "title": "LoR-VP: Low-Rank Visual Prompting for Efficient Vision Model Adaptation",
        "link": "https://arxiv.org/abs/2502.00896",
        "author": "Can Jin, Ying Li, Mingyu Zhao, Shiyu Zhao, Zhenting Wang, Xiaoxiao He, Ligong Han, Tong Che, Dimitris N. Metaxas",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.00896v3 Announce Type: replace \nAbstract: Visual prompting has gained popularity as a method for adapting pre-trained models to specific tasks, particularly in the realm of parameter-efficient tuning. However, existing visual prompting techniques often pad the prompt parameters around the image, limiting the interaction between the visual prompts and the original image to a small set of patches while neglecting the inductive bias present in shared information across different patches. In this study, we conduct a thorough preliminary investigation to identify and address these limitations. We propose a novel visual prompt design, introducing Low-Rank matrix multiplication for Visual Prompting (LoR-VP), which enables shared and patch-specific information across rows and columns of image pixels. Extensive experiments across seven network architectures and four datasets demonstrate significant improvements in both performance and efficiency compared to state-of-the-art visual prompting methods, achieving up to 6 times faster training times, utilizing 18 times fewer visual prompt parameters, and delivering a 3.1% improvement in performance. The code is available as https://github.com/jincan333/LoR-VP."
      },
      {
        "id": "oai:arXiv.org:2502.01436v2",
        "title": "Towards Safer Chatbots: A Framework for Policy Compliance Evaluation of Custom GPTs",
        "link": "https://arxiv.org/abs/2502.01436",
        "author": "David Rodriguez, William Seymour, Jose M. Del Alamo, Jose Such",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.01436v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) have gained unprecedented prominence, achieving widespread adoption across diverse domains and integrating deeply into society. The capability to fine-tune general-purpose LLMs, such as Generative Pre-trained Transformers (GPT), for specific tasks has facilitated the emergence of numerous Custom GPTs. These tailored models are increasingly made available through dedicated marketplaces, such as OpenAI's GPT Store. However, their black-box nature introduces significant safety and compliance risks. In this work, we present a scalable framework for the automated evaluation of Custom GPTs against OpenAI's usage policies, which define the permissible behaviors of these systems. Our framework integrates three core components: (1) automated discovery and data collection of models from the GPT store, (2) a red-teaming prompt generator tailored to specific policy categories and the characteristics of each target GPT, and (3) an LLM-as-a-judge technique to analyze each prompt-response pair for potential policy violations. We validate our framework with a manually annotated ground truth, and evaluate it through a large-scale study with 782 Custom GPTs across three categories: Romantic, Cybersecurity, and Academic GPTs. Our manual annotation process achieved an F1 score of 0.975 in identifying policy violations, confirming the reliability of the framework's assessments. The results reveal that 58.7% of the analyzed models exhibit indications of non-compliance, exposing weaknesses in the GPT store's review and approval processes. Furthermore, our findings indicate that a model's popularity does not correlate with compliance, and non-compliance issues largely stem from behaviors inherited from base models rather than user-driven customizations. We believe this approach is extendable to other chatbot platforms and policy domains, improving LLM-based systems safety."
      },
      {
        "id": "oai:arXiv.org:2502.02275v3",
        "title": "A User's Guide to Sampling Strategies for Sliced Optimal Transport",
        "link": "https://arxiv.org/abs/2502.02275",
        "author": "Keanu Sisouk, Julie Delon, Julien Tierny",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.02275v3 Announce Type: replace \nAbstract: This paper serves as a user's guide to sampling strategies for sliced optimal transport. We provide reminders and additional regularity results on the Sliced Wasserstein distance. We detail the construction methods, generation time complexity, theoretical guarantees, and conditions for each strategy. Additionally, we provide insights into their suitability for sliced optimal transport in theory. Extensive experiments on both simulated and real-world data offer a representative comparison of the strategies, culminating in practical recommendations for their best usage."
      },
      {
        "id": "oai:arXiv.org:2502.03897v3",
        "title": "UniForm: A Unified Multi-Task Diffusion Transformer for Audio-Video Generation",
        "link": "https://arxiv.org/abs/2502.03897",
        "author": "Lei Zhao, Linfeng Feng, Dongxu Ge, Rujin Chen, Fangqiu Yi, Chi Zhang, Xiao-Lei Zhang, Xuelong Li",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.03897v3 Announce Type: replace \nAbstract: With the rise of diffusion models, audio-video generation has been revolutionized. However, most existing methods rely on separate modules for each modality, with limited exploration of unified generative architectures. In addition, many are confined to a single task and small-scale datasets. To address these limitations, we first propose UniForm, a unified multi-task diffusion transformer that jointly generates audio and visual modalities in a shared latent space. A single diffusion process models both audio and video, capturing the inherent correlations between sound and vision. Second, we introduce task-specific noise schemes and task tokens, enabling a single model to support multiple tasks, including text-to-audio-video, audio-to-video, and video-to-audio generation. Furthermore, by leveraging large language models and a large-scale text-audio-video combined dataset, UniForm achieves greater generative diversity than prior approaches. Extensive experiments show that UniForm achieves the state-of-the-art performance across audio-video generation tasks, producing content that is both well-aligned and close to real-world data distributions. Our demos are available at https://uniform-t2av.github.io/."
      },
      {
        "id": "oai:arXiv.org:2502.05075v2",
        "title": "Discrepancies are Virtue: Weak-to-Strong Generalization through Lens of Intrinsic Dimension",
        "link": "https://arxiv.org/abs/2502.05075",
        "author": "Yijun Dong, Yicheng Li, Yunai Li, Jason D. Lee, Qi Lei",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.05075v2 Announce Type: replace \nAbstract: Weak-to-strong (W2S) generalization is a type of finetuning (FT) where a strong (large) student model is trained on pseudo-labels generated by a weak teacher. Surprisingly, W2S FT often outperforms the weak teacher. We seek to understand this phenomenon through the observation that FT often occurs in intrinsically low-dimensional spaces. Leveraging the low intrinsic dimensionality of FT, we analyze W2S in the ridgeless regression setting from a variance reduction perspective. For a strong student - weak teacher pair with sufficiently expressive low-dimensional feature subspaces $\\mathcal{V}_s, \\mathcal{V}_w$, we provide an exact characterization of the variance that dominates the generalization error of W2S. This unveils a virtue of discrepancy between the strong and weak models in W2S: the variance of the weak teacher is inherited by the strong student in $\\mathcal{V}_s \\cap \\mathcal{V}_w$, while reduced by a factor of $\\dim(\\mathcal{V}_s)/N$ in the subspace of discrepancy $\\mathcal{V}_w \\setminus \\mathcal{V}_s$ with $N$ pseudo-labels for W2S. Further, our analysis casts light on the sample complexities and the scaling of performance gap recovery in W2S. The analysis is supported with experiments on synthetic regression and real vision and NLP tasks."
      },
      {
        "id": "oai:arXiv.org:2502.05424v2",
        "title": "SAMGPT: Text-free Graph Foundation Model for Multi-domain Pre-training and Cross-domain Adaptation",
        "link": "https://arxiv.org/abs/2502.05424",
        "author": "Xingtong Yu, Zechuan Gong, Chang Zhou, Yuan Fang, Hui Zhang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.05424v2 Announce Type: replace \nAbstract: Graphs are able to model interconnected entities in many online services, supporting a wide range of applications on the Web. This raises an important question: How can we train a graph foundational model on multiple source domains and adapt to an unseen target domain? A major obstacle is that graphs from different domains often exhibit divergent characteristics. Some studies leverage large language models to align multiple domains based on textual descriptions associated with the graphs, limiting their applicability to text-attributed graphs. For text-free graphs, a few recent works attempt to align different feature distributions across domains, while generally neglecting structural differences. In this work, we propose a novel Structure Alignment framework for text-free Multi-domain Graph Pre-Training and cross-domain adaptation (SAMGPT). It is designed to learn multi-domain knowledge from graphs originating in multiple source domains, which can then be adapted to address applications in an unseen target domain. Specifically, we introduce a set of structure tokens to harmonize structure-based aggregation across source domains during the pre-training phase. Next, for cross-domain adaptation, we design dual prompts, namely, holistic prompts and specific prompts, which adapt unified multi-domain structural knowledge and fine-grained, domain-specific information, respectively, to a target domain. Finally, we conduct comprehensive experiments on seven public datasets to evaluate and analyze the effectiveness of SAMGPT."
      },
      {
        "id": "oai:arXiv.org:2502.07677v3",
        "title": "Auto-Drafting Police Reports from Noisy ASR Outputs: A Trust-Centered LLM Approach",
        "link": "https://arxiv.org/abs/2502.07677",
        "author": "Param Kulkarni, Yingchi Liu, Hao-Ming Fu, Shaohua Yang, Isuru Gunasekara, Matt Peloquin, Noah Spitzer-Williams, Xiaotian Zhou, Xiaozhong Liu, Zhengping Ji, Yasser Ibrahim",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.07677v3 Announce Type: replace \nAbstract: Achieving a delicate balance between fostering trust in law enforcement and protecting the rights of both officers and civilians continues to emerge as a pressing research and product challenge in the world today. In the pursuit of fairness and transparency, this study presents an innovative AI-driven system designed to generate police report drafts from complex, noisy, and multi-role dialogue data. Our approach intelligently extracts key elements of law enforcement interactions and includes them in the draft, producing structured narratives that are not only high in quality but also reinforce accountability and procedural clarity. This framework holds the potential to transform the reporting process, ensuring greater oversight, consistency, and fairness in future policing practices. A demonstration video of our system can be accessed at https://drive.google.com/file/d/1kBrsGGR8e3B5xPSblrchRGj-Y-kpCHNO/view?usp=sharing"
      },
      {
        "id": "oai:arXiv.org:2502.08736v2",
        "title": "Recurrent Memory for Online Interdomain Gaussian Processes",
        "link": "https://arxiv.org/abs/2502.08736",
        "author": "Wenlong Chen, Naoki Kiyohara, Harrison Bo Hua Zhu, Yingzhen Li",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.08736v2 Announce Type: replace \nAbstract: We propose a novel online Gaussian process (GP) model that is capable of capturing long-term memory in sequential data in an online regression setting. Our model, Online HiPPO Sparse Variational Gaussian Process Regression (OHSGPR), leverages the HiPPO (High-order Polynomial Projection Operators) framework, which is popularized in the RNN domain due to its long-range memory modeling capabilities. We interpret the HiPPO time-varying orthogonal projections as inducing variables with time-dependent orthogonal polynomial basis functions, which allows the SGPR inducing points to memorize the process history. We show that the HiPPO framework fits naturally into the interdomain GP framework and demonstrate that the kernel matrices can also be updated online in a recurrence form based on the ODE evolution of HiPPO. We evaluate our method on time series regression tasks, showing that it outperforms the existing online GP method in terms of predictive performance and computational efficiency"
      },
      {
        "id": "oai:arXiv.org:2502.09525v2",
        "title": "Robust Learning of Multi-index Models via Iterative Subspace Approximation",
        "link": "https://arxiv.org/abs/2502.09525",
        "author": "Ilias Diakonikolas, Giannis Iakovidis, Daniel M. Kane, Nikos Zarifis",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.09525v2 Announce Type: replace \nAbstract: We study the task of learning Multi-Index Models (MIMs) with label noise under the Gaussian distribution. A $K$-MIM is any function $f$ that only depends on a $K$-dimensional subspace. We focus on well-behaved MIMs with finite ranges that satisfy certain regularity properties. Our main contribution is a general robust learner that is qualitatively optimal in the Statistical Query (SQ) model. Our algorithm iteratively constructs better approximations to the defining subspace by computing low-degree moments conditional on the projection to the subspace computed thus far, and adding directions with relatively large empirical moments. This procedure efficiently finds a subspace $V$ so that $f(\\mathbf{x})$ is close to a function of the projection of $\\mathbf{x}$ onto $V$. Conversely, for functions for which these conditional moments do not help, we prove an SQ lower bound suggesting that no efficient learner exists. As applications, we provide faster robust learners for the following concept classes:\n  * {\\bf Multiclass Linear Classifiers} We give a constant-factor approximate agnostic learner with sample complexity $N = O(d) 2^{\\mathrm{poly}(K/\\epsilon)}$ and computational complexity $\\mathrm{poly}(N ,d)$. This is the first constant-factor agnostic learner for this class whose complexity is a fixed-degree polynomial in $d$.\n  * {\\bf Intersections of Halfspaces} We give an approximate agnostic learner for this class achieving 0-1 error $K \\tilde{O}(\\mathrm{OPT}) + \\epsilon$ with sample complexity $N=O(d^2) 2^{\\mathrm{poly}(K/\\epsilon)}$ and computational complexity $\\mathrm{poly}(N ,d)$. This is the first agnostic learner for this class with near-linear error dependence and complexity a fixed-degree polynomial in $d$.\n  Furthermore, we show that in the presence of random classification noise, the complexity of our algorithm scales polynomially with $1/\\epsilon$."
      },
      {
        "id": "oai:arXiv.org:2502.11093v2",
        "title": "Text-Promptable Propagation for Referring Medical Image Sequence Segmentation",
        "link": "https://arxiv.org/abs/2502.11093",
        "author": "Runtian Yuan, Mohan Chen, Jilan Xu, Ling Zhou, Qingqiu Li, Yuejie Zhang, Rui Feng, Tao Zhang, Shang Gao",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.11093v2 Announce Type: replace \nAbstract: Referring Medical Image Sequence Segmentation (Ref-MISS) is a novel and challenging task that aims to segment anatomical structures in medical image sequences (\\emph{e.g.} endoscopy, ultrasound, CT, and MRI) based on natural language descriptions. This task holds significant clinical potential and offers a user-friendly advancement in medical imaging interpretation. Existing 2D and 3D segmentation models struggle to explicitly track objects of interest across medical image sequences, and lack support for nteractive, text-driven guidance. To address these limitations, we propose Text-Promptable Propagation (TPP), a model designed for referring medical image sequence segmentation. TPP captures the intrinsic relationships among sequential images along with their associated textual descriptions. Specifically, it enables the recognition of referred objects through cross-modal referring interaction, and maintains continuous tracking across the sequence via Transformer-based triple propagation, using text embeddings as queries. To support this task, we curate a large-scale benchmark, Ref-MISS-Bench, which covers 4 imaging modalities and 20 different organs and lesions. Experimental results on this benchmark demonstrate that TPP consistently outperforms state-of-the-art methods in both medical segmentation and referring video object segmentation."
      },
      {
        "id": "oai:arXiv.org:2502.11901v2",
        "title": "Building A Proof-Oriented Programmer That Is 64% Better Than GPT-4o Under Data Scarcity",
        "link": "https://arxiv.org/abs/2502.11901",
        "author": "Dylan Zhang, Justin Wang, Tianran Sun",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.11901v2 Announce Type: replace \nAbstract: Existing LMs struggle with proof-oriented programming due to data scarcity, which manifest in two key ways: (1) a lack of sufficient corpora for proof-oriented programming languages such as F*, and (2) the absence of large-scale, project-level proof-oriented implementations that can teach the model the intricate reasoning process when performing proof-oriented programming. We present the first on synthetic data augmentation for project level proof oriented programming for both generation and repair. Our method addresses data scarcity by synthesizing basic proof-oriented programming problems for proficiency in that language; incorporating diverse coding data for reasoning capability elicitation and creating new proofs and repair data within existing repositories. This approach enables language models to both synthesize and repair proofs for function- and repository-level code. We show that our fine-tuned 14B parameter model, PoPilot, can exceed the performance of the models that outperforms GPT-4o in project-level proof-oriented programming by 64% relative margin, and can improve GPT-4o's performance by 54% by repairing its outputs over GPT-4o's self-repair."
      },
      {
        "id": "oai:arXiv.org:2502.12110v4",
        "title": "A-MEM: Agentic Memory for LLM Agents",
        "link": "https://arxiv.org/abs/2502.12110",
        "author": "Wujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Juntao Tan, Yongfeng Zhang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.12110v4 Announce Type: replace \nAbstract: While large language model (LLM) agents can effectively use external tools for complex real-world tasks, they require memory systems to leverage historical experiences. Current memory systems enable basic storage and retrieval but lack sophisticated memory organization, despite recent attempts to incorporate graph databases. Moreover, these systems' fixed operations and structures limit their adaptability across diverse tasks. To address this limitation, this paper proposes a novel agentic memory system for LLM agents that can dynamically organize memories in an agentic way. Following the basic principles of the Zettelkasten method, we designed our memory system to create interconnected knowledge networks through dynamic indexing and linking. When a new memory is added, we generate a comprehensive note containing multiple structured attributes, including contextual descriptions, keywords, and tags. The system then analyzes historical memories to identify relevant connections, establishing links where meaningful similarities exist. Additionally, this process enables memory evolution - as new memories are integrated, they can trigger updates to the contextual representations and attributes of existing historical memories, allowing the memory network to continuously refine its understanding. Our approach combines the structured organization principles of Zettelkasten with the flexibility of agent-driven decision making, allowing for more adaptive and context-aware memory management. Empirical experiments on six foundation models show superior improvement against existing SOTA baselines. The source code for evaluating performance is available at https://github.com/WujiangXu/AgenticMemory, while the source code of agentic memory system is available at https://github.com/agiresearch/A-mem."
      },
      {
        "id": "oai:arXiv.org:2502.12486v4",
        "title": "EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via Reinforcement Learning",
        "link": "https://arxiv.org/abs/2502.12486",
        "author": "Xiaoqian Liu, Ke Wang, Yongbin Li, Yuchuan Wu, Wentao Ma, Aobo Kong, Fei Huang, Jianbin Jiao, Junge Zhang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.12486v4 Announce Type: replace \nAbstract: Large Language Models (LLMs) have shown impressive reasoning capabilities in well-defined problems with clear solutions, such as mathematics and coding. However, they still struggle with complex real-world scenarios like business negotiations, which require strategic reasoning-an ability to navigate dynamic environments and align long-term goals amidst uncertainty. Existing methods for strategic reasoning face challenges in adaptability, scalability, and transferring strategies to new contexts. To address these issues, we propose explicit policy optimization (EPO) for strategic reasoning, featuring an LLM that provides strategies in open-ended action space and can be plugged into arbitrary LLM agents to motivate goal-directed behavior. To improve adaptability and policy transferability, we train the strategic reasoning model via multi-turn reinforcement learning (RL) using process rewards and iterative self-play, without supervised fine-tuning (SFT) as a preliminary step. Experiments across social and physical domains demonstrate EPO's ability of long-term goal alignment through enhanced strategic reasoning, achieving state-of-the-art performance on social dialogue and web navigation tasks. Our findings reveal various collaborative reasoning mechanisms emergent in EPO and its effectiveness in generating novel strategies, underscoring its potential for strategic reasoning in real-world applications."
      },
      {
        "id": "oai:arXiv.org:2502.15335v2",
        "title": "Stepwise Informativeness Search for Efficient and Effective LLM Reasoning",
        "link": "https://arxiv.org/abs/2502.15335",
        "author": "Siyuan Wang, Enda Zhao, Zhongyu Wei, Xiang Ren",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.15335v2 Announce Type: replace \nAbstract: Advances in Large Language Models (LLMs) have significantly improved multi-step reasoning through generating free-text rationales. However, recent studies show that LLMs tend to lose focus over the middle of long contexts. This raises concerns that as reasoning progresses, LLMs may overlook information in earlier steps when decoding subsequent steps, leading to generate unreliable and redundant rationales. To address this, we propose guiding LLMs to generate more accurate and concise step-by-step rationales by (1) proactively referencing information from underutilized prior steps, and (2) minimizing redundant information between new and existing steps. We introduce stepwise informativeness search, an inference-time tree search framework incorporating two selection heuristics: grounding-guided selection which prioritizes steps paying higher attention over underutilized steps; and novelty-guided selection which encourages steps with novel conclusions. During rationale generation, we use a self-grounding strategy that prompts LLMs to explicitly reference relevant prior steps to provide premises before deduction at each step. Experimental results on four reasoning datasets demonstrate that our approach improves reasoning accuracy by generating higher-quality rationales with reduced errors and redundancy."
      },
      {
        "id": "oai:arXiv.org:2502.17055v2",
        "title": "Stable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam",
        "link": "https://arxiv.org/abs/2502.17055",
        "author": "Tianjin Huang, Haotian Hu, Zhenyu Zhang, Gaojie Jin, Xiang Li, Li Shen, Tianlong Chen, Lu Liu, Qingsong Wen, Zhangyang Wang, Shiwei Liu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.17055v2 Announce Type: replace \nAbstract: This paper comprehensively evaluates several recently proposed optimizers for 4-bit training, revealing that low-bit precision amplifies sensitivity to learning rates and often causes unstable gradient norms, leading to divergence at higher learning rates. Among these, SPAM, a recent optimizer featuring momentum reset and spike-aware gradient clipping, achieves the best performance across various bit levels, but struggles to stabilize gradient norms, requiring careful learning rate tuning. To address these limitations, we propose Stable-SPAM, which incorporates enhanced gradient normalization and clipping techniques. In particular, Stable-SPAM (1) adaptively updates the clipping threshold for spiked gradients by tracking their historical maxima; (2) normalizes the entire gradient matrix based on its historical $l_2$-norm statistics; and $(3)$ inherits momentum reset from SPAM to periodically reset the first and second moments of Adam, mitigating the accumulation of spiked gradients. Extensive experiments show that Stable-SPAM effectively stabilizes gradient norms in 4-bit LLM training, delivering superior performance compared to Adam and SPAM. Notably, our 4-bit LLaMA-1B model trained with Stable-SPAM outperforms the BF16 LLaMA-1B trained with Adam by up to $2$ perplexity. Furthermore, when both models are trained in 4-bit, Stable-SPAM achieves the same loss as Adam while requiring only about half the training steps. Code is available at https://github.com/TianjinYellow/StableSPAM.git."
      },
      {
        "id": "oai:arXiv.org:2502.17189v2",
        "title": "IGDA: Interactive Graph Discovery through Large Language Model Agents",
        "link": "https://arxiv.org/abs/2502.17189",
        "author": "Alex Havrilla, David Alvarez-Melis, Nicolo Fusi",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.17189v2 Announce Type: replace \nAbstract: Large language models ($\\textbf{LLMs}$) have emerged as a powerful method for discovery. Instead of utilizing numerical data, LLMs utilize associated variable $\\textit{semantic metadata}$ to predict variable relationships. Simultaneously, LLMs demonstrate impressive abilities to act as black-box optimizers when given an objective $f$ and sequence of trials. We study LLMs at the intersection of these two capabilities by applying LLMs to the task of $\\textit{interactive graph discovery}$: given a ground truth graph $G^*$ capturing variable relationships and a budget of $I$ edge experiments over $R$ rounds, minimize the distance between the predicted graph $\\hat{G}_R$ and $G^*$ at the end of the $R$-th round. To solve this task we propose $\\textbf{IGDA}$, a LLM-based pipeline incorporating two key components: 1) an LLM uncertainty-driven method for edge experiment selection 2) a local graph update strategy utilizing binary feedback from experiments to improve predictions for unselected neighboring edges. Experiments on eight different real-world graphs show our approach often outperforms all baselines including a state-of-the-art numerical method for interactive graph discovery. Further, we conduct a rigorous series of ablations dissecting the impact of each pipeline component. Finally, to assess the impact of memorization, we apply our interactive graph discovery strategy to a complex, new (as of July 2024) causal graph on protein transcription factors, finding strong performance in a setting where memorization is impossible. Overall, our results show IGDA to be a powerful method for graph discovery complementary to existing numerically driven approaches."
      },
      {
        "id": "oai:arXiv.org:2502.20263v2",
        "title": "Vector-Quantized Vision Foundation Models for Object-Centric Learning",
        "link": "https://arxiv.org/abs/2502.20263",
        "author": "Rongzhen Zhao, Vivienne Wang, Juho Kannala, Joni Pajarinen",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.20263v2 Announce Type: replace \nAbstract: Perceiving visual scenes as objects and background -- like humans do -- Object-Centric Learning (OCL) aggregates image or video feature maps into object-level feature vectors, termed \\textit{slots}. OCL's self-supervision of reconstructing the input from these aggregated slots struggles with complex object textures, thus Vision Foundation Model (VFM) representations are used as the aggregation input and reconstruction target. However, existing methods leverage VFM representations in diverse ways and often fail to fully exploit their potential. In response, we propose a clean architecture -- Vector-Quantized VFMs for OCL (VQ-VFM-OCL, or VVO) -- that unifies mainstream OCL methods. The key to our unification is simple yet effective, just shared quantizing the same VFM representation as the reconstruction target. Through mathematical modeling and statistical verification, we further analyze why VFM representations facilitate OCL aggregation and how their shared quantization as reconstruction targets strengthens OCL supervision. Experiments show that across different VFMs, aggregators and decoders, our VVO consistently outperforms baselines in object discovery and recognition, as well as downstream visual prediction and reasoning. The source code is available in supplemental files."
      },
      {
        "id": "oai:arXiv.org:2503.00234v2",
        "title": "Towards Fairness for the Right Reasons: Using Saliency Maps to Evaluate Bias Removal in Neural Networks",
        "link": "https://arxiv.org/abs/2503.00234",
        "author": "Lukasz Sztukiewicz, Ignacy St\\k{e}pka, Micha{\\l} Wili\\'nski, Jerzy Stefanowski",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.00234v2 Announce Type: replace \nAbstract: The widespread adoption of machine learning systems has raised critical concerns about fairness and bias, making mitigating harmful biases essential for AI development. In this paper, we investigate the relationship between fairness improvement and the removal of harmful biases in neural networks applied to computer vision tasks. First, we introduce a set of novel XAI-based metrics that analyze saliency maps to assess shifts in a model's decision-making process. Then, we demonstrate that successful debiasing methods systematically redirect model focus away from protected attributes. Additionally, we show that techniques originally developed for artifact removal can be effectively repurposed for fairness. These findings underscore the importance of ensuring that models are fair for the right reasons, contributing to the development of more ethical and trustworthy AI systems."
      },
      {
        "id": "oai:arXiv.org:2503.00399v3",
        "title": "Extremely low-bitrate Image Compression Semantically Disentangled by LMMs from a Human Perception Perspective",
        "link": "https://arxiv.org/abs/2503.00399",
        "author": "Juan Song, Lijie Yang, Mingtao Feng",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.00399v3 Announce Type: replace \nAbstract: It remains a significant challenge to compress images at extremely low bitrate while achieving both semantic consistency and high perceptual quality. Inspired by human progressive perception mechanism, we propose a Semantically Disentangled Image Compression framework (SEDIC) in this paper. Initially, an extremely compressed reference image is obtained through a learned image encoder. Then we leverage LMMs to extract essential semantic components, including overall descriptions, object detailed description, and semantic segmentation masks. We propose a training-free Object Restoration model with Attention Guidance (ORAG) built on pre-trained ControlNet to restore object details conditioned by object-level text descriptions and semantic masks. Based on the proposed ORAG, we design a multistage semantic image decoder to progressively restore the details object by object, starting from the extremely compressed reference image, ultimately generating high-quality and high-fidelity reconstructions. Experimental results demonstrate that SEDIC significantly outperforms state-of-the-art approaches, achieving superior perceptual quality and semantic consistency at extremely low-bitrates ($\\le$ 0.05 bpp)."
      },
      {
        "id": "oai:arXiv.org:2503.00771v2",
        "title": "Evaluating Personalized Tool-Augmented LLMs from the Perspectives of Personalization and Proactivity",
        "link": "https://arxiv.org/abs/2503.00771",
        "author": "Yupu Hao, Pengfei Cao, Zhuoran Jin, Huanxuan Liao, Yubo Chen, Kang Liu, Jun Zhao",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.00771v2 Announce Type: replace \nAbstract: Personalized tool utilization is essential for aligning large language models (LLMs) with user preference in interaction scenarios with various tools. However, most of the current benchmarks primarily focus on either personalization of text generation or direct tool-utilizing, without considering both. In this work, we introduce a novel benchmark ETAPP for evaluating personalized tool invocation, establishing a sandbox environment, and a comprehensive dataset of 800 testing cases covering diverse user profiles. To improve the accuracy of our evaluation, we propose a key-point-based LLM evaluation method, mitigating biases in the LLM-as-a-judge system by manually annotating key points for each test case and providing them to LLM as the reference. Additionally, we evaluate the excellent LLMs and provide an in-depth analysis. Furthermore, we investigate the impact of different tool-invoking strategies on LLMs' personalization performance and the effects of fine-tuning in our task. The effectiveness of our preference-setting and key-point-based evaluation method is also validated. Our findings offer insights into improving personalized LLM agents. Our Code is available at https://github.com/hypasd-art/ETAPP."
      },
      {
        "id": "oai:arXiv.org:2503.00984v2",
        "title": "Machine Learning for Health symposium 2024 -- Findings track",
        "link": "https://arxiv.org/abs/2503.00984",
        "author": "Stefan Hegselmann, Helen Zhou, Elizabeth Healey, Trenton Chang, Caleb Ellington, Vishwali Mhasawade, Sana Tonekaboni, Peniel Argaw, Haoran Zhang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.00984v2 Announce Type: replace \nAbstract: A collection of the accepted Findings papers that were presented at the 4th Machine Learning for Health symposium (ML4H 2024), which was held on December 15-16, 2024, in Vancouver, BC, Canada. ML4H 2024 invited high-quality submissions describing innovative research in a variety of health-related disciplines including healthcare, biomedicine, and public health. Works could be submitted to either the archival Proceedings track, or the non-archival Findings track. The Proceedings track targeted mature, cohesive works with technical sophistication and high-impact relevance to health. The Findings track promoted works that would spark new insights, collaborations, and discussions at ML4H. Both tracks were given the opportunity to share their work through the in-person poster session. All the manuscripts submitted to ML4H Symposium underwent a double-blind peer-review process."
      },
      {
        "id": "oai:arXiv.org:2503.03506v3",
        "title": "Opinion: Revisiting synthetic data classifications from a privacy perspective",
        "link": "https://arxiv.org/abs/2503.03506",
        "author": "Vibeke Binz Vallevik, Serena Elizabeth Marshall, Aleksandar Babic, Jan Franz Nygaard",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.03506v3 Announce Type: replace \nAbstract: Synthetic data is emerging as a cost-effective solution necessary to meet the increasing data demands of AI development, created either from existing knowledge or derived from real data. The traditional classification of synthetic data types into hybrid, partial or fully synthetic datasets has limited value and does not reflect the ever-increasing methods to generate synthetic data. The generation method and their source jointly shape the characteristics of synthetic data, which in turn determines its practical applications. We make a case for an alternative approach to grouping synthetic data types that better reflect privacy perspectives in order to facilitate regulatory guidance in the generation and processing of synthetic data. This approach to classification provides flexibility to new advancements like deep generative methods and offers a more practical framework for future applications."
      },
      {
        "id": "oai:arXiv.org:2503.04104v2",
        "title": "LLMs Can Generate a Better Answer by Aggregating Their Own Responses",
        "link": "https://arxiv.org/abs/2503.04104",
        "author": "Zichong Li, Xinyu Feng, Yuheng Cai, Zixuan Zhang, Tianyi Liu, Chen Liang, Weizhu Chen, Haoyu Wang, Tuo Zhao",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.04104v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) have shown remarkable capabilities across tasks, yet they often require additional prompting techniques when facing complex problems. While approaches like self-correction and response selection have emerged as popular solutions, recent studies have shown these methods perform poorly when relying on the LLM itself to provide feedback or selection criteria. We argue this limitation stems from the fact that common LLM post-training procedures lack explicit supervision for discriminative judgment tasks. In this paper, we propose Generative Self-Aggregation (GSA), a novel prompting method that improves answer quality without requiring the model's discriminative capabilities. GSA first samples multiple diverse responses from the LLM, then aggregates them to obtain an improved solution. Unlike previous approaches, our method does not require the LLM to correct errors or compare response quality; instead, it leverages the model's generative abilities to synthesize a new response based on the context of multiple samples. While GSA shares similarities with the self-consistency (SC) approach for response aggregation, SC requires specific verifiable tokens to enable majority voting. In contrast, our approach is more general and can be applied to open-ended tasks. Empirical evaluation demonstrates that GSA effectively improves response quality across various tasks, including mathematical reasoning, knowledge-based problems, and open-ended generation tasks such as code synthesis and conversational responses."
      },
      {
        "id": "oai:arXiv.org:2503.04350v2",
        "title": "EDCA - An Evolutionary Data-Centric AutoML Framework for Efficient Pipelines",
        "link": "https://arxiv.org/abs/2503.04350",
        "author": "Joana Sim\\~oes, Jo\\~ao Correia",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.04350v2 Announce Type: replace \nAbstract: Automated Machine Learning (AutoML) gained popularity due to the increased demand for Machine Learning (ML) specialists, allowing them to apply ML techniques effortlessly and quickly. AutoML implementations use optimisation methods to identify the most effective ML solution for a given dataset, aiming to improve one or more predefined metrics. However, most implementations focus on model selection and hyperparameter tuning. Despite being an important factor in obtaining high-performance ML systems, data quality is usually an overlooked part of AutoML and continues to be a manual and time-consuming task. This work presents EDCA, an Evolutionary Data Centric AutoML framework. In addition to the traditional tasks such as selecting the best models and hyperparameters, EDCA enhances the given data by optimising data processing tasks such as data reduction and cleaning according to the problems' needs. All these steps create an ML pipeline that is optimised by an evolutionary algorithm. To assess its effectiveness, EDCA was compared to FLAML and TPOT, two frameworks at the top of the AutoML benchmarks. The frameworks were evaluated in the same conditions using datasets from AMLB classification benchmarks. EDCA achieved statistically similar results in performance to FLAML and TPOT but used significantly less data to train the final solutions. Moreover, EDCA experimental results reveal that a good performance can be achieved using less data and efficient ML algorithm aspects that align with Green AutoML guidelines"
      },
      {
        "id": "oai:arXiv.org:2503.04707v2",
        "title": "Iris Style Transfer: Enhancing Iris Recognition with Style Features and Privacy Preservation through Neural Style Transfer",
        "link": "https://arxiv.org/abs/2503.04707",
        "author": "Mengdi Wang, Efe Bozkir, Enkelejda Kasneci",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.04707v2 Announce Type: replace \nAbstract: Iris texture is widely regarded as a gold standard biometric modality for authentication and identification. The demand for robust iris recognition methods, coupled with growing security and privacy concerns regarding iris attacks, has escalated recently. Inspired by neural style transfer, an advanced technique that leverages neural networks to separate content and style features, we hypothesize that iris texture's style features provide a reliable foundation for recognition and are more resilient to variations like rotation and perspective shifts than traditional approaches. Our experimental results support this hypothesis, showing a significantly higher classification accuracy compared to conventional features. Further, we propose using neural style transfer to obfuscate the identifiable iris style features, ensuring the protection of sensitive biometric information while maintaining the utility of eye images for tasks like eye segmentation and gaze estimation. This work opens new avenues for iris-oriented, secure, and privacy-aware biometric systems."
      },
      {
        "id": "oai:arXiv.org:2503.04844v4",
        "title": "Narrative Context Protocol: an Author-centric Storytelling Framework for Generative AI",
        "link": "https://arxiv.org/abs/2503.04844",
        "author": "Hank Gerba",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.04844v4 Announce Type: replace \nAbstract: Generative AI promises to finally realize dynamic, personalized storytelling technologies across a range of media. To date, experimentation with generative AI in the field of procedural narrative generation has been quite promising from a technical perspective. However, fundamental narrative dilemmas remain, such as the balance between player agency and narrative coherence, and no rigorous narrative standard has been proposed to specifically leverage the strengths of generative AI. In this paper, we propose the Narrative Context Protocol (NCP), an open and extensible standard designed to place writers at the center of future narrative design workflows and enable interoperability across authoring platforms. By encoding an author's intent according to an objective narrative model, the NCP enables narrative portability as well as intent-based constraints for generative systems."
      },
      {
        "id": "oai:arXiv.org:2503.06730v2",
        "title": "Adaptive Test-Time Intervention for Concept Bottleneck Models",
        "link": "https://arxiv.org/abs/2503.06730",
        "author": "Matthew Shen, Aliyah Hsu, Abhineet Agarwal, Bin Yu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.06730v2 Announce Type: replace \nAbstract: Concept bottleneck models (CBM) aim to improve model interpretability by predicting human level \"concepts\" in a bottleneck within a deep learning model architecture. However, how the predicted concepts are used in predicting the target still either remains black-box or is simplified to maintain interpretability at the cost of prediction performance. We propose to use Fast Interpretable Greedy Sum-Trees (FIGS) to obtain Binary Distillation (BD). This new method, called FIGS-BD, distills a binary-augmented concept-to-target portion of the CBM into an interpretable tree-based model, while maintaining the competitive prediction performance of the CBM teacher. FIGS-BD can be used in downstream tasks to explain and decompose CBM predictions into interpretable binary-concept-interaction attributions and guide adaptive test-time intervention. Across 4 datasets, we demonstrate that our adaptive test-time intervention identifies key concepts that significantly improve performance for realistic human-in-the-loop settings that only allow for limited concept interventions."
      },
      {
        "id": "oai:arXiv.org:2503.08673v2",
        "title": "Keypoint Detection and Description for Raw Bayer Images",
        "link": "https://arxiv.org/abs/2503.08673",
        "author": "Jiakai Lin, Jinchang Zhang, Guoyu Lu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.08673v2 Announce Type: replace \nAbstract: Keypoint detection and local feature description are fundamental tasks in robotic perception, critical for applications such as SLAM, robot localization, feature matching, pose estimation, and 3D mapping. While existing methods predominantly operate on RGB images, we propose a novel network that directly processes raw images, bypassing the need for the Image Signal Processor (ISP). This approach significantly reduces hardware requirements and memory consumption, which is crucial for robotic vision systems. Our method introduces two custom-designed convolutional kernels capable of performing convolutions directly on raw images, preserving inter-channel information without converting to RGB. Experimental results show that our network outperforms existing algorithms on raw images, achieving higher accuracy and stability under large rotations and scale variations. This work represents the first attempt to develop a keypoint detection and feature description network specifically for raw images, offering a more efficient solution for resource-constrained environments."
      },
      {
        "id": "oai:arXiv.org:2503.08676v2",
        "title": "Language-Depth Navigated Thermal and Visible Image Fusion",
        "link": "https://arxiv.org/abs/2503.08676",
        "author": "Jinchang Zhang, Zijun Li, Guoyu Lu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.08676v2 Announce Type: replace \nAbstract: Depth-guided multimodal fusion combines depth information from visible and infrared images, significantly enhancing the performance of 3D reconstruction and robotics applications. Existing thermal-visible image fusion mainly focuses on detection tasks, ignoring other critical information such as depth. By addressing the limitations of single modalities in low-light and complex environments, the depth information from fused images not only generates more accurate point cloud data, improving the completeness and precision of 3D reconstruction, but also provides comprehensive scene understanding for robot navigation, localization, and environmental perception. This supports precise recognition and efficient operations in applications such as autonomous driving and rescue missions. We introduce a text-guided and depth-driven infrared and visible image fusion network. The model consists of an image fusion branch for extracting multi-channel complementary information through a diffusion model, equipped with a text-guided module, and two auxiliary depth estimation branches. The fusion branch uses CLIP to extract semantic information and parameters from depth-enriched image descriptions to guide the diffusion model in extracting multi-channel features and generating fused images. These fused images are then input into the depth estimation branches to calculate depth-driven loss, optimizing the image fusion network. This framework aims to integrate vision-language and depth to directly generate color-fused images from multimodal inputs."
      },
      {
        "id": "oai:arXiv.org:2503.08976v2",
        "title": "Not All Edges are Equally Robust: Evaluating the Robustness of Ranking-Based Federated Learning",
        "link": "https://arxiv.org/abs/2503.08976",
        "author": "Zirui Gong, Yanjun Zhang, Leo Yu Zhang, Zhaoxi Zhang, Yong Xiang, Shirui Pan",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.08976v2 Announce Type: replace \nAbstract: Federated Ranking Learning (FRL) is a state-of-the-art FL framework that stands out for its communication efficiency and resilience to poisoning attacks. It diverges from the traditional FL framework in two ways: 1) it leverages discrete rankings instead of gradient updates, significantly reducing communication costs and limiting the potential space for malicious updates, and 2) it uses majority voting on the server side to establish the global ranking, ensuring that individual updates have minimal influence since each client contributes only a single vote. These features enhance the system's scalability and position FRL as a promising paradigm for FL training.\n  However, our analysis reveals that FRL is not inherently robust, as certain edges are particularly vulnerable to poisoning attacks. Through a theoretical investigation, we prove the existence of these vulnerable edges and establish a lower bound and an upper bound for identifying them in each layer. Based on this finding, we introduce a novel local model poisoning attack against FRL, namely the Vulnerable Edge Manipulation (VEM) attack. The VEM attack focuses on identifying and perturbing the most vulnerable edges in each layer and leveraging an optimization-based approach to maximize the attack's impact. Through extensive experiments on benchmark datasets, we demonstrate that our attack achieves an overall 53.23% attack impact and is 3.7x more impactful than existing methods. Our findings highlight significant vulnerabilities in ranking-based FL systems and underline the urgency for the development of new robust FL frameworks."
      },
      {
        "id": "oai:arXiv.org:2503.08980v2",
        "title": "I Predict Therefore I Am: Is Next Token Prediction Enough to Learn Human-Interpretable Concepts from Data?",
        "link": "https://arxiv.org/abs/2503.08980",
        "author": "Yuhang Liu, Dong Gong, Erdun Gao, Zhen Zhang, Biwei Huang, Mingming Gong, Anton van den Hengel, Javen Qinfeng Shi",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.08980v2 Announce Type: replace \nAbstract: The remarkable achievements of large language models (LLMs) have led many to conclude that they exhibit a form of intelligence. This is as opposed to explanations of their capabilities based on their ability to perform relatively simple manipulations of vast volumes of data. To illuminate the distinction between these explanations, we introduce a novel generative model that generates tokens on the basis of human interpretable concepts represented as latent discrete variables. Under mild conditions, even when the mapping from the latent space to the observed space is non-invertible, we establish an identifiability result: the representations learned by LLMs through next-token prediction can be approximately modeled as the logarithm of the posterior probabilities of these latent discrete concepts, up to an invertible linear transformation. This theoretical finding not only provides evidence that LLMs capture underlying generative factors, but also strongly reinforces the linear representation hypothesis, which posits that LLMs learn linear representations of human-interpretable concepts. Empirically, we validate our theoretical results through evaluations on both simulation data and the Pythia, Llama, and DeepSeek model families."
      },
      {
        "id": "oai:arXiv.org:2503.09329v2",
        "title": "Energy Optimized Piecewise Polynomial Approximation Utilizing Modern Machine Learning Optimizers",
        "link": "https://arxiv.org/abs/2503.09329",
        "author": "Hannes Waclawek, Stefan Huber",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.09329v2 Announce Type: replace \nAbstract: This work explores an extension of machine learning-optimized piecewise polynomial approximation by incorporating energy optimization as an additional objective. Traditional closed-form solutions enable continuity and approximation targets but lack flexibility in accommodating complex optimization goals. By leveraging modern gradient descent optimizers within TensorFlow, we introduce a framework that minimizes elastic strain energy in cam profiles, leading to smoother motion. Experimental results confirm the effectiveness of this approach, demonstrating its potential to Pareto-efficiently trade approximation quality against energy consumption."
      },
      {
        "id": "oai:arXiv.org:2503.11297v2",
        "title": "GMG: A Video Prediction Method Based on Global Focus and Motion Guided",
        "link": "https://arxiv.org/abs/2503.11297",
        "author": "Yuhao Du, Hui Liu, Haoxiang Peng, Xinyuan Cheng, Chenrong Wu, Jiankai Zhang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.11297v2 Announce Type: replace \nAbstract: Recent years, weather forecasting has gained significant attention. However, accurately predicting weather remains a challenge due to the rapid variability of meteorological data and potential teleconnections. Current spatiotemporal forecasting models primarily rely on convolution operations or sliding windows for feature extraction. These methods are limited by the size of the convolutional kernel or sliding window, making it difficult to capture and identify potential teleconnection features in meteorological data. Additionally, weather data often involve non-rigid bodies, whose motion processes are accompanied by unpredictable deformations, further complicating the forecasting task. In this paper, we propose the GMG model to address these two core challenges. The Global Focus Module, a key component of our model, enhances the global receptive field, while the Motion Guided Module adapts to the growth or dissipation processes of non-rigid bodies. Through extensive evaluations, our method demonstrates competitive performance across various complex tasks, providing a novel approach to improving the predictive accuracy of complex spatiotemporal data."
      },
      {
        "id": "oai:arXiv.org:2503.12047v2",
        "title": "PSGait: Gait Recognition using Parsing Skeleton",
        "link": "https://arxiv.org/abs/2503.12047",
        "author": "Hangrui Xu, Chuanrui Zhang, Zhengxian Wu, Peng Jiao, Haoqian Wang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.12047v2 Announce Type: replace \nAbstract: Gait recognition has emerged as a robust biometric modality due to its non-intrusive nature and resilience to occlusion. Conventional gait recognition methods typically rely on silhouettes or skeletons. Despite their success in gait recognition for controlled laboratory environments, they usually fail in real-world scenarios due to their limited information entropy for gait representations. To achieve accurate gait recognition in the wild, we propose a novel gait representation, named Parsing Skeleton. This representation innovatively introduces the skeleton-guided human parsing method to capture fine-grained body dynamics, so they have much higher information entropy to encode the shapes and dynamics of fine-grained human parts during walking. Moreover, to effectively explore the capability of the Parsing Skeleton representation, we propose a novel Parsing Skeleton-based gait recognition framework, named PSGait, which takes Parsing Skeletons and silhouettes as input. By fusing these two modalities, the resulting image sequences are fed into gait recognition models for enhanced individual differentiation. We conduct comprehensive benchmarks on various datasets to evaluate our model. PSGait outperforms existing state-of-the-art multimodal methods that utilize both skeleton and silhouette inputs while significantly reducing computational resources. Furthermore, as a plug-and-play method, PSGait leads to a maximum improvement of 10.9% in Rank-1 accuracy across various gait recognition models. These results demonstrate that Parsing Skeleton offers a lightweight, effective, and highly generalizable representation for gait recognition in the wild."
      },
      {
        "id": "oai:arXiv.org:2503.12419v2",
        "title": "EgoEvGesture: Gesture Recognition Based on Egocentric Event Camera",
        "link": "https://arxiv.org/abs/2503.12419",
        "author": "Luming Wang, Hao Shi, Xiaoting Yin, Kailun Yang, Kaiwei Wang, Jian Bai",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.12419v2 Announce Type: replace \nAbstract: Egocentric gesture recognition is a pivotal technology for enhancing natural human-computer interaction, yet traditional RGB-based solutions suffer from motion blur and illumination variations in dynamic scenarios. While event cameras show distinct advantages in handling high dynamic range with ultra-low power consumption, existing RGB-based architectures face inherent limitations in processing asynchronous event streams due to their synchronous frame-based nature. Moreover, from an egocentric perspective, event cameras record data that includes events generated by both head movements and hand gestures, thereby increasing the complexity of gesture recognition. To address this, we propose a novel network architecture specifically designed for event data processing, incorporating (1) a lightweight CNN with asymmetric depthwise convolutions to reduce parameters while preserving spatiotemporal features, (2) a plug-and-play state-space model as context block that decouples head movement noise from gesture dynamics, and (3) a parameter-free Bins-Temporal Shift Module (BSTM) that shifts features along bins and temporal dimensions to fuse sparse events efficiently. We further establish the EgoEvGesture dataset, the first large-scale dataset for egocentric gesture recognition using event cameras. Experimental results demonstrate that our method achieves 62.7% accuracy tested on unseen subjects with only 7M parameters, 3.1% higher than state-of-the-art approaches. Notable misclassifications in freestyle motions stem from high inter-personal variability and unseen test patterns differing from training data. Moreover, our approach achieved a remarkable accuracy of 97.0% on the DVS128 Gesture, demonstrating the effectiveness and generalization capability of our method on public datasets. The dataset and models are made available at https://github.com/3190105222/EgoEv_Gesture."
      },
      {
        "id": "oai:arXiv.org:2503.12885v2",
        "title": "DreamRenderer: Taming Multi-Instance Attribute Control in Large-Scale Text-to-Image Models",
        "link": "https://arxiv.org/abs/2503.12885",
        "author": "Dewei Zhou, Mingwei Li, Zongxin Yang, Yi Yang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.12885v2 Announce Type: replace \nAbstract: Image-conditioned generation methods, such as depth- and canny-conditioned approaches, have demonstrated remarkable abilities for precise image synthesis. However, existing models still struggle to accurately control the content of multiple instances (or regions). Even state-of-the-art models like FLUX and 3DIS face challenges, such as attribute leakage between instances, which limits user control. To address these issues, we introduce DreamRenderer, a training-free approach built upon the FLUX model. DreamRenderer enables users to control the content of each instance via bounding boxes or masks, while ensuring overall visual harmony. We propose two key innovations: 1) Bridge Image Tokens for Hard Text Attribute Binding, which uses replicated image tokens as bridge tokens to ensure that T5 text embeddings, pre-trained solely on text data, bind the correct visual attributes for each instance during Joint Attention; 2) Hard Image Attribute Binding applied only to vital layers. Through our analysis of FLUX, we identify the critical layers responsible for instance attribute rendering and apply Hard Image Attribute Binding only in these layers, using soft binding in the others. This approach ensures precise control while preserving image quality. Evaluations on the COCO-POS and COCO-MIG benchmarks demonstrate that DreamRenderer improves the Image Success Ratio by 17.7% over FLUX and enhances the performance of layout-to-image models like GLIGEN and 3DIS by up to 26.8%. Project Page: https://limuloo.github.io/DreamRenderer/."
      },
      {
        "id": "oai:arXiv.org:2503.13208v3",
        "title": "Improving Complex Reasoning with Dynamic Prompt Corruption: A soft prompt Optimization Approach",
        "link": "https://arxiv.org/abs/2503.13208",
        "author": "Sinan Fan, Liang Xie, Chen Shen, Ge Teng, Xiaosong Yuan, Xiaofeng Zhang, Chenxi Huang, Wenxiao Wang, Xiaofei He, Jieping Ye",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.13208v3 Announce Type: replace \nAbstract: Prompt-tuning (PT) for large language models (LLMs) can facilitate the performance on various conventional NLP tasks with significantly fewer trainable parameters. However, our investigation reveals that PT provides limited improvement and may even degrade the primitive performance of LLMs on complex reasoning tasks. Such a phenomenon suggests that soft prompts can positively impact certain instances while negatively affecting others, particularly during the later phases of reasoning. To address these challenges, We first identify an information accumulation within the soft prompts. Through detailed analysis, we demonstrate that this phenomenon is often accompanied by erroneous information flow patterns in the deeper layers of the model, which ultimately lead to incorrect reasoning outcomes. we propose a novel method called Dynamic Prompt Corruption (DPC) to take better advantage of soft prompts in complex reasoning tasks, which dynamically adjusts the influence of soft prompts based on their impact on the reasoning process. Specifically, DPC consists of two stages: Dynamic Trigger and Dynamic Corruption. First, Dynamic Trigger measures the impact of soft prompts, identifying whether beneficial or detrimental. Then, Dynamic Corruption mitigates the negative effects of soft prompts by selectively masking key tokens that interfere with the reasoning process. We validate the proposed approach through extensive experiments on various LLMs and reasoning tasks, including GSM8K, MATH, and AQuA. Experimental results demonstrate that DPC can consistently enhance the performance of PT, achieving 4%-8% accuracy gains compared to vanilla prompt tuning, highlighting the effectiveness of our approach and its potential to enhance complex reasoning in LLMs."
      },
      {
        "id": "oai:arXiv.org:2503.13423v2",
        "title": "SuperBPE: Space Travel for Language Models",
        "link": "https://arxiv.org/abs/2503.13423",
        "author": "Alisa Liu, Jonathan Hayase, Valentin Hofmann, Sewoong Oh, Noah A. Smith, Yejin Choi",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.13423v2 Announce Type: replace \nAbstract: The assumption across nearly all language model (LM) tokenization schemes is that tokens should be subwords, i.e., contained within word boundaries. While providing a seemingly reasonable inductive bias, is this common practice limiting the potential of modern LMs? Whitespace is not a reliable delimiter of meaning, as evidenced by multi-word expressions (e.g., \"by the way\"), crosslingual variation in the number of words needed to express a concept (e.g., \"spacesuit helmet\" in German is \"raumanzughelm\"), and languages that do not use whitespace at all (e.g., Chinese). To explore the potential of tokenization beyond subwords, we introduce a \"superword\" tokenizer, SuperBPE, which incorporates a simple pretokenization curriculum into the byte-pair encoding (BPE) algorithm to first learn subwords, then superwords that bridge whitespace. This brings dramatic improvements in encoding efficiency: when fixing the vocabulary size to 200k, SuperBPE encodes a fixed piece of text with up to 33% fewer tokens than BPE on average. In experiments, we pretrain 8B transformer LMs from scratch while fixing the model size, vocabulary size, and train compute, varying *only* the algorithm for learning the vocabulary. Our model trained with SuperBPE achieves an average +4.0% absolute improvement over the BPE baseline across 30 downstream tasks (including +8.2% on MMLU), while simultaneously requiring 27% less compute at inference time. In analysis, we find that SuperBPE results in segmentations of text that are more uniform in per-token difficulty. Qualitatively, this may be because SuperBPE tokens often capture common multi-word expressions that function semantically as a single unit. SuperBPE is a straightforward, local modification to tokenization that improves both encoding efficiency and downstream performance, yielding better language models overall."
      },
      {
        "id": "oai:arXiv.org:2503.14301v2",
        "title": "FeNeC: Enhancing Continual Learning via Feature Clustering with Neighbor- or Logit-Based Classification",
        "link": "https://arxiv.org/abs/2503.14301",
        "author": "Kamil Ksi\\k{a}\\.zek, Hubert Jastrz\\k{e}bski, Bartosz Trojan, Krzysztof Pniaczek, Micha{\\l} Karp, Jacek Tabor",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.14301v2 Announce Type: replace \nAbstract: The ability of deep learning models to learn continuously is essential for adapting to new data categories and evolving data distributions. In recent years, approaches leveraging frozen feature extractors after an initial learning phase have been extensively studied. Many of these methods estimate per-class covariance matrices and prototypes based on backbone-derived feature representations. Within this paradigm, we introduce FeNeC (Feature Neighborhood Classifier) and FeNeC-Log, its variant based on the log-likelihood function. Our approach generalizes the existing concept by incorporating data clustering to capture greater intra-class variability. Utilizing the Mahalanobis distance, our models classify samples either through a nearest neighbor approach or trainable logit values assigned to consecutive classes. Our proposition may be reduced to the existing approaches in a special case while extending them with the ability of more flexible adaptation to data. We demonstrate that two FeNeC variants achieve competitive performance in scenarios where task identities are unknown and establish state-of-the-art results on several benchmarks."
      },
      {
        "id": "oai:arXiv.org:2503.14911v2",
        "title": "Derm1M: A Million-scale Vision-Language Dataset Aligned with Clinical Ontology Knowledge for Dermatology",
        "link": "https://arxiv.org/abs/2503.14911",
        "author": "Siyuan Yan, Ming Hu, Yiwen Jiang, Xieji Li, Hao Fei, Philipp Tschandl, Harald Kittler, Zongyuan Ge",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.14911v2 Announce Type: replace \nAbstract: The emergence of vision-language models has transformed medical AI, enabling unprecedented advances in diagnostic capability and clinical applications. However, progress in dermatology has lagged behind other medical domains due to the lack of standard image-text pairs. Existing dermatological datasets are limited in both scale and depth, offering only single-label annotations across a narrow range of diseases instead of rich textual descriptions, and lacking the crucial clinical context needed for real-world applications. To address these limitations, we present Derm1M, the first large-scale vision-language dataset for dermatology, comprising 1,029,761 image-text pairs. Built from diverse educational resources and structured around a standard ontology collaboratively developed by experts, Derm1M provides comprehensive coverage for over 390 skin conditions across four hierarchical levels and 130 clinical concepts with rich contextual information such as medical history, symptoms, and skin tone. To demonstrate Derm1M potential in advancing both AI research and clinical application, we pretrained a series of CLIP-like models, collectively called DermLIP, on this dataset. The DermLIP family significantly outperforms state-of-the-art foundation models on eight diverse datasets across multiple tasks, including zero-shot skin disease classification, clinical and artifacts concept identification, few-shot/full-shot learning, and cross-modal retrieval. Our dataset and code will be publicly available at https://github.com/SiyuanYan1/Derm1M upon acceptance."
      },
      {
        "id": "oai:arXiv.org:2503.15166v2",
        "title": "Machine Unlearning in Hyperbolic vs. Euclidean Multimodal Contrastive Learning: Adapting Alignment Calibration to MERU",
        "link": "https://arxiv.org/abs/2503.15166",
        "author": "\\`Alex Pujol Vidal, Sergio Escalera, Kamal Nasrollahi, Thomas B. Moeslund",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.15166v2 Announce Type: replace \nAbstract: Machine unlearning methods have become increasingly important for selective concept removal in large pre-trained models. While recent work has explored unlearning in Euclidean contrastive vision-language models, the effectiveness of concept removal in hyperbolic spaces remains unexplored. This paper investigates machine unlearning in hyperbolic contrastive learning by adapting Alignment Calibration to MERU, a model that embeds images and text in hyperbolic space to better capture semantic hierarchies. Through systematic experiments and ablation studies, we demonstrate that hyperbolic geometry offers distinct advantages for concept removal, achieving near perfect forgetting with reasonable performance on retained concepts, particularly when scaling to multiple concept removal. Our approach introduces hyperbolic-specific components including entailment calibration and norm regularization that leverage the unique properties of hyperbolic space. Comparative analysis with Euclidean models reveals fundamental differences in unlearning dynamics, with hyperbolic unlearning reorganizing the semantic hierarchy while Euclidean approaches merely disconnect cross-modal associations. These findings not only advance machine unlearning techniques but also provide insights into the geometric properties that influence concept representation and removal in multimodal models. Source code available at https://github.com/alex-pv01/HAC"
      },
      {
        "id": "oai:arXiv.org:2503.15793v3",
        "title": "DNR Bench: Benchmarking Over-Reasoning in Reasoning LLMs",
        "link": "https://arxiv.org/abs/2503.15793",
        "author": "Masoud Hashemi, Oluwanifemi Bamgbose, Sathwik Tejaswi Madhusudhan, Jishnu Sethumadhavan Nair, Aman Tiwari, Vikas Yadav",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.15793v3 Announce Type: replace \nAbstract: Test-time scaling has significantly improved large language model performance, enabling deeper reasoning to solve complex problems. However, this increased reasoning capability also leads to excessive token generation and unnecessary problem-solving attempts. We introduce Don\\'t Answer Bench (DNA Bench), a new benchmark designed to evaluate LLMs ability to robustly understand the tricky reasoning triggers and avoiding unnecessary generation. DNA Bench consists of 150 adversarially designed prompts that are easy for humans to understand and respond to, but surprisingly not for many of the recent prominent LLMs. DNA Bench tests models abilities across different capabilities, such as instruction adherence, hallucination avoidance, redundancy filtering, and unanswerable question recognition. We evaluate reasoning LLMs (RLMs), including DeepSeek-R1, OpenAI O3-mini, Claude-3.7-sonnet and compare them against a powerful non-reasoning model, e.g., GPT-4o. Our experiments reveal that RLMs generate up to 70x more tokens than necessary, often failing at tasks that simpler non-reasoning models handle efficiently with higher accuracy. Our findings underscore the need for more effective training and inference strategies in RLMs."
      },
      {
        "id": "oai:arXiv.org:2503.16316v2",
        "title": "On the Cone Effect in the Learning Dynamics",
        "link": "https://arxiv.org/abs/2503.16316",
        "author": "Zhanpeng Zhou, Yongyi Yang, Jie Ren, Mahito Sugiyama, Junchi Yan",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.16316v2 Announce Type: replace \nAbstract: Understanding the learning dynamics of neural networks is a central topic in the deep learning community. In this paper, we take an empirical perspective to study the learning dynamics of neural networks in real-world settings. Specifically, we investigate the evolution process of the empirical Neural Tangent Kernel (eNTK) during training. Our key findings reveal a two-phase learning process: i) in Phase I, the eNTK evolves significantly, signaling the rich regime, and ii) in Phase II, the eNTK keeps evolving but is constrained in a narrow space, a phenomenon we term the cone effect. This two-phase framework builds on the hypothesis proposed by Fort et al. (2020), but we uniquely identify the cone effect in Phase II, demonstrating its significant performance advantages over fully linearized training."
      },
      {
        "id": "oai:arXiv.org:2503.16340v4",
        "title": "Deep learning framework for action prediction reveals multi-timescale locomotor control",
        "link": "https://arxiv.org/abs/2503.16340",
        "author": "Wei-Chen Wang, Antoine De Comite, Alexandra Voloshina, Monica Daley, Nidhi Seethapathi",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.16340v4 Announce Type: replace \nAbstract: Modeling human movement in real-world tasks is a fundamental goal for motor control, biomechanics, and rehabilitation engineering. However, existing models of essential tasks like locomotion are not applicable across varying terrain, mechanical conditions, and sensory contexts. This is at least in part due to simplifying assumptions like linear and fixed timescales mappings between inputs and future actions, which may not be broadly applicable. Here, we develop a deep learning-based framework for action prediction, outperforming traditional models across multiple contexts (walking and running, treadmill and overground, varying terrains) and input modalities (multiple body states, visual gaze). We find that neural network architectures with flexible input history-dependence, like GRU and Transformer, and with architecture-dependent trial embeddings perform best overall. By quantifying the model's predictions relative to an autoregressive baseline, we identify context- and modality-dependent timescales. These analyses reveal that there is greater reliance on fast-timescale predictions in complex terrain, gaze predicts future foot placement before body states, and the full-body state predictions precede those by center-of-mass states. This deep learning framework for human action prediction provides quantifiable insights into the control of real-world locomotion and can be extended to other actions, contexts, and populations."
      },
      {
        "id": "oai:arXiv.org:2503.16535v2",
        "title": "Vision-Language Embodiment for Monocular Depth Estimation",
        "link": "https://arxiv.org/abs/2503.16535",
        "author": "Jinchang Zhang, Guoyu Lu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.16535v2 Announce Type: replace \nAbstract: Depth estimation is a core problem in robotic perception and vision tasks, but 3D reconstruction from a single image presents inherent uncertainties. Current depth estimation models primarily rely on inter-image relationships for supervised training, often overlooking the intrinsic information provided by the camera itself. We propose a method that embodies the camera model and its physical characteristics into a deep learning model, computing embodied scene depth through real-time interactions with road environments. The model can calculate embodied scene depth in real-time based on immediate environmental changes using only the intrinsic properties of the camera, without any additional equipment. By combining embodied scene depth with RGB image features, the model gains a comprehensive perspective on both geometric and visual details. Additionally, we incorporate text descriptions containing environmental content and depth information as priors for scene understanding, enriching the model's perception of objects. This integration of image and language - two inherently ambiguous modalities - leverages their complementary strengths for monocular depth estimation. The real-time nature of the embodied language and depth prior model ensures that the model can continuously adjust its perception and behavior in dynamic environments. Experimental results show that the embodied depth estimation method enhances model performance across different scenes."
      },
      {
        "id": "oai:arXiv.org:2503.17039v2",
        "title": "Summarization Metrics for Spanish and Basque: Do Automatic Scores and LLM-Judges Correlate with Humans?",
        "link": "https://arxiv.org/abs/2503.17039",
        "author": "Jeremy Barnes, Naiara Perez, Alba Bonet-Jover, Bego\\~na Altuna",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.17039v2 Announce Type: replace \nAbstract: Studies on evaluation metrics and LLM-as-a-Judge models for automatic text summarization have largely been focused on English, limiting our understanding of their effectiveness in other languages. Through our new dataset BASSE (BAsque and Spanish Summarization Evaluation), we address this situation by collecting human judgments on 2,040 abstractive summaries in Basque and Spanish, generated either manually or by five LLMs with four different prompts. For each summary, annotators evaluated five criteria on a 5-point Likert scale: coherence, consistency, fluency, relevance, and 5W1H. We use these data to reevaluate traditional automatic metrics used for evaluating summaries, as well as several LLM-as-a-Judge models that show strong performance on this task in English. Our results show that currently proprietary judge LLMs have the highest correlation with human judgments, followed by criteria-specific automatic metrics, while open-sourced judge LLMs perform poorly. We release BASSE and our code publicly, along with the first large-scale Basque summarization dataset containing 22,525 news articles with their subheads."
      },
      {
        "id": "oai:arXiv.org:2503.17417v2",
        "title": "Generative Modeling of Class Probability for Multi-Modal Representation Learning",
        "link": "https://arxiv.org/abs/2503.17417",
        "author": "Jungkyoo Shin, Bumsoo Kim, Eunwoo Kim",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.17417v2 Announce Type: replace \nAbstract: Multi-modal understanding plays a crucial role in artificial intelligence by enabling models to jointly interpret inputs from different modalities. However, conventional approaches such as contrastive learning often struggle with modality discrepancies, leading to potential misalignments. In this paper, we propose a novel class anchor alignment approach that leverages class probability distributions for multi-modal representation learning. Our method, Class-anchor-ALigned generative Modeling (CALM), encodes class anchors as prompts to generate and align class probability distributions for each modality, enabling more effective alignment. Furthermore, we introduce a cross-modal probabilistic variational autoencoder to model uncertainty in the alignment, enhancing the ability to capture deeper relationships between modalities and data variations. Extensive experiments on four benchmark datasets demonstrate that our approach significantly outperforms state-of-the-art methods, especially in out-of-domain evaluations. This highlights its superior generalization capabilities in multi-modal representation learning."
      },
      {
        "id": "oai:arXiv.org:2503.18826v2",
        "title": "Interpretable and Fair Mechanisms for Abstaining Classifiers",
        "link": "https://arxiv.org/abs/2503.18826",
        "author": "Daphne Lenders, Andrea Pugnana, Roberto Pellungrini, Toon Calders, Dino Pedreschi, Fosca Giannotti",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.18826v2 Announce Type: replace \nAbstract: Abstaining classifiers have the option to refrain from providing a prediction for instances that are difficult to classify. The abstention mechanism is designed to trade off the classifier's performance on the accepted data while ensuring a minimum number of predictions. In this setting, often fairness concerns arise when the abstention mechanism solely reduces errors for the majority groups of the data, resulting in increased performance differences across demographic groups. While there exist a bunch of methods that aim to reduce discrimination when abstaining, there is no mechanism that can do so in an explainable way. In this paper, we fill this gap by introducing Interpretable and Fair Abstaining Classifier IFAC, an algorithm that can reject predictions both based on their uncertainty and their unfairness. By rejecting possibly unfair predictions, our method reduces error and positive decision rate differences across demographic groups of the non-rejected data. Since the unfairness-based rejections are based on an interpretable-by-design method, i.e., rule-based fairness checks and situation testing, we create a transparent process that can empower human decision-makers to review the unfair predictions and make more just decisions for them. This explainable aspect is especially important in light of recent AI regulations, mandating that any high-risk decision task should be overseen by human experts to reduce discrimination risks."
      },
      {
        "id": "oai:arXiv.org:2503.18982v2",
        "title": "Generative Data Imputation for Sparse Learner Performance Data Using Generative Adversarial Imputation Networks",
        "link": "https://arxiv.org/abs/2503.18982",
        "author": "Liang Zhang, Jionghao Lin, John Sabatini, Diego Zapata-Rivera, Carol Forsyth, Yang Jiang, John Hollander, Xiangen Hu, Arthur C. Graesser",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.18982v2 Announce Type: replace \nAbstract: Learner performance data collected by Intelligent Tutoring Systems (ITSs), such as responses to questions, is essential for modeling and predicting learners' knowledge states. However, missing responses due to skips or incomplete attempts create data sparsity, challenging accurate assessment and personalized instruction. To address this, we propose a generative imputation approach using Generative Adversarial Imputation Networks (GAIN). Our method features a three-dimensional (3D) framework (learners, questions, and attempts), flexibly accommodating various sparsity levels. Enhanced by convolutional neural networks and optimized with a least squares loss function, the GAIN-based method aligns input and output dimensions to question-attempt matrices along the learners' dimension. Extensive experiments using datasets from AutoTutor Adult Reading Comprehension (ARC), ASSISTments, and MATHia demonstrate that our approach significantly outperforms tensor factorization and alternative GAN methods in imputation accuracy across different attempt scenarios. Bayesian Knowledge Tracing (BKT) further validates the effectiveness of the imputed data by estimating learning parameters: initial knowledge (P(L0)), learning rate (P(T)), guess rate (P(G)), and slip rate (P(S)). Results indicate the imputed data enhances model fit and closely mirrors original distributions, capturing underlying learning behaviors reliably. Kullback-Leibler (KL) divergence assessments confirm minimal divergence, showing the imputed data preserves essential learning characteristics effectively. These findings underscore GAIN's capability as a robust imputation tool in ITSs, alleviating data sparsity and supporting adaptive, individualized instruction, ultimately leading to more precise and responsive learner assessments and improved educational outcomes."
      },
      {
        "id": "oai:arXiv.org:2503.20299v2",
        "title": "Finding Near-Optimal Maximum Set of Disjoint $k$-Cliques in Real-World Social Networks",
        "link": "https://arxiv.org/abs/2503.20299",
        "author": "Wenqing Lin, Xin Chen, Haoxuan Xie, Sibo Wang, Siqiang Luo",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.20299v2 Announce Type: replace \nAbstract: A $k$-clique is a dense graph, consisting of $k$ fully-connected nodes, that finds numerous applications, such as community detection and network analysis. In this paper, we study a new problem, that finds a maximum set of disjoint $k$-cliques in a given large real-world graph with a user-defined fixed number $k$, which can contribute to a good performance of teaming collaborative events in online games. However, this problem is NP-hard when $k \\geq 3$, making it difficult to solve. To address that, we propose an efficient lightweight method that avoids significant overheads and achieves a $k$-approximation to the optimal, which is equipped with several optimization techniques, including the ordering method, degree estimation in the clique graph, and a lightweight implementation. Besides, to handle dynamic graphs that are widely seen in real-world social networks, we devise an efficient indexing method with careful swapping operations, leading to the efficient maintenance of a near-optimal result with frequent updates in the graph. In various experiments on several large graphs, our proposed approaches significantly outperform the competitors by up to 2 orders of magnitude in running time and 13.3\\% in the number of computed disjoint $k$-cliques, which demonstrates the superiority of the proposed approaches in terms of efficiency and effectiveness."
      },
      {
        "id": "oai:arXiv.org:2503.20737v2",
        "title": "Ontology-based Semantic Similarity Measures for Clustering Medical Concepts in Drug Safety",
        "link": "https://arxiv.org/abs/2503.20737",
        "author": "Jeffery L Painter, Fran\\c{c}ois Haguinet, Gregory E Powell, Andrew Bate",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.20737v2 Announce Type: replace \nAbstract: Semantic similarity measures (SSMs) are widely used in biomedical research but remain underutilized in pharmacovigilance. This study evaluates six ontology-based SSMs for clustering MedDRA Preferred Terms (PTs) in drug safety data. Using the Unified Medical Language System (UMLS), we assess each method's ability to group PTs around medically meaningful centroids. A high-throughput framework was developed with a Java API and Python and R interfaces support large-scale similarity computations. Results show that while path-based methods perform moderately with F1 scores of 0.36 for WUPALMER and 0.28 for LCH, intrinsic information content (IC)-based measures, especially INTRINSIC-LIN and SOKAL, consistently yield better clustering accuracy (F1 score of 0.403). Validated against expert review and standard MedDRA queries (SMQs), our findings highlight the promise of IC-based SSMs in enhancing pharmacovigilance workflows by improving early signal detection and reducing manual review."
      },
      {
        "id": "oai:arXiv.org:2503.21106v4",
        "title": "Function Alignment: A New Theory of Mind and Intelligence, Part I: Foundations",
        "link": "https://arxiv.org/abs/2503.21106",
        "author": "Gus G. Xia",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.21106v4 Announce Type: replace \nAbstract: This paper introduces function alignment, a novel theory of mind and intelligence that is both intuitively compelling and structurally grounded. It explicitly models how meaning, interpretation, and analogy emerge from interactions among layered representations, forming a coherent framework capable not only of modeling minds but also of serving as a blueprint for building them. One of the key theoretical insights derived from function alignment is bounded interpretability, which provides a unified explanation for previously fragmented ideas in cognitive science, such as bounded rationality, symbol grounding, and analogy-making. Beyond modeling, the function alignment framework bridges disciplines often kept apart, linking computational architecture, psychological theory, and even contemplative traditions such as Zen. Rather than building on any philosophical systems, it offers a structural foundation upon which multiple ways of understanding the mind may be reconstructed."
      },
      {
        "id": "oai:arXiv.org:2503.22121v2",
        "title": "Detecting Localized Deepfake Manipulations Using Action Unit-Guided Video Representations",
        "link": "https://arxiv.org/abs/2503.22121",
        "author": "Tharun Anand, Siva Sankar Sajeev, Pravin Nair",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.22121v2 Announce Type: replace \nAbstract: With rapid advancements in generative modeling, deepfake techniques are increasingly narrowing the gap between real and synthetic videos, raising serious privacy and security concerns. Beyond traditional face swapping and reenactment, an emerging trend in recent state-of-the-art deepfake generation methods involves localized edits such as subtle manipulations of specific facial features like raising eyebrows, altering eye shapes, or modifying mouth expressions. These fine-grained manipulations pose a significant challenge for existing detection models, which struggle to capture such localized variations. To the best of our knowledge, this work presents the first detection approach explicitly designed to generalize to localized edits in deepfake videos by leveraging spatiotemporal representations guided by facial action units. Our method leverages a cross-attention-based fusion of representations learned from pretext tasks like random masking and action unit detection, to create an embedding that effectively encodes subtle, localized changes. Comprehensive evaluations across multiple deepfake generation methods demonstrate that our approach, despite being trained solely on the traditional FF+ dataset, sets a new benchmark in detecting recent deepfake-generated videos with fine-grained local edits, achieving a $20\\%$ improvement in accuracy over current state-of-the-art detection methods. Additionally, our method delivers competitive performance on standard datasets, highlighting its robustness and generalization across diverse types of local and global forgeries."
      },
      {
        "id": "oai:arXiv.org:2503.22268v2",
        "title": "Segment Any Motion in Videos",
        "link": "https://arxiv.org/abs/2503.22268",
        "author": "Nan Huang, Wenzhao Zheng, Chenfeng Xu, Kurt Keutzer, Shanghang Zhang, Angjoo Kanazawa, Qianqian Wang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.22268v2 Announce Type: replace \nAbstract: Moving object segmentation is a crucial task for achieving a high-level understanding of visual scenes and has numerous downstream applications. Humans can effortlessly segment moving objects in videos. Previous work has largely relied on optical flow to provide motion cues; however, this approach often results in imperfect predictions due to challenges such as partial motion, complex deformations, motion blur and background distractions. We propose a novel approach for moving object segmentation that combines long-range trajectory motion cues with DINO-based semantic features and leverages SAM2 for pixel-level mask densification through an iterative prompting strategy. Our model employs Spatio-Temporal Trajectory Attention and Motion-Semantic Decoupled Embedding to prioritize motion while integrating semantic support. Extensive testing on diverse datasets demonstrates state-of-the-art performance, excelling in challenging scenarios and fine-grained segmentation of multiple objects. Our code is available at https://motion-seg.github.io/."
      },
      {
        "id": "oai:arXiv.org:2503.23167v2",
        "title": "Graph ODEs and Beyond: A Comprehensive Survey on Integrating Differential Equations with Graph Neural Networks",
        "link": "https://arxiv.org/abs/2503.23167",
        "author": "Zewen Liu, Xiaoda Wang, Bohan Wang, Zijie Huang, Carl Yang, Wei Jin",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.23167v2 Announce Type: replace \nAbstract: Graph Neural Networks (GNNs) and differential equations (DEs) are two rapidly advancing areas of research that have shown remarkable synergy in recent years. GNNs have emerged as powerful tools for learning on graph-structured data, while differential equations provide a principled framework for modeling continuous dynamics across time and space. The intersection of these fields has led to innovative approaches that leverage the strengths of both, enabling applications in physics-informed learning, spatiotemporal modeling, and scientific computing. This survey aims to provide a comprehensive overview of the burgeoning research at the intersection of GNNs and DEs. We will categorize existing methods, discuss their underlying principles, and highlight their applications across domains such as molecular modeling, traffic prediction, and epidemic spreading. Furthermore, we identify open challenges and outline future research directions to advance this interdisciplinary field. A comprehensive paper list is provided at https://github.com/Emory-Melody/Awesome-Graph-NDEs. This survey serves as a resource for researchers and practitioners seeking to understand and contribute to the fusion of GNNs and DEs"
      },
      {
        "id": "oai:arXiv.org:2503.23174v3",
        "title": "TRA: Better Length Generalisation with Threshold Relative Attention",
        "link": "https://arxiv.org/abs/2503.23174",
        "author": "Mattia Opper, Roland Fernandez, Paul Smolensky, Jianfeng Gao",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.23174v3 Announce Type: replace \nAbstract: Transformers struggle with length generalisation, displaying poor performance even on basic tasks. We test whether these limitations can be explained through two key failures of the self-attention mechanism. The first is the inability to fully remove irrelevant information. The second is tied to position, even if the dot product between a key and query is highly negative (i.e. an irrelevant key) learned positional biases may unintentionally up-weight such information - dangerous when distances become out of distribution. Put together, these two failure cases lead to compounding generalisation difficulties. We test whether they can be mitigated through the combination of a) selective sparsity - completely removing irrelevant keys from the attention softmax and b) contextualised relative distance - distance is only considered as between the query and the keys that matter. We show how refactoring the attention mechanism with these two mitigations in place can substantially improve generalisation capabilities of decoder only transformers."
      },
      {
        "id": "oai:arXiv.org:2503.23726v2",
        "title": "PDSL: Privacy-Preserved Decentralized Stochastic Learning with Heterogeneous Data Distribution",
        "link": "https://arxiv.org/abs/2503.23726",
        "author": "Lina Wang, Yunsheng Yuan, Chunxiao Wang, Feng Li",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.23726v2 Announce Type: replace \nAbstract: In the paradigm of decentralized learning, a group of agents collaborates to learn a global model using distributed datasets without a central server. However, due to the heterogeneity of the local data across the different agents, learning a robust global model is rather challenging. Moreover, the collaboration of the agents relies on their gradient information exchange, which poses a risk of privacy leakage. In this paper, to address these issues, we propose PDSL, a novel privacy-preserved decentralized stochastic learning algorithm with heterogeneous data distribution. On one hand, we innovate in utilizing the notion of Shapley values such that each agent can precisely measure the contributions of its heterogeneous neighbors to the global learning goal; on the other hand, we leverage the notion of differential privacy to prevent each agent from suffering privacy leakage when it contributes gradient information to its neighbors. We conduct both solid theoretical analysis and extensive experiments to demonstrate the efficacy of our PDSL algorithm in terms of privacy preservation and convergence."
      },
      {
        "id": "oai:arXiv.org:2503.24043v2",
        "title": "Frequency-Aware Attention-LSTM for PM$_{2.5}$ Time Series Forecasting",
        "link": "https://arxiv.org/abs/2503.24043",
        "author": "Jiahui LU, Shuang Wu, Zhenkai Qin, Dongze Wu, Guifang Yang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.24043v2 Announce Type: replace \nAbstract: To enhance the accuracy and robustness of PM$_{2.5}$ concentration forecasting, this paper introduces FALNet, a Frequency-Aware LSTM Network that integrates frequency-domain decomposition, temporal modeling, and attention-based refinement. The model first applies STL and FFT to extract trend, seasonal, and denoised residual components, effectively filtering out high-frequency noise. The filtered residuals are then fed into a stacked LSTM to capture long-term dependencies, followed by a multi-head attention mechanism that dynamically focuses on key time steps. Experiments conducted on real-world urban air quality datasets demonstrate that FALNet consistently outperforms conventional models across standard metrics such as MAE, RMSE, and $R^2$. The model shows strong adaptability in capturing sharp fluctuations during pollution peaks and non-stationary conditions. These results validate the effectiveness and generalizability of FALNet for real-time air pollution prediction, environmental risk assessment, and decision-making support."
      },
      {
        "id": "oai:arXiv.org:2503.24157v2",
        "title": "LLM4FS: Leveraging Large Language Models for Feature Selection and How to Improve It",
        "link": "https://arxiv.org/abs/2503.24157",
        "author": "Jianhao Li, Xianchao Xiu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.24157v2 Announce Type: replace \nAbstract: Recent advances in large language models (LLMs) have provided new opportunities for decision-making, particularly in the task of automated feature selection. In this paper, we first comprehensively evaluate LLM-based feature selection methods, covering the state-of-the-art DeepSeek-R1, GPT-o3-mini, and GPT-4.5. Then, we propose a novel hybrid strategy called LLM4FS that integrates LLMs with traditional data-driven methods. Specifically, input data samples into LLMs, and directly call traditional data-driven techniques such as random forest and forward sequential selection. Notably, our analysis reveals that the hybrid strategy leverages the contextual understanding of LLMs and the high statistical reliability of traditional data-driven methods to achieve excellent feature selection performance, even surpassing LLMs and traditional data-driven methods. Finally, we point out the limitations of its application in decision-making."
      },
      {
        "id": "oai:arXiv.org:2504.00698v2",
        "title": "Command A: An Enterprise-Ready Large Language Model",
        "link": "https://arxiv.org/abs/2504.00698",
        "author": "Team Cohere,  :,  Aakanksha, Arash Ahmadian, Marwan Ahmed, Jay Alammar, Milad Alizadeh, Yazeed Alnumay, Sophia Althammer, Arkady Arkhangorodsky, Viraat Aryabumi, Dennis Aumiller, Rapha\\\"el Avalos, Zahara Aviv, Sammie Bae, Saurabh Baji, Alexandre Barbet, Max Bartolo, Bj\\\"orn Bebensee, Neeral Beladia, Walter Beller-Morales, Alexandre B\\'erard, Andrew Berneshawi, Anna Bialas, Phil Blunsom, Matt Bobkin, Adi Bongale, Sam Braun, Maxime Brunet, Samuel Cahyawijaya, David Cairuz, Jon Ander Campos, Cassie Cao, Kris Cao, Roman Castagn\\'e, Juli\\'an Cendrero, Leila Chan Currie, Yash Chandak, Diane Chang, Giannis Chatziveroglou, Hongyu Chen, Claire Cheng, Alexis Chevalier, Justin T. Chiu, Eugene Cho, Eugene Choi, Eujeong Choi, Tim Chung, Volkan Cirik, Ana Cismaru, Pierre Clavier, Henry Conklin, Lucas Crawhall-Stein, Devon Crouse, Andres Felipe Cruz-Salinas, Ben Cyrus, Daniel D'souza, Hugo Dalla-Torre, John Dang, William Darling, Omar Darwiche Domingues, Saurabh Dash, Antoine Debugne, Th\\'eo Dehaze, Shaan Desai, Joan Devassy, Rishit Dholakia, Kyle Duffy, Ali Edalati, Ace Eldeib, Abdullah Elkady, Sarah Elsharkawy, Irem Erg\\\"un, Beyza Ermis, Marzieh Fadaee, Boyu Fan, Lucas Fayoux, Yannis Flet-Berliac, Nick Frosst, Matthias Gall\\'e, Wojciech Galuba, Utsav Garg, Matthieu Geist, Mohammad Gheshlaghi Azar, Ellen Gilsenan-McMahon, Seraphina Goldfarb-Tarrant, Tomas Goldsack, Aidan Gomez, Victor Machado Gonzaga, Nithya Govindarajan, Manoj Govindassamy, Nathan Grinsztajn, Nikolas Gritsch, Patrick Gu, Shangmin Guo, Kilian Haefeli, Rod Hajjar, Tim Hawes, Jingyi He, Sebastian Hofst\\\"atter, Sungjin Hong, Sara Hooker, Tom Hosking, Stephanie Howe, Eric Hu, Renjie Huang, Hemant Jain, Ritika Jain, Nick Jakobi, Madeline Jenkins, JJ Jordan, Dhruti Joshi, Jason Jung, Trushant Kalyanpur, Siddhartha Rao Kamalakara, Julia Kedrzycki, Gokce Keskin, Edward Kim, Joon Kim, Wei-Yin Ko, Tom Kocmi, Michael Kozakov, Wojciech Kry\\'sci\\'nski, Arnav Kumar Jain, Komal Kumar Teru, Sander Land, Michael Lasby, Olivia Lasche, Justin Lee, Patrick Lewis, Jeffrey Li, Jonathan Li, Hangyu Lin, Acyr Locatelli, Kevin Luong, Raymond Ma, Luk\\'a\\v{s} Mach, Marina Machado, Joanne Magbitang, Brenda Malacara Lopez, Aryan Mann, Kelly Marchisio, Olivia Markham, Alexandre Matton, Alex McKinney, Dominic McLoughlin, Jozef Mokry, Adrien Morisot, Autumn Moulder, Harry Moynehan, Maximilian Mozes, Vivek Muppalla, Lidiya Murakhovska, Hemangani Nagarajan, Alekhya Nandula, Hisham Nasir, Shauna Nehra, Josh Netto-Rosen, Daniel Ohashi, James Owers-Bardsley, Jason Ozuzu, Dennis Padilla, Gloria Park, Sam Passaglia, Jeremy Pekmez, Laura Penstone, Aleksandra Piktus, Case Ploeg, Andrew Poulton, Youran Qi, Shubha Raghvendra, Miguel Ramos, Ekagra Ranjan, Pierre Richemond, C\\'ecile Robert-Michon, Aur\\'elien Rodriguez, Sudip Roy, Sebastian Ruder, Laura Ruis, Louise Rust, Anubhav Sachan, Alejandro Salamanca, Kailash Karthik Saravanakumar, Isha Satyakam, Alice Schoenauer Sebag, Priyanka Sen, Sholeh Sepehri, Preethi Seshadri, Ye Shen, Tom Sherborne, Sylvie Shang Shi, Sanal Shivaprasad, Vladyslav Shmyhlo, Anirudh Shrinivason, Inna Shteinbuk, Amir Shukayev, Mathieu Simard, Ella Snyder, Ava Spataru, Victoria Spooner, Trisha Starostina, Florian Strub, Yixuan Su, Jimin Sun, Dwarak Talupuru, Eugene Tarassov, Elena Tommasone, Jennifer Tracey, Billy Trend, Evren Tumer, Ahmet \\\"Ust\\\"un, Bharat Venkitesh, David Venuto, Pat Verga, Maxime Voisin, Alex Wang, Donglu Wang, Shijian Wang, Edmond Wen, Naomi White, Jesse Willman, Marysia Winkels, Chen Xia, Jessica Xie, Minjie Xu, Bowen Yang, Tan Yi-Chern, Ivan Zhang, Zhenyu Zhao, Zhoujie Zhao",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.00698v2 Announce Type: replace \nAbstract: In this report we describe the development of Command A, a powerful large language model purpose-built to excel at real-world enterprise use cases. Command A is an agent-optimised and multilingual-capable model, with support for 23 languages of global business, and a novel hybrid architecture balancing efficiency with top of the range performance. It offers best-in-class Retrieval Augmented Generation (RAG) capabilities with grounding and tool use to automate sophisticated business processes. These abilities are achieved through a decentralised training approach, including self-refinement algorithms and model merging techniques. We also include results for Command R7B which shares capability and architectural similarities to Command A. Weights for both models have been released for research purposes. This technical report details our original training pipeline and presents an extensive evaluation of our models across a suite of enterprise-relevant tasks and public benchmarks, demonstrating excellent performance and efficiency."
      },
      {
        "id": "oai:arXiv.org:2504.01243v2",
        "title": "FUSION: Frequency-guided Underwater Spatial Image recOnstructioN",
        "link": "https://arxiv.org/abs/2504.01243",
        "author": "Jaskaran Singh Walia, Shravan Venkatraman, Pavithra LK",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.01243v2 Announce Type: replace \nAbstract: Underwater images suffer from severe degradations, including color distortions, reduced visibility, and loss of structural details due to wavelength-dependent attenuation and scattering. Existing enhancement methods primarily focus on spatial-domain processing, neglecting the frequency domain's potential to capture global color distributions and long-range dependencies. To address these limitations, we propose FUSION, a dual-domain deep learning framework that jointly leverages spatial and frequency domain information. FUSION independently processes each RGB channel through multi-scale convolutional kernels and adaptive attention mechanisms in the spatial domain, while simultaneously extracting global structural information via FFT-based frequency attention. A Frequency Guided Fusion module integrates complementary features from both domains, followed by inter-channel fusion and adaptive channel recalibration to ensure balanced color distributions. Extensive experiments on benchmark datasets (UIEB, EUVP, SUIM-E) demonstrate that FUSION achieves state-of-the-art performance, consistently outperforming existing methods in reconstruction fidelity (highest PSNR of 23.717 dB and SSIM of 0.883 on UIEB), perceptual quality (lowest LPIPS of 0.112 on UIEB), and visual enhancement metrics (best UIQM of 3.414 on UIEB), while requiring significantly fewer parameters (0.28M) and lower computational complexity, demonstrating its suitability for real-time underwater imaging applications."
      },
      {
        "id": "oai:arXiv.org:2504.01648v2",
        "title": "ProtoGuard-guided PROPEL: Class-Aware Prototype Enhancement and Progressive Labeling for Incremental 3D Point Cloud Segmentation",
        "link": "https://arxiv.org/abs/2504.01648",
        "author": "Haosheng Li, Yuecong Xu, Junjie Chen, Kemi Ding",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.01648v2 Announce Type: replace \nAbstract: 3D point cloud semantic segmentation technology has been widely used. However, in real-world scenarios, the environment is evolving. Thus, offline-trained segmentation models may lead to catastrophic forgetting of previously seen classes. Class-incremental learning (CIL) is designed to address the problem of catastrophic forgetting. While point clouds are common, we observe high similarity and unclear boundaries between different classes. Meanwhile, they are known to be imbalanced in class distribution. These lead to issues including misclassification between similar classes and the long-tail problem, which have not been adequately addressed in previous CIL methods. We thus propose ProtoGuard and PROPEL (Progressive Refinement Of PsEudo-Labels). In the base-class training phase, ProtoGuard maintains geometric and semantic prototypes for each class, which are combined into prototype features using an attention mechanism. In the novel-class training phase, PROPEL inherits the base feature extractor and classifier, guiding pseudo-label propagation and updates based on density distribution and semantic similarity. Extensive experiments show that our approach achieves remarkable results on both the S3DIS and ScanNet datasets, improving the mIoU of 3D point cloud segmentation by a maximum of 20.39% under the 5-step CIL scenario on S3DIS."
      },
      {
        "id": "oai:arXiv.org:2504.01659v3",
        "title": "Robust Unsupervised Domain Adaptation for 3D Point Cloud Segmentation Under Source Adversarial Attacks",
        "link": "https://arxiv.org/abs/2504.01659",
        "author": "Haosheng Li, Junjie Chen, Yuecong Xu, Kemi Ding",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.01659v3 Announce Type: replace \nAbstract: Unsupervised domain adaptation (UDA) frameworks have shown good generalization capabilities for 3D point cloud semantic segmentation models on clean data. However, existing works overlook adversarial robustness when the source domain itself is compromised. To comprehensively explore the robustness of the UDA frameworks, we first design a stealthy adversarial point cloud generation attack that can significantly contaminate datasets with only minor perturbations to the point cloud surface. Based on that, we propose a novel dataset, AdvSynLiDAR, comprising synthesized contaminated LiDAR point clouds. With the generated corrupted data, we further develop the Adversarial Adaptation Framework (AAF) as the countermeasure. Specifically, by extending the key point sensitive (KPS) loss towards the Robust Long-Tail loss (RLT loss) and utilizing a decoder branch, our approach enables the model to focus on long-tail classes during the pre-training phase and leverages high-confidence decoded point cloud information to restore point cloud structures during the adaptation phase. We evaluated our AAF method on the AdvSynLiDAR dataset, where the results demonstrate that our AAF method can mitigate performance degradation under source adversarial perturbations for UDA in the 3D point cloud segmentation application."
      },
      {
        "id": "oai:arXiv.org:2504.01668v2",
        "title": "Overlap-Aware Feature Learning for Robust Unsupervised Domain Adaptation for 3D Semantic Segmentation",
        "link": "https://arxiv.org/abs/2504.01668",
        "author": "Junjie Chen, Yuecong Xu, Haosheng Li, Kemi Ding",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.01668v2 Announce Type: replace \nAbstract: 3D point cloud semantic segmentation (PCSS) is a cornerstone for environmental perception in robotic systems and autonomous driving, enabling precise scene understanding through point-wise classification. While unsupervised domain adaptation (UDA) mitigates label scarcity in PCSS, existing methods critically overlook the inherent vulnerability to real-world perturbations (e.g., snow, fog, rain) and adversarial distortions. This work first identifies two intrinsic limitations that undermine current PCSS-UDA robustness: (a) unsupervised features overlap from unaligned boundaries in shared-class regions and (b) feature structure erosion caused by domain-invariant learning that suppresses target-specific patterns. To address the proposed problems, we propose a tripartite framework consisting of: 1) a robustness evaluation model quantifying resilience against adversarial attack/corruption types through robustness metrics; 2) an invertible attention alignment module (IAAM) enabling bidirectional domain mapping while preserving discriminative structure via attention-guided overlap suppression; and 3) a contrastive memory bank with quality-aware contrastive learning that progressively refines pseudo-labels with feature quality for more discriminative representations. Extensive experiments on SynLiDAR-to-SemanticPOSS adaptation demonstrate a maximum mIoU improvement of 14.3\\% under adversarial attack."
      },
      {
        "id": "oai:arXiv.org:2504.02544v2",
        "title": "Fourier Sliced-Wasserstein Embedding for Multisets and Measures",
        "link": "https://arxiv.org/abs/2504.02544",
        "author": "Tal Amir, Nadav Dym",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02544v2 Announce Type: replace \nAbstract: We present the Fourier Sliced-Wasserstein (FSW) embedding - a novel method to embed multisets and measures over $\\mathbb{R}^d$ into Euclidean space.\n  Our proposed embedding approximately preserves the sliced Wasserstein distance on distributions, thereby yielding geometrically meaningful representations that better capture the structure of the input. Moreover, it is injective on measures and bi-Lipschitz on multisets - a significant advantage over prevalent methods based on sum- or max-pooling, which are provably not bi-Lipschitz, and, in many cases, not even injective. The required output dimension for these guarantees is near-optimal: roughly $2 N d$, where $N$ is the maximal input multiset size.\n  Furthermore, we prove that it is impossible to embed distributions over $\\mathbb{R}^d$ into Euclidean space in a bi-Lipschitz manner. Thus, the metric properties of our embedding are, in a sense, the best possible.\n  Through numerical experiments, we demonstrate that our method yields superior multiset representations that improve performance in practical learning tasks. Specifically, we show that (a) a simple combination of the FSW embedding with an MLP achieves state-of-the-art performance in learning the (non-sliced) Wasserstein distance; and (b) replacing max-pooling with the FSW embedding makes PointNet significantly more robust to parameter reduction, with only minor performance degradation even after a 40-fold reduction."
      },
      {
        "id": "oai:arXiv.org:2504.03036v2",
        "title": "IPA-CHILDES & G2P+: Feature-Rich Resources for Cross-Lingual Phonology and Phonemic Language Modeling",
        "link": "https://arxiv.org/abs/2504.03036",
        "author": "Z\\'ebulon Goriely, Paula Buttery",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03036v2 Announce Type: replace \nAbstract: In this paper, we introduce two resources: (i) G2P+, a tool for converting orthographic datasets to a consistent phonemic representation; and (ii) IPA CHILDES, a phonemic dataset of child-centered speech across 31 languages. Prior tools for grapheme-to-phoneme conversion result in phonemic vocabularies that are inconsistent with established phonemic inventories, an issue which G2P+ addresses by leveraging the inventories in the Phoible database. Using this tool, we augment CHILDES with phonemic transcriptions to produce IPA CHILDES. This new resource fills several gaps in existing phonemic datasets, which often lack multilingual coverage, spontaneous speech, and a focus on child-directed language. We demonstrate the utility of this dataset for phonological research by training phoneme language models on 11 languages and probing them for distinctive features, finding that the distributional properties of phonemes are sufficient to learn major class and place features cross-lingually."
      },
      {
        "id": "oai:arXiv.org:2504.03162v2",
        "title": "Beyond Progress Measures: Theoretical Insights into the Mechanism of Grokking",
        "link": "https://arxiv.org/abs/2504.03162",
        "author": "Zihan Gu, Ruoyu Chen, Hua Zhang, Yue Hu, Xiaochun Cao",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03162v2 Announce Type: replace \nAbstract: Grokking, referring to the abrupt improvement in test accuracy after extended overfitting, offers valuable insights into the mechanisms of model generalization. Existing researches based on progress measures imply that grokking relies on understanding the optimization dynamics when the loss function is dominated solely by the weight decay term. However, we find that this optimization merely leads to token uniformity, which is not a sufficient condition for grokking. In this work, we investigate the grokking mechanism underlying the Transformer in the task of prime number operations. Based on theoretical analysis and experimental validation, we present the following insights: (i) The weight decay term encourages uniformity across all tokens in the embedding space when it is minimized. (ii) The occurrence of grokking is jointly determined by the uniformity of the embedding space and the distribution of the training dataset. Building on these insights, we provide a unified perspective for understanding various previously proposed progress measures and introduce a novel, concise, and effective progress measure that could trace the changes in test loss more accurately. Finally, to demonstrate the versatility of our theoretical framework, we design a dedicated dataset to validate our theory on ResNet-18, successfully showcasing the occurrence of grokking. The code is released at https://github.com/Qihuai27/Grokking-Insight."
      },
      {
        "id": "oai:arXiv.org:2504.03338v2",
        "title": "BabyLM's First Words: Word Segmentation as a Phonological Probing Task",
        "link": "https://arxiv.org/abs/2504.03338",
        "author": "Z\\'ebulon Goriely, Paula Buttery",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03338v2 Announce Type: replace \nAbstract: Language models provide a key framework for studying linguistic theories based on prediction, but phonological analysis using large language models (LLMs) is difficult; there are few phonological benchmarks beyond English and the standard input representation used in LLMs (subwords of graphemes) is not suitable for analyzing the representation of phonemes. In this work, we demonstrate how word segmentation can be used as a phonological probing task, allowing us to study the representations learned by phoneme-based language models trained on child-directed speech across 31 languages. Following computational models of word segmentation, we present unsupervised methods for extracting word boundaries from a trained model using the observation that prediction-error peaks at the start of words. We also use linear probes to identify that these models implicitly track word boundaries, even when they do not appear in training. This cross-lingual work corroborates statistical learning theories of acquisition and empirically motivates new methods for training subword tokenizers."
      },
      {
        "id": "oai:arXiv.org:2504.03786v2",
        "title": "Do \"New Snow Tablets\" Contain Snow? Large Language Models Over-Rely on Names to Identify Ingredients of Chinese Drugs",
        "link": "https://arxiv.org/abs/2504.03786",
        "author": "Sifan Li, Yujun Cai, Bryan Hooi, Nanyun Peng, Yiwei Wang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03786v2 Announce Type: replace \nAbstract: Traditional Chinese Medicine (TCM) has seen increasing adoption in healthcare, with specialized Large Language Models (LLMs) emerging to support clinical applications. A fundamental requirement for these models is accurate identification of TCM drug ingredients. In this paper, we evaluate how general and TCM-specialized LLMs perform when identifying ingredients of Chinese drugs. Our systematic analysis reveals consistent failure patterns: models often interpret drug names literally, overuse common herbs regardless of relevance, and exhibit erratic behaviors when faced with unfamiliar formulations. LLMs also fail to understand the verification task. These findings demonstrate that current LLMs rely primarily on drug names rather than possessing systematic pharmacological knowledge. To address these limitations, we propose a Retrieval Augmented Generation (RAG) approach focused on ingredient names. Experiments across 220 TCM formulations show our method significantly improves accuracy from approximately 50% to 82% in ingredient verification tasks. Our work highlights critical weaknesses in current TCM-specific LLMs and offers a practical solution for enhancing their clinical reliability."
      },
      {
        "id": "oai:arXiv.org:2504.04569v2",
        "title": "KnowsLM: A framework for evaluation of small language models for knowledge augmentation and humanised conversations",
        "link": "https://arxiv.org/abs/2504.04569",
        "author": "Chitranshu Harbola, Anupam Purwar",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.04569v2 Announce Type: replace \nAbstract: In the evolving landscape of conversational AI, generating concise, context-aware, and human-like dialogue using small and medium-sized language models (LLMs) remains a complex challenge. This study investigates the influence of LoRA rank, dataset scale, and prompt prefix design on both knowledge retention and stylistic alignment. While fine-tuning improves fluency and enables stylistic customization, its ability to integrate unseen knowledge is constrained -- particularly with smaller datasets. Conversely, RAG-augmented models, equipped to incorporate external documents at inference, demonstrated superior factual accuracy on out-of-distribution prompts, though they lacked the stylistic consistency achieved by fine-tuning. Evaluations by LLM-based judges across knowledge accuracy, conversational quality, and conciseness suggest that fine-tuning is best suited for tone adaptation, whereas RAG excels at real-time knowledge augmentation."
      },
      {
        "id": "oai:arXiv.org:2504.04840v2",
        "title": "Unsupervised Ego- and Exo-centric Dense Procedural Activity Captioning via Gaze Consensus Adaptation",
        "link": "https://arxiv.org/abs/2504.04840",
        "author": "Zhaofeng Shi, Heqian Qiu, Lanxiao Wang, Qingbo Wu, Fanman Meng, Hongliang Li",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.04840v2 Announce Type: replace \nAbstract: Even from an early age, humans naturally adapt between exocentric (Exo) and egocentric (Ego) perspectives to understand daily procedural activities. Inspired by this cognitive ability, we propose a novel Unsupervised Ego-Exo Dense Procedural Activity Captioning (UE$^{2}$DPAC) task, which aims to transfer knowledge from the labeled source view to predict the time segments and descriptions of action sequences for the target view without annotations. Despite previous works endeavoring to address the fully-supervised single-view or cross-view dense video captioning, they lapse in the proposed task due to the significant inter-view gap caused by temporal misalignment and irrelevant object interference. Hence, we propose a Gaze Consensus-guided Ego-Exo Adaptation Network (GCEAN) that injects the gaze information into the learned representations for the fine-grained Ego-Exo alignment. Specifically, we propose a Score-based Adversarial Learning Module (SALM) that incorporates a discriminative scoring network and compares the scores of distinct views to learn unified view-invariant representations from a global level. Then, the Gaze Consensus Construction Module (GCCM) utilizes the gaze to progressively calibrate the learned representations to highlight the regions of interest and extract the corresponding temporal contexts. Moreover, we adopt hierarchical gaze-guided consistency losses to construct gaze consensus for the explicit temporal and spatial adaptation between the source and target views. To support our research, we propose a new EgoMe-UE$^{2}$DPAC benchmark, and extensive experiments demonstrate the effectiveness of our method, which outperforms many related methods by a large margin. The code will be released."
      },
      {
        "id": "oai:arXiv.org:2504.05045v3",
        "title": "Attention-Augmented Inverse Reinforcement Learning with Graph Convolutions for Multi-Agent Task Allocation",
        "link": "https://arxiv.org/abs/2504.05045",
        "author": "Huilin Yin, Zhikun Yang, Linchuan Zhang, Daniel Watzenig",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.05045v3 Announce Type: replace \nAbstract: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.\n  Multi-agent task allocation (MATA) plays a vital role in cooperative multi-agent systems, with significant implications for applications such as logistics, search and rescue, and robotic coordination. Although traditional deep reinforcement learning (DRL) methods have been shown to be promising, their effectiveness is hindered by a reliance on manually designed reward functions and inefficiencies in dynamic environments. In this paper, an inverse reinforcement learning (IRL)-based framework is proposed, in which multi-head self-attention (MHSA) and graph attention mechanisms are incorporated to enhance reward function learning and task execution efficiency. Expert demonstrations are utilized to infer optimal reward densities, allowing dependence on handcrafted designs to be reduced and adaptability to be improved. Extensive experiments validate the superiority of the proposed method over widely used multi-agent reinforcement learning (MARL) algorithms in terms of both cumulative rewards and task execution efficiency."
      },
      {
        "id": "oai:arXiv.org:2504.05089v2",
        "title": "Climplicit: Climatic Implicit Embeddings for Global Ecological Tasks",
        "link": "https://arxiv.org/abs/2504.05089",
        "author": "Johannes Dollinger, Damien Robert, Elena Plekhanova, Lukas Drees, Jan Dirk Wegner",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.05089v2 Announce Type: replace \nAbstract: Deep learning on climatic data holds potential for macroecological applications. However, its adoption remains limited among scientists outside the deep learning community due to storage, compute, and technical expertise barriers. To address this, we introduce Climplicit, a spatio-temporal geolocation encoder pretrained to generate implicit climatic representations anywhere on Earth. By bypassing the need to download raw climatic rasters and train feature extractors, our model uses x3500 less disk space and significantly reduces computational needs for downstream tasks. We evaluate our Climplicit embeddings on biomes classification, species distribution modeling, and plant trait regression. We find that single-layer probing our Climplicit embeddings consistently performs better or on par with training a model from scratch on downstream tasks and overall better than alternative geolocation encoding models."
      },
      {
        "id": "oai:arXiv.org:2504.05172v2",
        "title": "Attention-Based Multiscale Temporal Fusion Network for Uncertain-Mode Fault Diagnosis in Multimode Processes",
        "link": "https://arxiv.org/abs/2504.05172",
        "author": "Guangqiang Li, M. Amine Atoui, Xiangshun Li",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.05172v2 Announce Type: replace \nAbstract: Fault diagnosis in multimode processes plays a critical role in ensuring the safe operation of industrial systems across multiple modes. It faces a great challenge yet to be addressed - that is, the significant distributional differences among monitoring data from multiple modes make it difficult for the models to extract shared feature representations related to system health conditions. In response to this problem, this paper introduces a novel method called attention-based multiscale temporal fusion network. The multiscale depthwise convolution and gated recurrent unit are employed to extract multiscale contextual local features and long-short-term features. Instance normalization is applied to suppress mode-specific information. Furthermore, a temporal attention mechanism is designed to focus on critical time points with higher cross-mode shared information, thereby enhancing the accuracy of fault diagnosis. The proposed model is applied to Tennessee Eastman process dataset and three-phase flow facility dataset. The experiments demonstrate that the proposed model achieves superior diagnostic performance and maintains a small model size. The source code will be available on GitHub at https://github.com/GuangqiangLi/AMTFNet."
      },
      {
        "id": "oai:arXiv.org:2504.05226v2",
        "title": "Proposing TAGbank as a Corpus of Tree-Adjoining Grammar Derivations",
        "link": "https://arxiv.org/abs/2504.05226",
        "author": "Jungyeul Park",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.05226v2 Announce Type: replace \nAbstract: The development of lexicalized grammars, particularly Tree-Adjoining Grammar (TAG), has significantly advanced our understanding of syntax and semantics in natural language processing (NLP). While existing syntactic resources like the Penn Treebank and Universal Dependencies offer extensive annotations for phrase-structure and dependency parsing, there is a lack of large-scale corpora grounded in lexicalized grammar formalisms. To address this gap, we introduce TAGbank, a corpus of TAG derivations automatically extracted from existing syntactic treebanks. This paper outlines a methodology for mapping phrase-structure annotations to TAG derivations, leveraging the generative power of TAG to support parsing, grammar induction, and semantic analysis. Our approach builds on the work of CCGbank, extending it to incorporate the unique structural properties of TAG, including its transparent derivation trees and its ability to capture long-distance dependencies. We also discuss the challenges involved in the extraction process, including ensuring consistency across treebank schemes and dealing with language-specific syntactic idiosyncrasies. Finally, we propose the future extension of TAGbank to include multilingual corpora, focusing on the Penn Korean and Penn Chinese Treebanks, to explore the cross-linguistic application of TAG's formalism. By providing a robust, derivation-based resource, TAGbank aims to support a wide range of computational tasks and contribute to the theoretical understanding of TAG's generative capacity."
      },
      {
        "id": "oai:arXiv.org:2504.05579v2",
        "title": "TAPNext: Tracking Any Point (TAP) as Next Token Prediction",
        "link": "https://arxiv.org/abs/2504.05579",
        "author": "Artem Zholus, Carl Doersch, Yi Yang, Skanda Koppula, Viorica Patraucean, Xu Owen He, Ignacio Rocco, Mehdi S. M. Sajjadi, Sarath Chandar, Ross Goroshin",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.05579v2 Announce Type: replace \nAbstract: Tracking Any Point (TAP) in a video is a challenging computer vision problem with many demonstrated applications in robotics, video editing, and 3D reconstruction. Existing methods for TAP rely heavily on complex tracking-specific inductive biases and heuristics, limiting their generality and potential for scaling. To address these challenges, we present TAPNext, a new approach that casts TAP as sequential masked token decoding. Our model is causal, tracks in a purely online fashion, and removes tracking-specific inductive biases. This enables TAPNext to run with minimal latency, and removes the temporal windowing required by many existing state of art trackers. Despite its simplicity, TAPNext achieves a new state-of-the-art tracking performance among both online and offline trackers. Finally, we present evidence that many widely used tracking heuristics emerge naturally in TAPNext through end-to-end training. The TAPNext model and code can be found at https://tap-next.github.io/."
      },
      {
        "id": "oai:arXiv.org:2504.05670v2",
        "title": "Dual Boost-Driven Graph-Level Clustering Network",
        "link": "https://arxiv.org/abs/2504.05670",
        "author": "John Smith, Wenxuan Tu, Junlong Wu, Wenxin Zhang, Jingxin Liu, Haotian Wang, Jieren Cheng, Huajie Lei, Guangzhen Yao, Lingren Wang, Mengfei Li, Renda Han, Yu Li",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.05670v2 Announce Type: replace \nAbstract: Graph-level clustering remains a pivotal yet formidable challenge in graph learning. Recently, the integration of deep learning with representation learning has demonstrated notable advancements, yielding performance enhancements to a certain degree. However, existing methods suffer from at least one of the following issues: 1. the original graph structure has noise, and 2. during feature propagation and pooling processes, noise is gradually aggregated into the graph-level embeddings through information propagation. Consequently, these two limitations mask clustering-friendly information, leading to suboptimal graph-level clustering performance. To this end, we propose a novel Dual Boost-Driven Graph-Level Clustering Network (DBGCN) to alternately promote graph-level clustering and filtering out interference information in a unified framework. Specifically, in the pooling step, we evaluate the contribution of features at the global and optimize them using a learnable transformation matrix to obtain high-quality graph-level representation, such that the model's reasoning capability can be improved. Moreover, to enable reliable graph-level clustering, we first identify and suppress information detrimental to clustering by evaluating similarities between graph-level representations, providing more accurate guidance for multi-view fusion. Extensive experiments demonstrated that DBGCN outperforms the state-of-the-art graph-level clustering methods on six benchmark datasets."
      },
      {
        "id": "oai:arXiv.org:2504.06006v2",
        "title": "Optuna vs Code Llama: Are LLMs a New Paradigm for Hyperparameter Tuning?",
        "link": "https://arxiv.org/abs/2504.06006",
        "author": "Roman Kochnev, Arash Torabi Goodarzi, Zofia Antonina Bentyn, Dmitry Ignatov, Radu Timofte",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.06006v2 Announce Type: replace \nAbstract: Optimal hyperparameter selection is critical for maximizing neural network performance, especially as models grow in complexity. This work investigates the viability of leveraging large language models (LLMs) for hyperparameter optimization by fine-tuning a parameter-efficient version of Code Llama using LoRA. The adapted LLM is capable of generating accurate and efficient hyperparameter recommendations tailored to diverse neural network architectures. Unlike traditional approaches such as Optuna, which rely on computationally intensive trial-and-error procedures, our method achieves competitive or superior results in terms of Root Mean Square Error (RMSE) while significantly reducing computational overhead. Our findings demonstrate that LLM-based optimization not only matches the performance of state-of-the-art techniques like Tree-structured Parzen Estimators (TPE) but also substantially accelerates the tuning process. This positions LLMs as a promising alternative for rapid experimentation, particularly in resource-constrained environments such as edge devices and mobile platforms, where computational efficiency is essential. In addition to improved efficiency, the method offers time savings and consistent performance across various tasks, highlighting its robustness and generalizability. All generated hyperparameters are included in the LEMUR Neural Network (NN) Dataset, which is publicly available and serves as an open-source benchmark for hyperparameter optimization research."
      },
      {
        "id": "oai:arXiv.org:2504.06141v2",
        "title": "Adversarial Training of Reward Models",
        "link": "https://arxiv.org/abs/2504.06141",
        "author": "Alexander Bukharin, Haifeng Qian, Shengyang Sun, Adithya Renduchintala, Soumye Singhal, Zhilin Wang, Oleksii Kuchaiev, Olivier Delalleau, Tuo Zhao",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.06141v2 Announce Type: replace \nAbstract: Reward modeling has emerged as a promising approach for the scalable alignment of language models. However, contemporary reward models (RMs) often lack robustness, awarding high rewards to low-quality, out-of-distribution (OOD) samples. This can lead to reward hacking, where policies exploit unintended shortcuts to maximize rewards, undermining alignment. To address this challenge, we introduce Adv-RM, a novel adversarial training framework that automatically identifies adversarial examples -- responses that receive high rewards from the target RM but are OOD and of low quality. By leveraging reinforcement learning, Adv-RM trains a policy to generate adversarial examples that reliably expose vulnerabilities in large state-of-the-art reward models such as Nemotron 340B RM. Incorporating these adversarial examples into the reward training process improves the robustness of RMs, mitigating reward hacking and enhancing downstream performance in RLHF. We demonstrate that Adv-RM significantly outperforms conventional RM training, increasing stability and enabling more effective RLHF training in both synthetic and real-data settings."
      },
      {
        "id": "oai:arXiv.org:2504.06160v3",
        "title": "Navigating the Rabbit Hole: Emergent Biases in LLM-Generated Attack Narratives Targeting Mental Health Groups",
        "link": "https://arxiv.org/abs/2504.06160",
        "author": "Rijul Magu, Arka Dutta, Sean Kim, Ashiqur R. KhudaBukhsh, Munmun De Choudhury",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.06160v3 Announce Type: replace \nAbstract: Large Language Models (LLMs) have been shown to demonstrate imbalanced biases against certain groups. However, the study of unprovoked targeted attacks by LLMs towards at-risk populations remains underexplored. Our paper presents three novel contributions: (1) the explicit evaluation of LLM-generated attacks on highly vulnerable mental health groups; (2) a network-based framework to study the propagation of relative biases; and (3) an assessment of the relative degree of stigmatization that emerges from these attacks. Our analysis of a recently released large-scale bias audit dataset reveals that mental health entities occupy central positions within attack narrative networks, as revealed by a significantly higher mean centrality of closeness (p-value = 4.06e-10) and dense clustering (Gini coefficient = 0.7). Drawing from sociological foundations of stigmatization theory, our stigmatization analysis indicates increased labeling components for mental health disorder-related targets relative to initial targets in generation chains. Taken together, these insights shed light on the structural predilections of large language models to heighten harmful discourse and highlight the need for suitable approaches for mitigation."
      },
      {
        "id": "oai:arXiv.org:2504.06280v2",
        "title": "Different Paths, Same Destination: Designing New Physics-Inspired Dynamical Systems with Engineered Stability to Minimize the Ising Hamiltonian",
        "link": "https://arxiv.org/abs/2504.06280",
        "author": "E. M. H. E. B. Ekanayake, N. Shukla",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.06280v2 Announce Type: replace \nAbstract: Oscillator Ising machines (OIMs) represent an exemplar case of using physics-inspired non-linear dynamical systems to solve computationally challenging combinatorial optimization problems (COPs). The computational performance of such systems is highly sensitive to the underlying dynamical properties, the topology of the input graph, and their relative compatibility. In this work, we explore the concept of designing different dynamical systems that minimize the same objective function but exhibit drastically different dynamical properties. Our goal is to leverage this diversification in dynamics to reduce the sensitivity of the computational performance to the underlying graph, and subsequently, enhance the overall effectiveness of such physics-based computational methods. To this end, we introduce a novel dynamical system, the Dynamical Ising Machine (DIM), which, like the OIM, minimizes the Ising Hamiltonian but offers significantly different dynamical properties. We analyze the characteristic properties of the DIM and compare them with those of the OIM. We also show that the relative performance of each model is dependent on the input graph. Our work illustrates that using multiple dynamical systems with varying properties to solve the same COP enables an effective method that is less sensitive to the input graph, while producing robust solutions."
      },
      {
        "id": "oai:arXiv.org:2504.06313v2",
        "title": "Text-to-Image Models and Their Representation of People from Different Nationalities Engaging in Activities",
        "link": "https://arxiv.org/abs/2504.06313",
        "author": "Abdulkareem Alsudais",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.06313v2 Announce Type: replace \nAbstract: The primary objective of this paper is to investigate how a popular Text-to-Image (T2I) model represents people from 208 different nationalities when prompted to generate images of individuals performing typical activities. Two scenarios were developed, and images were generated based on input prompts that specified nationalities. The results show that in one scenario, the majority of images, and in the other, a substantial portion, depict individuals wearing traditional attire. This suggests that the model emphasizes such characteristics even when they are impractical for the given activity. A statistically significant relationship was observed between this representation pattern and the regions associated with the specified countries. This indicates that the issue disproportionately affects certain areas, particularly the Middle East & North Africa and Sub-Saharan Africa. A notable association with income groups was also found. CLIP was used to measure alignment scores between generated images and various prompts and captions. The findings indicate statistically significant higher scores for images featuring individuals in traditional attire in one scenario. The study also examined revised prompts (additional contextual information automatically added to the original input prompts) to assess their potential influence on how individuals are represented in the generated images, finding that the word \"traditional\" was commonly added to revised prompts. These findings provide valuable insights into how T2I models represent individuals from various countries and highlight potential areas for improvement in future models."
      },
      {
        "id": "oai:arXiv.org:2504.06521v2",
        "title": "DUKAE: DUal-level Knowledge Accumulation and Ensemble for Pre-Trained Model-Based Continual Learning",
        "link": "https://arxiv.org/abs/2504.06521",
        "author": "Songze Li, Tonghua Su, Xu-Yao Zhang, Qixing Xu, Zhongjie Wang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.06521v2 Announce Type: replace \nAbstract: Pre-trained model-based continual learning (PTMCL) has garnered growing attention, as it enables more rapid acquisition of new knowledge by leveraging the extensive foundational understanding inherent in pre-trained model (PTM). Most existing PTMCL methods use Parameter-Efficient Fine-Tuning (PEFT) to learn new knowledge while consolidating existing memory. However, they often face some challenges. A major challenge lies in the misalignment of classification heads, as the classification head of each task is trained within a distinct feature space, leading to inconsistent decision boundaries across tasks and, consequently, increased forgetting. Another critical limitation stems from the restricted feature-level knowledge accumulation, with feature learning typically restricted to the initial task only, which constrains the model's representation capabilities. To address these issues, we propose a method named DUal-level Knowledge Accumulation and Ensemble (DUKAE) that leverages both feature-level and decision-level knowledge accumulation by aligning classification heads into a unified feature space through Gaussian distribution sampling and introducing an adaptive expertise ensemble to fuse knowledge across feature subspaces. Extensive experiments on CIFAR-100, ImageNet-R, CUB-200, and Cars-196 datasets demonstrate the superior performance of our approach."
      },
      {
        "id": "oai:arXiv.org:2504.06958v3",
        "title": "VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement Fine-Tuning",
        "link": "https://arxiv.org/abs/2504.06958",
        "author": "Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, Limin Wang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.06958v3 Announce Type: replace \nAbstract: Recent advancements in reinforcement learning have significantly advanced the reasoning capabilities of multimodal large language models (MLLMs). While approaches such as Group Relative Policy Optimization (GRPO) and rule-based reward mechanisms demonstrate promise in text and image domains, their application to video understanding remains limited. This paper presents a systematic exploration of Reinforcement Fine-Tuning (RFT) with GRPO for video MLLMs, aiming to enhance spatio-temporal perception while maintaining general capabilities. Our experiments reveal that RFT is highly data-efficient for task-specific improvements. Through multi-task RFT on spatio-temporal perception objectives with limited samples, we develop VideoChat-R1, a powerful video MLLM that achieves state-of-the-art performance on spatio-temporal perception tasks without sacrificing chat ability, while exhibiting emerging spatio-temporal reasoning abilities. Compared to Qwen2.5-VL-7B, VideoChat-R1 boosts performance several-fold in tasks like temporal grounding (+31.8) and object tracking (+31.2). Additionally, it significantly improves on general QA benchmarks such as VideoMME (+0.9), MVBench (+1.0), and Perception Test (+0.9). Our findings underscore the potential of RFT for specialized task enhancement of Video MLLMs. We hope our work offers valuable insights for future RL research in video MLLMs."
      },
      {
        "id": "oai:arXiv.org:2504.07029v2",
        "title": "Distilling Textual Priors from LLM to Efficient Image Fusion",
        "link": "https://arxiv.org/abs/2504.07029",
        "author": "Ran Zhang, Xuanhua He, Ke Cao, Liu Liu, Li Zhang, Man Zhou, Jie Zhang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07029v2 Announce Type: replace \nAbstract: Multi-modality image fusion aims to synthesize a single, comprehensive image from multiple source inputs. Traditional approaches, such as CNNs and GANs, offer efficiency but struggle to handle low-quality or complex inputs. Recent advances in text-guided methods leverage large model priors to overcome these limitations, but at the cost of significant computational overhead, both in memory and inference time. To address this challenge, we propose a novel framework for distilling large model priors, eliminating the need for text guidance during inference while dramatically reducing model size. Our framework utilizes a teacher-student architecture, where the teacher network incorporates large model priors and transfers this knowledge to a smaller student network via a tailored distillation process. Additionally, we introduce spatial-channel cross-fusion module to enhance the model's ability to leverage textual priors across both spatial and channel dimensions. Our method achieves a favorable trade-off between computational efficiency and fusion quality. The distilled network, requiring only 10% of the parameters and inference time of the teacher network, retains 90% of its performance and outperforms existing SOTA methods. Extensive experiments demonstrate the effectiveness of our approach. The implementation will be made publicly available as an open-source resource."
      },
      {
        "id": "oai:arXiv.org:2504.07282v2",
        "title": "RAISE: Reinforenced Adaptive Instruction Selection For Large Language Models",
        "link": "https://arxiv.org/abs/2504.07282",
        "author": "Lv Qingsong, Yangning Li, Zihua Lan, Zishan Xu, Jiwei Tang, Yinghui Li, Wenhao Jiang, Hai-Tao Zheng, Philip S. Yu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07282v2 Announce Type: replace \nAbstract: In the instruction fine-tuning of large language models (LLMs), it has become a consensus that a few high-quality instructions are superior to a large number of low-quality instructions. At present, many instruction selection methods have been proposed, but most of these methods select instruction based on heuristic quality metrics, and only consider data selection before training. These designs lead to insufficient optimization of instruction fine-tuning, and fixed heuristic indicators are often difficult to optimize for specific tasks. So we designed a dynamic, task-objective-driven instruction selection framework RAISE(Reinforenced Adaptive Instruction SElection), which incorporates the entire instruction fine-tuning process into optimization, selecting instruction at each step based on the expected impact of instruction on model performance improvement. Our approach is well interpretable and has strong task-specific optimization capabilities. By modeling dynamic instruction selection as a sequential decision-making process, we use RL to train our selection strategy. Extensive experiments and result analysis prove the superiority of our method compared with other instruction selection methods. Notably, RAISE achieves superior performance by updating only 1\\% of the training steps compared to full-data training, demonstrating its efficiency and effectiveness."
      },
      {
        "id": "oai:arXiv.org:2504.07288v2",
        "title": "MDIT: A Model-free Data Interpolation Method for Diverse Instruction Tuning",
        "link": "https://arxiv.org/abs/2504.07288",
        "author": "Yangning Li, Zihua Lan, Lv Qingsong, Yinghui Li, Hai-Tao Zheng",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07288v2 Announce Type: replace \nAbstract: As Large Language Models (LLMs) are increasingly applied across various tasks, instruction tuning has emerged as a critical method for enhancing model performance. However, current data management strategies face substantial challenges in generating diverse and comprehensive data, restricting further improvements in model performance. To address this gap, we propose MDIT, a novel model-free data interpolation method for diverse instruction tuning, which generates varied and high-quality instruction data by performing task interpolation. Moreover, it contains diversity-based clustering strategies to ensure the diversity of the training data. Extensive experiments show that our method achieves superior performance in multiple benchmark tasks. The LLMs finetuned with MDIT show significant improvements in numerous tasks such as general question answering, math reasoning, and code generation. MDIT offers an efficient and automatic data synthetic method, generating diverse instruction data without depending on external resources while expanding the application potential of LLMs in complex environments."
      },
      {
        "id": "oai:arXiv.org:2504.07316v2",
        "title": "Alice: Proactive Learning with Teacher's Demonstrations for Weak-to-Strong Generalization",
        "link": "https://arxiv.org/abs/2504.07316",
        "author": "Shujin Wu, Cheng Qian, Yi R. Fung, Paul Pu Liang, Heng Ji",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07316v2 Announce Type: replace \nAbstract: The growing capabilities of large language models (LLMs) present a key challenge of maintaining effective human oversight. Weak-to-strong generalization (W2SG) offers a promising framework for supervising increasingly capable LLMs using weaker ones. Traditional W2SG methods rely on passive learning, where a weak teacher provides noisy demonstrations to train a strong student. This hinders students from employing their knowledge during training and reaching their full potential. In this work, we introduce Alice (pro{A}ctive {l}earning w{i}th tea{c}her's D{e}monstrations), a framework that leverages complementary knowledge between teacher and student to enhance the learning process. We probe the knowledge base of the teacher model by eliciting their uncertainty, and then use these insights together with teachers' responses as demonstrations to guide student models in self-generating improved responses for supervision. In addition, for situations with significant capability gaps between teacher and student models, we introduce cascade Alice, which employs a hierarchical training approach where weak teachers initially supervise intermediate models, who then guide stronger models in sequence. Experimental results demonstrate that our method significantly enhances the W2SG performance, yielding substantial improvements in three key tasks compared to the original W2SG: knowledge-based reasoning (+4.0%), mathematical reasoning (+22.62%), and logical reasoning (+12.11%). This highlights the effectiveness of our new W2SG paradigm that enables more robust knowledge transfer and supervision outcome."
      },
      {
        "id": "oai:arXiv.org:2504.07334v2",
        "title": "Objaverse++: Curated 3D Object Dataset with Quality Annotations",
        "link": "https://arxiv.org/abs/2504.07334",
        "author": "Chendi Lin, Heshan Liu, Qunshu Lin, Zachary Bright, Shitao Tang, Yihui He, Minghao Liu, Ling Zhu, Cindy Le",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07334v2 Announce Type: replace \nAbstract: This paper presents Objaverse++, a curated subset of Objaverse enhanced with detailed attribute annotations by human experts. Recent advances in 3D content generation have been driven by large-scale datasets such as Objaverse, which contains over 800,000 3D objects collected from the Internet. Although Objaverse represents the largest available 3D asset collection, its utility is limited by the predominance of low-quality models. To address this limitation, we manually annotate 10,000 3D objects with detailed attributes, including aesthetic quality scores, texture color classifications, multi-object composition flags, transparency characteristics, etc. Then, we trained a neural network capable of annotating the tags for the rest of the Objaverse dataset. Through experiments and a user study on generation results, we demonstrate that models pre-trained on our quality-focused subset achieve better performance than those trained on the larger dataset of Objaverse in image-to-3D generation tasks. In addition, by comparing multiple subsets of training data filtered by our tags, our results show that the higher the data quality, the faster the training loss converges. These findings suggest that careful curation and rich annotation can compensate for the raw dataset size, potentially offering a more efficient path to develop 3D generative models. We release our enhanced dataset of approximately 500,000 curated 3D models to facilitate further research on various downstream tasks in 3D computer vision. In the near future, we aim to extend our annotations to cover the entire Objaverse dataset."
      },
      {
        "id": "oai:arXiv.org:2504.07392v2",
        "title": "ID-Booth: Identity-consistent Face Generation with Diffusion Models",
        "link": "https://arxiv.org/abs/2504.07392",
        "author": "Darian Toma\\v{s}evi\\'c, Fadi Boutros, Chenhao Lin, Naser Damer, Vitomir \\v{S}truc, Peter Peer",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07392v2 Announce Type: replace \nAbstract: Recent advances in generative modeling have enabled the generation of high-quality synthetic data that is applicable in a variety of domains, including face recognition. Here, state-of-the-art generative models typically rely on conditioning and fine-tuning of powerful pretrained diffusion models to facilitate the synthesis of realistic images of a desired identity. Yet, these models often do not consider the identity of subjects during training, leading to poor consistency between generated and intended identities. In contrast, methods that employ identity-based training objectives tend to overfit on various aspects of the identity, and in turn, lower the diversity of images that can be generated. To address these issues, we present in this paper a novel generative diffusion-based framework, called ID-Booth. ID-Booth consists of a denoising network responsible for data generation, a variational auto-encoder for mapping images to and from a lower-dimensional latent space and a text encoder that allows for prompt-based control over the generation procedure. The framework utilizes a novel triplet identity training objective and enables identity-consistent image generation while retaining the synthesis capabilities of pretrained diffusion models. Experiments with a state-of-the-art latent diffusion model and diverse prompts reveal that our method facilitates better intra-identity consistency and inter-identity separability than competing methods, while achieving higher image diversity. In turn, the produced data allows for effective augmentation of small-scale datasets and training of better-performing recognition models in a privacy-preserving manner. The source code for the ID-Booth framework is publicly available at https://github.com/dariant/ID-Booth."
      },
      {
        "id": "oai:arXiv.org:2504.07615v2",
        "title": "VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model",
        "link": "https://arxiv.org/abs/2504.07615",
        "author": "Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, Ruochen Xu, Tiancheng Zhao",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07615v2 Announce Type: replace \nAbstract: Recently DeepSeek R1 has shown that reinforcement learning (RL) can substantially improve the reasoning capabilities of Large Language Models (LLMs) through a simple yet effective design. The core of R1 lies in its rule-based reward formulation, which leverages tasks with deterministic ground-truth answers to enable precise and stable reward computation. In the visual domain, we similarly observe that a wide range of visual understanding tasks are inherently equipped with well-defined ground-truth annotations. This property makes them naturally compatible with rule-based reward mechanisms. Motivated by this observation, we investigate the extension of R1-style reinforcement learning to Vision-Language Models (VLMs), aiming to enhance their visual reasoning capabilities. To this end, we develop VLM-R1, a dedicated framework designed to harness RL for improving VLMs' performance on general vision-language tasks. Using this framework, we further explore the feasibility of applying RL to visual domain. Experimental results indicate that the RL-based model not only delivers competitive performance on visual understanding tasks but also surpasses Supervised Fine-Tuning (SFT) in generalization ability. Furthermore, we conduct comprehensive ablation studies that uncover a series of noteworthy insights, including the presence of reward hacking in object detection, the emergence of the \"OD aha moment\", the impact of training data quality, and the scaling behavior of RL across different model sizes. Through these analyses, we aim to deepen the understanding of how reinforcement learning enhances the capabilities of vision-language models, and we hope our findings and open-source contributions will support continued progress in the vision-language RL community. Our code and model are available at https://github.com/om-ai-lab/VLM-R1"
      },
      {
        "id": "oai:arXiv.org:2504.07633v2",
        "title": "Kernel Logistic Regression Learning for High-Capacity Hopfield Networks",
        "link": "https://arxiv.org/abs/2504.07633",
        "author": "Akira Tamamori",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07633v2 Announce Type: replace \nAbstract: Hebbian learning limits Hopfield network storage capacity (pattern-to-neuron ratio around 0.14). We propose Kernel Logistic Regression (KLR) learning. Unlike linear methods, KLR uses kernels to implicitly map patterns to high-dimensional feature space, enhancing separability. By learning dual variables, KLR dramatically improves storage capacity, achieving perfect recall even when pattern numbers exceed neuron numbers (up to ratio 1.5 shown), and enhances noise robustness. KLR demonstrably outperforms Hebbian and linear logistic regression approaches."
      },
      {
        "id": "oai:arXiv.org:2504.07822v2",
        "title": "DG-STMTL: A Novel Graph Convolutional Network for Multi-Task Spatio-Temporal Traffic Forecasting",
        "link": "https://arxiv.org/abs/2504.07822",
        "author": "Wanna Cui, Peizheng Wang, Faliang Yin",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07822v2 Announce Type: replace \nAbstract: Spatio-temporal traffic prediction is crucial in intelligent transportation systems. The key challenge of accurate prediction is how to model the complex spatio-temporal dependencies and adapt to the inherent dynamics in data. Traditional Graph Convolutional Networks (GCNs) often struggle with static adjacency matrices that introduce domain bias or learnable matrices that may be overfitting to specific patterns. This challenge becomes more complex when considering Multi-Task Learning (MTL). While MTL has the potential to enhance prediction accuracy through task synergies, it can also face significant hurdles due to task interference. To overcome these challenges, this study introduces a novel MTL framework, Dynamic Group-wise Spatio-Temporal Multi-Task Learning (DG-STMTL). DG-STMTL proposes a hybrid adjacency matrix generation module that combines static matrices with dynamic ones through a task-specific gating mechanism. We also introduce a group-wise GCN module to enhance the modelling capability of spatio-temporal dependencies. We conduct extensive experiments on two real-world datasets to evaluate our method. Results show that our method outperforms other state-of-the-arts, indicating its effectiveness and robustness."
      },
      {
        "id": "oai:arXiv.org:2504.07835v2",
        "title": "Pychop: Emulating Low-Precision Arithmetic in Numerical Methods and Neural Networks",
        "link": "https://arxiv.org/abs/2504.07835",
        "author": "Erin Carson, Xinye Chen",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07835v2 Announce Type: replace \nAbstract: Motivated by the growing demand for low-precision arithmetic in computational science, we exploit lower-precision emulation in Python -- widely regarded as the dominant programming language for numerical analysis and machine learning. Low-precision training has revolutionized deep learning by enabling more efficient computation and reduced memory and energy consumption while maintaining model fidelity. To better enable numerical experimentation with and exploration of low precision computation, we developed the Pychop library, which supports customizable floating-point formats and a comprehensive set of rounding modes in Python, allowing users to benefit from fast, low-precision emulation in numerous applications. Pychop also introduces interfaces for both PyTorch and JAX, enabling efficient low-precision emulation on GPUs for neural network training and inference with unparalleled flexibility.\n  In this paper, we offer a comprehensive exposition of the design, implementation, validation, and practical application of Pychop, establishing it as a foundational tool for advancing efficient mixed-precision algorithms. Furthermore, we present empirical results on low-precision emulation for image classification and object detection using published datasets, illustrating the sensitivity of the use of low precision and offering valuable insights into its impact. Pychop enables in-depth investigations into the effects of numerical precision, facilitates the development of novel hardware accelerators, and integrates seamlessly into existing deep learning workflows. Software and experimental code are publicly available at https://github.com/inEXASCALE/pychop."
      },
      {
        "id": "oai:arXiv.org:2504.07983v2",
        "title": "Psychological Health Knowledge-Enhanced LLM-based Social Network Crisis Intervention Text Transfer Recognition Method",
        "link": "https://arxiv.org/abs/2504.07983",
        "author": "Shurui Wu, Xinyi Huang, Dingxin Lu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07983v2 Announce Type: replace \nAbstract: As the prevalence of mental health crises increases on social media platforms, identifying and preventing potential harm has become an urgent challenge. This study introduces a large language model (LLM)-based text transfer recognition method for social network crisis intervention, enhanced with domain-specific mental health knowledge. We propose a multi-level framework that incorporates transfer learning using BERT, and integrates mental health knowledge, sentiment analysis, and behavior prediction techniques. The framework includes a crisis annotation tool trained on social media datasets from real-world events, enabling the model to detect nuanced emotional cues and identify psychological crises. Experimental results show that the proposed method outperforms traditional models in crisis detection accuracy and exhibits greater sensitivity to subtle emotional and contextual variations."
      },
      {
        "id": "oai:arXiv.org:2504.08300v2",
        "title": "Large language models could be rote learners",
        "link": "https://arxiv.org/abs/2504.08300",
        "author": "Yuyang Xu, Renjun Hu, Haochao Ying, Jian Wu, Xing Shi, Wei Lin",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08300v2 Announce Type: replace \nAbstract: Multiple-choice question (MCQ) benchmarks are widely used for evaluating Large Language Models (LLMs), yet their reliability is undermined by benchmark contamination. In this study, we reframe contamination as an inherent aspect of learning and seek to disentangle genuine capability acquisition from superficial memorization in LLM evaluation. First, by analyzing model performance under different memorization conditions, we uncover a counterintuitive trend: LLMs perform worse on memorized MCQs than on non-memorized ones, indicating the coexistence of two distinct learning phenomena, i.e., rote memorization and genuine capability learning. To disentangle them, we propose TrinEval, a novel evaluation framework that reformulates MCQs into an alternative trinity format, reducing memorization while preserving knowledge assessment. Experiments validate TrinEval's effectiveness in reformulation, and its evaluation reveals that common LLMs may memorize by rote 20.5% of knowledge points (in MMLU on average)."
      },
      {
        "id": "oai:arXiv.org:2504.08410v2",
        "title": "PMNI: Pose-free Multi-view Normal Integration for Reflective and Textureless Surface Reconstruction",
        "link": "https://arxiv.org/abs/2504.08410",
        "author": "Mingzhi Pei, Xu Cao, Xiangyi Wang, Heng Guo, Zhanyu Ma",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08410v2 Announce Type: replace \nAbstract: Reflective and textureless surfaces remain a challenge in multi-view 3D reconstruction. Both camera pose calibration and shape reconstruction often fail due to insufficient or unreliable cross-view visual features. To address these issues, we present PMNI (Pose-free Multi-view Normal Integration), a neural surface reconstruction method that incorporates rich geometric information by leveraging surface normal maps instead of RGB images. By enforcing geometric constraints from surface normals and multi-view shape consistency within a neural signed distance function (SDF) optimization framework, PMNI simultaneously recovers accurate camera poses and high-fidelity surface geometry. Experimental results on synthetic and real-world datasets show that our method achieves state-of-the-art performance in the reconstruction of reflective surfaces, even without reliable initial camera poses."
      },
      {
        "id": "oai:arXiv.org:2504.08593v2",
        "title": "Hands-On: Segmenting Individual Signs from Continuous Sequences",
        "link": "https://arxiv.org/abs/2504.08593",
        "author": "Low Jian He, Harry Walsh, Ozge Mercanoglu Sincan, Richard Bowden",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08593v2 Announce Type: replace \nAbstract: This work tackles the challenge of continuous sign language segmentation, a key task with huge implications for sign language translation and data annotation. We propose a transformer-based architecture that models the temporal dynamics of signing and frames segmentation as a sequence labeling problem using the Begin-In-Out (BIO) tagging scheme. Our method leverages the HaMeR hand features, and is complemented with 3D Angles. Extensive experiments show that our model achieves state-of-the-art results on the DGS Corpus, while our features surpass prior benchmarks on BSLCorpus."
      },
      {
        "id": "oai:arXiv.org:2504.08727v2",
        "title": "Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections of Images",
        "link": "https://arxiv.org/abs/2504.08727",
        "author": "Boyang Deng, Songyou Peng, Kyle Genova, Gordon Wetzstein, Noah Snavely, Leonidas Guibas, Thomas Funkhouser",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08727v2 Announce Type: replace \nAbstract: We present a system using Multimodal LLMs (MLLMs) to analyze a large database with tens of millions of images captured at different times, with the aim of discovering patterns in temporal changes. Specifically, we aim to capture frequent co-occurring changes (\"trends\") across a city over a certain period. Unlike previous visual analyses, our analysis answers open-ended queries (e.g., \"what are the frequent types of changes in the city?\") without any predetermined target subjects or training labels. These properties cast prior learning-based or unsupervised visual analysis tools unsuitable. We identify MLLMs as a novel tool for their open-ended semantic understanding capabilities. Yet, our datasets are four orders of magnitude too large for an MLLM to ingest as context. So we introduce a bottom-up procedure that decomposes the massive visual analysis problem into more tractable sub-problems. We carefully design MLLM-based solutions to each sub-problem. During experiments and ablation studies with our system, we find it significantly outperforms baselines and is able to discover interesting trends from images captured in large cities (e.g., \"addition of outdoor dining,\", \"overpass was painted blue,\" etc.). See more results and interactive demos at https://boyangdeng.com/visual-chronicles."
      },
      {
        "id": "oai:arXiv.org:2011.10510v4",
        "title": "Seismic Facies Analysis: A Deep Domain Adaptation Approach",
        "link": "https://arxiv.org/abs/2011.10510",
        "author": "M Quamer Nasim, Tannistha Maiti, Ayush Srivastava, Tarry Singh, Jie Mei",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2011.10510v4 Announce Type: replace-cross \nAbstract: Deep neural networks (DNNs) can learn accurately from large quantities of labeled input data, but often fail to do so when labelled data are scarce. DNNs sometimes fail to generalize ontest data sampled from different input distributions. Unsupervised Deep Domain Adaptation (DDA)techniques have been proven useful when no labels are available, and when distribution shifts are observed in the target domain (TD). In the present study, experiments are performed on seismic images of the F3 block 3D dataset from offshore Netherlands (source domain; SD) and Penobscot 3D survey data from Canada (target domain; TD). Three geological classes from SD and TD that have similar reflection patterns are considered. A deep neural network architecture named EarthAdaptNet (EAN) is proposed to semantically segment the seismic images when few classes have data scarcity, and we use a transposed residual unit to replace the traditional dilated convolution in the decoder block. The EAN achieved a pixel-level accuracy >84% and an accuracy of ~70% for the minority classes, showing improved performance compared to existing architectures. In addition, we introduce the CORAL (Correlation Alignment) method to the EAN to create an unsupervised deep domain adaptation network (EAN-DDA) for the classification of seismic reflections from F3 and Penobscot, to demonstrate possible approaches when labelled data are unavailable. Maximum class accuracy achieved was ~99% for class 2 of Penobscot, with an overall accuracy>50%. Taken together, the EAN-DDA has the potential to classify target domain seismic facies classes with high accuracy."
      },
      {
        "id": "oai:arXiv.org:2209.03440v2",
        "title": "Deep Learning-Based Automatic Diagnosis System for Developmental Dysplasia of the Hip",
        "link": "https://arxiv.org/abs/2209.03440",
        "author": "Yang Li, Leo Yan Li-Han, Hua Tian",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2209.03440v2 Announce Type: replace-cross \nAbstract: Objective: The clinical diagnosis of developmental dysplasia of the hip (DDH) typically involves manually measuring key radiological angles -- Center-Edge (CE), Tonnis, and Sharp angles -- from pelvic radiographs, a process that is time-consuming and susceptible to variability. This study aims to develop an automated system that integrates these measurements to enhance the accuracy and consistency of DDH diagnosis.\n  Methods and procedures: We developed an end-to-end deep learning model for keypoint detection that accurately identifies eight anatomical keypoints from pelvic radiographs, enabling the automated calculation of CE, Tonnis, and Sharp angles. To support the diagnostic decision, we introduced a novel data-driven scoring system that combines the information from all three angles into a comprehensive and explainable diagnostic output.\n  Results: The system demonstrated superior consistency in angle measurements compared to a cohort of eight moderately experienced orthopedists. The intraclass correlation coefficients for the CE, Tonnis, and Sharp angles were 0.957 (95% CI: 0.952--0.962), 0.942 (95% CI: 0.937--0.947), and 0.966 (95% CI: 0.964--0.968), respectively. The system achieved a diagnostic F1 score of 0.863 (95% CI: 0.851--0.876), significantly outperforming the orthopedist group (0.777, 95% CI: 0.737--0.817, p = 0.005), as well as using clinical diagnostic criteria for each angle individually (p<0.001).\n  Conclusion: The proposed system provides reliable and consistent automated measurements of radiological angles and an explainable diagnostic output for DDH, outperforming moderately experienced clinicians.\n  Clinical impact: This AI-powered solution reduces the variability and potential errors of manual measurements, offering clinicians a more consistent and interpretable tool for DDH diagnosis."
      },
      {
        "id": "oai:arXiv.org:2210.04979v4",
        "title": "Label-free segmentation from cardiac ultrasound using self-supervised learning",
        "link": "https://arxiv.org/abs/2210.04979",
        "author": "Danielle L. Ferreira, Connor Lau, Zaynaf Salaymang, Rima Arnaout",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2210.04979v4 Announce Type: replace-cross \nAbstract: Segmentation and measurement of cardiac chambers is critical in cardiac ultrasound but is laborious and poorly reproducible. Neural networks can assist, but supervised approaches require the same laborious manual annotations. We built a pipeline for self-supervised (no manual labels) segmentation combining computer vision, clinical domain knowledge, and deep learning. We trained on 450 echocardiograms (93,000 images) and tested on 8,393 echocardiograms (4,476,266 images; mean 61 years, 51% female), using the resulting segmentations to calculate biometrics. We also tested against external images from an additional 10,030 patients with available manual tracings of the left ventricle. r2 between clinically measured and pipeline-predicted measurements were similar to reported inter-clinician variation and comparable to supervised learning across several different measurements (r2 0.56-0.84). Average accuracy for detecting abnormal chamber size and function was 0.85 (range 0.71-0.97) compared to clinical measurements. A subset of test echocardiograms (n=553) had corresponding cardiac MRIs, where MRI is the gold standard. Correlation between pipeline and MRI measurements was similar to that between clinical echocardiogram and MRI. Finally, the pipeline accurately segments the left ventricle with an average Dice score of 0.89 (95% CI [0.89]) in the external, manually labeled dataset. Our results demonstrate a manual-label free, clinically valid, and highly scalable method for segmentation from ultrasound, a noisy but globally important imaging modality."
      },
      {
        "id": "oai:arXiv.org:2302.05816v3",
        "title": "A Policy Gradient Framework for Stochastic Optimal Control Problems with Global Convergence Guarantee",
        "link": "https://arxiv.org/abs/2302.05816",
        "author": "Mo Zhou, Jianfeng Lu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2302.05816v3 Announce Type: replace-cross \nAbstract: We consider policy gradient methods for stochastic optimal control problem in continuous time. In particular, we analyze the gradient flow for the control, viewed as a continuous time limit of the policy gradient method. We prove the global convergence of the gradient flow and establish a convergence rate under some regularity assumptions. The main novelty in the analysis is the notion of local optimal control function, which is introduced to characterize the local optimality of the iterate."
      },
      {
        "id": "oai:arXiv.org:2304.08400v2",
        "title": "ATHEENA: A Toolflow for Hardware Early-Exit Network Automation",
        "link": "https://arxiv.org/abs/2304.08400",
        "author": "Benjamin Biggs, Christos-Savvas Bouganis, George A. Constantinides",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2304.08400v2 Announce Type: replace-cross \nAbstract: The continued need for improvements in accuracy, throughput, and efficiency of Deep Neural Networks has resulted in a multitude of methods that make the most of custom architectures on FPGAs. These include the creation of hand-crafted networks and the use of quantization and pruning to reduce extraneous network parameters. However, with the potential of static solutions already well exploited, we propose to shift the focus to using the varying difficulty of individual data samples to further improve efficiency and reduce average compute for classification. Input-dependent computation allows for the network to make runtime decisions to finish a task early if the result meets a confidence threshold. Early-Exit network architectures have become an increasingly popular way to implement such behaviour in software.\n  We create: A Toolflow for Hardware Early-Exit Network Automation (ATHEENA), an automated FPGA toolflow that leverages the probability of samples exiting early from such networks to scale the resources allocated to different sections of the network. The toolflow uses the data-flow model of fpgaConvNet, extended to support Early-Exit networks as well as Design Space Exploration to optimize the generated streaming architecture hardware with the goal of increasing throughput/reducing area while maintaining accuracy. Experimental results on three different networks demonstrate a throughput increase of $2.00\\times$ to $2.78\\times$ compared to an optimized baseline network implementation with no early exits. Additionally, the toolflow can achieve a throughput matching the same baseline with as low as $46\\%$ of the resources the baseline requires."
      },
      {
        "id": "oai:arXiv.org:2310.17471v2",
        "title": "Toward 6G Native-AI Network: Foundation Model based Cloud-Edge-End Collaboration Framework",
        "link": "https://arxiv.org/abs/2310.17471",
        "author": "Xiang Chen, Zhiheng Guo, Xijun Wang, Howard H. Yang, Chenyuan Feng, Shuangfeng Han, Xiaoyun Wang, Tony Q. S. Quek",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2310.17471v2 Announce Type: replace-cross \nAbstract: Future wireless communication networks are in a position to move beyond data-centric, device-oriented connectivity and offer intelligent, immersive experiences based on multi-agent collaboration, especially in the context of the thriving development of pre-trained foundation models (PFM) and the evolving vision of 6G native artificial intelligence (AI). Therefore, redefining modes of collaboration between devices and agents, and constructing native intelligence libraries become critically important in 6G. In this paper, we analyze the challenges of achieving 6G native AI from the perspectives of data, AI models, and operational paradigm. Then, we propose a 6G native AI framework based on foundation models, provide an integration method for the expert knowledge, present the customization for two kinds of PFM, and outline a novel operational paradigm for the native AI framework. As a practical use case, we apply this framework for orchestration, achieving the maximum sum rate within a cell-free massive MIMO system, and presenting preliminary evaluation results. Finally, we outline research directions for achieving native AI in 6G."
      },
      {
        "id": "oai:arXiv.org:2311.13094v3",
        "title": "Newton-CG methods for nonconvex unconstrained optimization with H\\\"older continuous Hessian",
        "link": "https://arxiv.org/abs/2311.13094",
        "author": "Chuan He, Heng Huang, Zhaosong Lu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2311.13094v3 Announce Type: replace-cross \nAbstract: In this paper we consider a nonconvex unconstrained optimization problem minimizing a twice differentiable objective function with H\\\"older continuous Hessian. Specifically, we first propose a Newton-conjugate gradient (Newton-CG) method for finding an approximate first- and second-order stationary point of this problem, assuming the associated the H\\\"older parameters are explicitly known. Then we develop a parameter-free Newton-CG method without requiring any prior knowledge of these parameters. To the best of our knowledge, this method is the first parameter-free second-order method achieving the best-known iteration and operation complexity for finding an approximate first- and second-order stationary point of this problem. Finally, we present preliminary numerical results to demonstrate the superior practical performance of our parameter-free Newton-CG method over a well-known regularized Newton method."
      },
      {
        "id": "oai:arXiv.org:2312.03792v2",
        "title": "PCDP-SGD: Improving the Convergence of Differentially Private SGD via Projection in Advance",
        "link": "https://arxiv.org/abs/2312.03792",
        "author": "Haichao Sha (Renmin University of China), Ruixuan Liu (Renmin University of China), Yixuan Liu (Renmin University of China), Hong Chen (Renmin University of China)",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2312.03792v2 Announce Type: replace-cross \nAbstract: The paradigm of Differentially Private SGD~(DP-SGD) can provide a theoretical guarantee for training data in both centralized and federated settings. However, the utility degradation caused by DP-SGD limits its wide application in high-stakes tasks, such as medical image diagnosis. In addition to the necessary perturbation, the convergence issue is attributed to the information loss on the gradient clipping. In this work, we propose a general framework PCDP-SGD, which aims to compress redundant gradient norms and preserve more crucial top gradient components via projection operation before gradient clipping. Additionally, we extend PCDP-SGD as a fundamental component in differential privacy federated learning~(DPFL) for mitigating the data heterogeneous challenge and achieving efficient communication. We prove that pre-projection enhances the convergence of DP-SGD by reducing the dependence of clipping error and bias to a fraction of the top gradient eigenspace, and in theory, limits cross-client variance to improve the convergence under heterogeneous federation. Experimental results demonstrate that PCDP-SGD achieves higher accuracy compared with state-of-the-art DP-SGD variants in computer vision tasks. Moreover, PCDP-SGD outperforms current federated learning frameworks when DP is guaranteed on local training sets."
      },
      {
        "id": "oai:arXiv.org:2312.10068v2",
        "title": "Artificial Neural Network for Estimation of Physical Parameters of Sea Water using LiDAR Waveforms",
        "link": "https://arxiv.org/abs/2312.10068",
        "author": "Saad Ahmed Jamal",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2312.10068v2 Announce Type: replace-cross \nAbstract: Light Detection and Ranging (LiDAR) are fast emerging sensors in the field of Earth Observation. It is a remote sensing technology that utilizes laser beams to measure distances and create detailed three-dimensional representations of objects and environments. The potential of Full Waveform LiDAR is much greater than just height estimation and 3D reconstruction only. Overall shape of signal provides important information about properties of water body. However, the shape of FWL is unexplored as most LiDAR software work on point cloud by utilizing the maximum value within the waveform. Existing techniques in the field of LiDAR data analysis include depth estimation through inverse modeling and regression of logarithmic intensity and depth for approximating the attenuation coefficient. However, these methods suffer from limitations in accuracy. Depth estimation through inverse modeling provides only approximate values and does not account for variations in surface properties, while the regression approach for the attenuation coefficient is only able to generalize a value through several data points which lacks precision and may lead to significant errors in estimation. Additionally, there is currently no established modeling method available for predicting bottom reflectance. This research proposed a novel solution based on neural networks for parameter estimation in LIDAR data analysis. By leveraging the power of neural networks, the proposed solution successfully learned the inversion model, was able to do prediction of parameters such as depth, attenuation coefficient, and bottom reflectance. Performance of model was validated by testing it on real LiDAR data. In future, more data availability would enable more accuracy and reliability of such models."
      },
      {
        "id": "oai:arXiv.org:2312.15856v3",
        "title": "SplatMesh: Interactive 3D Segmentation and Editing Using Mesh-Based Gaussian Splatting",
        "link": "https://arxiv.org/abs/2312.15856",
        "author": "Kaichen Zhou, Lanqing Hong, Xinhai Chang, Yingji Zhong, Enze Xie, Hao Dong, Zhihao Li, Yongxin Yang, Zhenguo Li, Wei Zhang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2312.15856v3 Announce Type: replace-cross \nAbstract: A key challenge in fine-grained 3D-based interactive editing is the absence of an efficient representation that balances diverse modifications with high-quality view synthesis under a given memory constraint. While 3D meshes provide robustness for various modifications, they often yield lower-quality view synthesis compared to 3D Gaussian Splatting, which, in turn, suffers from instability during extensive editing. A straightforward combination of these two representations results in suboptimal performance and fails to meet memory constraints. In this paper, we introduce SplatMesh, a novel fine-grained interactive 3D segmentation and editing algorithm that integrates 3D Gaussian Splat with a precomputed mesh and could adjust the memory request based on the requirement. Specifically, given a mesh, \\method simplifies it while considering both color and shape, ensuring it meets memory constraints. Then, SplatMesh aligns Gaussian splats with the simplified mesh by treating each triangle as a new reference point. By segmenting and editing the simplified mesh, we can effectively edit the Gaussian splats as well, which will lead to extensive experiments on real and synthetic datasets, coupled with illustrative visual examples, highlighting the superiority of our approach in terms of representation quality and editing performance. Code of our paper can be found here: https://github.com/kaichen-z/SplatMesh."
      },
      {
        "id": "oai:arXiv.org:2401.03692v4",
        "title": "Boosting Column Generation with Graph Neural Networks for Joint Rider Trip Planning and Crew Shift Scheduling",
        "link": "https://arxiv.org/abs/2401.03692",
        "author": "Jiawei Lu, Tinghan Ye, Wenbo Chen, Pascal Van Hentenryck",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2401.03692v4 Announce Type: replace-cross \nAbstract: Optimizing service schedules is pivotal to the reliable, efficient, and inclusive on-demand mobility. This pressing challenge is further exacerbated by the increasing needs of an aging population, the oversubscription of existing services, and the lack of effective solution methods. This study addresses the intricacies of service scheduling, by jointly optimizing rider trip planning and crew scheduling for a complex dynamic mobility service. The resulting optimization problems are extremely challenging computationally for state-of-the-art methods. To address this fundamental gap, this paper introduces the Joint Rider Trip Planning and Crew Shift Scheduling Problem (JRTPCSSP) and a novel solution method, called Attention and Gated GNN-Informed Column Generation (AGGNNI-CG), that hybridizes column generation and machine learning to obtain near-optimal solutions to the JRTPCSSP with real-life constraints of the application. The key idea of the machine-learning component is to dramatically reduce the number of paths to explore in the pricing problem, accelerating the most time-consuming component of the column generation. The machine learning component is a graph neural network with an attention mechanism and a gated architecture, which is particularly suited to cater for the different input sizes coming from daily operations. AGGNNI-CG has been applied to a challenging, real-world dataset from the Paratransit system of Chatham County in Georgia. It produces substantial improvements compared to the baseline column generation approach, which typically cannot produce high-quality feasible solutions in reasonable time on large-scale complex instances. AGGNNI-CG also produces significant improvements in service quality compared to the existing system."
      },
      {
        "id": "oai:arXiv.org:2402.01677v4",
        "title": "Embedding Ontologies via Incorporating Extensional and Intensional Knowledge",
        "link": "https://arxiv.org/abs/2402.01677",
        "author": "Keyu Wang, Guilin Qi, Jiaoyan Chen, Yi Huang, Tianxing Wu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2402.01677v4 Announce Type: replace-cross \nAbstract: Ontologies contain rich knowledge within domain, which can be divided into two categories, namely extensional knowledge and intensional knowledge. Extensional knowledge provides information about the concrete instances that belong to specific concepts in the ontology, while intensional knowledge details inherent properties, characteristics, and semantic associations among concepts. However, existing ontology embedding approaches fail to take both extensional knowledge and intensional knowledge into fine consideration simultaneously. In this paper, we propose a novel ontology embedding approach named EIKE (Extensional and Intensional Knowledge Embedding) by representing ontologies in two spaces, called extensional space and intensional space. EIKE presents a unified framework for embedding instances, concepts and their relations in an ontology, applying a geometry-based method to model extensional knowledge and a pretrained language model to model intensional knowledge, which can capture both structure information and textual information. Experimental results show that EIKE significantly outperforms state-of-the-art methods in three datasets for both triple classification and link prediction, indicating that EIKE provides a more comprehensive and representative perspective of the domain."
      },
      {
        "id": "oai:arXiv.org:2405.00252v2",
        "title": "Q-Newton: Hybrid Quantum-Classical Scheduling for Accelerating Neural Network Training with Newton's Gradient Descent",
        "link": "https://arxiv.org/abs/2405.00252",
        "author": "Pingzhi Li, Junyu Liu, Hanrui Wang, Tianlong Chen",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2405.00252v2 Announce Type: replace-cross \nAbstract: Optimization techniques in deep learning are predominantly led by first-order gradient methodologies, such as SGD. However, neural network training can greatly benefit from the rapid convergence characteristics of second-order optimization. Newton's GD stands out in this category, by rescaling the gradient using the inverse Hessian. Nevertheless, one of its major bottlenecks is matrix inversion, which is notably time-consuming in $O(N^3)$ time with weak scalability.\n  Matrix inversion can be translated into solving a series of linear equations. Given that quantum linear solver algorithms (QLSAs), leveraging the principles of quantum superposition and entanglement, can operate within a $\\text{polylog}(N)$ time frame, they present a promising approach with exponential acceleration. Specifically, one of the most recent QLSAs demonstrates a complexity scaling of $O(d\\cdot\\kappa \\log(N\\cdot\\kappa/\\epsilon))$, depending on: {size~$N$, condition number~$\\kappa$, error tolerance~$\\epsilon$, quantum oracle sparsity~$d$} of the matrix. However, this also implies that their potential exponential advantage may be hindered by certain properties (i.e. $\\kappa$ and $d$).\n  We propose Q-Newton, a hybrid quantum-classical scheduler for accelerating neural network training with Newton's GD. Q-Newton utilizes a streamlined scheduling module that coordinates between quantum and classical linear solvers, by estimating & reducing $\\kappa$ and constructing $d$ for the quantum solver.\n  Our evaluation showcases the potential for Q-Newton to significantly reduce the total training time compared to commonly used optimizers like SGD. We hypothesize a future scenario where the gate time of quantum machines is reduced, possibly realized by attoseconds physics. Our evaluation establishes an ambitious and promising target for the evolution of quantum computing."
      },
      {
        "id": "oai:arXiv.org:2405.05336v3",
        "title": "Joint semi-supervised and contrastive learning enables domain generalization and multi-domain segmentation",
        "link": "https://arxiv.org/abs/2405.05336",
        "author": "Alvaro Gomariz, Yusuke Kikuchi, Yun Yvonna Li, Thomas Albrecht, Andreas Maunz, Daniela Ferrara, Huanxiang Lu, Orcun Goksel",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2405.05336v3 Announce Type: replace-cross \nAbstract: Despite their effectiveness, current deep learning models face challenges with images coming from different domains with varying appearance and content. We introduce SegCLR, a versatile framework designed to segment images across different domains, employing supervised and contrastive learning simultaneously to effectively learn from both labeled and unlabeled data. We demonstrate the superior performance of SegCLR through a comprehensive evaluation involving three diverse clinical datasets of 3D retinal Optical Coherence Tomography (OCT) images, for the slice-wise segmentation of fluids with various network configurations and verification across 10 different network initializations. In an unsupervised domain adaptation context, SegCLR achieves results on par with a supervised upper-bound model trained on the intended target domain. Notably, we discover that the segmentation performance of SegCLR framework is marginally impacted by the abundance of unlabeled data from the target domain, thereby we also propose an effective domain generalization extension of SegCLR, known also as zero-shot domain adaptation, which eliminates the need for any target domain information. This shows that our proposed addition of contrastive loss in standard supervised training for segmentation leads to superior models, inherently more generalizable to both in- and out-of-domain test data. We additionally propose a pragmatic solution for SegCLR deployment in realistic scenarios with multiple domains containing labeled data. Accordingly, our framework pushes the boundaries of deep-learning based segmentation in multi-domain applications, regardless of data availability - labeled, unlabeled, or nonexistent."
      },
      {
        "id": "oai:arXiv.org:2405.08825v4",
        "title": "Thermodynamic limit in learning period three",
        "link": "https://arxiv.org/abs/2405.08825",
        "author": "Yuichiro Terasaki, Kohei Nakajima",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2405.08825v4 Announce Type: replace-cross \nAbstract: A continuous one-dimensional map with period three includes all periods. This raises the following question: Can we obtain any types of periodic orbits solely by learning three data points? In this paper, we report the answer to be yes. Considering a random neural network in its thermodynamic limit, we first show that almost all learned periods are unstable, and each network has its own characteristic attractors (which can even be untrained ones). The latently acquired dynamics, which are unstable within the trained network, serve as a foundation for the diversity of characteristic attractors and may even lead to the emergence of attractors of all periods after learning. When the neural network interpolation is quadratic, a universal post-learning bifurcation scenario appears, which is consistent with a topological conjugacy between the trained network and the classical logistic map. In addition to universality, we explore specific properties of certain networks, including the singular behavior of the scale of weight at the infinite limit, the finite-size effects, and the symmetry in learning period three."
      },
      {
        "id": "oai:arXiv.org:2405.14139v2",
        "title": "Contribute to balance, wire in accordance: Emergence of backpropagation from a simple, bio-plausible neuroplasticity rule",
        "link": "https://arxiv.org/abs/2405.14139",
        "author": "Xinhao Fan, Shreesh P Mysore",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2405.14139v2 Announce Type: replace-cross \nAbstract: Backpropagation (BP) has been pivotal in advancing machine learning and remains essential in computational applications and comparative studies of biological and artificial neural networks. Despite its widespread use, the implementation of BP in the brain remains elusive, and its biological plausibility is often questioned due to inherent issues such as the need for symmetry of weights between forward and backward connections, and the requirement of distinct forward and backward phases of computation. Here, we introduce a novel neuroplasticity rule that offers a potential mechanism for implementing BP in the brain. Similar in general form to the classical Hebbian rule, this rule is based on the core principles of maintaining the balance of excitatory and inhibitory inputs as well as on retrograde signaling, and operates over three progressively slower timescales: neural firing, retrograde signaling, and neural plasticity. We hypothesize that each neuron possesses an internal state, termed credit, in addition to its firing rate. After achieving equilibrium in firing rates, neurons receive credits based on their contribution to the E-I balance of postsynaptic neurons through retrograde signaling. As the network's credit distribution stabilizes, connections from those presynaptic neurons are strengthened that significantly contribute to the balance of postsynaptic neurons. We demonstrate mathematically that our learning rule precisely replicates BP in layered neural networks without any approximations. Simulations on artificial neural networks reveal that this rule induces varying community structures in networks, depending on the learning rate. This simple theoretical framework presents a biologically plausible implementation of BP, with testable assumptions and predictions that may be evaluated through biological experiments."
      },
      {
        "id": "oai:arXiv.org:2406.01708v2",
        "title": "SnatchML: Hijacking ML models without Training Access",
        "link": "https://arxiv.org/abs/2406.01708",
        "author": "Mahmoud Ghorbel, Halima Bouzidi, Ioan Marius Bilasco, Ihsen Alouani",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2406.01708v2 Announce Type: replace-cross \nAbstract: Model hijacking can cause significant accountability and security risks since the owner of a hijacked model can be framed for having their model offer illegal or unethical services. Prior works consider model hijacking as a training time attack, whereby an adversary requires full access to the ML model training. In this paper, we consider a stronger threat model for an inference-time hijacking attack, where the adversary has no access to the training phase of the victim model. Our intuition is that ML models, which are typically over-parameterized, might have the capacity to (unintentionally) learn more than the intended task they are trained for. We propose SnatchML, a new training-free model hijacking attack, that leverages the extra capacity learnt by the victim model to infer different tasks that can be semantically related or unrelated to the original one. Our results on models deployed on AWS Sagemaker showed that SnatchML can deliver high accuracy on hijacking tasks. Interestingly, while all previous approaches are limited by the number of classes in the benign task, SnatchML can hijack models for tasks that contain more classes than the original. We explore different methods to mitigate this risk; We propose meta-unlearning, which is designed to help the model unlearn a potentially malicious task while training for the original task. We also provide insights on over-parametrization as a possible inherent factor that facilitates model hijacking, and accordingly, we propose a compression-based countermeasure to counteract this attack. We believe this work offers a previously overlooked perspective on model hijacking attacks, presenting a stronger threat model and higher applicability in real-world contexts."
      },
      {
        "id": "oai:arXiv.org:2406.08315v2",
        "title": "Improving Policy Optimization via $\\varepsilon$-Retrain",
        "link": "https://arxiv.org/abs/2406.08315",
        "author": "Luca Marzari, Priya L. Donti, Changliu Liu, Enrico Marchesini",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2406.08315v2 Announce Type: replace-cross \nAbstract: We present $\\varepsilon$-retrain, an exploration strategy encouraging a behavioral preference while optimizing policies with monotonic improvement guarantees. To this end, we introduce an iterative procedure for collecting retrain areas -- parts of the state space where an agent did not satisfy the behavioral preference. Our method switches between the typical uniform restart state distribution and the retrain areas using a decaying factor $\\varepsilon$, allowing agents to retrain on situations where they violated the preference. We also employ formal verification of neural networks to provably quantify the degree to which agents adhere to these behavioral preferences. Experiments over hundreds of seeds across locomotion, power network, and navigation tasks show that our method yields agents that exhibit significant performance and sample efficiency improvements."
      },
      {
        "id": "oai:arXiv.org:2406.18145v4",
        "title": "Beyond Statistical Estimation: Differentially Private Individual Computation via Shuffling",
        "link": "https://arxiv.org/abs/2406.18145",
        "author": "Shaowei Wang, Changyu Dong, Xiangfu Song, Jin Li, Zhili Zhou, Di Wang, Han Wu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2406.18145v4 Announce Type: replace-cross \nAbstract: In data-driven applications, preserving user privacy while enabling valuable computations remains a critical challenge. Technologies like differential privacy have been pivotal in addressing these concerns. The shuffle model of DP requires no trusted curators and can achieve high utility by leveraging the privacy amplification effect yielded from shuffling. These benefits have led to significant interest in the shuffle model. However, the computation tasks in the shuffle model are limited to statistical estimation, making it inapplicable to real-world scenarios in which each user requires a personalized output. This paper introduces a novel paradigm termed Private Individual Computation (PIC), expanding the shuffle model to support a broader range of permutation-equivariant computations. PIC enables personalized outputs while preserving privacy, and enjoys privacy amplification through shuffling. We propose a concrete protocol that realizes PIC. By using one-time public keys, our protocol enables users to receive their outputs without compromising anonymity, which is essential for privacy amplification. Additionally, we present an optimal randomizer, the Minkowski Response, designed for the PIC model to enhance utility. We formally prove the security and privacy properties of the PIC protocol. Theoretical analysis and empirical evaluations demonstrate PIC's capability in handling non-statistical computation tasks, and the efficacy of PIC and the Minkowski randomizer in achieving superior utility compared to existing solutions."
      },
      {
        "id": "oai:arXiv.org:2407.18449v3",
        "title": "Towards A Generalizable Pathology Foundation Model via Unified Knowledge Distillation",
        "link": "https://arxiv.org/abs/2407.18449",
        "author": "Jiabo Ma, Zhengrui Guo, Fengtao Zhou, Yihui Wang, Yingxue Xu, Jinbang Li, Fang Yan, Yu Cai, Zhengjie Zhu, Cheng Jin, Yi Lin, Xinrui Jiang, Chenglong Zhao, Danyi Li, Anjia Han, Zhenhui Li, Ronald Cheong Kin Chan, Jiguang Wang, Peng Fei, Kwang-Ting Cheng, Shaoting Zhang, Li Liang, Hao Chen",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2407.18449v3 Announce Type: replace-cross \nAbstract: Foundation models pretrained on large-scale datasets are revolutionizing the field of computational pathology (CPath). The generalization ability of foundation models is crucial for the success in various downstream clinical tasks. However, current foundation models have only been evaluated on a limited type and number of tasks, leaving their generalization ability and overall performance unclear. To address this gap, we established a most comprehensive benchmark to evaluate the performance of off-the-shelf foundation models across six distinct clinical task types, encompassing a total of 72 specific tasks, including slide-level classification, survival prediction, ROI-tissue classification, ROI retrieval, visual question answering, and report generation. Our findings reveal that existing foundation models excel at certain task types but struggle to effectively handle the full breadth of clinical tasks. To improve the generalization of pathology foundation models, we propose a unified knowledge distillation framework consisting of both expert and self-knowledge distillation, where the former allows the model to learn from the knowledge of multiple expert models, while the latter leverages self-distillation to enable image representation learning via local-global alignment. Based on this framework, we curated a dataset of 96,000 whole slide images (WSIs) and developed a Generalizable Pathology Foundation Model (GPFM). This advanced model was trained on a substantial dataset comprising 190 million images extracted from approximately 72,000 publicly available slides, encompassing 34 major tissue types. Evaluated on the established benchmark, GPFM achieves an impressive average rank of 1.6, with 42 tasks ranked 1st, while the second-best model, UNI, attains an average rank of 3.7, with only 6 tasks ranked 1st."
      },
      {
        "id": "oai:arXiv.org:2408.02479v2",
        "title": "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
        "link": "https://arxiv.org/abs/2408.02479",
        "author": "Haolin Jin, Linghan Huang, Haipeng Cai, Jun Yan, Bo Li, Huaming Chen",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2408.02479v2 Announce Type: replace-cross \nAbstract: With the rise of large language models (LLMs), researchers are increasingly exploring their applications in var ious vertical domains, such as software engineering. LLMs have achieved remarkable success in areas including code generation and vulnerability detection. However, they also exhibit numerous limitations and shortcomings. LLM-based agents, a novel tech nology with the potential for Artificial General Intelligence (AGI), combine LLMs as the core for decision-making and action-taking, addressing some of the inherent limitations of LLMs such as lack of autonomy and self-improvement. Despite numerous studies and surveys exploring the possibility of using LLMs in software engineering, it lacks a clear distinction between LLMs and LLM based agents. It is still in its early stage for a unified standard and benchmarking to qualify an LLM solution as an LLM-based agent in its domain. In this survey, we broadly investigate the current practice and solutions for LLMs and LLM-based agents for software engineering. In particular we summarise six key topics: requirement engineering, code generation, autonomous decision-making, software design, test generation, and software maintenance. We review and differentiate the work of LLMs and LLM-based agents from these six topics, examining their differences and similarities in tasks, benchmarks, and evaluation metrics. Finally, we discuss the models and benchmarks used, providing a comprehensive analysis of their applications and effectiveness in software engineering. We anticipate this work will shed some lights on pushing the boundaries of LLM-based agents in software engineering for future research."
      },
      {
        "id": "oai:arXiv.org:2408.08926v4",
        "title": "Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risks of Language Models",
        "link": "https://arxiv.org/abs/2408.08926",
        "author": "Andy K. Zhang, Neil Perry, Riya Dulepet, Joey Ji, Celeste Menders, Justin W. Lin, Eliot Jones, Gashon Hussein, Samantha Liu, Donovan Jasper, Pura Peetathawatchai, Ari Glenn, Vikram Sivashankar, Daniel Zamoshchin, Leo Glikbarg, Derek Askaryar, Mike Yang, Teddy Zhang, Rishi Alluri, Nathan Tran, Rinnara Sangpisit, Polycarpos Yiorkadjis, Kenny Osele, Gautham Raghupathi, Dan Boneh, Daniel E. Ho, Percy Liang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2408.08926v4 Announce Type: replace-cross \nAbstract: Language Model (LM) agents for cybersecurity that are capable of autonomously identifying vulnerabilities and executing exploits have potential to cause real-world impact. Policymakers, model providers, and researchers in the AI and cybersecurity communities are interested in quantifying the capabilities of such agents to help mitigate cyberrisk and investigate opportunities for penetration testing. Toward that end, we introduce Cybench, a framework for specifying cybersecurity tasks and evaluating agents on those tasks. We include 40 professional-level Capture the Flag (CTF) tasks from 4 distinct CTF competitions, chosen to be recent, meaningful, and spanning a wide range of difficulties. Each task includes its own description, starter files, and is initialized in an environment where an agent can execute commands and observe outputs. Since many tasks are beyond the capabilities of existing LM agents, we introduce subtasks for each task, which break down a task into intermediary steps for a more detailed evaluation. To evaluate agent capabilities, we construct a cybersecurity agent and evaluate 8 models: GPT-4o, OpenAI o1-preview, Claude 3 Opus, Claude 3.5 Sonnet, Mixtral 8x22b Instruct, Gemini 1.5 Pro, Llama 3 70B Chat, and Llama 3.1 405B Instruct. For the top performing models (GPT-4o and Claude 3.5 Sonnet), we further investigate performance across 4 agent scaffolds (structed bash, action-only, pseudoterminal, and web search). Without subtask guidance, agents leveraging Claude 3.5 Sonnet, GPT-4o, OpenAI o1-preview, and Claude 3 Opus successfully solved complete tasks that took human teams up to 11 minutes to solve. In comparison, the most difficult task took human teams 24 hours and 54 minutes to solve. All code and data are publicly available at https://cybench.github.io."
      },
      {
        "id": "oai:arXiv.org:2408.13336v2",
        "title": "Oscillatory and Excitable Dynamics in an Opinion Model with Group Opinions",
        "link": "https://arxiv.org/abs/2408.13336",
        "author": "Corbit R. Sampson, Juan G. Restrepo, Mason A. Porter",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2408.13336v2 Announce Type: replace-cross \nAbstract: In traditional models of opinion dynamics, each agent in a network has an opinion and changes in opinions arise from pairwise (i.e., dyadic) interactions between agents. However, in many situations, groups of individuals possess a collective opinion that can differ from the opinions of its constituent individuals. In this paper, we study the effects of group opinions on opinion dynamics. We formulate a hypergraph model in which both individual agents and groups of 3 agents have opinions, and we examine how opinions evolve through both dyadic interactions and group memberships. In some parameter regimes, we find that the presence of group opinions can lead to oscillatory and excitable opinion dynamics. In the oscillatory regime, the mean opinion of the agents in a network has self-sustained oscillations. In the excitable regime, finite-size effects create large but short-lived opinion swings (as in social fads). We develop a mean-field approximation of our model and obtain good agreement with direct numerical simulations. We also show -- both numerically and via our mean-field description -- that oscillatory dynamics occur only when the number of dyadic and polyadic interactions per agent are not completely correlated. Our results illustrate how polyadic structures, such as groups of agents, can have important effects on collective opinion dynamics."
      },
      {
        "id": "oai:arXiv.org:2408.14254v2",
        "title": "Integrated Brain Connectivity Analysis with fMRI, DTI, and sMRI Powered by Interpretable Graph Neural Networks",
        "link": "https://arxiv.org/abs/2408.14254",
        "author": "Gang Qu, Ziyu Zhou, Vince D. Calhoun, Aiying Zhang, Yu-Ping Wang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2408.14254v2 Announce Type: replace-cross \nAbstract: Multimodal neuroimaging modeling has becomes a widely used approach but confronts considerable challenges due to heterogeneity, which encompasses variability in data types, scales, and formats across modalities. This variability necessitates the deployment of advanced computational methods to integrate and interpret these diverse datasets within a cohesive analytical framework. In our research, we amalgamate functional magnetic resonance imaging, diffusion tensor imaging, and structural MRI into a cohesive framework. This integration capitalizes on the unique strengths of each modality and their inherent interconnections, aiming for a comprehensive understanding of the brain's connectivity and anatomical characteristics. Utilizing the Glasser atlas for parcellation, we integrate imaging derived features from various modalities: functional connectivity from fMRI, structural connectivity from DTI, and anatomical features from sMRI within consistent regions. Our approach incorporates a masking strategy to differentially weight neural connections, thereby facilitating a holistic amalgamation of multimodal imaging data. This technique enhances interpretability at connectivity level, transcending traditional analyses centered on singular regional attributes. The model is applied to the Human Connectome Project's Development study to elucidate the associations between multimodal imaging and cognitive functions throughout youth. The analysis demonstrates improved predictive accuracy and uncovers crucial anatomical features and essential neural connections, deepening our understanding of brain structure and function."
      },
      {
        "id": "oai:arXiv.org:2409.01990v5",
        "title": "Designing Large Foundation Models for Efficient Training and Inference: A Survey",
        "link": "https://arxiv.org/abs/2409.01990",
        "author": "Dong Liu, Yanxuan Yu, Yite Wang, Jing Wu, Zhongwei Wan, Sina Alinejad, Benjamin Lengerich, Ying Nian Wu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2409.01990v5 Announce Type: replace-cross \nAbstract: This paper focuses on modern efficient training and inference technologies on foundation models and illustrates them from two perspectives: model and system design. Model and System Design optimize LLM training and inference from different aspects to save computational resources, making LLMs more efficient, affordable, and more accessible. The paper list repository is available at https://github.com/NoakLiu/Efficient-Foundation-Models-Survey."
      },
      {
        "id": "oai:arXiv.org:2409.06560v2",
        "title": "A Primer on Variational Inference for Physics-Informed Deep Generative Modelling",
        "link": "https://arxiv.org/abs/2409.06560",
        "author": "Alex Glyn-Davies, Arnaud Vadeboncoeur, O. Deniz Akyildiz, Ieva Kazlauskaite, Mark Girolami",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2409.06560v2 Announce Type: replace-cross \nAbstract: Variational inference (VI) is a computationally efficient and scalable methodology for approximate Bayesian inference. It strikes a balance between accuracy of uncertainty quantification and practical tractability. It excels at generative modelling and inversion tasks due to its built-in Bayesian regularisation and flexibility, essential qualities for physics related problems. For such problems, the underlying physical model determines the dependence between variables of interest, which in turn will require a tailored derivation for the central VI learning objective. Furthermore, in many physical inference applications this structure has rich meaning and is essential for accurately capturing the dynamics of interest. In this paper, we provide an accessible and thorough technical introduction to VI for forward and inverse problems, guiding the reader through standard derivations of the VI framework and how it can best be realized through deep learning. We then review and unify recent literature exemplifying the flexibility allowed by VI. This paper is designed for a general scientific audience looking to solve physics-based problems with an emphasis on uncertainty quantification"
      },
      {
        "id": "oai:arXiv.org:2409.09487v2",
        "title": "Evaluating probabilistic and data-driven inference models for fiber-coupled NV-diamond temperature sensors",
        "link": "https://arxiv.org/abs/2409.09487",
        "author": "Shraddha Rajpal, Zeeshan Ahmed, Tyrus Berry",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2409.09487v2 Announce Type: replace-cross \nAbstract: We evaluate the impact of inference model on uncertainties when using continuous wave Optically Detected Magnetic Resonance (ODMR) measurements to infer temperature. Our approach leverages a probabilistic feedforward inference model designed to maximize the likelihood of observed ODMR spectra through automatic differentiation. This model effectively utilizes the temperature dependence of spin Hamiltonian parameters to infer temperature from spectral features in the ODMR data. We achieve prediction uncertainty of $\\pm$ 1 K across a temperature range of 243 K to 323 K. To benchmark our probabilistic model, we compare it with a non-parametric peak-finding technique and data-driven methodologies such as Principal Component Regression (PCR) and a 1D Convolutional Neural Network (CNN). We find that when validated against out-of-sample dataset that encompasses the same temperature range as the training dataset, data driven methods can show uncertainties that are as much as 0.67 K lower without incorporating expert-level understanding of the spectroscopic-temperature relationship. However, our results show that the probabilistic model outperforms both PCR and CNN when tasked with extrapolating beyond the temperature range used in training set, indicating robustness and generalizability. In contrast, data-driven methods like PCR and CNN demonstrate up to ten times worse uncertainties when tasked with extrapolating outside their training data range."
      },
      {
        "id": "oai:arXiv.org:2409.11267v2",
        "title": "Integrating Reinforcement Learning and Model Predictive Control with Applications to Microgrids",
        "link": "https://arxiv.org/abs/2409.11267",
        "author": "Caio Fabio Oliveira da Silva, Azita Dabiri, Bart De Schutter",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2409.11267v2 Announce Type: replace-cross \nAbstract: This work proposes an approach that integrates reinforcement learning and model predictive control (MPC) to solve finite-horizon optimal control problems in mixed-logical dynamical systems efficiently. Optimization-based control of such systems with discrete and continuous decision variables entails the online solution of mixed-integer linear programs, which suffer from the curse of dimensionality. Our approach aims to mitigate this issue by decoupling the decision on the discrete variables from the decision on the continuous variables. In the proposed approach, reinforcement learning determines the discrete decision variables and simplifies the online optimization problem of the MPC controller from a mixed-integer linear program to a linear program, significantly reducing the computational time. A fundamental contribution of this work is the definition of the decoupled Q-function, which plays a crucial role in making the learning problem tractable in a combinatorial action space. We motivate the use of recurrent neural networks to approximate the decoupled Q-function and show how they can be employed in a reinforcement learning setting. Simulation experiments on a microgrid system using real-world data demonstrate that the proposed method substantially reduces the online computation time of MPC while maintaining high feasibility and low suboptimality."
      },
      {
        "id": "oai:arXiv.org:2409.15672v2",
        "title": "Language-based Audio Moment Retrieval",
        "link": "https://arxiv.org/abs/2409.15672",
        "author": "Hokuto Munakata, Taichi Nishimura, Shota Nakada, Tatsuya Komatsu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2409.15672v2 Announce Type: replace-cross \nAbstract: In this paper, we propose and design a new task called audio moment retrieval (AMR). Unlike conventional language-based audio retrieval tasks that search for short audio clips from an audio database, AMR aims to predict relevant moments in untrimmed long audio based on a text query. Given the lack of prior work in AMR, we first build a dedicated dataset, Clotho-Moment, consisting of large-scale simulated audio recordings with moment annotations. We then propose a DETR-based model, named Audio Moment DETR (AM-DETR), as a fundamental framework for AMR tasks. This model captures temporal dependencies within audio features, inspired by similar video moment retrieval tasks, thus surpassing conventional clip-level audio retrieval methods. Additionally, we provide manually annotated datasets to properly measure the effectiveness and robustness of our methods on real data. Experimental results show that AM-DETR, trained with Clotho-Moment, outperforms a baseline model that applies a clip-level audio retrieval method with a sliding window on all metrics, particularly improving Recall1@0.7 by 9.00 points. Our datasets and code are publicly available in https://h-munakata.github.io/Language-based-Audio-Moment-Retrieval."
      },
      {
        "id": "oai:arXiv.org:2409.19777v2",
        "title": "Automatic debiasing of neural networks via moment-constrained learning",
        "link": "https://arxiv.org/abs/2409.19777",
        "author": "Christian L. Hines, Oliver J. Hines",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2409.19777v2 Announce Type: replace-cross \nAbstract: Causal and nonparametric estimands in economics and biostatistics can often be viewed as the mean of a linear functional applied to an unknown outcome regression function. Naively learning the regression function and taking a sample mean of the target functional results in biased estimators, and a rich debiasing literature has developed where one additionally learns the so-called Riesz representer (RR) of the target estimand (targeted learning, double ML, automatic debiasing etc.). Learning the RR via its derived functional form can be challenging, e.g. due to extreme inverse probability weights or the need to learn conditional density functions. Such challenges have motivated recent advances in automatic debiasing (AD), where the RR is learned directly via minimization of a bespoke loss. We propose moment-constrained learning as a new RR learning approach that addresses some shortcomings in AD, constraining the predicted moments and improving the robustness of RR estimates to optimization hyperparamters. Though our approach is not tied to a particular class of learner, we illustrate it using neural networks, and evaluate on the problems of average treatment/derivative effect estimation using semi-synthetic data. Our numerical experiments show improved performance versus state of the art benchmarks."
      },
      {
        "id": "oai:arXiv.org:2410.03974v2",
        "title": "Robust Barycenter Estimation using Semi-Unbalanced Neural Optimal Transport",
        "link": "https://arxiv.org/abs/2410.03974",
        "author": "Milena Gazdieva, Jaemoo Choi, Alexander Kolesov, Jaewoong Choi, Petr Mokrov, Alexander Korotin",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.03974v2 Announce Type: replace-cross \nAbstract: Aggregating data from multiple sources can be formalized as an Optimal Transport (OT) barycenter problem, which seeks to compute the average of probability distributions with respect to OT discrepancies. However, in real-world scenarios, the presence of outliers and noise in the data measures can significantly hinder the performance of traditional statistical methods for estimating OT barycenters. To address this issue, we propose a novel scalable approach for estimating the robust continuous barycenter, leveraging the dual formulation of the (semi-)unbalanced OT problem. To the best of our knowledge, this paper is the first attempt to develop an algorithm for robust barycenters under the continuous distribution setup. Our method is framed as a min-max optimization problem and is adaptable to general cost functions. We rigorously establish the theoretical underpinnings of the proposed method and demonstrate its robustness to outliers and class imbalance through a number of illustrative experiments. Our source code is publicly available at https://github.com/milenagazdieva/U-NOTBarycenters."
      },
      {
        "id": "oai:arXiv.org:2410.08437v3",
        "title": "Autonomous Evaluation of LLMs for Truth Maintenance and Reasoning Tasks",
        "link": "https://arxiv.org/abs/2410.08437",
        "author": "Rushang Karia, Daniel Bramblett, Daksh Dobhal, Siddharth Srivastava",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.08437v3 Announce Type: replace-cross \nAbstract: This paper presents AutoEval, a novel benchmark for scaling Large Language Model (LLM) assessment in formal tasks with clear notions of correctness, such as truth maintenance in translation and logical reasoning. AutoEval is the first benchmarking paradigm that offers several key advantages necessary for scaling objective evaluation of LLMs without human labeling: (a) ability to evaluate LLMs of increasing sophistication by auto-generating tasks at different levels of difficulty; (b) auto-generation of ground truth that eliminates dependence on expensive and time-consuming human annotation; (c) the use of automatically generated, randomized datasets that mitigate the ability of successive LLMs to overfit to static datasets used in many contemporary benchmarks. Empirical analysis shows that an LLM's performance on AutoEval is highly indicative of its performance on a diverse array of other benchmarks focusing on translation and reasoning tasks, making it a valuable autonomous evaluation paradigm in settings where hand-curated datasets can be hard to obtain and/or update."
      },
      {
        "id": "oai:arXiv.org:2410.10212v2",
        "title": "Large Language Model-Enhanced Reinforcement Learning for Generic Bus Holding Control Strategies",
        "link": "https://arxiv.org/abs/2410.10212",
        "author": "Jiajie Yu, Yuhong Wang, Wei Ma",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.10212v2 Announce Type: replace-cross \nAbstract: Bus holding control is a widely-adopted strategy for maintaining stability and improving the operational efficiency of bus systems. Traditional model-based methods often face challenges with the low accuracy of bus state prediction and passenger demand estimation. In contrast, Reinforcement Learning (RL), as a data-driven approach, has demonstrated great potential in formulating bus holding strategies. RL determines the optimal control strategies in order to maximize the cumulative reward, which reflects the overall control goals. However, translating sparse and delayed control goals in real-world tasks into dense and real-time rewards for RL is challenging, normally requiring extensive manual trial-and-error. In view of this, this study introduces an automatic reward generation paradigm by leveraging the in-context learning and reasoning capabilities of Large Language Models (LLMs). This new paradigm, termed the LLM-enhanced RL, comprises several LLM-based modules: reward initializer, reward modifier, performance analyzer, and reward refiner. These modules cooperate to initialize and iteratively improve the reward function according to the feedback from training and test results for the specified RL-based task. Ineffective reward functions generated by the LLM are filtered out to ensure the stable evolution of the RL agents' performance over iterations. To evaluate the feasibility of the proposed LLM-enhanced RL paradigm, it is applied to extensive bus holding control scenarios that vary in the number of bus lines, stops, and passenger demand. The results demonstrate the superiority, generalization capability, and robustness of the proposed paradigm compared to vanilla RL strategies, the LLM-based controller, physics-based feedback controllers, and optimization-based controllers. This study sheds light on the great potential of utilizing LLMs in various smart mobility applications."
      },
      {
        "id": "oai:arXiv.org:2410.19449v3",
        "title": "Learned Reference-based Diffusion Sampling for multi-modal distributions",
        "link": "https://arxiv.org/abs/2410.19449",
        "author": "Maxence Noble, Louis Grenioux, Marylou Gabri\\'e, Alain Oliviero Durmus",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.19449v3 Announce Type: replace-cross \nAbstract: Over the past few years, several approaches utilizing score-based diffusion have been proposed to sample from probability distributions, that is without having access to exact samples and relying solely on evaluations of unnormalized densities. The resulting samplers approximate the time-reversal of a noising diffusion process, bridging the target distribution to an easy-to-sample base distribution. In practice, the performance of these methods heavily depends on key hyperparameters that require ground truth samples to be accurately tuned. Our work aims to highlight and address this fundamental issue, focusing in particular on multi-modal distributions, which pose significant challenges for existing sampling methods. Building on existing approaches, we introduce Learned Reference-based Diffusion Sampler (LRDS), a methodology specifically designed to leverage prior knowledge on the location of the target modes in order to bypass the obstacle of hyperparameter tuning. LRDS proceeds in two steps by (i) learning a reference diffusion model on samples located in high-density space regions and tailored for multimodality, and (ii) using this reference model to foster the training of a diffusion-based sampler. We experimentally demonstrate that LRDS best exploits prior knowledge on the target distribution compared to competing algorithms on a variety of challenging distributions."
      },
      {
        "id": "oai:arXiv.org:2411.00264v2",
        "title": "TurtleBench: A Visual Programming Benchmark in Turtle Geometry",
        "link": "https://arxiv.org/abs/2411.00264",
        "author": "Sina Rismanchian, Yasaman Razeghi, Sameer Singh, Shayan Doroudi",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.00264v2 Announce Type: replace-cross \nAbstract: Humans have the ability to reason about geometric patterns in images and scenes from a young age. However, developing large multimodal models (LMMs) capable of similar reasoning remains a challenge, highlighting the need for robust evaluation methods to assess these capabilities. We introduce \\Turtle, a benchmark designed to evaluate LMMs' capacity to interpret geometric patterns -- given visual examples, textual instructions, or both -- and generate precise code outputs. Inspired by turtle geometry, a notion used to teach children foundational coding and geometric concepts, TurtleBench features tasks with patterned shapes that have underlying algorithmic logic. Our evaluation reveals that leading LMMs struggle significantly with these tasks, with GPT-4o achieving only 19\\% accuracy on the simplest tasks and few-shot prompting only marginally improves their performance ($<2\\%$). \\Turtle highlights the gap between human and AI performance in intuitive and visual geometrical understanding, setting the stage for future research in this area. \\Turtle stands as one of the few benchmarks to evaluate the integration of visual understanding and code generation capabilities in LMMs, setting the stage for future research. Code and Dataset for this paper is provided here: \\href{https://github.com/sinaris76/TurtleBench}{https://github.com/sinaris76/TurtleBench}"
      },
      {
        "id": "oai:arXiv.org:2411.02253v3",
        "title": "Towards safe Bayesian optimization with Wiener kernel regression",
        "link": "https://arxiv.org/abs/2411.02253",
        "author": "Oleksii Molodchyk, Johannes Teutsch, Timm Faulwasser",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.02253v3 Announce Type: replace-cross \nAbstract: Bayesian Optimization (BO) is a data-driven strategy for minimizing/maximizing black-box functions based on probabilistic surrogate models. In the presence of safety constraints, the performance of BO crucially relies on tight probabilistic error bounds related to the uncertainty surrounding the surrogate model. For the case of Gaussian Process surrogates and Gaussian measurement noise, we present a novel error bound based on the recently proposed Wiener kernel regression. We prove that under rather mild assumptions, the proposed error bound is tighter than bounds previously documented in the literature, leading to enlarged safety regions. We draw upon a numerical example to demonstrate the efficacy of the proposed error bound in safe BO."
      },
      {
        "id": "oai:arXiv.org:2411.10614v2",
        "title": "To Shuffle or not to Shuffle: Auditing DP-SGD with Shuffling",
        "link": "https://arxiv.org/abs/2411.10614",
        "author": "Meenatchi Sundaram Muthu Selva Annamalai, Borja Balle, Jamie Hayes, Emiliano De Cristofaro",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.10614v2 Announce Type: replace-cross \nAbstract: The Differentially Private Stochastic Gradient Descent (DP-SGD) algorithm allows the training of machine learning (ML) models with formal Differential Privacy (DP) guarantees. Since DP-SGD processes training data in batches, it employs Poisson sub-sampling to select each batch at every step. However, it has become common practice to replace sub-sampling with shuffling owing to better compatibility and computational overhead. At the same time, we do not know how to compute tight theoretical guarantees for shuffling; thus, DP guarantees of models privately trained with shuffling are often reported as though Poisson sub-sampling was used.\n  This prompts the need to verify whether gaps exist between the theoretical DP guarantees reported by state-of-the-art models and their actual leakage. To do so, we introduce a novel DP auditing procedure to analyze DP-SGD with shuffling and show that DP models trained with this approach have considerably overestimated privacy guarantees (up to 4 times). In the process, we assess the impact on privacy leakage of several parameters, including batch size, privacy budget, and threat model. Finally, we study two common variations of the shuffling procedure that result in even further privacy leakage (up to 10 times). Overall, our work attests to the risk of using shuffling instead of Poisson sub-sampling vis-\\`a-vis privacy leakage from DP-SGD."
      },
      {
        "id": "oai:arXiv.org:2411.11688v2",
        "title": "Conceptwm: A Diffusion Model Watermark for Concept Protection",
        "link": "https://arxiv.org/abs/2411.11688",
        "author": "Liangqi Lei, Keke Gai, Jing Yu, Liehuang Zhu, Qi Wu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.11688v2 Announce Type: replace-cross \nAbstract: The personalization techniques of diffusion models succeed in generating specific concepts but also pose threats to copyright protection and illegal use. Model Watermarking is an effective method to prevent the unauthorized use of subject-driven or style-driven image generation, safeguarding concept copyrights. However, under the goal of concept-oriented protection, current watermarking schemes typically add watermarks to all images rather than applying them in a refined manner targeted at specific concepts. Additionally, the personalization techniques of diffusion models can easily remove watermarks. Existing watermarking methods struggle to achieve fine-grained watermark embedding with a few images of specific concept and prevent removal of watermarks through personalized fine-tuning. Therefore, we introduce a novel concept-oriented watermarking framework that seamlessly embeds imperceptible watermarks into the concept of diffusion models. We introduce Fidelity-preserving Latent Watermarking (FLW) to generate latent watermarks based on image characteristics and the Adversarial Watermarking Modulation module to prevent \"jailbreaking\" via personalized finetuning. To enhance U-Net's efficiency in learning watermark patterns with limited samples, we propose Efficient Concept Watermark Finetuning, which alternates optimization of model parameters for both watermark embedding and concept learning. We conduct extensive experiments and ablation studies to verify our framework. Our code is available at https://anonymous.4open.science/r/Conceptwm-4EB3/."
      },
      {
        "id": "oai:arXiv.org:2411.15669v2",
        "title": "Implicit High-Order Moment Tensor Estimation and Learning Latent Variable Models",
        "link": "https://arxiv.org/abs/2411.15669",
        "author": "Ilias Diakonikolas, Daniel M. Kane",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.15669v2 Announce Type: replace-cross \nAbstract: We study the task of learning latent-variable models. A common algorithmic technique for this task is the method of moments. Unfortunately, moment-based approaches are hampered by the fact that the moment tensors of super-constant degree cannot even be written down in polynomial time. Motivated by such learning applications, we develop a general efficient algorithm for {\\em implicit moment tensor computation}. Our framework generalizes the work of~\\cite{LL21-opt} which developed an efficient algorithm for the specific moment tensors that arise in clustering mixtures of spherical Gaussians.\n  By leveraging our implicit moment estimation algorithm, we obtain the first $\\mathrm{poly}(d, k)$-time learning algorithms for the following models.\n  * {\\bf Mixtures of Linear Regressions} We give a $\\mathrm{poly}(d, k, 1/\\epsilon)$-time algorithm for this task, where $\\epsilon$ is the desired error.\n  * {\\bf Mixtures of Spherical Gaussians} For density estimation, we give a $\\mathrm{poly}(d, k, 1/\\epsilon)$-time learning algorithm, where $\\epsilon$ is the desired total variation error, under the condition that the means lie in a ball of radius $O(\\sqrt{\\log k})$. For parameter estimation, we give a $\\mathrm{poly}(d, k, 1/\\epsilon)$-time algorithm under the {\\em optimal} mean separation of $\\Omega(\\log^{1/2}(k/\\epsilon))$.\n  * {\\bf Positive Linear Combinations of Non-Linear Activations} We give a general algorithm for this task with complexity $\\mathrm{poly}(d, k) g(\\epsilon)$, where $\\epsilon$ is the desired error and the function $g$ depends on the Hermite concentration of the target class of functions. Specifically, for positive linear combinations of ReLU activations, our algorithm has complexity $\\mathrm{poly}(d, k) 2^{\\mathrm{poly}(1/\\epsilon)}$."
      },
      {
        "id": "oai:arXiv.org:2412.00324v2",
        "title": "Table Integration in Data Lakes Unleashed: Pairwise Integrability Judgment, Integrable Set Discovery, and Multi-Tuple Conflict Resolution",
        "link": "https://arxiv.org/abs/2412.00324",
        "author": "Daomin Ji, Hui Luo, Zhifeng Bao, Shane Culpepper",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.00324v2 Announce Type: replace-cross \nAbstract: Table integration aims to create a comprehensive table by consolidating tuples containing relevant information. In this work, we investigate the challenge of integrating multiple tables from a data lake, focusing on three core tasks: 1) pairwise integrability judgment, which determines whether a tuple pair is integrable, accounting for any occurrences of semantic equivalence or typographical errors; 2) integrable set discovery, which identifies all integrable sets in a table based on pairwise integrability judgments established in the first task; 3) multi-tuple conflict resolution, which resolves conflicts between multiple tuples during integration. To this end, we train a binary classifier to address the task of pairwise integrability judgment. Given the scarcity of labeled data in data lakes, we propose a self-supervised adversarial contrastive learning algorithm to perform classification, which incorporates data augmentation methods and adversarial examples to autonomously generate new training data. Upon the output of pairwise integrability judgment, each integrable set can be considered as a community, a densely connected sub-graph where nodes and edges correspond to tuples in the table and their pairwise integrability respectively, we proceed to investigate various community detection algorithms to address the integrable set discovery objective. Moving forward to tackle multi-tuple conflict resolution, we introduce an innovative in-context learning methodology. This approach capitalizes on the knowledge embedded within large language models (LLMs) to effectively resolve conflicts that arise when integrating multiple tuples. Notably, our method minimizes the need for annotated data, making it particularly suited for scenarios where labeled datasets are scarce."
      },
      {
        "id": "oai:arXiv.org:2412.02091v2",
        "title": "The Problem of Social Cost in Multi-Agent General Reinforcement Learning: Survey and Synthesis",
        "link": "https://arxiv.org/abs/2412.02091",
        "author": "Kee Siong Ng, Samuel Yang-Zhao, Timothy Cadogan-Cowper",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.02091v2 Announce Type: replace-cross \nAbstract: The AI safety literature is full of examples of powerful AI agents that, in blindly pursuing a specific and usually narrow objective, ends up with unacceptable and even catastrophic collateral damage to others. In this paper, we consider the problem of social harms that can result from actions taken by learning and utility-maximising agents in a multi-agent environment. The problem of measuring social harms or impacts in such multi-agent settings, especially when the agents are artificial generally intelligent (AGI) agents, was listed as an open problem in Everitt et al, 2018. We attempt a partial answer to that open problem in the form of market-based mechanisms to quantify and control the cost of such social harms. The proposed setup captures many well-studied special cases and is more general than existing formulations of multi-agent reinforcement learning with mechanism design in two ways: (i) the underlying environment is a history-based general reinforcement learning environment like in AIXI; (ii) the reinforcement-learning agents participating in the environment can have different learning strategies and planning horizons. To demonstrate the practicality of the proposed setup, we survey some key classes of learning algorithms and present a few applications, including a discussion of the Paperclips problem and pollution control with a cap-and-trade system."
      },
      {
        "id": "oai:arXiv.org:2412.07236v5",
        "title": "CBraMod: A Criss-Cross Brain Foundation Model for EEG Decoding",
        "link": "https://arxiv.org/abs/2412.07236",
        "author": "Jiquan Wang, Sha Zhao, Zhiling Luo, Yangxuan Zhou, Haiteng Jiang, Shijian Li, Tao Li, Gang Pan",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.07236v5 Announce Type: replace-cross \nAbstract: Electroencephalography (EEG) is a non-invasive technique to measure and record brain electrical activity, widely used in various BCI and healthcare applications. Early EEG decoding methods rely on supervised learning, limited by specific tasks and datasets, hindering model performance and generalizability. With the success of large language models, there is a growing body of studies focusing on EEG foundation models. However, these studies still leave challenges: Firstly, most of existing EEG foundation models employ full EEG modeling strategy. It models the spatial and temporal dependencies between all EEG patches together, but ignores that the spatial and temporal dependencies are heterogeneous due to the unique structural characteristics of EEG signals. Secondly, existing EEG foundation models have limited generalizability on a wide range of downstream BCI tasks due to varying formats of EEG data, making it challenging to adapt to. To address these challenges, we propose a novel foundation model called CBraMod. Specifically, we devise a criss-cross transformer as the backbone to thoroughly leverage the structural characteristics of EEG signals, which can model spatial and temporal dependencies separately through two parallel attention mechanisms. And we utilize an asymmetric conditional positional encoding scheme which can encode positional information of EEG patches and be easily adapted to the EEG with diverse formats. CBraMod is pre-trained on a very large corpus of EEG through patch-based masked EEG reconstruction. We evaluate CBraMod on up to 10 downstream BCI tasks (12 public datasets). CBraMod achieves the state-of-the-art performance across the wide range of tasks, proving its strong capability and generalizability. The source code is publicly available at https://github.com/wjq-learning/CBraMod."
      },
      {
        "id": "oai:arXiv.org:2412.07919v2",
        "title": "Identifying Quantum Mechanical Statistics in Italian Corpora",
        "link": "https://arxiv.org/abs/2412.07919",
        "author": "Diederik Aerts, Jonito Aerts Argu\\\"elles, Lester Beltran, Massimiliano Sassoli de Bianchi, Sandro Sozzo",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.07919v2 Announce Type: replace-cross \nAbstract: We present a theoretical and empirical investigation of the statistical behaviour of the words in a text produced by human language. To this aim, we analyse the word distribution of various texts of Italian language selected from a specific literary corpus. We firstly generalise a theoretical framework elaborated by ourselves to identify 'quantum mechanical statistics' in large-size texts. Then, we show that, in all analysed texts, words distribute according to 'Bose--Einstein statistics' and show significant deviations from 'Maxwell--Boltzmann statistics'. Next, we introduce an effect of 'word randomization' which instead indicates that the difference between the two statistical models is not as pronounced as in the original cases. These results confirm the empirical patterns obtained in texts of English language and strongly indicate that identical words tend to 'clump together' as a consequence of their meaning, which can be explained as an effect of 'quantum entanglement' produced through a phenomenon of 'contextual updating'. More, word randomization can be seen as the linguistic-conceptual equivalent of an increase of temperature which destroys 'coherence' and makes classical statistics prevail over quantum statistics. Some insights into the origin of quantum statistics in physics are finally provided."
      },
      {
        "id": "oai:arXiv.org:2412.10137v3",
        "title": "Constraint-Aware Zero-Shot Vision-Language Navigation in Continuous Environments",
        "link": "https://arxiv.org/abs/2412.10137",
        "author": "Kehan Chen, Dong An, Yan Huang, Rongtao Xu, Yifei Su, Yonggen Ling, Ian Reid, Liang Wang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.10137v3 Announce Type: replace-cross \nAbstract: We address the task of Vision-Language Navigation in Continuous Environments (VLN-CE) under the zero-shot setting. Zero-shot VLN-CE is particularly challenging due to the absence of expert demonstrations for training and minimal environment structural prior to guide navigation. To confront these challenges, we propose a Constraint-Aware Navigator (CA-Nav), which reframes zero-shot VLN-CE as a sequential, constraint-aware sub-instruction completion process. CA-Nav continuously translates sub-instructions into navigation plans using two core modules: the Constraint-Aware Sub-instruction Manager (CSM) and the Constraint-Aware Value Mapper (CVM). CSM defines the completion criteria for decomposed sub-instructions as constraints and tracks navigation progress by switching sub-instructions in a constraint-aware manner. CVM, guided by CSM's constraints, generates a value map on the fly and refines it using superpixel clustering to improve navigation stability. CA-Nav achieves the state-of-the-art performance on two VLN-CE benchmarks, surpassing the previous best method by 12 percent and 13 percent in Success Rate on the validation unseen splits of R2R-CE and RxR-CE, respectively. Moreover, CA-Nav demonstrates its effectiveness in real-world robot deployments across various indoor scenes and instructions."
      },
      {
        "id": "oai:arXiv.org:2412.12807v2",
        "title": "Ask for More Than Bayes Optimal: A Theory of Indecisions for Classification",
        "link": "https://arxiv.org/abs/2412.12807",
        "author": "Mohamed Ndaoud, Peter Radchenko, Bradley Rava",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.12807v2 Announce Type: replace-cross \nAbstract: Selective classification is a powerful tool for automated decision-making in high-risk scenarios, allowing classifiers to make only highly confident decisions while abstaining when uncertainty is too high. Given a target classification accuracy, our goal is to minimize the number of indecisions, which are observations that we do not automate. For problems that are hard, the target accuracy may not be achievable without using indecisions. In contrast, by using indecisions, we are able to control the misclassification rate to any user-specified level, even below the Bayes optimal error rate, while minimizing the frequency of identifying an indecision. We provide a full characterization of the minimax risk in selective classification, proving key continuity and monotonicity properties that enable optimal indecision selection. Our results extend to hypothesis testing, where we control type II error given a fixed type I error, introducing a novel perspective in selective inference. We analyze the impact of estimating the regression function $\\eta$, showing that plug-in classifiers remain consistent and that accuracy-based calibration effectively controls indecision levels. Additionally, we develop finite-sample calibration methods and identify cases where no training data is needed under the Monotone Likelihood Ratio (MLR) property. In the binary Gaussian mixture model, we establish sharp phase transition results, demonstrating that minimal indecisions can yield near-optimal accuracy even with suboptimal class separation. These findings highlight the potential of selective classification to significantly reduce misclassification rates with a relatively small cost in terms of indecisions."
      },
      {
        "id": "oai:arXiv.org:2412.15004v3",
        "title": "From Vulnerabilities to Remediation: A Systematic Literature Review of LLMs in Code Security",
        "link": "https://arxiv.org/abs/2412.15004",
        "author": "Enna Basic, Alberto Giaretta",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.15004v3 Announce Type: replace-cross \nAbstract: Large Language Models (LLMs) have emerged as powerful tools for automating various programming tasks, including security-related ones, such as detecting and fixing vulnerabilities. Despite their promising capabilities, when required to produce or modify pre-existing code, LLMs could introduce vulnerabilities unbeknown to the programmer. When analyzing code, they could miss clear vulnerabilities or signal nonexistent ones. In this Systematic Literature Review (SLR), we aim to investigate both the security benefits and potential drawbacks of using LLMs for a variety of code-related tasks. In particular, first we focus on the types of vulnerabilities that could be introduced by LLMs, when used for producing code. Second, we analyze the capabilities of LLMs to detect and fix vulnerabilities, in any given code, and how the prompting strategy of choice impacts their performance in these two tasks. Last, we provide an in-depth analysis on how data poisoning attacks on LLMs can impact performance in the aforementioned tasks."
      },
      {
        "id": "oai:arXiv.org:2412.18417v2",
        "title": "Ultra-Low Complexity On-Orbit Compression for Remote Sensing Imagery via Block Modulated Imaging",
        "link": "https://arxiv.org/abs/2412.18417",
        "author": "Zhibin Wang, Yanxin Cai, Jiayi Zhou, Yangming Zhang, Tianyu Li, Wei Li, Xun Liu, Guoqing Wang, Yang Yang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.18417v2 Announce Type: replace-cross \nAbstract: The growing field of remote sensing faces a challenge: the ever-increasing size and volume of imagery data are exceeding the storage and transmission capabilities of satellite platforms. Efficient compression of remote sensing imagery is a critical solution to alleviate these burdens on satellites. However, existing compression methods are often too computationally expensive for satellites. With the continued advancement of compressed sensing theory, single-pixel imaging emerges as a powerful tool that brings new possibilities for on-orbit image compression. However, it still suffers from prolonged imaging times and the inability to perform high-resolution imaging, hindering its practical application. This paper advances the study of compressed sensing in remote sensing image compression, proposing Block Modulated Imaging (BMI). By requiring only a single exposure, BMI significantly enhances imaging acquisition speeds. Additionally, BMI obviates the need for digital micromirror devices and surpasses limitations in image resolution. Furthermore, we propose a novel decoding network specifically designed to reconstruct images compressed under the BMI framework. Leveraging the gated 3D convolutions and promoting efficient information flow across stages through a Two-Way Cross-Attention module, our decoding network exhibits demonstrably superior reconstruction performance. Extensive experiments conducted on multiple renowned remote sensing datasets unequivocally demonstrate the efficacy of our proposed method. To further validate its practical applicability, we developed and tested a prototype of the BMI-based camera, which has shown promising potential for on-orbit image compression. The code is available at https://github.com/Johnathan218/BMNet."
      },
      {
        "id": "oai:arXiv.org:2412.19711v2",
        "title": "Causal machine learning for heterogeneous treatment effects in the presence of missing outcome data",
        "link": "https://arxiv.org/abs/2412.19711",
        "author": "Matthew Pryce, Karla Diaz-Ordaz, Ruth H. Keogh, Stijn Vansteelandt",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.19711v2 Announce Type: replace-cross \nAbstract: When estimating heterogeneous treatment effects, missing outcome data can complicate treatment effect estimation, causing certain subgroups of the population to be poorly represented. In this work, we discuss this commonly overlooked problem and consider the impact that missing at random (MAR) outcome data has on causal machine learning estimators for the conditional average treatment effect (CATE). We propose two de-biased machine learning estimators for the CATE, the mDR-learner and mEP-learner, which address the issue of under-representation by integrating inverse probability of censoring weights into the DR-learner and EP-learner respectively. We show that under reasonable conditions, these estimators are oracle efficient, and illustrate their favorable performance through simulated data settings, comparing them to existing CATE estimators, including comparison to estimators which use common missing data techniques. We present an example of their application using the GBSG2 trial, exploring treatment effect heterogeneity when comparing hormonal therapies to non-hormonal therapies among breast cancer patients post surgery, and offer guidance on the decisions a practitioner must make when implementing these estimators."
      },
      {
        "id": "oai:arXiv.org:2501.02406v3",
        "title": "Zero-Shot Statistical Tests for LLM-Generated Text Detection using Finite Sample Concentration Inequalities",
        "link": "https://arxiv.org/abs/2501.02406",
        "author": "Tara Radvand, Mojtaba Abdolmaleki, Mohamed Mostagir, Ambuj Tewari",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.02406v3 Announce Type: replace-cross \nAbstract: Verifying the provenance of content is crucial to the function of many organizations, e.g., educational institutions, social media platforms, firms, etc. This problem is becoming increasingly challenging as text generated by Large Language Models (LLMs) becomes almost indistinguishable from human-generated content. In addition, many institutions utilize in-house LLMs and want to ensure that external, non-sanctioned LLMs do not produce content within the institution. We answer the following question: Given a piece of text, can we identify whether it was produced by LLM $A$ or $B$ (where $B$ can be a human)? We model LLM-generated text as a sequential stochastic process with complete dependence on history and design zero-shot statistical tests to distinguish between (i) the text generated by two different sets of LLMs $A$ (in-house) and $B$ (non-sanctioned) and also (ii) LLM-generated and human-generated texts. We prove that our tests' type I and type II errors decrease exponentially as text length increases. For designing our tests for a given string, we demonstrate that if the string is generated by the evaluator model $A$, the log-perplexity of the string under $A$ converges to the average entropy of the string under $A$, except with an exponentially small probability in the string length. We also show that if $B$ generates the text, except with an exponentially small probability in string length, the log-perplexity of the string under $A$ converges to the average cross-entropy of $B$ and $A$. For our experiments: First, we present experiments using open-source LLMs to support our theoretical results, and then we provide experiments in a black-box setting with adversarial attacks. Practically, our work enables guaranteed finding of the origin of harmful or false LLM-generated text, which can be useful for combating misinformation and compliance with emerging AI regulations."
      },
      {
        "id": "oai:arXiv.org:2501.04870v2",
        "title": "Deep Transfer $Q$-Learning for Offline Non-Stationary Reinforcement Learning",
        "link": "https://arxiv.org/abs/2501.04870",
        "author": "Jinhang Chai, Elynn Chen, Jianqing Fan",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.04870v2 Announce Type: replace-cross \nAbstract: In dynamic decision-making scenarios across business and healthcare, leveraging sample trajectories from diverse populations can significantly enhance reinforcement learning (RL) performance for specific target populations, especially when sample sizes are limited. While existing transfer learning methods primarily focus on linear regression settings, they lack direct applicability to reinforcement learning algorithms. This paper pioneers the study of transfer learning for dynamic decision scenarios modeled by non-stationary finite-horizon Markov decision processes, utilizing neural networks as powerful function approximators and backward inductive learning. We demonstrate that naive sample pooling strategies, effective in regression settings, fail in Markov decision processes.To address this challenge, we introduce a novel ``re-weighted targeting procedure'' to construct ``transferable RL samples'' and propose ``transfer deep $Q^*$-learning'', enabling neural network approximation with theoretical guarantees. We assume that the reward functions are transferable and deal with both situations in which the transition densities are transferable or nontransferable. Our analytical techniques for transfer learning in neural network approximation and transition density transfers have broader implications, extending to supervised transfer learning with neural networks and domain shift scenarios. Empirical experiments on both synthetic and real datasets corroborate the advantages of our method, showcasing its potential for improving decision-making through strategically constructing transferable RL samples in non-stationary reinforcement learning contexts."
      },
      {
        "id": "oai:arXiv.org:2501.06089v2",
        "title": "Towards Developing Socially Compliant Automated Vehicles: Advances, Expert Insights, and A Conceptual Framework",
        "link": "https://arxiv.org/abs/2501.06089",
        "author": "Yongqi Dong, Bart van Arem, Haneen Farah",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.06089v2 Announce Type: replace-cross \nAbstract: Automated Vehicles (AVs) hold promise for revolutionizing transportation by improving road safety, traffic efficiency, and overall mobility. Despite the steady advancement in high-level AVs in recent years, the transition to full automation entails a period of mixed traffic, where AVs of varying automation levels coexist with human-driven vehicles (HDVs). Making AVs socially compliant and understood by human drivers is expected to improve the safety and efficiency of mixed traffic. Thus, ensuring AVs' compatibility with HDVs and social acceptance is crucial for their successful and seamless integration into mixed traffic. However, research in this critical area of developing Socially Compliant AVs (SCAVs) remains sparse. This study carries out the first comprehensive scoping review to assess the current state of the art in developing SCAVs, identifying key concepts, methodological approaches, and research gaps. An informal expert interview was also conducted to discuss the literature review results and identify critical research gaps and expectations towards SCAVs. Based on the scoping review and expert interview input, a conceptual framework is proposed for the development of SCAVs. The conceptual framework is evaluated using an online survey targeting researchers, technicians, policymakers, and other relevant professionals worldwide. The survey results provide valuable validation and insights, affirming the significance of the proposed conceptual framework in tackling the challenges of integrating AVs into mixed-traffic environments. Additionally, future research perspectives and suggestions are discussed, contributing to the research and development agenda of SCAVs."
      },
      {
        "id": "oai:arXiv.org:2501.13878v3",
        "title": "Eye Gaze as a Signal for Conveying User Attention in Contextual AI Systems",
        "link": "https://arxiv.org/abs/2501.13878",
        "author": "Ethan Wilson, Naveen Sendhilnathan, Charlie S. Burlingham, Yusuf Mansour, Robert Cavin, Sai Deep Tetali, Ajoy Savio Fernandes, Michael J. Proulx",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.13878v3 Announce Type: replace-cross \nAbstract: Advanced multimodal AI agents can now collaborate with users to solve challenges in the world. Yet, these emerging contextual AI systems rely on explicit communication channels between the user and system. We hypothesize that implicit communication of the user's interests and intent would reduce friction and improve user experience when collaborating with AI agents. In this work, we explore the potential of wearable eye tracking to convey signals about user attention. We measure the eye tracking signal quality requirements to effectively map gaze traces to physical objects, then conduct experiments that provide visual scanpath history as additional context when querying vision language models. Our results show that eye tracking provides high value as a user attention signal and can convey important context about the user's current task and interests, improving understanding of contextual AI agents."
      },
      {
        "id": "oai:arXiv.org:2502.02332v2",
        "title": "Coreset-Based Task Selection for Sample-Efficient Meta-Reinforcement Learning",
        "link": "https://arxiv.org/abs/2502.02332",
        "author": "Donglin Zhan, Leonardo F. Toso, James Anderson",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.02332v2 Announce Type: replace-cross \nAbstract: We study task selection to enhance sample efficiency in model-agnostic meta-reinforcement learning (MAML-RL). Traditional meta-RL typically assumes that all available tasks are equally important, which can lead to task redundancy when they share significant similarities. To address this, we propose a coreset-based task selection approach that selects a weighted subset of tasks based on how diverse they are in gradient space, prioritizing the most informative and diverse tasks. Such task selection reduces the number of samples needed to find an $\\epsilon$-close stationary solution by a factor of O(1/$\\epsilon$). Consequently, it guarantees a faster adaptation to unseen tasks while focusing training on the most relevant tasks. As a case study, we incorporate task selection to MAML-LQR (Toso et al., 2024b), and prove a sample complexity reduction proportional to O(log(1/$\\epsilon$)) when the task specific cost also satisfy gradient dominance. Our theoretical guarantees underscore task selection as a key component for scalable and sample-efficient meta-RL. We numerically validate this trend across multiple RL benchmark problems, illustrating the benefits of task selection beyond the LQR baseline."
      },
      {
        "id": "oai:arXiv.org:2502.03377v2",
        "title": "Energy-Efficient Flying LoRa Gateways: A Multi-Agent Reinforcement Learning Approach",
        "link": "https://arxiv.org/abs/2502.03377",
        "author": "Abdullahi Isa Ahmed, Jamal Bentahar, El Mehdi Amhoud",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.03377v2 Announce Type: replace-cross \nAbstract: As next-generation Internet of Things (NG-IoT) networks continue to grow, the number of connected devices is rapidly increasing, along with their energy demands. This creates challenges for resource management and sustainability. Energy-efficient communication, particularly for power-limited IoT devices, is therefore a key research focus. In this paper, we deployed flying LoRa gateways mounted on unmanned aerial vehicles (UAVs) to collect data from LoRa end devices and transmit it to a central server. Our primary objective is to maximize the global system energy efficiency of wireless LoRa networks by joint optimization of transmission power, spreading factor, bandwidth, and user association. To solve this challenging problem, we model the problem as a partially observable Markov decision process (POMDP), where each flying LoRa GW acts as a learning agent using a cooperative multi-agent reinforcement learning (MARL). Simulation results demonstrate that our proposed method, based on the multi-agent proximal policy optimization (MAPPO) algorithm, significantly improves the global system EE and surpasses the conventional MARL schemes."
      },
      {
        "id": "oai:arXiv.org:2502.04493v2",
        "title": "LUND-PROBE -- LUND Prostate Radiotherapy Open Benchmarking and Evaluation dataset",
        "link": "https://arxiv.org/abs/2502.04493",
        "author": "Viktor Rogowski, Lars E Olsson, Jonas Scherman, Emilia Persson, Mustafa Kadhim, Sacha af Wetterstedt, Adalsteinn Gunnlaugsson, Martin P. Nilsson, Nandor Vass, Mathieu Moreau, Maria Gebre Medhin, Sven B\\\"ack, Per Munck af Rosensch\\\"old, Silke Engelholm, Christian Jamtheim Gustafsson",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.04493v2 Announce Type: replace-cross \nAbstract: Radiotherapy treatment for prostate cancer relies on computed tomography (CT) and/or magnetic resonance imaging (MRI) for segmentation of target volumes and organs at risk (OARs). Manual segmentation of these volumes is regarded as the gold standard for ground truth in machine learning applications but to acquire such data is tedious and time-consuming. A publicly available clinical dataset is presented, comprising MRI- and synthetic CT (sCT) images, target and OARs segmentations, and radiotherapy dose distributions for 432 prostate cancer patients treated with MRI-guided radiotherapy. An extended dataset with 35 patients is also included, with the addition of deep learning (DL)-generated segmentations, DL segmentation uncertainty maps, and DL segmentations manually adjusted by four radiation oncologists. The publication of these resources aims to aid research within the fields of automated radiotherapy treatment planning, segmentation, inter-observer analyses, and DL model uncertainty investigation. The dataset is hosted on the AIDA Data Hub and offers a free-to-use resource for the scientific community, valuable for the advancement of medical imaging and prostate cancer radiotherapy research."
      },
      {
        "id": "oai:arXiv.org:2502.05776v3",
        "title": "Dynamic Pricing in the Linear Valuation Model using Shape Constraints",
        "link": "https://arxiv.org/abs/2502.05776",
        "author": "Daniele Bracale, Moulinath Banerjee, Yuekai Sun, Kevin Stoll, Salam Turki",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.05776v3 Announce Type: replace-cross \nAbstract: We propose a shape-constrained approach to dynamic pricing for censored data in the linear valuation model eliminating the need for tuning parameters commonly required by existing methods. Previous works have addressed the challenge of unknown market noise distribution $F_0$ using strategies ranging from kernel methods to reinforcement learning algorithms, such as bandit techniques and upper confidence bounds (UCB), under the assumption that $F_0$ satisfies Lipschitz (or stronger) conditions. In contrast, our method relies on isotonic regression under the weaker assumption that $F_0$ is $\\alpha$-H\\\"older continuous for some $\\alpha \\in (0,1]$, for which we derive a regret upper bound. Simulations and experiments with real-world data obtained by Welltower Inc (a major healthcare Real Estate Investment Trust) consistently demonstrate that our method attains lower empirical regret in comparison to several existing methods in the literature while offering the advantage of being tuning-parameter free."
      },
      {
        "id": "oai:arXiv.org:2502.06564v2",
        "title": "Nearly Optimal Robust Covariance and Scatter Matrix Estimation Beyond Gaussians",
        "link": "https://arxiv.org/abs/2502.06564",
        "author": "Gleb Novikov",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.06564v2 Announce Type: replace-cross \nAbstract: We study the problem of computationally efficient robust estimation of the covariance/scatter matrix of elliptical distributions -- that is, affine transformations of spherically symmetric distributions -- under the strong contamination model in the high-dimensional regime $d \\gtrsim 1/\\varepsilon^2$, where $d$ is the dimension and $\\varepsilon$ is the fraction of adversarial corruptions.\n  We propose an algorithm that, under a very mild assumption on the scatter matrix $\\Sigma$, and given a nearly optimal number of samples $n = \\tilde{O}(d^2/\\varepsilon^2)$, computes in polynomial time an estimator $\\hat{\\Sigma}$ such that, with high probability, \\[ \\left\\| \\Sigma^{-1/2} \\hat{\\Sigma} \\Sigma^{-1/2} - Id \\right\\|_{\\text F} \\le O(\\varepsilon \\log(1/\\varepsilon))\\,. \\]\n  As an application of our result, we obtain the first efficiently computable, nearly optimal robust covariance estimators that extend beyond the Gaussian case. Specifically, for elliptical distributions satisfying the Hanson--Wright inequality (such as Gaussians and uniform distributions over ellipsoids), our estimator $\\hat{\\Sigma}$ of the covariance $\\Sigma$ achieves the same error guarantee as in the Gaussian case. Moreover, for elliptical distributions with sub-exponential tails (such as the multivariate Laplace distribution), we construct an estimator $\\hat{\\Sigma}$ satisfying the spectral norm bound \\[ \\left\\| \\Sigma^{-1/2} \\hat{\\Sigma} \\Sigma^{-1/2} - Id \\right\\| \\le O(\\varepsilon \\log(1/\\varepsilon))\\,. \\]\n  Our approach is based on estimating the covariance of the spatial sign of elliptical distributions. The estimation proceeds in several stages, one of which involves a novel spectral covariance filtering algorithm. This algorithm combines covariance filtering techniques with degree-4 sum-of-squares relaxations, and we believe it may be of independent interest for future applications."
      },
      {
        "id": "oai:arXiv.org:2502.12736v2",
        "title": "Cross-Domain Continual Learning for Edge Intelligence in Wireless ISAC Networks",
        "link": "https://arxiv.org/abs/2502.12736",
        "author": "Jingzhi Hu, Xin Li, Zhou Su, Jun Luo",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.12736v2 Announce Type: replace-cross \nAbstract: In wireless networks with integrated sensing and communications (ISAC), edge intelligence (EI) is expected to be developed at edge devices (ED) for sensing user activities based on channel state information (CSI). However, due to the CSI being highly specific to users' characteristics, the CSI-activity relationship is notoriously domain dependent, essentially demanding EI to learn sufficient datasets from various domains in order to gain cross-domain sensing capability. This poses a crucial challenge owing to the EDs' limited resources, for which storing datasets across all domains will be a significant burden. In this paper, we propose the EdgeCL framework, enabling the EI to continually learn-then-discard each incoming dataset, while remaining resilient to catastrophic forgetting. We design a transformer-based discriminator for handling sequences of noisy and nonequispaced CSI samples. Besides, we propose a distilled core-set based knowledge retention method with robustness-enhanced optimization to train the discriminator, preserving its performance for previous domains while preventing future forgetting. Experimental evaluations show that EdgeCL achieves 89% of performance compared to cumulative training while consuming only 3% of its memory, mitigating forgetting by 79%."
      },
      {
        "id": "oai:arXiv.org:2502.17494v5",
        "title": "External Large Foundation Model: How to Efficiently Serve Trillions of Parameters for Online Ads Recommendation",
        "link": "https://arxiv.org/abs/2502.17494",
        "author": "Mingfu Liang, Xi Liu, Rong Jin, Boyang Liu, Qiuling Suo, Qinghai Zhou, Song Zhou, Laming Chen, Hua Zheng, Zhiyuan Li, Shali Jiang, Jiyan Yang, Xiaozhen Xia, Fan Yang, Yasmine Badr, Ellie Wen, Shuyu Xu, Hansey Chen, Zhengyu Zhang, Jade Nie, Chunzhi Yang, Zhichen Zeng, Weilin Zhang, Xingliang Huang, Qianru Li, Shiquan Wang, Evelyn Lyu, Wenjing Lu, Rui Zhang, Wenjun Wang, Jason Rudy, Mengyue Hang, Kai Wang, Yinbin Ma, Shuaiwen Wang, Sihan Zeng, Tongyi Tang, Xiaohan Wei, Longhao Jin, Jamey Zhang, Marcus Chen, Jiayi Zhang, Angie Huang, Chi Zhang, Zhengli Zhao, Jared Yang, Qiang Jin, Xian Chen, Amit Anand Amlesahwaram, Lexi Song, Liang Luo, Yuchen Hao, Nan Xiao, Yavuz Yetim, Luoshang Pan, Gaoxiang Liu, Yuxi Hu, Yuzhen Huang, Jackie Xu, Rich Zhu, Xin Zhang, Yiqun Liu, Hang Yin, Yuxin Chen, Buyun Zhang, Xiaoyi Liu, Xingyuan Wang, Wenguang Mao, Zhijing Li, Zhehui Zhou, Feifan Gu, Qin Huang, Chonglin Sun, Nancy Yu, Shuo Gu, Shupin Mao, Benjamin Au, Jingzheng Qin, Peggy Yao, Jae-Woo Choi, Bin Gao, Ernest Wang, Lei Zhang, Wen-Yen Chen, Ted Lee, Jay Zha, Yi Meng, Alex Gong, Edison Gao, Alireza Vahdatpour, Yiping Han, Yantao Yao, Toshinari Kureha, Shuo Chang, Musharaf Sultan, John Bocharov, Sagar Chordia, Xiaorui Gan, Peng Sun, Rocky Liu, Bo Long, Wenlin Chen, Santanu Kolay, Huayu Li",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.17494v5 Announce Type: replace-cross \nAbstract: Ads recommendation is a prominent service of online advertising systems and has been actively studied. Recent studies indicate that scaling-up and advanced design of the recommendation model can bring significant performance improvement. However, with a larger model scale, such prior studies have a significantly increasing gap from industry as they often neglect two fundamental challenges in industrial-scale applications. First, training and inference budgets are restricted for the model to be served, exceeding which may incur latency and impair user experience. Second, large-volume data arrive in a streaming mode with data distributions dynamically shifting, as new users/ads join and existing users/ads leave the system. We propose the External Large Foundation Model (ExFM) framework to address the overlooked challenges. Specifically, we develop external distillation and a data augmentation system (DAS) to control the computational cost of training/inference while maintaining high performance. We design the teacher in a way like a foundation model (FM) that can serve multiple students as vertical models (VMs) to amortize its building cost. We propose Auxiliary Head and Student Adapter to mitigate the data distribution gap between FM and VMs caused by the streaming data issue. Comprehensive experiments on internal industrial-scale applications and public datasets demonstrate significant performance gain by ExFM."
      },
      {
        "id": "oai:arXiv.org:2502.19390v2",
        "title": "Multi-modal Contrastive Learning for Tumor-specific Missing Modality Synthesis",
        "link": "https://arxiv.org/abs/2502.19390",
        "author": "Minjoo Lim, Bogyeong Kang, Tae-Eui Kam",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.19390v2 Announce Type: replace-cross \nAbstract: Multi-modal magnetic resonance imaging (MRI) is essential for providing complementary information about brain anatomy and pathology, leading to more accurate diagnoses. However, obtaining high-quality multi-modal MRI in a clinical setting is difficult due to factors such as time constraints, high costs, and patient movement artifacts. To overcome this difficulty, there is increasing interest in developing generative models that can synthesize missing target modality images from the available source ones. Therefore, our team, PLAVE, design a generative model for missing MRI that integrates multi-modal contrastive learning with a focus on critical tumor regions. Specifically, we integrate multi-modal contrastive learning, tailored for multiple source modalities, and enhance its effectiveness by selecting features based on entropy during the contrastive learning process. Additionally, our network not only generates the missing target modality images but also predicts segmentation outputs, simultaneously. This approach improves the generator's capability to precisely generate tumor regions, ultimately improving performance in downstream segmentation tasks. By leveraging a combination of contrastive, segmentation, and additional self-representation losses, our model effectively reflects target-specific information and generate high-quality target images. Consequently, our results in the Brain MR Image Synthesis challenge demonstrate that the proposed model excelled in generating the missing modality."
      },
      {
        "id": "oai:arXiv.org:2502.19460v2",
        "title": "Practical Evaluation of Copula-based Survival Metrics: Beyond the Independent Censoring Assumption",
        "link": "https://arxiv.org/abs/2502.19460",
        "author": "Christian Marius Lillelund, Shi-ang Qi, Russell Greiner",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.19460v2 Announce Type: replace-cross \nAbstract: Conventional survival metrics, such as Harrell's concordance index and the Brier Score, rely on the independent censoring assumption for valid inference in the presence of right-censored data. However, when instances are censored for reasons related to the event of interest, this assumption no longer holds, as this kind of dependent censoring biases the marginal survival estimates of popular nonparametric estimators. In this paper, we propose three copula-based metrics to evaluate survival models in the presence of dependent censoring, and design a framework to create realistic, semi-synthetic datasets with dependent censoring to facilitate the evaluation of the metrics. Our empirical analyses in synthetic and semi-synthetic datasets show that our metrics can give error estimates that are closer to the true error, mainly in terms of prediction accuracy."
      },
      {
        "id": "oai:arXiv.org:2503.03736v2",
        "title": "Opportunistic Routing in Wireless Communications via Learnable State-Augmented Policies",
        "link": "https://arxiv.org/abs/2503.03736",
        "author": "Sourajit Das, Kirtan Gopal Panda, Navid NaderiAlizadeh",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.03736v2 Announce Type: replace-cross \nAbstract: This paper addresses the challenge of packet-based information routing in large-scale wireless communication networks. The problem is framed as a constrained statistical learning task, where each network node operates using only local information. Opportunistic routing exploits the broadcast nature of wireless communication to dynamically select optimal forwarding nodes, enabling the information to reach the destination through multiple relay nodes simultaneously. To solve this, we propose a State-Augmentation (SA) based distributed optimization approach aimed at maximizing the total information handled by the source nodes in the network. The problem formulation leverages Graph Neural Networks (GNNs), which perform graph convolutions based on the topological connections between network nodes. Using an unsupervised learning paradigm, we extract routing policies from the GNN architecture, enabling optimal decisions for source nodes across various flows. Numerical experiments demonstrate that the proposed method achieves superior performance when training a GNN-parameterized model, particularly when compared to baseline algorithms. Additionally, applying the method to real-world network topologies and wireless ad-hoc network test beds validates its effectiveness, highlighting the robustness and transferability of GNNs."
      },
      {
        "id": "oai:arXiv.org:2503.07901v2",
        "title": "Intelligent Framework for Human-Robot Collaboration: Dynamic Ergonomics and Adaptive Decision-Making",
        "link": "https://arxiv.org/abs/2503.07901",
        "author": "Francesco Iodice, Elena De Momi, Arash Ajoudani",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.07901v2 Announce Type: replace-cross \nAbstract: The integration of collaborative robots into industrial environments has improved productivity, but has also highlighted significant challenges related to operator safety and ergonomics. This paper proposes an innovative framework that integrates advanced visual perception, continuous ergonomic monitoring, and adaptive Behaviour Tree decision-making to overcome the limitations of traditional methods that typically operate as isolated components. Our approach synthesizes deep learning models, advanced tracking algorithms, and dynamic ergonomic assessments into a modular, scalable, and adaptive system. Experimental validation demonstrates the framework's superiority over existing solutions across multiple dimensions: the visual perception module outperformed previous detection models with 72.4% mAP@50:95; the system achieved high accuracy in recognizing operator intentions (92.5%); it promptly classified ergonomic risks with minimal latency (0.57 seconds); and it dynamically managed robotic interventions with exceptionally responsive decision-making capabilities (0.07 seconds), representing a 56% improvement over benchmark systems. This comprehensive solution provides a robust platform for enhancing human-robot collaboration in industrial environments by prioritizing ergonomic safety, operational efficiency, and real-time adaptability."
      },
      {
        "id": "oai:arXiv.org:2503.08643v2",
        "title": "Rethinking Diffusion Model in High Dimension",
        "link": "https://arxiv.org/abs/2503.08643",
        "author": "Zhenxin Zheng, Zhenjie Zheng",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.08643v2 Announce Type: replace-cross \nAbstract: Curse of Dimensionality is an unavoidable challenge in statistical probability models, yet diffusion models seem to overcome this limitation, achieving impressive results in high-dimensional data generation. Diffusion models assume that they can learn the statistical properties of the underlying probability distribution, enabling sampling from this distribution to generate realistic samples. But is this really how they work? To address this question, this paper conducts a detailed analysis of the objective function and inference methods of diffusion models, leading to several important conclusions that help answer the above question: 1) In high-dimensional sparse scenarios, the target of the objective function fitting degrades from a weighted sum of multiple samples to a single sample. 2) The mainstream inference methods can all be represented within a simple unified framework, without requiring statistical concepts such as Markov chains and SDE, while aligning with the degraded objective function. 3) Guided by this simple framework, more efficient inference methods can be discovered."
      },
      {
        "id": "oai:arXiv.org:2503.08915v2",
        "title": "Reconstruct Anything Model: a lightweight foundation model for computational imaging",
        "link": "https://arxiv.org/abs/2503.08915",
        "author": "Matthieu Terris, Samuel Hurault, Maxime Song, Julian Tachella",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.08915v2 Announce Type: replace-cross \nAbstract: Most existing learning-based methods for solving imaging inverse problems can be roughly divided into two classes: iterative algorithms, such as plug-and-play and diffusion methods, that leverage pretrained denoisers, and unrolled architectures that are trained end-to-end for specific imaging problems. Iterative methods in the first class are computationally costly and often provide suboptimal reconstruction performance, whereas unrolled architectures are generally specific to a single inverse problem and require expensive training. In this work, we propose a novel non-iterative, lightweight architecture that incorporates knowledge about the forward operator (acquisition physics and noise parameters) without relying on unrolling. Our model is trained to solve a wide range of inverse problems beyond denoising, including deblurring, magnetic resonance imaging, computed tomography, inpainting, and super-resolution. The proposed model can be easily adapted to unseen inverse problems or datasets with a few fine-tuning steps (up to a few images) in a self-supervised way, without ground-truth references. Throughout a series of experiments, we demonstrate state-of-the-art performance from medical imaging to low-photon imaging and microscopy."
      },
      {
        "id": "oai:arXiv.org:2503.11562v2",
        "title": "Designing Neural Synthesizers for Low-Latency Interaction",
        "link": "https://arxiv.org/abs/2503.11562",
        "author": "Franco Caspe, Jordie Shier, Mark Sandler, Charalampos Saitis, Andrew McPherson",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.11562v2 Announce Type: replace-cross \nAbstract: Neural Audio Synthesis (NAS) models offer interactive musical control over high-quality, expressive audio generators. While these models can operate in real-time, they often suffer from high latency, making them unsuitable for intimate musical interaction. The impact of architectural choices in deep learning models on audio latency remains largely unexplored in the NAS literature. In this work, we investigate the sources of latency and jitter typically found in interactive NAS models. We then apply this analysis to the task of timbre transfer using RAVE, a convolutional variational autoencoder for audio waveforms introduced by Caillon et al. in 2021. Finally, we present an iterative design approach for optimizing latency. This culminates with a model we call BRAVE (Bravely Realtime Audio Variational autoEncoder), which is low-latency and exhibits better pitch and loudness replication while showing timbre modification capabilities similar to RAVE. We implement it in a specialized inference framework for low-latency, real-time inference and present a proof-of-concept audio plugin compatible with audio signals from musical instruments. We expect the challenges and guidelines described in this document to support NAS researchers in designing models for low-latency inference from the ground up, enriching the landscape of possibilities for musicians."
      },
      {
        "id": "oai:arXiv.org:2503.12768v2",
        "title": "Dynamic-Dark SLAM: RGB-Thermal Cooperative Robot Vision Strategy for Multi-Person Tracking in Both Well-Lit and Low-Light Scenes",
        "link": "https://arxiv.org/abs/2503.12768",
        "author": "Tatsuro Sakai, Kanji Tanaka, Jonathan Tay Yu Liang, Muhammad Adil Luqman, Daiki Iwata",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.12768v2 Announce Type: replace-cross \nAbstract: In robot vision, thermal cameras hold great potential for recognizing humans even in complete darkness. However, their application to multi-person tracking (MPT) has been limited due to data scarcity and the inherent difficulty of distinguishing individuals. In this study, we propose a cooperative MPT system that utilizes co-located RGB and thermal cameras, where pseudo-annotations (bounding boxes and person IDs) are used to train both RGB and thermal trackers. Evaluation experiments demonstrate that the thermal tracker performs robustly in both bright and dark environments. Moreover, the results suggest that a tracker-switching strategy -- guided by a binary brightness classifier -- is more effective for information integration than a tracker-fusion approach. As an application example, we present an image change pattern recognition (ICPR) method, the ``human-as-landmark,'' which combines two key properties: the thermal recognizability of humans in dark environments and the rich landmark characteristics -- appearance, geometry, and semantics -- of static objects (occluders). Whereas conventional SLAM focuses on mapping static landmarks in well-lit environments, the present study takes a first step toward a new Human-Only SLAM paradigm, ``DD-SLAM,'' which aims to map even dynamic landmarks in complete darkness."
      },
      {
        "id": "oai:arXiv.org:2503.12899v2",
        "title": "A Semantic-based Optimization Approach for Repairing LLMs: Case Study on Code Generation",
        "link": "https://arxiv.org/abs/2503.12899",
        "author": "Jian Gu, Aldeida Aleti, Chunyang Chen, Hongyu Zhang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.12899v2 Announce Type: replace-cross \nAbstract: Language Models (LMs) are widely used in software engineering for code generation, but they may produce code with errors. Rather than repairing the generated code, an alternative way is to address the underlying failures of models. LM repair offers a lightweight solution to this challenge: it requires minimal data, reduces computational costs, and reduces the side effects. Unlike retraining, LM repair focuses on applying tailored updates to targeted neurons, making it ideal for scenarios with limited resources, high-performance demands, or strict safety requirements. In this paper, we propose \\ul{S}emantic \\ul{T}argeting for \\ul{A}nalytical \\ul{R}epair (\\textsc{STAR}), a pioneering and novel semantic-based optimization approach for repairing LLMs. \\textsc{STAR} realizes main operations in LM repair methods in an optimization process, including locating ``buggy neurons'', solving ``neuron patches'', and patching ``buggy neurons''. Correspondingly, it computes the deltas of weight matrix as the prior information to guide optimization; and attributes the targeted layers and neurons leveraging statistical insights. The neuron patches are computed with a solid semantic-based analytical formula, which directly bridges the changes to logits with the deltas of neurons, by steering latent representations. Compared to the prior work of LM repair (\\textsc{MINT}) and optimization methods (\\textsc{SGD}), \\textsc{STAR} integrates their strengths while mitigating their limitations. \\textsc{STAR} supports solving multiple failures together, significantly improving the usefulness. Evaluated on three code generation tasks using popular code LMs, \\textsc{STAR} demonstrates superior effectiveness. Additionally, \\textsc{STAR} exhibits better efficiency. In terms of side effects, namely the balance between generalization and specificity, \\textsc{STAR} outperforms prior work by a significant margin."
      },
      {
        "id": "oai:arXiv.org:2503.13116v2",
        "title": "VeriLeaky: Navigating IP Protection vs Utility in Fine-Tuning for LLM-Driven Verilog Coding",
        "link": "https://arxiv.org/abs/2503.13116",
        "author": "Zeng Wang, Minghao Shao, Mohammed Nabeel, Prithwish Basu Roy, Likhitha Mankali, Jitendra Bhandari, Ramesh Karri, Ozgur Sinanoglu, Muhammad Shafique, Johann Knechtel",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.13116v2 Announce Type: replace-cross \nAbstract: Large language models (LLMs) offer significant potential for coding, yet fine-tuning (FT) with curated data is essential for niche languages like Verilog. Using proprietary intellectual property (IP) for FT presents a serious risk, as FT data can be leaked through LLM inference. This leads to a critical dilemma for design houses: seeking to build externally accessible LLMs offering competitive Verilog coding, how can they leverage in-house IP to enhance FT utility while ensuring IP protection?\n  For the first time in the literature, we study this dilemma. Using LLaMA 3.1-8B, we conduct in-house FT on a baseline Verilog dataset (RTLCoder) supplemented with our own in-house IP, which is validated through multiple tape-outs. To rigorously assess IP leakage, we quantify structural similarity (AST/Dolos) and functional equivalence (Synopsys Formality) between generated codes and our in-house IP. We show that our IP can indeed be leaked, confirming the threat. As defense, we evaluate logic locking of Verilog codes (ASSURE). This offers some level of protection, yet reduces the IP's utility for FT and degrades the LLM's performance. Our study shows the need for novel strategies that are both effective and minimally disruptive to FT, an essential effort for enabling design houses to fully utilize their proprietary IP toward LLM-driven Verilog coding."
      },
      {
        "id": "oai:arXiv.org:2503.13572v2",
        "title": "VeriContaminated: Assessing LLM-Driven Verilog Coding for Data Contamination",
        "link": "https://arxiv.org/abs/2503.13572",
        "author": "Zeng Wang, Minghao Shao, Jitendra Bhandari, Likhitha Mankali, Ramesh Karri, Ozgur Sinanoglu, Muhammad Shafique, Johann Knechtel",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.13572v2 Announce Type: replace-cross \nAbstract: Large Language Models (LLMs) have revolutionized code generation, achieving exceptional results on various established benchmarking frameworks. However, concerns about data contamination - where benchmark data inadvertently leaks into pre-training or fine-tuning datasets - raise questions about the validity of these evaluations. While this issue is known, limiting the industrial adoption of LLM-driven software engineering, hardware coding has received little to no attention regarding these risks. For the first time, we analyze state-of-the-art (SOTA) evaluation frameworks for Verilog code generation (VerilogEval and RTLLM), using established methods for contamination detection (CCD and Min-K% Prob). We cover SOTA commercial and open-source LLMs (CodeGen2.5, Minitron 4b, Mistral 7b, phi-4 mini, LLaMA-{1,2,3.1}, GPT-{2,3.5,4o}, Deepseek-Coder, and CodeQwen 1.5), in baseline and fine-tuned models (RTLCoder and Verigen). Our study confirms that data contamination is a critical concern. We explore mitigations and the resulting trade-offs for code quality vs fairness (i.e., reducing contamination toward unbiased benchmarking)."
      },
      {
        "id": "oai:arXiv.org:2503.15638v2",
        "title": "Using machine learning to measure evidence of students' sensemaking in physics courses",
        "link": "https://arxiv.org/abs/2503.15638",
        "author": "Kaitlin Gili, Kyle Heuton, Astha Shah, Michael C. Hughes",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.15638v2 Announce Type: replace-cross \nAbstract: In the education system, problem-solving correctness is often inappropriately conflated with student learning. Advances in both Physics Education Research (PER) and Machine Learning (ML) provide the initial tools to develop a more meaningful and efficient measurement scheme for whether physics students are engaging in sensemaking: a learning process of figuring out the how and why for a particular phenomena. In this work, we contribute such a measurement scheme, which quantifies the evidence of students' physical sensemaking given their written explanations for their solutions to physics problems. We outline how the proposed human annotation scheme can be automated into a deployable ML model using language encoders and shared probabilistic classifiers. The procedure is scalable for a large number of problems and students. We implement three unique language encoders with logistic regression, and provide a deployability analysis on 385 real student explanations from the 2023 Introduction to Physics course at Tufts University. Furthermore, we compute sensemaking scores for all students, and analyze these measurements alongside their corresponding problem-solving accuracies. We find no linear relationship between these two variables, supporting the hypothesis that one is not a reliable proxy for the other. We discuss how sensemaking scores can be used alongside problem-solving accuracies to provide a more nuanced snapshot of student performance in physics class."
      },
      {
        "id": "oai:arXiv.org:2503.21809v2",
        "title": "Enhancing Predictive Accuracy in Tennis: Integrating Fuzzy Logic and CV-GRNN for Dynamic Match Outcome and Player Momentum Analysis",
        "link": "https://arxiv.org/abs/2503.21809",
        "author": "Kechen Li, Jiaming Liu, Zhenyu Wu, Tianbo Ji",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.21809v2 Announce Type: replace-cross \nAbstract: The predictive analysis of match outcomes and player momentum in professional tennis has long been a subject of scholarly debate. In this paper, we introduce a novel approach to game prediction by combining a multi-level fuzzy evaluation model with a CV-GRNN model. We first identify critical statistical indicators via Principal Component Analysis and then develop a two-tier fuzzy model based on the Wimbledon data. In addition, the results of Pearson Correlation Coefficient indicate that the momentum indicators, such as Player Win Streak and Score Difference, have a strong correlation among them, revealing insightful trends among players transitioning between losing and winning streaks. Subsequently, we refine the CV-GRNN model by incorporating 15 statistically significant indicators, resulting in an increase in accuracy to 86.64% and a decrease in MSE by 49.21%. This consequently strengthens the methodological framework for predicting tennis match outcomes, emphasizing its practical utility and potential for adaptation in various athletic contexts."
      },
      {
        "id": "oai:arXiv.org:2503.23600v2",
        "title": "Online Convex Optimization and Integral Quadratic Constraints: A new approach to regret analysis",
        "link": "https://arxiv.org/abs/2503.23600",
        "author": "Fabian Jakob, Andrea Iannelli",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.23600v2 Announce Type: replace-cross \nAbstract: We propose a novel approach for analyzing dynamic regret of first-order constrained online convex optimization algorithms for strongly convex and Lipschitz-smooth objectives. Crucially, we provide a general analysis that is applicable to a wide range of first-order algorithms that can be expressed as an interconnection of a linear dynamical system in feedback with a first-order oracle. By leveraging Integral Quadratic Constraints (IQCs), we derive a semi-definite program which, when feasible, provides a regret guarantee for the online algorithm. For this, the concept of variational IQCs is introduced as the generalization of IQCs to time-varying monotone operators. Our bounds capture the temporal rate of change of the problem in the form of the path length of the time-varying minimizer and the objective function variation. In contrast to standard results in OCO, our results do not require nerither the assumption of gradient boundedness, nor that of a bounded feasible set. Numerical analyses showcase the ability of the approach to capture the dependence of the regret on the function class condition number."
      },
      {
        "id": "oai:arXiv.org:2504.01007v2",
        "title": "Data-Driven Safety Verification using Barrier Certificates and Matrix Zonotopes",
        "link": "https://arxiv.org/abs/2504.01007",
        "author": "Mohammed Adib Oumer, Amr Alanwar, Majid Zamani",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.01007v2 Announce Type: replace-cross \nAbstract: Ensuring safety in cyber-physical systems (CPSs) is a critical challenge, especially when system models are difficult to obtain or cannot be fully trusted due to uncertainty, modeling errors, or environmental disturbances. Traditional model-based approaches rely on precise system dynamics, which may not be available in real-world scenarios. To address this, we propose a data-driven safety verification framework that leverages matrix zonotopes and barrier certificates to verify system safety directly from noisy data. Instead of trusting a single unreliable model, we construct a set of models that capture all possible system dynamics that align with the observed data, ensuring that the true system model is always contained within this set. This model set is compactly represented using matrix zonotopes, enabling efficient computation and propagation of uncertainty. By integrating this representation into a barrier certificate framework, we establish rigorous safety guarantees without requiring an explicit system model. Numerical experiments demonstrate the effectiveness of our approach in verifying safety for dynamical systems with unknown models, showcasing its potential for real-world CPS applications."
      },
      {
        "id": "oai:arXiv.org:2504.03175v2",
        "title": "Mathematical Modeling of Option Pricing with an Extended Black-Scholes Framework",
        "link": "https://arxiv.org/abs/2504.03175",
        "author": "Nikhil Shivakumar Nayak",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03175v2 Announce Type: replace-cross \nAbstract: This study investigates enhancing option pricing by extending the Black-Scholes model to include stochastic volatility and interest rate variability within the Partial Differential Equation (PDE). The PDE is solved using the finite difference method. The extended Black-Scholes model and a machine learning-based LSTM model are developed and evaluated for pricing Google stock options. Both models were backtested using historical market data. While the LSTM model exhibited higher predictive accuracy, the finite difference method demonstrated superior computational efficiency. This work provides insights into model performance under varying market conditions and emphasizes the potential of hybrid approaches for robust financial modeling."
      },
      {
        "id": "oai:arXiv.org:2504.03758v2",
        "title": "Improved visual-information-driven model for crowd simulation and its modular application",
        "link": "https://arxiv.org/abs/2504.03758",
        "author": "Xuanwen Liang, Jiayu Chen, Eric Wai Ming Lee, Wei Xie",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03758v2 Announce Type: replace-cross \nAbstract: Data-driven crowd simulation models offer advantages in enhancing the accuracy and realism of simulations, and improving their generalizability is essential for promoting application. Current data-driven approaches are primarily designed for a single scenario, with very few models validated across more than two scenarios. It is still an open question to develop data-driven crowd simulation models with strong generalizibility. We notice that the key to addressing this challenge lies in effectively and accurately capturing the core common influential features that govern pedestrians' navigation across diverse scenarios. Particularly, we believe that visual information is one of the most dominant influencing features. In light of this, this paper proposes a data-driven model incorporating a refined visual information extraction method and exit cues to enhance generalizability. The proposed model is examined on four common fundamental modules: bottleneck, corridor, corner and T-junction. The evaluation results demonstrate that our model performs excellently across these scenarios, aligning with pedestrian movement in real-world experiments, and significantly outperforms the classical knowledge-driven model. Furthermore, we introduce a modular approach to apply our proposed model in composite scenarios, and the results regarding trajectories and fundamental diagrams indicate that our simulations closely match real-world patterns in the composite scenario. The research outcomes can provide inspiration for the development of data-driven crowd simulation models with high generalizability and advance the application of data-driven approaches.This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible."
      },
      {
        "id": "oai:arXiv.org:2504.04639v2",
        "title": "Ineffectiveness for Search and Undecidability of PCSP Meta-Problems",
        "link": "https://arxiv.org/abs/2504.04639",
        "author": "Alberto Larrauri",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.04639v2 Announce Type: replace-cross \nAbstract: It is an open question whether the search and decision versions of promise CSPs are equivalent. Most known algorithms for PCSPs solve only their \\emph{decision} variant, and it is unknown whether they can be adapted to solve \\emph{search} as well. The main approaches, called BLP, AIP and BLP+AIP, handle a PCSP by finding a solution to a relaxation of some integer program. We prove that rounding those solutions to a proper search certificate can be as hard as any problem in the class TFNP. In other words, these algorithms are ineffective for search. Building on the algebraic approach to PCSPs, we find sufficient conditions that imply ineffectiveness for search. Our tools are tailored to algorithms that are characterized by minions in a suitable way, and can also be used to prove undecidability results for meta-problems. This way, we show that the families of templates solvable via BLP, AIP, and BLP+AIP are undecidable.\n  Using the same techniques we also analyze several algebraic conditions that are known to guarantee the tractability of finite-template CSPs. We prove that several meta-problems related to cyclic polymorphims and WNUs are undecidable for PCSPs. In particular, there is no algorithm deciding whether a finite PCSP template (1) admits cyclic a polymorphism, (2) admits a WNU."
      },
      {
        "id": "oai:arXiv.org:2504.05108v3",
        "title": "Algorithm Discovery With LLMs: Evolutionary Search Meets Reinforcement Learning",
        "link": "https://arxiv.org/abs/2504.05108",
        "author": "Anja Surina, Amin Mansouri, Lars Quaedvlieg, Amal Seddas, Maryna Viazovska, Emmanuel Abbe, Caglar Gulcehre",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.05108v3 Announce Type: replace-cross \nAbstract: Discovering efficient algorithms for solving complex problems has been an outstanding challenge in mathematics and computer science, requiring substantial human expertise over the years. Recent advancements in evolutionary search with large language models (LLMs) have shown promise in accelerating the discovery of algorithms across various domains, particularly in mathematics and optimization. However, existing approaches treat the LLM as a static generator, missing the opportunity to update the model with the signal obtained from evolutionary exploration. In this work, we propose to augment LLM-based evolutionary search by continuously refining the search operator - the LLM - through reinforcement learning (RL) fine-tuning. Our method leverages evolutionary search as an exploration strategy to discover improved algorithms, while RL optimizes the LLM policy based on these discoveries. Our experiments on three combinatorial optimization tasks - bin packing, traveling salesman, and the flatpack problem - show that combining RL and evolutionary search improves discovery efficiency of improved algorithms, showcasing the potential of RL-enhanced evolutionary strategies to assist computer scientists and mathematicians for more efficient algorithm design."
      },
      {
        "id": "oai:arXiv.org:2504.06818v2",
        "title": "Deep Neural Koopman Operator-based Economic Model Predictive Control of Shipboard Carbon Capture System",
        "link": "https://arxiv.org/abs/2504.06818",
        "author": "Minghao Han, Xunyuan Yin",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.06818v2 Announce Type: replace-cross \nAbstract: Shipboard carbon capture is a promising solution to help reduce carbon emissions in international shipping. In this work, we propose a data-driven dynamic modeling and economic predictive control approach within the Koopman framework. This integrated modeling and control approach is used to achieve safe and energy-efficient process operation of shipboard post-combustion carbon capture plants. Specifically, we propose a deep neural Koopman operator modeling approach, based on which a Koopman model with time-varying model parameters is established. This Koopman model predicts the overall economic operational cost and key system outputs, based on accessible partial state measurements. By leveraging this learned model, a constrained economic predictive control scheme is developed. Despite time-varying parameters involved in the formulated model, the formulated optimization problem associated with the economic predictive control design is convex, and it can be solved efficiently during online control implementations. Extensive tests are conducted on a high-fidelity simulation environment for shipboard post-combustion carbon capture processes. Four ship operational conditions are taken into account. The results show that the proposed method significantly improves the overall economic operational performance and carbon capture rate. Additionally, the proposed method guarantees safe operation by ensuring that hard constraints on the system outputs are satisfied."
      },
      {
        "id": "oai:arXiv.org:2504.06923v2",
        "title": "The Importance of Being Discrete: Measuring the Impact of Discretization in End-to-End Differentially Private Synthetic Data",
        "link": "https://arxiv.org/abs/2504.06923",
        "author": "Georgi Ganev, Meenatchi Sundaram Muthu Selva Annamalai, Sofiane Mahiou, Emiliano De Cristofaro",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.06923v2 Announce Type: replace-cross \nAbstract: Differentially Private (DP) generative marginal models are often used in the wild to release synthetic tabular datasets in lieu of sensitive data while providing formal privacy guarantees. These models approximate low-dimensional marginals or query workloads; crucially, they require the training data to be pre-discretized, i.e., continuous values need to first be partitioned into bins. However, as the range of values (or their domain) is often inferred directly from the training data, with the number of bins and bin edges typically defined arbitrarily, this approach can ultimately break end-to-end DP guarantees and may not always yield optimal utility.\n  In this paper, we present an extensive measurement study of four discretization strategies in the context of DP marginal generative models. More precisely, we design DP versions of three discretizers (uniform, quantile, and k-means) and reimplement the PrivTree algorithm. We find that optimizing both the choice of discretizer and bin count can improve utility, on average, by almost 30% across six DP marginal models, compared to the default strategy and number of bins, with PrivTree being the best-performing discretizer in the majority of cases. We demonstrate that, while DP generative models with non-private discretization remain vulnerable to membership inference attacks, applying DP during discretization effectively mitigates this risk. Finally, we propose an optimized approach for automatically selecting the optimal number of bins, achieving high utility while reducing both privacy budget consumption and computational overhead."
      },
      {
        "id": "oai:arXiv.org:2504.07210v2",
        "title": "MESA: Text-Driven Terrain Generation Using Latent Diffusion and Global Copernicus Data",
        "link": "https://arxiv.org/abs/2504.07210",
        "author": "Paul Borne--Pons (Adobe Research, ESA), Mikolaj Czerkawski (Asterisk Labs, ESA), Rosalie Martin (Adobe Research), Romain Rouffet (Adobe Research)",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07210v2 Announce Type: replace-cross \nAbstract: Terrain modeling has traditionally relied on procedural techniques, which often require extensive domain expertise and handcrafted rules. In this paper, we present MESA - a novel data-centric alternative by training a diffusion model on global remote sensing data. This approach leverages large-scale geospatial information to generate high-quality terrain samples from text descriptions, showcasing a flexible and scalable solution for terrain generation. The model's capabilities are demonstrated through extensive experiments, highlighting its ability to generate realistic and diverse terrain landscapes. The dataset produced to support this work, the Major TOM Core-DEM extension dataset, is released openly as a comprehensive resource for global terrain data. The results suggest that data-driven models, trained on remote sensing data, can provide a powerful tool for realistic terrain modeling and generation."
      },
      {
        "id": "oai:arXiv.org:2504.07993v2",
        "title": "Towards Simple Machine Learning Baselines for GNSS RFI Detection",
        "link": "https://arxiv.org/abs/2504.07993",
        "author": "Viktor Ivanov, Richard C. Wilson, Maurizio Scaramuzza",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07993v2 Announce Type: replace-cross \nAbstract: Machine learning research in GNSS radio frequency interference (RFI) detection often lacks a clear empirical justification for the choice of deep learning architectures over simpler machine learning approaches. In this work, we argue for a change in research direction-from developing ever more complex deep learning models to carefully assessing their real-world effectiveness in comparison to interpretable and lightweight machine learning baselines. Our findings reveal that state-of-the-art deep learning models frequently fail to outperform simple, well-engineered machine learning methods in the context of GNSS RFI detection. Leveraging a unique large-scale dataset collected by the Swiss Air Force and Swiss Air-Rescue (Rega), and preprocessed by Swiss Air Navigation Services Ltd. (Skyguide), we demonstrate that a simple baseline model achieves 91\\% accuracy in detecting GNSS RFI, outperforming more complex deep learning counterparts. These results highlight the effectiveness of pragmatic solutions and offer valuable insights to guide future research in this critical application domain."
      },
      {
        "id": "oai:arXiv.org:2504.08178v2",
        "title": "A Piecewise Lyapunov Analysis of Sub-quadratic SGD: Applications to Robust and Quantile Regression",
        "link": "https://arxiv.org/abs/2504.08178",
        "author": "Yixuan Zhang (Lucy),  Dongyan (Lucy),  Huo, Yudong Chen, Qiaomin Xie",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08178v2 Announce Type: replace-cross \nAbstract: Motivated by robust and quantile regression problems, we investigate the stochastic gradient descent (SGD) algorithm for minimizing an objective function $f$ that is locally strongly convex with a sub--quadratic tail. This setting covers many widely used online statistical methods. We introduce a novel piecewise Lyapunov function that enables us to handle functions $f$ with only first-order differentiability, which includes a wide range of popular loss functions such as Huber loss. Leveraging our proposed Lyapunov function, we derive finite-time moment bounds under general diminishing stepsizes, as well as constant stepsizes. We further establish the weak convergence, central limit theorem and bias characterization under constant stepsize, providing the first geometrical convergence result for sub--quadratic SGD. Our results have wide applications, especially in online statistical methods. In particular, we discuss two applications of our results. 1) Online robust regression: We consider a corrupted linear model with sub--exponential covariates and heavy--tailed noise. Our analysis provides convergence rates comparable to those for corrupted models with Gaussian covariates and noise. 2) Online quantile regression: Importantly, our results relax the common assumption in prior work that the conditional density is continuous and provide a more fine-grained analysis for the moment bounds."
      },
      {
        "id": "oai:arXiv.org:2504.08201v2",
        "title": "Neural Encoding and Decoding at Scale",
        "link": "https://arxiv.org/abs/2504.08201",
        "author": "Yizi Zhang, Yanchen Wang, Mehdi Azabou, Alexandre Andre, Zixuan Wang, Hanrui Lyu, The International Brain Laboratory, Eva Dyer, Liam Paninski, Cole Hurwitz",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08201v2 Announce Type: replace-cross \nAbstract: Recent work has demonstrated that large-scale, multi-animal models are powerful tools for characterizing the relationship between neural activity and behavior. Current large-scale approaches, however, focus exclusively on either predicting neural activity from behavior (encoding) or predicting behavior from neural activity (decoding), limiting their ability to capture the bidirectional relationship between neural activity and behavior. To bridge this gap, we introduce a multimodal, multi-task model that enables simultaneous Neural Encoding and Decoding at Scale (NEDS). Central to our approach is a novel multi-task-masking strategy, which alternates between neural, behavioral, within-modality, and cross-modality masking. We pretrain our method on the International Brain Laboratory (IBL) repeated site dataset, which includes recordings from 83 animals performing the same visual decision-making task. In comparison to other large-scale models, we demonstrate that NEDS achieves state-of-the-art performance for both encoding and decoding when pretrained on multi-animal data and then fine-tuned on new animals. Surprisingly, NEDS's learned embeddings exhibit emergent properties: even without explicit training, they are highly predictive of the brain regions in each recording. Altogether, our approach is a step towards a foundation model of the brain that enables seamless translation between neural activity and behavior."
      },
      {
        "id": "oai:arXiv.org:2504.08254v2",
        "title": "Understanding the Impact of Data Domain Extraction on Synthetic Data Privacy",
        "link": "https://arxiv.org/abs/2504.08254",
        "author": "Georgi Ganev, Meenatchi Sundaram Muthu Selva Annamalai, Sofiane Mahiou, Emiliano De Cristofaro",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08254v2 Announce Type: replace-cross \nAbstract: Privacy attacks, particularly membership inference attacks (MIAs), are widely used to assess the privacy of generative models for tabular synthetic data, including those with Differential Privacy (DP) guarantees. These attacks often exploit outliers, which are especially vulnerable due to their position at the boundaries of the data domain (e.g., at the minimum and maximum values). However, the role of data domain extraction in generative models and its impact on privacy attacks have been overlooked. In this paper, we examine three strategies for defining the data domain: assuming it is externally provided (ideally from public data), extracting it directly from the input data, and extracting it with DP mechanisms. While common in popular implementations and libraries, we show that the second approach breaks end-to-end DP guarantees and leaves models vulnerable. While using a provided domain (if representative) is preferable, extracting it with DP can also defend against popular MIAs, even at high privacy budgets."
      },
      {
        "id": "oai:arXiv.org:2504.08525v2",
        "title": "Task Memory Engine (TME): A Structured Memory Framework with Graph-Aware Extensions for Multi-Step LLM Agent Tasks",
        "link": "https://arxiv.org/abs/2504.08525",
        "author": "Ye Ye",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08525v2 Announce Type: replace-cross \nAbstract: Large Language Models (LLMs) are increasingly used as autonomous agents for multi-step tasks. However, most existing frameworks fail to maintain a structured understanding of the task state, often relying on linear prompt concatenation or shallow memory buffers. This leads to brittle performance, frequent hallucinations, and poor long-range coherence. In this work, we propose the Task Memory Engine (TME), a lightweight and structured memory module that tracks task execution using a hierarchical Task Memory Tree (TMT). Each node in the tree corresponds to a task step, storing relevant input, output, status, and sub-task relationships. We introduce a prompt synthesis method that dynamically generates LLM prompts based on the active node path, significantly improving execution consistency and contextual grounding. Through case studies and comparative experiments on multi-step agent tasks, we demonstrate that TME leads to better task completion accuracy and more interpretable behavior with minimal implementation overhead. A reference implementation of the core TME components is available at https://github.com/biubiutomato/TME-Agent, including basic examples and structured memory integration. While the current implementation uses a tree-based structure, TME is designed to be graph-aware, supporting reusable substeps, converging task paths, and shared dependencies. This lays the groundwork for future DAG-based memory architectures."
      },
      {
        "id": "oai:arXiv.org:2504.08619v2",
        "title": "Analyzing 16,193 LLM Papers for Fun and Profits",
        "link": "https://arxiv.org/abs/2504.08619",
        "author": "Zhiqiu Xia, Lang Zhu, Bingzhe Li, Feng Chen, Qiannan Li, Hang Liu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08619v2 Announce Type: replace-cross \nAbstract: Large Language Models (LLMs) are reshaping the landscape of computer science research, driving significant shifts in research priorities across diverse conferences and fields. This study provides a comprehensive analysis of the publication trend of LLM-related papers in 77 top-tier computer science conferences over the past six years (2019-2024). We approach this analysis from four distinct perspectives: (1) We investigate how LLM research is driving topic shifts within major conferences. (2) We adopt a topic modeling approach to identify various areas of LLM-related topic growth and reveal the topics of concern at different conferences. (3) We explore distinct contribution patterns of academic and industrial institutions. (4) We study the influence of national origins on LLM development trajectories. Synthesizing the findings from these diverse analytical angles, we derive ten key insights that illuminate the dynamics and evolution of the LLM research ecosystem."
      },
      {
        "id": "oai:arXiv.org:2504.08696v2",
        "title": "SeaView: Software Engineering Agent Visual Interface for Enhanced Workflow",
        "link": "https://arxiv.org/abs/2504.08696",
        "author": "Timothy Bula, Saurabh Pujar, Luca Buratti, Mihaela Bornea, Avirup Sil",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08696v2 Announce Type: replace-cross \nAbstract: Auto-regressive LLM-based software engineering (SWE) agents, henceforth SWE agents, have made tremendous progress (>60% on SWE-Bench Verified) on real-world coding challenges including GitHub issue resolution. SWE agents use a combination of reasoning, environment interaction and self-reflection to resolve issues thereby generating \"trajectories\". Analysis of SWE agent trajectories is difficult, not only as they exceed LLM sequence length (sometimes, greater than 128k) but also because it involves a relatively prolonged interaction between an LLM and the environment managed by the agent. In case of an agent error, it can be hard to decipher, locate and understand its scope. Similarly, it can be hard to track improvements or regression over multiple runs or experiments. While a lot of research has gone into making these SWE agents reach state-of-the-art, much less focus has been put into creating tools to help analyze and visualize agent output. We propose a novel tool called SeaView: Software Engineering Agent Visual Interface for Enhanced Workflow, with a vision to assist SWE-agent researchers to visualize and inspect their experiments. SeaView's novel mechanisms help compare experimental runs with varying hyper-parameters or LLMs, and quickly get an understanding of LLM or environment related problems. Based on our user study, experienced researchers spend between 10 and 30 minutes to gather the information provided by SeaView, while researchers with little experience can spend between 30 minutes to 1 hour to diagnose their experiment."
      }
    ]
  },
  "https://rss.arxiv.org/rss/cs.SD+eess.AS": {
    "feed": {
      "title": "cs.SD, eess.AS updates on arXiv.org",
      "link": "http://rss.arxiv.org/rss/cs.SD+eess.AS",
      "updated": "Tue, 15 Apr 2025 04:02:02 +0000",
      "published": "Tue, 15 Apr 2025 00:00:00 -0400"
    },
    "entries": [
      {
        "id": "oai:arXiv.org:2504.08907v1",
        "title": "Spatial Audio Processing with Large Language Model on Wearable Devices",
        "link": "https://arxiv.org/abs/2504.08907",
        "author": "Ayushi Mishra, Yang Bai, Priyadarshan Narayanasamy, Nakul Garg, Nirupam Roy",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08907v1 Announce Type: new \nAbstract: Integrating spatial context into large language models (LLMs) has the potential to revolutionize human-computer interaction, particularly in wearable devices. In this work, we present a novel system architecture that incorporates spatial speech understanding into LLMs, enabling contextually aware and adaptive applications for wearable technologies. Our approach leverages microstructure-based spatial sensing to extract precise Direction of Arrival (DoA) information using a monaural microphone. To address the lack of existing dataset for microstructure-assisted speech recordings, we synthetically create a dataset called OmniTalk by using the LibriSpeech dataset. This spatial information is fused with linguistic embeddings from OpenAI's Whisper model, allowing each modality to learn complementary contextual representations. The fused embeddings are aligned with the input space of LLaMA-3.2 3B model and fine-tuned with lightweight adaptation technique LoRA to optimize for on-device processing. SING supports spatially-aware automatic speech recognition (ASR), achieving a mean error of $25.72^\\circ$-a substantial improvement compared to the 88.52$^\\circ$ median error in existing work-with a word error rate (WER) of 5.3. SING also supports soundscaping, for example, inference how many people were talking and their directions, with up to 5 people and a median DoA error of 16$^\\circ$. Our system demonstrates superior performance in spatial speech understanding while addressing the challenges of power efficiency, privacy, and hardware constraints, paving the way for advanced applications in augmented reality, accessibility, and immersive experiences."
      },
      {
        "id": "oai:arXiv.org:2504.08997v1",
        "title": "Beyond Global Metrics: A Fairness Analysis for Interpretable Voice Disorder Detection Systems",
        "link": "https://arxiv.org/abs/2504.08997",
        "author": "Mariel Estevez, Cyntia Bonomi, Dayana Ribas, Alfonso Ortega, Luciana Ferrer",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08997v1 Announce Type: new \nAbstract: We conducted a comprehensive analysis of an Automatic Voice Disorders Detection (AVDD) system using existing voice disorder datasets with available demographic metadata. The study involved analysing system performance across various demographic groups, particularly focusing on gender and age-based cohorts. Performance evaluation was based on multiple metrics, including normalised costs and cross-entropy. We employed calibration techniques trained separately on predefined demographic groups to address group-dependent miscalibration. Analysis revealed significant performance disparities across groups despite strong global metrics. The system showed systematic biases, misclassifying healthy speakers over 55 as having a voice disorder and speakers with disorders aged 14-30 as healthy. Group-specific calibration improved posterior probability quality, reducing overconfidence. For young disordered speakers, low severity scores were identified as contributing to poor system performance. For older speakers, age-related voice characteristics and potential limitations in the pretrained Hubert model used as feature extractor likely affected results. The study demonstrates that global performance metrics are insufficient for evaluating AVDD system performance. Group-specific analysis may unmask problems in system performance which are hidden within global metrics. Further, group-dependent calibration strategies help mitigate biases, resulting in a more reliable indication of system confidence. These findings emphasize the need for demographic-specific evaluation and calibration in voice disorder detection systems, while providing a methodological framework applicable to broader biomedical classification tasks where demographic metadata is available."
      },
      {
        "id": "oai:arXiv.org:2504.09081v1",
        "title": "SIFT-50M: A Large-Scale Multilingual Dataset for Speech Instruction Fine-Tuning",
        "link": "https://arxiv.org/abs/2504.09081",
        "author": "Prabhat Pandey, Rupak Vignesh Swaminathan, K V Vijay Girish, Arunasish Sen, Jian Xie, Grant P. Strimel, Andreas Schwarz",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09081v1 Announce Type: new \nAbstract: We introduce SIFT (Speech Instruction Fine-Tuning), a 50M-example dataset designed for instruction fine-tuning and pre-training of speech-text large language models (LLMs). SIFT-50M is built from publicly available speech corpora, which collectively contain 14K hours of speech, and leverages LLMs along with off-the-shelf expert models. The dataset spans five languages, encompassing a diverse range of speech understanding as well as controllable speech generation instructions. Using SIFT-50M, we train SIFT-LLM, which outperforms existing speech-text LLMs on instruction-following benchmarks while achieving competitive performance on foundational speech tasks. To support further research, we also introduce EvalSIFT, a benchmark dataset specifically designed to evaluate the instruction-following capabilities of speech-text LLMs."
      },
      {
        "id": "oai:arXiv.org:2504.09219v1",
        "title": "Generation of Musical Timbres using a Text-Guided Diffusion Model",
        "link": "https://arxiv.org/abs/2504.09219",
        "author": "Weixuan Yuan, Qadeer Khan, Vladimir Golkov",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09219v1 Announce Type: new \nAbstract: In recent years, text-to-audio systems have achieved remarkable success, enabling the generation of complete audio segments directly from text descriptions. While these systems also facilitate music creation, the element of human creativity and deliberate expression is often limited. In contrast, the present work allows composers, arrangers, and performers to create the basic building blocks for music creation: audio of individual musical notes for use in electronic instruments and DAWs. Through text prompts, the user can specify the timbre characteristics of the audio. We introduce a system that combines a latent diffusion model and multi-modal contrastive learning to generate musical timbres conditioned on text descriptions. By jointly generating the magnitude and phase of the spectrogram, our method eliminates the need for subsequently running a phase retrieval algorithm, as related methods do.\n  Audio examples, source code, and a web app are available at https://wxuanyuan.github.io/Musical-Note-Generation/"
      },
      {
        "id": "oai:arXiv.org:2504.09225v1",
        "title": "AMNet: An Acoustic Model Network for Enhanced Mandarin Speech Synthesis",
        "link": "https://arxiv.org/abs/2504.09225",
        "author": "Yubing Cao, Yinfeng Yu, Yongming Li, Liejun Wang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09225v1 Announce Type: new \nAbstract: This paper presents AMNet, an Acoustic Model Network designed to improve the performance of Mandarin speech synthesis by incorporating phrase structure annotation and local convolution modules. AMNet builds upon the FastSpeech 2 architecture while addressing the challenge of local context modeling, which is crucial for capturing intricate speech features such as pauses, stress, and intonation. By embedding a phrase structure parser into the model and introducing a local convolution module, AMNet enhances the model's sensitivity to local information. Additionally, AMNet decouples tonal characteristics from phonemes, providing explicit guidance for tone modeling, which improves tone accuracy and pronunciation. Experimental results demonstrate that AMNet outperforms baseline models in subjective and objective evaluations. The proposed model achieves superior Mean Opinion Scores (MOS), lower Mel Cepstral Distortion (MCD), and improved fundamental frequency fitting $F0 (R^2)$, confirming its ability to generate high-quality, natural, and expressive Mandarin speech."
      },
      {
        "id": "oai:arXiv.org:2504.09381v1",
        "title": "DiTSE: High-Fidelity Generative Speech Enhancement via Latent Diffusion Transformers",
        "link": "https://arxiv.org/abs/2504.09381",
        "author": "Heitor R. Guimar\\~aes, Jiaqi Su, Rithesh Kumar, Tiago H. Falk, Zeyu Jin",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09381v1 Announce Type: new \nAbstract: Real-world speech recordings suffer from degradations such as background noise and reverberation. Speech enhancement aims to mitigate these issues by generating clean high-fidelity signals. While recent generative approaches for speech enhancement have shown promising results, they still face two major challenges: (1) content hallucination, where plausible phonemes generated differ from the original utterance; and (2) inconsistency, failing to preserve speaker's identity and paralinguistic features from the input speech. In this work, we introduce DiTSE (Diffusion Transformer for Speech Enhancement), which addresses quality issues of degraded speech in full bandwidth. Our approach employs a latent diffusion transformer model together with robust conditioning features, effectively addressing these challenges while remaining computationally efficient. Experimental results from both subjective and objective evaluations demonstrate that DiTSE achieves state-of-the-art audio quality that, for the first time, matches real studio-quality audio from the DAPS dataset. Furthermore, DiTSE significantly improves the preservation of speaker identity and content fidelity, reducing hallucinations across datasets compared to state-of-the-art enhancers. Audio samples are available at: http://hguimaraes.me/DiTSE"
      },
      {
        "id": "oai:arXiv.org:2504.09516v1",
        "title": "FSSUAVL: A Discriminative Framework using Vision Models for Federated Self-Supervised Audio and Image Understanding",
        "link": "https://arxiv.org/abs/2504.09516",
        "author": "Yasar Abbas Ur Rehman, Kin Wai Lau, Yuyang Xie, Ma Lan, JiaJun Shen",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09516v1 Announce Type: new \nAbstract: Recent studies have demonstrated that vision models can effectively learn multimodal audio-image representations when paired. However, the challenge of enabling deep models to learn representations from unpaired modalities remains unresolved. This issue is especially pertinent in scenarios like Federated Learning (FL), where data is often decentralized, heterogeneous, and lacks a reliable guarantee of paired data. Previous attempts tackled this issue through the use of auxiliary pretrained encoders or generative models on local clients, which invariably raise computational cost with increasing number modalities. Unlike these approaches, in this paper, we aim to address the task of unpaired audio and image recognition using \\texttt{FSSUAVL}, a single deep model pretrained in FL with self-supervised contrastive learning (SSL). Instead of aligning the audio and image modalities, \\texttt{FSSUAVL} jointly discriminates them by projecting them into a common embedding space using contrastive SSL. This extends the utility of \\texttt{FSSUAVL} to paired and unpaired audio and image recognition tasks. Our experiments with CNN and ViT demonstrate that \\texttt{FSSUAVL} significantly improves performance across various image- and audio-based downstream tasks compared to using separate deep models for each modality. Additionally, \\texttt{FSSUAVL}'s capacity to learn multimodal feature representations allows for integrating auxiliary information, if available, to enhance recognition accuracy."
      },
      {
        "id": "oai:arXiv.org:2504.09839v1",
        "title": "SafeSpeech: Robust and Universal Voice Protection Against Malicious Speech Synthesis",
        "link": "https://arxiv.org/abs/2504.09839",
        "author": "Zhisheng Zhang, Derui Wang, Qianyi Yang, Pengyang Huang, Junhan Pu, Yuxin Cao, Kai Ye, Jie Hao, Yixian Yang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09839v1 Announce Type: new \nAbstract: Speech synthesis technology has brought great convenience, while the widespread usage of realistic deepfake audio has triggered hazards. Malicious adversaries may unauthorizedly collect victims' speeches and clone a similar voice for illegal exploitation (\\textit{e.g.}, telecom fraud). However, the existing defense methods cannot effectively prevent deepfake exploitation and are vulnerable to robust training techniques. Therefore, a more effective and robust data protection method is urgently needed. In response, we propose a defensive framework, \\textit{\\textbf{SafeSpeech}}, which protects the users' audio before uploading by embedding imperceptible perturbations on original speeches to prevent high-quality synthetic speech. In SafeSpeech, we devise a robust and universal proactive protection technique, \\textbf{S}peech \\textbf{PE}rturbative \\textbf{C}oncealment (\\textbf{SPEC}), that leverages a surrogate model to generate universally applicable perturbation for generative synthetic models. Moreover, we optimize the human perception of embedded perturbation in terms of time and frequency domains. To evaluate our method comprehensively, we conduct extensive experiments across advanced models and datasets, both subjectively and objectively. Our experimental results demonstrate that SafeSpeech achieves state-of-the-art (SOTA) voice protection effectiveness and transferability and is highly robust against advanced adaptive adversaries. Moreover, SafeSpeech has real-time capability in real-world tests. The source code is available at \\href{https://github.com/wxzyd123/SafeSpeech}{https://github.com/wxzyd123/SafeSpeech}."
      },
      {
        "id": "oai:arXiv.org:2504.09885v1",
        "title": "Separate to Collaborate: Dual-Stream Diffusion Model for Coordinated Piano Hand Motion Synthesis",
        "link": "https://arxiv.org/abs/2504.09885",
        "author": "Zihao Liu, Mingwen Ou, Zunnan Xu, Jiaqi Huang, Haonan Han, Ronghui Li, Xiu Li",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09885v1 Announce Type: new \nAbstract: Automating the synthesis of coordinated bimanual piano performances poses significant challenges, particularly in capturing the intricate choreography between the hands while preserving their distinct kinematic signatures. In this paper, we propose a dual-stream neural framework designed to generate synchronized hand gestures for piano playing from audio input, addressing the critical challenge of modeling both hand independence and coordination. Our framework introduces two key innovations: (i) a decoupled diffusion-based generation framework that independently models each hand's motion via dual-noise initialization, sampling distinct latent noise for each while leveraging a shared positional condition, and (ii) a Hand-Coordinated Asymmetric Attention (HCAA) mechanism suppresses symmetric (common-mode) noise to highlight asymmetric hand-specific features, while adaptively enhancing inter-hand coordination during denoising. The system operates hierarchically: it first predicts 3D hand positions from audio features and then generates joint angles through position-aware diffusion models, where parallel denoising streams interact via HCAA. Comprehensive evaluations demonstrate that our framework outperforms existing state-of-the-art methods across multiple metrics."
      },
      {
        "id": "oai:arXiv.org:2504.10309v1",
        "title": "AutoStyle-TTS: Retrieval-Augmented Generation based Automatic Style Matching Text-to-Speech Synthesis",
        "link": "https://arxiv.org/abs/2504.10309",
        "author": "Dan Luo, Chengyuan Ma, Weiqin Li, Jun Wang, Wei Chen, Zhiyong Wu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10309v1 Announce Type: new \nAbstract: With the advancement of speech synthesis technology, users have higher expectations for the naturalness and expressiveness of synthesized speech. But previous research ignores the importance of prompt selection. This study proposes a text-to-speech (TTS) framework based on Retrieval-Augmented Generation (RAG) technology, which can dynamically adjust the speech style according to the text content to achieve more natural and vivid communication effects. We have constructed a speech style knowledge database containing high-quality speech samples in various contexts and developed a style matching scheme. This scheme uses embeddings, extracted by Llama, PER-LLM-Embedder,and Moka, to match with samples in the knowledge database, selecting the most appropriate speech style for synthesis. Furthermore, our empirical research validates the effectiveness of the proposed method. Our demo can be viewed at: https://thuhcsi.github.io/icme2025-AutoStyle-TTS"
      },
      {
        "id": "oai:arXiv.org:2504.10344v1",
        "title": "ALMTokenizer: A Low-bitrate and Semantic-rich Audio Codec Tokenizer for Audio Language Modeling",
        "link": "https://arxiv.org/abs/2504.10344",
        "author": "Dongchao Yang, Songxiang Liu, Haohan Guo, Jiankun Zhao, Yuanyuan Wang, Helin Wang, Zeqian Ju, Xubo Liu, Xueyuan Chen, Xu Tan, Xixin Wu, Helen Meng",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10344v1 Announce Type: new \nAbstract: Recent advancements in audio language models have underscored the pivotal role of audio tokenization, which converts audio signals into discrete tokens, thereby facilitating the application of language model architectures to the audio domain. In this study, we introduce ALMTokenizer, a novel low-bitrate and semantically rich audio codec tokenizer for audio language models. Prior methods, such as Encodec, typically encode individual audio frames into discrete tokens without considering the use of context information across frames. Unlike these methods, we introduce a novel query-based compression strategy to capture holistic information with a set of learnable query tokens by explicitly modeling the context information across frames. This design not only enables the codec model to capture more semantic information but also encodes the audio signal with fewer token sequences. Additionally, to enhance the semantic information in audio codec models, we introduce the following: (1) A masked autoencoder (MAE) loss, (2) Vector quantization based on semantic priors, and (3) An autoregressive (AR) prediction loss. As a result, ALMTokenizer achieves competitive reconstruction performance relative to state-of-the-art approaches while operating at a lower bitrate. Within the same audio language model framework, ALMTokenizer outperforms previous tokenizers in audio understanding and generation tasks."
      },
      {
        "id": "oai:arXiv.org:2504.10352v1",
        "title": "Pseudo-Autoregressive Neural Codec Language Models for Efficient Zero-Shot Text-to-Speech Synthesis",
        "link": "https://arxiv.org/abs/2504.10352",
        "author": "Yifan Yang, Shujie Liu, Jinyu Li, Yuxuan Hu, Haibin Wu, Hui Wang, Jianwei Yu, Lingwei Meng, Haiyang Sun, Yanqing Liu, Yan Lu, Kai Yu, Xie Chen",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10352v1 Announce Type: new \nAbstract: Recent zero-shot text-to-speech (TTS) systems face a common dilemma: autoregressive (AR) models suffer from slow generation and lack duration controllability, while non-autoregressive (NAR) models lack temporal modeling and typically require complex designs. In this paper, we introduce a novel pseudo-autoregressive (PAR) codec language modeling approach that unifies AR and NAR modeling. Combining explicit temporal modeling from AR with parallel generation from NAR, PAR generates dynamic-length spans at fixed time steps. Building on PAR, we propose PALLE, a two-stage TTS system that leverages PAR for initial generation followed by NAR refinement. In the first stage, PAR progressively generates speech tokens along the time dimension, with each step predicting all positions in parallel but only retaining the left-most span. In the second stage, low-confidence tokens are iteratively refined in parallel, leveraging the global contextual information. Experiments demonstrate that PALLE, trained on LibriTTS, outperforms state-of-the-art systems trained on large-scale data, including F5-TTS, E2-TTS, and MaskGCT, on the LibriSpeech test-clean set in terms of speech quality, speaker similarity, and intelligibility, while achieving up to ten times faster inference speed. Audio samples are available at https://anonymous-palle.github.io."
      },
      {
        "id": "oai:arXiv.org:2504.09209v1",
        "title": "EchoMask: Speech-Queried Attention-based Mask Modeling for Holistic Co-Speech Motion Generation",
        "link": "https://arxiv.org/abs/2504.09209",
        "author": "Xiangyue Zhang, Jianfang Li, Jiaxu Zhang, Jianqiang Ren, Liefeng Bo, Zhigang Tu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09209v1 Announce Type: cross \nAbstract: Masked modeling framework has shown promise in co-speech motion generation. However, it struggles to identify semantically significant frames for effective motion masking. In this work, we propose a speech-queried attention-based mask modeling framework for co-speech motion generation. Our key insight is to leverage motion-aligned speech features to guide the masked motion modeling process, selectively masking rhythm-related and semantically expressive motion frames. Specifically, we first propose a motion-audio alignment module (MAM) to construct a latent motion-audio joint space. In this space, both low-level and high-level speech features are projected, enabling motion-aligned speech representation using learnable speech queries. Then, a speech-queried attention mechanism (SQA) is introduced to compute frame-level attention scores through interactions between motion keys and speech queries, guiding selective masking toward motion frames with high attention scores. Finally, the motion-aligned speech features are also injected into the generation network to facilitate co-speech motion generation. Qualitative and quantitative evaluations confirm that our method outperforms existing state-of-the-art approaches, successfully producing high-quality co-speech motion."
      },
      {
        "id": "oai:arXiv.org:2504.09980v1",
        "title": "Turn-taking annotation for quantitative and qualitative analyses of conversation",
        "link": "https://arxiv.org/abs/2504.09980",
        "author": "Anneliese Kelterer, Barbara Schuppler",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09980v1 Announce Type: cross \nAbstract: This paper has two goals. First, we present the turn-taking annotation layers created for 95 minutes of conversational speech of the Graz Corpus of Read and Spontaneous Speech (GRASS), available to the scientific community. Second, we describe the annotation system and the annotation process in more detail, so other researchers may use it for their own conversational data. The annotation system was developed with an interdisciplinary application in mind. It should be based on sequential criteria according to Conversation Analysis, suitable for subsequent phonetic analysis, thus time-aligned annotations were made Praat, and it should be suitable for automatic classification, which required the continuous annotation of speech and a label inventory that is not too large and results in a high inter-rater agreement. Turn-taking was annotated on two layers, Inter-Pausal Units (IPU) and points of potential completion (PCOMP; similar to transition relevance places). We provide a detailed description of the annotation process and of segmentation and labelling criteria. A detailed analysis of inter-rater agreement and common confusions shows that agreement for IPU annotation is near-perfect, that agreement for PCOMP annotations is substantial, and that disagreements often are either partial or can be explained by a different analysis of a sequence which also has merit. The annotation system can be applied to a variety of conversational data for linguistic studies and technological applications, and we hope that the annotations, as well as the annotation system will contribute to a stronger cross-fertilization between these disciplines."
      },
      {
        "id": "oai:arXiv.org:2407.04082v2",
        "title": "DASS: Distilled Audio State Space Models Are Stronger and More Duration-Scalable Learners",
        "link": "https://arxiv.org/abs/2407.04082",
        "author": "Saurabhchand Bhati, Yuan Gong, Leonid Karlinsky, Hilde Kuehne, Rogerio Feris, James Glass",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2407.04082v2 Announce Type: replace \nAbstract: State-space models (SSMs) have emerged as an alternative to Transformers for audio modeling due to their high computational efficiency with long inputs. While recent efforts on Audio SSMs have reported encouraging results, two main limitations remain: First, in 10-second short audio tagging tasks, Audio SSMs still underperform compared to Transformer-based models such as Audio Spectrogram Transformer (AST). Second, although Audio SSMs theoretically support long audio inputs, their actual performance with long audio has not been thoroughly evaluated. To address these limitations, in this paper, 1) We applied knowledge distillation in audio space model training, resulting in a model called Knowledge Distilled Audio SSM (DASS). To the best of our knowledge, it is the first SSM that outperforms the Transformers on AudioSet and achieves an mAP of 48.9; and 2) We designed a new test called Audio Needle In A Haystack (Audio NIAH). We find that DASS, trained with only 10-second audio clips, can retrieve sound events in audio recordings up to 2.5 hours long, while the AST model fails when the input is just 50 seconds, demonstrating SSMs are indeed more duration scalable. Code: https://github.com/Saurabhbhati/DASS, https://huggingface.co/saurabhati/DASS_small_AudioSet_48.9"
      },
      {
        "id": "oai:arXiv.org:2409.08723v2",
        "title": "FLAMO: An Open-Source Library for Frequency-Domain Differentiable Audio Processing",
        "link": "https://arxiv.org/abs/2409.08723",
        "author": "Gloria Dal Santo, Gian Marco De Bortoli, Karolina Prawda, Sebastian J. Schlecht, Vesa V\\\"alim\\\"aki",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2409.08723v2 Announce Type: replace \nAbstract: We present FLAMO, a Frequency-sampling Library for Audio-Module Optimization designed to implement and optimize differentiable linear time-invariant audio systems. The library is open-source and built on the frequency-sampling filter design method, allowing for the creation of differentiable modules that can be used stand-alone or within the computation graph of neural networks, simplifying the development of differentiable audio systems. It includes predefined filtering modules and auxiliary classes for constructing, training, and logging the optimized systems, all accessible through an intuitive interface. Practical application of these modules is demonstrated through two case studies: the optimization of an artificial reverberator and an active acoustics system for improved response coloration."
      },
      {
        "id": "oai:arXiv.org:2409.15672v2",
        "title": "Language-based Audio Moment Retrieval",
        "link": "https://arxiv.org/abs/2409.15672",
        "author": "Hokuto Munakata, Taichi Nishimura, Shota Nakada, Tatsuya Komatsu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2409.15672v2 Announce Type: replace \nAbstract: In this paper, we propose and design a new task called audio moment retrieval (AMR). Unlike conventional language-based audio retrieval tasks that search for short audio clips from an audio database, AMR aims to predict relevant moments in untrimmed long audio based on a text query. Given the lack of prior work in AMR, we first build a dedicated dataset, Clotho-Moment, consisting of large-scale simulated audio recordings with moment annotations. We then propose a DETR-based model, named Audio Moment DETR (AM-DETR), as a fundamental framework for AMR tasks. This model captures temporal dependencies within audio features, inspired by similar video moment retrieval tasks, thus surpassing conventional clip-level audio retrieval methods. Additionally, we provide manually annotated datasets to properly measure the effectiveness and robustness of our methods on real data. Experimental results show that AM-DETR, trained with Clotho-Moment, outperforms a baseline model that applies a clip-level audio retrieval method with a sliding window on all metrics, particularly improving Recall1@0.7 by 9.00 points. Our datasets and code are publicly available in https://h-munakata.github.io/Language-based-Audio-Moment-Retrieval."
      },
      {
        "id": "oai:arXiv.org:2412.08550v2",
        "title": "Sketch2Sound: Controllable Audio Generation via Time-Varying Signals and Sonic Imitations",
        "link": "https://arxiv.org/abs/2412.08550",
        "author": "Hugo Flores Garc\\'ia, Oriol Nieto, Justin Salamon, Bryan Pardo, Prem Seetharaman",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.08550v2 Announce Type: replace \nAbstract: We present Sketch2Sound, a generative audio model capable of creating high-quality sounds from a set of interpretable time-varying control signals: loudness, brightness, and pitch, as well as text prompts. Sketch2Sound can synthesize arbitrary sounds from sonic imitations (i.e.,~a vocal imitation or a reference sound-shape). Sketch2Sound can be implemented on top of any text-to-audio latent diffusion transformer (DiT), and requires only 40k steps of fine-tuning and a single linear layer per control, making it more lightweight than existing methods like ControlNet. To synthesize from sketchlike sonic imitations, we propose applying random median filters to the control signals during training, allowing Sketch2Sound to be prompted using controls with flexible levels of temporal specificity. We show that Sketch2Sound can synthesize sounds that follow the gist of input controls from a vocal imitation while retaining the adherence to an input text prompt and audio quality compared to a text-only baseline. Sketch2Sound allows sound artists to create sounds with the semantic flexibility of text prompts and the expressivity and precision of a sonic gesture or vocal imitation. Sound examples are available at https://hugofloresgarcia.art/sketch2sound/."
      },
      {
        "id": "oai:arXiv.org:2503.11562v2",
        "title": "Designing Neural Synthesizers for Low-Latency Interaction",
        "link": "https://arxiv.org/abs/2503.11562",
        "author": "Franco Caspe, Jordie Shier, Mark Sandler, Charalampos Saitis, Andrew McPherson",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.11562v2 Announce Type: replace \nAbstract: Neural Audio Synthesis (NAS) models offer interactive musical control over high-quality, expressive audio generators. While these models can operate in real-time, they often suffer from high latency, making them unsuitable for intimate musical interaction. The impact of architectural choices in deep learning models on audio latency remains largely unexplored in the NAS literature. In this work, we investigate the sources of latency and jitter typically found in interactive NAS models. We then apply this analysis to the task of timbre transfer using RAVE, a convolutional variational autoencoder for audio waveforms introduced by Caillon et al. in 2021. Finally, we present an iterative design approach for optimizing latency. This culminates with a model we call BRAVE (Bravely Realtime Audio Variational autoEncoder), which is low-latency and exhibits better pitch and loudness replication while showing timbre modification capabilities similar to RAVE. We implement it in a specialized inference framework for low-latency, real-time inference and present a proof-of-concept audio plugin compatible with audio signals from musical instruments. We expect the challenges and guidelines described in this document to support NAS researchers in designing models for low-latency inference from the ground up, enriching the landscape of possibilities for musicians."
      },
      {
        "id": "oai:arXiv.org:2504.04450v2",
        "title": "WaveNet-Volterra Neural Networks for Active Noise Control: A Fully Causal Approach",
        "link": "https://arxiv.org/abs/2504.04450",
        "author": "Lu Bai, Mengtong Li, Siyuan Lian, Kai Chen, Jing Lu",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.04450v2 Announce Type: replace \nAbstract: Active Noise Control (ANC) systems are challenged by nonlinear distortions, which degrade the performance of traditional adaptive filters. While deep learning-based ANC algorithms have emerged to address nonlinearity, existing approaches often overlook critical limitations: (1) end-to-end Deep Neural Network (DNN) models frequently violate causality constraints inherent to real-time ANC applications; (2) many studies compare DNN-based methods against simplified or low-order adaptive filters rather than fully optimized high-order counterparts. In this letter, we propose a causality-preserving time-domain ANC framework that synergizes WaveNet with Volterra Neural Networks (VNNs), explicitly addressing system nonlinearity while ensuring strict causal operation. Unlike prior DNN-based approaches, our method is benchmarked against both state-of-the-art deep learning architectures and rigorously optimized high-order adaptive filters, including Wiener solutions. Simulations demonstrate that the proposed framework achieves superior performance over existing DNN methods and traditional algorithms, revealing that prior claims of DNN superiority stem from incomplete comparisons with suboptimal traditional baselines. Source code is available at https://github.com/Lu-Baihh/WaveNet-VNNs-for-ANC.git."
      },
      {
        "id": "oai:arXiv.org:2310.14778v5",
        "title": "Audio-Visual Speaker Tracking: Progress, Challenges, and Future Directions",
        "link": "https://arxiv.org/abs/2310.14778",
        "author": "Jinzheng Zhao, Yong Xu, Xinyuan Qian, Davide Berghi, Peipei Wu, Meng Cui, Jianyuan Sun, Philip J. B. Jackson, Wenwu Wang",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2310.14778v5 Announce Type: replace-cross \nAbstract: Audio-visual speaker tracking has drawn increasing attention over the past few years due to its academic values and wide applications. Audio and visual modalities can provide complementary information for localization and tracking. With audio and visual information, the Bayesian-based filter and deep learning-based methods can solve the problem of data association, audio-visual fusion and track management. In this paper, we conduct a comprehensive overview of audio-visual speaker tracking. To our knowledge, this is the first extensive survey over the past five years. We introduce the family of Bayesian filters and summarize the methods for obtaining audio-visual measurements. In addition, the existing trackers and their performance on the AV16.3 dataset are summarized. In the past few years, deep learning techniques have thrived, which also boost the development of audio-visual speaker tracking. The influence of deep learning techniques in terms of measurement extraction and state estimation is also discussed. Finally, we discuss the connections between audio-visual speaker tracking and other areas such as speech separation and distributed speaker tracking."
      },
      {
        "id": "oai:arXiv.org:2312.09736v2",
        "title": "HEAR: Hearing Enhanced Audio Response for Video-grounded Dialogue",
        "link": "https://arxiv.org/abs/2312.09736",
        "author": "Sunjae Yoon, Dahyun Kim, Eunseop Yoon, Hee Suk Yoon, Junyeong Kim, Chnag D. Yoo",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2312.09736v2 Announce Type: replace-cross \nAbstract: Video-grounded Dialogue (VGD) aims to answer questions regarding a given multi-modal input comprising video, audio, and dialogue history. Although there have been numerous efforts in developing VGD systems to improve the quality of their responses, existing systems are competent only to incorporate the information in the video and text and tend to struggle in extracting the necessary information from the audio when generating appropriate responses to the question. The VGD system seems to be deaf, and thus, we coin this symptom of current systems' ignoring audio data as a deaf response. To overcome the deaf response problem, Hearing Enhanced Audio Response (HEAR) framework is proposed to perform sensible listening by selectively attending to audio whenever the question requires it. The HEAR framework enhances the accuracy and audibility of VGD systems in a model-agnostic manner. HEAR is validated on VGD datasets (i.e., AVSD@DSTC7 and AVSD@DSTC8) and shows effectiveness with various VGD systems."
      },
      {
        "id": "oai:arXiv.org:2502.03897v3",
        "title": "UniForm: A Unified Multi-Task Diffusion Transformer for Audio-Video Generation",
        "link": "https://arxiv.org/abs/2502.03897",
        "author": "Lei Zhao, Linfeng Feng, Dongxu Ge, Rujin Chen, Fangqiu Yi, Chi Zhang, Xiao-Lei Zhang, Xuelong Li",
        "published": "Tue, 15 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.03897v3 Announce Type: replace-cross \nAbstract: With the rise of diffusion models, audio-video generation has been revolutionized. However, most existing methods rely on separate modules for each modality, with limited exploration of unified generative architectures. In addition, many are confined to a single task and small-scale datasets. To address these limitations, we first propose UniForm, a unified multi-task diffusion transformer that jointly generates audio and visual modalities in a shared latent space. A single diffusion process models both audio and video, capturing the inherent correlations between sound and vision. Second, we introduce task-specific noise schemes and task tokens, enabling a single model to support multiple tasks, including text-to-audio-video, audio-to-video, and video-to-audio generation. Furthermore, by leveraging large language models and a large-scale text-audio-video combined dataset, UniForm achieves greater generative diversity than prior approaches. Extensive experiments show that UniForm achieves the state-of-the-art performance across audio-video generation tasks, producing content that is both well-aligned and close to real-world data distributions. Our demos are available at https://uniform-t2av.github.io/."
      }
    ]
  }
}