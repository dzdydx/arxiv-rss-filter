{
  "https://rss.arxiv.org/rss/cs.CL+cs.CV+cs.MM+cs.LG+cs.SI": {
    "feed": {
      "title": "cs.CL, cs.CV, cs.MM, cs.LG, cs.SI updates on arXiv.org",
      "link": "http://rss.arxiv.org/rss/cs.CL+cs.CV+cs.MM+cs.LG+cs.SI",
      "updated": "Fri, 11 Apr 2025 04:09:59 +0000",
      "published": "Fri, 11 Apr 2025 00:00:00 -0400"
    },
    "entries": [
      {
        "id": "oai:arXiv.org:2504.07100v1",
        "title": "EnDive: A Cross-Dialect Benchmark for Fairness and Performance in Large Language Models",
        "link": "https://arxiv.org/abs/2504.07100",
        "author": "Abhay Gupta, Jacob Cheung, Philip Meng, Shayan Sayyed, Austen Liao, Kevin Zhu, Sean O'Brien",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07100v1 Announce Type: new \nAbstract: The diversity of human language, shaped by social, cultural, and regional influences, presents significant challenges for natural language processing (NLP) systems. Existing benchmarks often overlook intra-language variations, leaving speakers of non-standard dialects underserved. To address this gap, we introduce EnDive (English Diversity), a benchmark that evaluates five widely-used large language models (LLMs) across tasks in language understanding, algorithmic reasoning, mathematics, and logic. Our framework translates Standard American English datasets into five underrepresented dialects using few-shot prompting with verified examples from native speakers, and compare these translations against rule-based methods via fluency assessments, preference tests, and semantic similarity metrics. Human evaluations confirm high translation quality, with average scores of at least 6.02/7 for faithfulness, fluency, and formality. By filtering out near-identical translations, we create a challenging dataset that reveals significant performance disparities - models consistently underperform on dialectal inputs compared to Standard American English. EnDive thus advances dialect-aware NLP by uncovering model biases and promoting more equitable language technologies."
      },
      {
        "id": "oai:arXiv.org:2504.07113v1",
        "title": "How Robust Are Router-LLMs? Analysis of the Fragility of LLM Routing Capabilities",
        "link": "https://arxiv.org/abs/2504.07113",
        "author": "Aly M. Kassem, Bernhard Sch\\\"olkopf, Zhijing Jin",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07113v1 Announce Type: new \nAbstract: Large language model (LLM) routing has emerged as a crucial strategy for balancing computational costs with performance by dynamically assigning queries to the most appropriate model based on query complexity. Despite recent advances showing that preference-data-based routers can outperform traditional methods, current evaluation benchmarks remain limited. They largely focus on general model capabilities while overlooking task-specific behaviors and critical concerns such as privacy, safety, and potential backdoor vulnerabilities introduced through preference data. In response, we propose the DSC benchmark: Diverse, Simple, and Categorized, an evaluation framework that categorizes router performance across a broad spectrum of query types, including coding, translation, mathematics, human instructions, general knowledge, and LLM jailbreaking. Additionally, it integrates privacy and safety assessments to reveal hidden risks. Our experiments on three preference-based routers and two commercial counterparts demonstrate that while these systems improve efficiency, they often make suboptimal, category-driven decisions. For instance, a BERT-based router directs all coding and mathematics queries to the most powerful LLM even when simpler models would suffice, while routing jailbreaking attempts to weaker models, thereby elevating safety risks."
      },
      {
        "id": "oai:arXiv.org:2504.07114v1",
        "title": "ChatBench: From Static Benchmarks to Human-AI Evaluation",
        "link": "https://arxiv.org/abs/2504.07114",
        "author": "Serina Chang, Ashton Anderson, Jake M. Hofman",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07114v1 Announce Type: new \nAbstract: With the rapid adoption of LLM-based chatbots, there is a pressing need to evaluate what humans and LLMs can achieve together. However, standard benchmarks, such as MMLU, measure LLM capabilities in isolation (i.e., \"AI-alone\"). Here, we design and conduct a user study to convert MMLU questions into user-AI conversations, by seeding the user with the question and having them carry out a conversation with the LLM to answer their question. We release ChatBench, a new dataset with AI-alone, user-alone, and user-AI data for 396 questions and two LLMs, including 144K answers and 7,336 user-AI conversations. We find that AI-alone accuracy fails to predict user-AI accuracy, with significant differences across multiple subjects (math, physics, and moral reasoning), and we analyze the user-AI conversations to provide insight into how they diverge from AI-alone benchmarks. Finally, we show that fine-tuning a user simulator on a subset of ChatBench improves its ability to estimate user-AI accuracies, increasing correlation on held-out questions by more than 20 points, creating possibilities for scaling interactive evaluation."
      },
      {
        "id": "oai:arXiv.org:2504.07115v1",
        "title": "EqualizeIR: Mitigating Linguistic Biases in Retrieval Models",
        "link": "https://arxiv.org/abs/2504.07115",
        "author": "Jiali Cheng, Hadi Amiri",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07115v1 Announce Type: new \nAbstract: This study finds that existing information retrieval (IR) models show significant biases based on the linguistic complexity of input queries, performing well on linguistically simpler (or more complex) queries while underperforming on linguistically more complex (or simpler) queries. To address this issue, we propose EqualizeIR, a framework to mitigate linguistic biases in IR models. EqualizeIR uses a linguistically biased weak learner to capture linguistic biases in IR datasets and then trains a robust model by regularizing and refining its predictions using the biased weak learner. This approach effectively prevents the robust model from overfitting to specific linguistic patterns in data. We propose four approaches for developing linguistically-biased models. Extensive experiments on several datasets show that our method reduces performance disparities across linguistically simple and complex queries, while improving overall retrieval performance."
      },
      {
        "id": "oai:arXiv.org:2504.07116v1",
        "title": "CLEAR: Contrasting Textual Feedback with Experts and Amateurs for Reasoning",
        "link": "https://arxiv.org/abs/2504.07116",
        "author": "Andrew Rufail, Daniel Kim, Sean O'Brien, Kevin Zhu",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07116v1 Announce Type: new \nAbstract: We introduce CLEAR (Contrasting Textual Feedback with Experts and Amateurs for Reasoning), a novel approach to language model reasoning that leverages the strengths of a larger (expert) model and smaller (amateur) model. The expert and amateur models each provide feedback on a model's initial output and are contrasted with each other into refined feedback. This feedback is subsequently applied to iteratively improve CLEAR's responses. Our experiments demonstrate that CLEAR outperforms state-of-the-art methods in several challenging reasoning tasks, including story outline improvement (up to 19.6% relative increase in interestingness), constrained generation (up to 18.5% increase in coverage), mathematical reasoning (up to 6.7% improvement in accuracy) and mitigation of toxicity (decrease of up to 22% in toxicity)."
      },
      {
        "id": "oai:arXiv.org:2504.07128v1",
        "title": "DeepSeek-R1 Thoughtology: Let's <think> about LLM Reasoning",
        "link": "https://arxiv.org/abs/2504.07128",
        "author": "Sara Vera Marjanovi\\'c, Arkil Patel, Vaibhav Adlakha, Milad Aghajohari, Parishad BehnamGhader, Mehar Bhatia, Aditi Khandelwal, Austin Kraft, Benno Krojer, Xing Han L\\`u, Nicholas Meade, Dongchan Shin, Amirhossein Kazemnejad, Gaurav Kamath, Marius Mosbach, Karolina Sta\\'nczak, Siva Reddy",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07128v1 Announce Type: new \nAbstract: Large Reasoning Models like DeepSeek-R1 mark a fundamental shift in how LLMs approach complex problems. Instead of directly producing an answer for a given input, DeepSeek-R1 creates detailed multi-step reasoning chains, seemingly \"thinking\" about a problem before providing an answer. This reasoning process is publicly available to the user, creating endless opportunities for studying the reasoning behaviour of the model and opening up the field of Thoughtology. Starting from a taxonomy of DeepSeek-R1's basic building blocks of reasoning, our analyses on DeepSeek-R1 investigate the impact and controllability of thought length, management of long or confusing contexts, cultural and safety concerns, and the status of DeepSeek-R1 vis-\\`a-vis cognitive phenomena, such as human-like language processing and world modelling. Our findings paint a nuanced picture. Notably, we show DeepSeek-R1 has a 'sweet spot' of reasoning, where extra inference time can impair model performance. Furthermore, we find a tendency for DeepSeek-R1 to persistently ruminate on previously explored problem formulations, obstructing further exploration. We also note strong safety vulnerabilities of DeepSeek-R1 compared to its non-reasoning counterpart, which can also compromise safety-aligned LLMs."
      },
      {
        "id": "oai:arXiv.org:2504.07151v1",
        "title": "Deep Sturm--Liouville: From Sample-Based to 1D Regularization with Learnable Orthogonal Basis Functions",
        "link": "https://arxiv.org/abs/2504.07151",
        "author": "David Vigouroux (IRIT, IRIT-ADRIA, UT3), Joseba Dalmau (IRIT, IRIT-ADRIA, UT3), Louis B\\'ethune (IRIT, IRIT-ADRIA, UT3), Victor Boutin",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07151v1 Announce Type: new \nAbstract: Although Artificial Neural Networks (ANNs) have achieved remarkable success across various tasks, they still suffer from limited generalization. We hypothesize that this limitation arises from the traditional sample-based (0--dimensionnal) regularization used in ANNs. To overcome this, we introduce \\textit{Deep Sturm--Liouville} (DSL), a novel function approximator that enables continuous 1D regularization along field lines in the input space by integrating the Sturm--Liouville Theorem (SLT) into the deep learning framework. DSL defines field lines traversing the input space, along which a Sturm--Liouville problem is solved to generate orthogonal basis functions, enforcing implicit regularization thanks to the desirable properties of SLT. These basis functions are linearly combined to construct the DSL approximator. Both the vector field and basis functions are parameterized by neural networks and learned jointly. We demonstrate that the DSL formulation naturally arises when solving a Rank-1 Parabolic Eigenvalue Problem. DSL is trained efficiently using stochastic gradient descent via implicit differentiation. DSL achieves competitive performance and demonstrate improved sample efficiency on diverse multivariate datasets including high-dimensional image datasets such as MNIST and CIFAR-10."
      },
      {
        "id": "oai:arXiv.org:2504.07155v1",
        "title": "Compound Fault Diagnosis for Train Transmission Systems Using Deep Learning with Fourier-enhanced Representation",
        "link": "https://arxiv.org/abs/2504.07155",
        "author": "Jonathan Adam Rico, Nagarajan Raghavan, Senthilnath Jayavelu",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07155v1 Announce Type: new \nAbstract: Fault diagnosis prevents train disruptions by ensuring the stability and reliability of their transmission systems. Data-driven fault diagnosis models have several advantages over traditional methods in terms of dealing with non-linearity, adaptability, scalability, and automation. However, existing data-driven models are trained on separate transmission components and only consider single faults due to the limitations of existing datasets. These models will perform worse in scenarios where components operate with each other at the same time, affecting each component's vibration signals. To address some of these challenges, we propose a frequency domain representation and a 1-dimensional convolutional neural network for compound fault diagnosis and applied it on the PHM Beijing 2024 dataset, which includes 21 sensor channels, 17 single faults, and 42 compound faults from 4 interacting components, that is, motor, gearbox, left axle box, and right axle box. Our proposed model achieved 97.67% and 93.93% accuracies on the test set with 17 single faults and on the test set with 42 compound faults, respectively."
      },
      {
        "id": "oai:arXiv.org:2504.07158v1",
        "title": "Holistic Capability Preservation: Towards Compact Yet Comprehensive Reasoning Models",
        "link": "https://arxiv.org/abs/2504.07158",
        "author": "Ling Team, Caizhi Tang, Chilin Fu, Chunwei Wu, Jia Guo, Jianwen Wang, Jingyu Hu, Liang Jiang, Meng Li, Peng Jiao, Pingping Liu, Shaomian Zheng, Shiwei Liang, Shuaicheng Li, Yalin Zhang, Yingting Wu, Yongkang Liu, Zhenyu Huang",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07158v1 Announce Type: new \nAbstract: This technical report presents Ring-Lite-Distill, a lightweight reasoning model derived from our open-source Mixture-of-Experts (MoE) Large Language Models (LLMs) Ling-Lite. This study demonstrates that through meticulous high-quality data curation and ingenious training paradigms, the compact MoE model Ling-Lite can be further trained to achieve exceptional reasoning capabilities, while maintaining its parameter-efficient architecture with only 2.75 billion activated parameters, establishing an efficient lightweight reasoning architecture. In particular, in constructing this model, we have not merely focused on enhancing advanced reasoning capabilities, exemplified by high-difficulty mathematical problem solving, but rather aimed to develop a reasoning model with more comprehensive competency coverage. Our approach ensures coverage across reasoning tasks of varying difficulty levels while preserving generic capabilities, such as instruction following, tool use, and knowledge retention. We show that, Ring-Lite-Distill's reasoning ability reaches a level comparable to DeepSeek-R1-Distill-Qwen-7B, while its general capabilities significantly surpass those of DeepSeek-R1-Distill-Qwen-7B. The models are accessible at https://huggingface.co/inclusionAI"
      },
      {
        "id": "oai:arXiv.org:2504.07165v1",
        "title": "Perception in Reflection",
        "link": "https://arxiv.org/abs/2504.07165",
        "author": "Yana Wei, Liang Zhao, Kangheng Lin, En Yu, Yuang Peng, Runpei Dong, Jianjian Sun, Haoran Wei, Zheng Ge, Xiangyu Zhang, Vishal M. Patel",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07165v1 Announce Type: new \nAbstract: We present a perception in reflection paradigm designed to transcend the limitations of current large vision-language models (LVLMs), which are expected yet often fail to achieve perfect perception initially. Specifically, we propose Reflective Perception (RePer), a dual-model reflection mechanism that systematically alternates between policy and critic models, enables iterative refinement of visual perception. This framework is powered by Reflective Perceptual Learning (RPL), which reinforces intrinsic reflective capabilities through a methodically constructed visual reflection dataset and reflective unlikelihood training. Comprehensive experimental evaluation demonstrates RePer's quantifiable improvements in image understanding, captioning precision, and hallucination reduction. Notably, RePer achieves strong alignment between model attention patterns and human visual focus, while RPL optimizes fine-grained and free-form preference alignment. These advancements establish perception in reflection as a robust paradigm for future multimodal agents, particularly in tasks requiring complex reasoning and multi-step manipulation."
      },
      {
        "id": "oai:arXiv.org:2504.07170v1",
        "title": "Trustworthy AI Must Account for Intersectionality",
        "link": "https://arxiv.org/abs/2504.07170",
        "author": "Jesse C. Cresswell",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07170v1 Announce Type: new \nAbstract: Trustworthy AI encompasses many aspirational aspects for aligning AI systems with human values, including fairness, privacy, robustness, explainability, and uncertainty quantification. However, efforts to enhance one aspect often introduce unintended trade-offs that negatively impact others, making it challenging to improve all aspects simultaneously. In this position paper, we review notable approaches to these five aspects and systematically consider every pair, detailing the negative interactions that can arise. For example, applying differential privacy to model training can amplify biases in the data, undermining fairness. Drawing on these findings, we take the position that addressing trustworthiness along each axis in isolation is insufficient. Instead, research on Trustworthy AI must account for intersectionality between aspects and adopt a holistic view across all relevant axes at once. To illustrate our perspective, we provide guidance on how researchers can work towards integrated trustworthiness, a case study on how intersectionality applies to the financial industry, and alternative views to our position."
      },
      {
        "id": "oai:arXiv.org:2504.07174v1",
        "title": "HypoEval: Hypothesis-Guided Evaluation for Natural Language Generation",
        "link": "https://arxiv.org/abs/2504.07174",
        "author": "Mingxuan Li, Hanchen Li, Chenhao Tan",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07174v1 Announce Type: new \nAbstract: Large language models (LLMs) have demonstrated great potential for automating the evaluation of natural language generation. Previous frameworks of LLM-as-a-judge fall short in two ways: they either use zero-shot setting without consulting any human input, which leads to low alignment, or fine-tune LLMs on labeled data, which requires a non-trivial number of samples. Moreover, previous methods often provide little reasoning behind automated evaluations. In this paper, we propose HypoEval, Hypothesis-guided Evaluation framework, which first uses a small corpus of human evaluations to generate more detailed rubrics for human judgments and then incorporates a checklist-like approach to combine LLM's assigned scores on each decomposed dimension to acquire overall scores. With only 30 human evaluations, HypoEval achieves state-of-the-art performance in alignment with both human rankings (Spearman correlation) and human scores (Pearson correlation), on average outperforming G-Eval by 11.86% and fine-tuned Llama-3.1-8B-Instruct with at least 3 times more human evaluations by 11.95%. Furthermore, we conduct systematic studies to assess the robustness of HypoEval, highlighting its effectiveness as a reliable and interpretable automated evaluation framework."
      },
      {
        "id": "oai:arXiv.org:2504.07198v1",
        "title": "Face-LLaVA: Facial Expression and Attribute Understanding through Instruction Tuning",
        "link": "https://arxiv.org/abs/2504.07198",
        "author": "Ashutosh Chaubey, Xulang Guan, Mohammad Soleymani",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07198v1 Announce Type: new \nAbstract: The human face plays a central role in social communication, necessitating the use of performant computer vision tools for human-centered applications. We propose Face-LLaVA, a multimodal large language model for face-centered, in-context learning, including facial expression and attribute recognition. Additionally, Face-LLaVA is able to generate natural language descriptions that can be used for reasoning. Leveraging existing visual databases, we first developed FaceInstruct-1M, a face-centered database for instruction tuning MLLMs for face processing. We then developed a novel face-specific visual encoder powered by Face-Region Guided Cross-Attention that integrates face geometry with local visual features. We evaluated the proposed method across nine different datasets and five different face processing tasks, including facial expression recognition, action unit detection, facial attribute detection, age estimation and deepfake detection. Face-LLaVA achieves superior results compared to existing open-source MLLMs and competitive performance compared to commercial solutions. Our model output also receives a higher reasoning rating by GPT under a zero-shot setting across all the tasks. Both our dataset and model wil be released at https://face-llava.github.io to support future advancements in social AI and foundational vision-language research."
      },
      {
        "id": "oai:arXiv.org:2504.07199v1",
        "title": "SemEval-2025 Task 5: LLMs4Subjects -- LLM-based Automated Subject Tagging for a National Technical Library's Open-Access Catalog",
        "link": "https://arxiv.org/abs/2504.07199",
        "author": "Jennifer D'Souza, Sameer Sadruddin, Holger Israel, Mathias Begoin, Diana Slawig",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07199v1 Announce Type: new \nAbstract: We present SemEval-2025 Task 5: LLMs4Subjects, a shared task on automated subject tagging for scientific and technical records in English and German using the GND taxonomy. Participants developed LLM-based systems to recommend top-k subjects, evaluated through quantitative metrics (precision, recall, F1-score) and qualitative assessments by subject specialists. Results highlight the effectiveness of LLM ensembles, synthetic data generation, and multilingual processing, offering insights into applying LLMs for digital library classification."
      },
      {
        "id": "oai:arXiv.org:2504.07228v1",
        "title": "ConceptCarve: Dynamic Realization of Evidence",
        "link": "https://arxiv.org/abs/2504.07228",
        "author": "Eylon Caplan, Dan Goldwasser",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07228v1 Announce Type: new \nAbstract: Finding evidence for human opinion and behavior at scale is a challenging task, often requiring an understanding of sophisticated thought patterns among vast online communities found on social media. For example, studying how gun ownership is related to the perception of Freedom, requires a retrieval system that can operate at scale over social media posts, while dealing with two key challenges: (1) identifying abstract concept instances, (2) which can be instantiated differently across different communities. To address these, we introduce ConceptCarve, an evidence retrieval framework that utilizes traditional retrievers and LLMs to dynamically characterize the search space during retrieval. Our experiments show that ConceptCarve surpasses traditional retrieval systems in finding evidence within a social media community. It also produces an interpretable representation of the evidence for that community, which we use to qualitatively analyze complex thought patterns that manifest differently across the communities."
      },
      {
        "id": "oai:arXiv.org:2504.07229v1",
        "title": "Visual-Aware Speech Recognition for Noisy Scenarios",
        "link": "https://arxiv.org/abs/2504.07229",
        "author": "Lakshmipathi Balaji, Karan Singla",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07229v1 Announce Type: new \nAbstract: Humans have the ability to utilize visual cues, such as lip movements and visual scenes, to enhance auditory perception, particularly in noisy environments. However, current Automatic Speech Recognition (ASR) or Audio-Visual Speech Recognition (AVSR) models often struggle in noisy scenarios. To solve this task, we propose a model that improves transcription by correlating noise sources to visual cues. Unlike works that rely on lip motion and require the speaker's visibility, we exploit broader visual information from the environment. This allows our model to naturally filter speech from noise and improve transcription, much like humans do in noisy scenarios. Our method re-purposes pretrained speech and visual encoders, linking them with multi-headed attention. This approach enables the transcription of speech and the prediction of noise labels in video inputs. We introduce a scalable pipeline to develop audio-visual datasets, where visual cues correlate to noise in the audio. We show significant improvements over existing audio-only models in noisy scenarios. Results also highlight that visual cues play a vital role in improved transcription accuracy."
      },
      {
        "id": "oai:arXiv.org:2504.07240v1",
        "title": "Prototype-Based Continual Learning with Label-free Replay Buffer and Cluster Preservation Loss",
        "link": "https://arxiv.org/abs/2504.07240",
        "author": "Agil Aghasanli, Yi Li, Plamen Angelov",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07240v1 Announce Type: new \nAbstract: Continual learning techniques employ simple replay sample selection processes and use them during subsequent tasks. Typically, they rely on labeled data. In this paper, we depart from this by automatically selecting prototypes stored without labels, preserving cluster structures in the latent space across tasks. By eliminating label dependence in the replay buffer and introducing cluster preservation loss, it is demonstrated that the proposed method can maintain essential information from previously encountered tasks while ensuring adaptation to new tasks. \"Push-away\" and \"pull-toward\" mechanisms over previously learned prototypes are also introduced for class-incremental and domain-incremental scenarios. These mechanisms ensure the retention of previously learned information as well as adaptation to new classes or domain shifts. The proposed method is evaluated on several benchmarks, including SplitCIFAR100, SplitImageNet32, SplitTinyImageNet, and SplitCaltech256 for class-incremental, as well as R-MNIST and CORe50 for domain-incremental setting using pre-extracted DINOv2 features. Experimental results indicate that the label-free replay-based technique outperforms state-of-the-art continual learning methods and, in some cases, even surpasses offline learning. An unsupervised variant of the proposed technique for the class-incremental setting, avoiding labels use even on incoming data, also demonstrated competitive performance, outperforming particular supervised baselines in some cases. These findings underscore the effectiveness of the proposed framework in retaining prior information and facilitating continual adaptation."
      },
      {
        "id": "oai:arXiv.org:2504.07247v1",
        "title": "Resource-efficient Inference with Foundation Model Programs",
        "link": "https://arxiv.org/abs/2504.07247",
        "author": "Lunyiu Nie, Zhimin Ding, Kevin Yu, Marco Cheung, Chris Jermaine, Swarat Chaudhuri",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07247v1 Announce Type: new \nAbstract: The inference-time resource costs of large language and vision models present a growing challenge in production deployments. We propose the use of foundation model programs, i.e., programs that can invoke foundation models with varying resource costs and performance, as an approach to this problem. Specifically, we present a method that translates a task into a program, then learns a policy for resource allocation that, on each input, selects foundation model \"backends\" for each program module. The policy uses smaller, cheaper backends to handle simpler subtasks, while allowing more complex subtasks to leverage larger, more capable models. We evaluate the method on two new \"streaming\" visual question-answering tasks in which a system answers a question on a sequence of inputs, receiving ground-truth feedback after each answer. Compared to monolithic multi-modal models, our implementation achieves up to 98% resource savings with minimal accuracy loss, demonstrating its potential for scalable and resource-efficient multi-modal inference."
      },
      {
        "id": "oai:arXiv.org:2504.07252v1",
        "title": "Few-Shot Adaptation of Grounding DINO for Agricultural Domain",
        "link": "https://arxiv.org/abs/2504.07252",
        "author": "Rajhans Singh, Rafael Bidese Puhl, Kshitiz Dhakal, Sudhir Sornapudi",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07252v1 Announce Type: new \nAbstract: Deep learning models are transforming agricultural applications by enabling automated phenotyping, monitoring, and yield estimation. However, their effectiveness heavily depends on large amounts of annotated training data, which can be labor and time intensive. Recent advances in open-set object detection, particularly with models like Grounding-DINO, offer a potential solution to detect regions of interests based on text prompt input. Initial zero-shot experiments revealed challenges in crafting effective text prompts, especially for complex objects like individual leaves and visually similar classes. To address these limitations, we propose an efficient few-shot adaptation method that simplifies the Grounding-DINO architecture by removing the text encoder module (BERT) and introducing a randomly initialized trainable text embedding. This method achieves superior performance across multiple agricultural datasets, including plant-weed detection, plant counting, insect identification, fruit counting, and remote sensing tasks. Specifically, it demonstrates up to a $\\sim24\\%$ higher mAP than fully fine-tuned YOLO models on agricultural datasets and outperforms previous state-of-the-art methods by $\\sim10\\%$ in remote sensing, under few-shot learning conditions. Our method offers a promising solution for automating annotation and accelerating the development of specialized agricultural AI solutions."
      },
      {
        "id": "oai:arXiv.org:2504.07260v1",
        "title": "Quantifying Epistemic Uncertainty in Absolute Pose Regression",
        "link": "https://arxiv.org/abs/2504.07260",
        "author": "Fereidoon Zangeneh, Amit Dekel, Alessandro Pieropan, Patric Jensfelt",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07260v1 Announce Type: new \nAbstract: Visual relocalization is the task of estimating the camera pose given an image it views. Absolute pose regression offers a solution to this task by training a neural network, directly regressing the camera pose from image features. While an attractive solution in terms of memory and compute efficiency, absolute pose regression's predictions are inaccurate and unreliable outside the training domain. In this work, we propose a novel method for quantifying the epistemic uncertainty of an absolute pose regression model by estimating the likelihood of observations within a variational framework. Beyond providing a measure of confidence in predictions, our approach offers a unified model that also handles observation ambiguities, probabilistically localizing the camera in the presence of repetitive structures. Our method outperforms existing approaches in capturing the relation between uncertainty and prediction error."
      },
      {
        "id": "oai:arXiv.org:2504.07261v1",
        "title": "Adapting to Online Distribution Shifts in Deep Learning: A Black-Box Approach",
        "link": "https://arxiv.org/abs/2504.07261",
        "author": "Dheeraj Baby, Boran Han, Shuai Zhang, Cuixiong Hu, Yuyang Wang, Yu-Xiang Wang",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07261v1 Announce Type: new \nAbstract: We study the well-motivated problem of online distribution shift in which the data arrive in batches and the distribution of each batch can change arbitrarily over time. Since the shifts can be large or small, abrupt or gradual, the length of the relevant historical data to learn from may vary over time, which poses a major challenge in designing algorithms that can automatically adapt to the best ``attention span'' while remaining computationally efficient. We propose a meta-algorithm that takes any network architecture and any Online Learner (OL) algorithm as input and produces a new algorithm which provably enhances the performance of the given OL under non-stationarity. Our algorithm is efficient (it requires maintaining only $O(\\log(T))$ OL instances) and adaptive (it automatically chooses OL instances with the ideal ``attention'' length at every timestamp). Experiments on various real-world datasets across text and image modalities show that our method consistently improves the accuracy of user specified OL algorithms for classification tasks. Key novel algorithmic ingredients include a \\emph{multi-resolution instance} design inspired by wavelet theory and a cross-validation-through-time technique. Both could be of independent interest."
      },
      {
        "id": "oai:arXiv.org:2504.07274v1",
        "title": "Language Modeling for the Future of Finance: A Quantitative Survey into Metrics, Tasks, and Data Opportunities",
        "link": "https://arxiv.org/abs/2504.07274",
        "author": "Nikita Tatarinov, Siddhant Sukhani, Agam Shah, Sudheer Chava",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07274v1 Announce Type: new \nAbstract: Recent advances in language modeling have led to growing interest in applying Natural Language Processing (NLP) techniques to financial problems, enabling new approaches to analysis and decision-making. To systematically examine this trend, we review 374 NLP research papers published between 2017 and 2024 across 38 conferences and workshops, with a focused analysis of 221 papers that directly address finance-related tasks. We evaluate these papers across 11 qualitative and quantitative dimensions, identifying key trends such as the increasing use of general-purpose language models, steady progress in sentiment analysis and information extraction, and emerging efforts around explainability and privacy-preserving methods. We also discuss the use of evaluation metrics, highlighting the importance of domain-specific ones to complement standard machine learning metrics. Our findings emphasize the need for more accessible, adaptive datasets and highlight the significance of incorporating financial crisis periods to strengthen model robustness under real-world conditions. This survey provides a structured overview of NLP research applied to finance and offers practical insights for researchers and practitioners working at this intersection."
      },
      {
        "id": "oai:arXiv.org:2504.07278v1",
        "title": "A Multi-Phase Analysis of Blood Culture Stewardship: Machine Learning Prediction, Expert Recommendation Assessment, and LLM Automation",
        "link": "https://arxiv.org/abs/2504.07278",
        "author": "Fatemeh Amrollahi, Nicholas Marshall, Fateme Nateghi Haredasht, Kameron C Black, Aydin Zahedivash, Manoj V Maddali, Stephen P. Ma, Amy Chang, MD Phar Stanley C Deresinski, Mary Kane Goldstein, Steven M. Asch, Niaz Banaei, Jonathan H Chen",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07278v1 Announce Type: new \nAbstract: Blood cultures are often over ordered without clear justification, straining healthcare resources and contributing to inappropriate antibiotic use pressures worsened by the global shortage. In study of 135483 emergency department (ED) blood culture orders, we developed machine learning (ML) models to predict the risk of bacteremia using structured electronic health record (EHR) data and provider notes via a large language model (LLM). The structured models AUC improved from 0.76 to 0.79 with note embeddings and reached 0.81 with added diagnosis codes. Compared to an expert recommendation framework applied by human reviewers and an LLM-based pipeline, our ML approach offered higher specificity without compromising sensitivity. The recommendation framework achieved sensitivity 86%, specificity 57%, while the LLM maintained high sensitivity (96%) but over classified negatives, reducing specificity (16%). These findings demonstrate that ML models integrating structured and unstructured data can outperform consensus recommendations, enhancing diagnostic stewardship beyond existing standards of care."
      },
      {
        "id": "oai:arXiv.org:2504.07282v1",
        "title": "RAISE: Reinforenced Adaptive Instruction Selection For Large Language Models",
        "link": "https://arxiv.org/abs/2504.07282",
        "author": "Lv Qingsong, Yangning Li, Zihua Lan, Zishan Xu, Jiwei Tang, Yinghui Li, Wenhao Jiang, Hai-Tao Zheng, Philip S. Yu",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07282v1 Announce Type: new \nAbstract: In the instruction fine-tuning of large language models (LLMs), it has become a consensus that a few high-quality instructions are superior to a large number of low-quality instructions. At present, many instruction selection methods have been proposed, but most of these methods select instruction based on heuristic quality metrics, and only consider data selection before training. These designs lead to insufficient optimization of instruction fine-tuning, and fixed heuristic indicators are often difficult to optimize for specific tasks. So we designed a dynamic, task-objective-driven instruction selection framework RAISE(Reinforenced Adaptive Instruction SElection), which incorporates the entire instruction fine-tuning process into optimization, selecting instruction at each step based on the expected impact of instruction on model performance improvement. Our approach is well interpretable and has strong task-specific optimization capabilities. By modeling dynamic instruction selection as a sequential decision-making process, we use RL to train our selection strategy. Extensive experiments and result analysis prove the superiority of our method compared with other instruction selection methods. Notably, RAISE achieves superior performance by updating only 1\\% of the training steps compared to full-data training, demonstrating its efficiency and effectiveness."
      },
      {
        "id": "oai:arXiv.org:2504.07288v1",
        "title": "MDIT: A Model-free Data Interpolation Method for Diverse Instruction Tuning",
        "link": "https://arxiv.org/abs/2504.07288",
        "author": "Yangning Li, Zihua Lan, Lv Qingsong, Yinghui Li, Hai-Tao Zheng",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07288v1 Announce Type: new \nAbstract: As Large Language Models (LLMs) are increasingly applied across various tasks, instruction tuning has emerged as a critical method for enhancing model performance. However, current data management strategies face substantial challenges in generating diverse and comprehensive data, restricting further improvements in model performance. To address this gap, we propose MDIT, a novel model-free data interpolation method for diverse instruction tuning, which generates varied and high-quality instruction data by performing task interpolation. Moreover, it contains diversity-based clustering strategies to ensure the diversity of the training data. Extensive experiments show that our method achieves superior performance in multiple benchmark tasks. The LLMs finetuned with MDIT show significant improvements in numerous tasks such as general question answering, math reasoning, and code generation. MDIT offers an efficient and automatic data synthetic method, generating diverse instruction data without depending on external resources while expanding the application potential of LLMs in complex environments."
      },
      {
        "id": "oai:arXiv.org:2504.07297v1",
        "title": "Data Fusion of Deep Learned Molecular Embeddings for Property Prediction",
        "link": "https://arxiv.org/abs/2504.07297",
        "author": "Robert J Appleton, Brian C Barnes, Alejandro Strachan",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07297v1 Announce Type: new \nAbstract: Data-driven approaches such as deep learning can result in predictive models for material properties with exceptional accuracy and efficiency. However, in many problems data is sparse, severely limiting their accuracy and applicability. To improve predictions, techniques such as transfer learning and multi-task learning have been used. The performance of multi-task learning models depends on the strength of the underlying correlations between tasks and the completeness of the dataset. We find that standard multi-task models tend to underperform when trained on sparse datasets with weakly correlated properties. To address this gap, we use data fusion techniques to combine the learned molecular embeddings of various single-task models and trained a multi-task model on this combined embedding. We apply this technique to a widely used benchmark dataset of quantum chemistry data for small molecules as well as a newly compiled sparse dataset of experimental data collected from literature and our own quantum chemistry and thermochemical calculations. The results show that the fused, multi-task models outperform standard multi-task models for sparse datasets and can provide enhanced prediction on data-limited properties compared to single-task models."
      },
      {
        "id": "oai:arXiv.org:2504.07301v1",
        "title": "CEC-MMR: Cross-Entropy Clustering Approach to Multi-Modal Regression",
        "link": "https://arxiv.org/abs/2504.07301",
        "author": "Krzysztof Byrski, Jacek Tabor, Przemys{\\l}aw Spurek, Marcin Mazur",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07301v1 Announce Type: new \nAbstract: In practical applications of regression analysis, it is not uncommon to encounter a multitude of values for each attribute. In such a situation, the univariate distribution, which is typically Gaussian, is suboptimal because the mean may be situated between modes, resulting in a predicted value that differs significantly from the actual data. Consequently, to address this issue, a mixture distribution with parameters learned by a neural network, known as a Mixture Density Network (MDN), is typically employed. However, this approach has an important inherent limitation, in that it is not feasible to ascertain the precise number of components with a reasonable degree of accuracy. In this paper, we introduce CEC-MMR, a novel approach based on Cross-Entropy Clustering (CEC), which allows for the automatic detection of the number of components in a regression problem. Furthermore, given an attribute and its value, our method is capable of uniquely identifying it with the underlying component. The experimental results demonstrate that CEC-MMR yields superior outcomes compared to classical MDNs."
      },
      {
        "id": "oai:arXiv.org:2504.07304v1",
        "title": "PAYADOR: A Minimalist Approach to Grounding Language Models on Structured Data for Interactive Storytelling and Role-playing Games",
        "link": "https://arxiv.org/abs/2504.07304",
        "author": "Santiago G\\'ongora, Luis Chiruzzo, Gonzalo M\\'endez, Pablo Gerv\\'as",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07304v1 Announce Type: new \nAbstract: Every time an Interactive Storytelling (IS) system gets a player input, it is facing the world-update problem. Classical approaches to this problem consist in mapping that input to known preprogrammed actions, what can severely constrain the free will of the player. When the expected experience has a strong focus on improvisation, like in Role-playing Games (RPGs), this problem is critical. In this paper we present PAYADOR, a different approach that focuses on predicting the outcomes of the actions instead of representing the actions themselves. To implement this approach, we ground a Large Language Model to a minimal representation of the fictional world, obtaining promising results. We make this contribution open-source, so it can be adapted and used for other related research on unleashing the co-creativity power of RPGs."
      },
      {
        "id": "oai:arXiv.org:2504.07307v1",
        "title": "Follow-the-Perturbed-Leader Achieves Best-of-Both-Worlds for the m-Set Semi-Bandit Problems",
        "link": "https://arxiv.org/abs/2504.07307",
        "author": "Jingxin Zhan, Zhihua Zhang",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07307v1 Announce Type: new \nAbstract: We consider a common case of the combinatorial semi-bandit problem, the $m$-set semi-bandit, where the learner exactly selects $m$ arms from the total $d$ arms. In the adversarial setting, the best regret bound, known to be $\\mathcal{O}(\\sqrt{nmd})$ for time horizon $n$, is achieved by the well-known Follow-the-Regularized-Leader (FTRL) policy, which, however, requires to explicitly compute the arm-selection probabilities by solving optimizing problems at each time step and sample according to it. This problem can be avoided by the Follow-the-Perturbed-Leader (FTPL) policy, which simply pulls the $m$ arms that rank among the $m$ smallest (estimated) loss with random perturbation. In this paper, we show that FTPL with a Fr\\'echet perturbation also enjoys the optimal regret bound $\\mathcal{O}(\\sqrt{nmd})$ in the adversarial setting and achieves best-of-both-world regret bounds, i.e., achieves a logarithmic regret for the stochastic setting."
      },
      {
        "id": "oai:arXiv.org:2504.07315v1",
        "title": "Multilingual MFA: Forced Alignment on Low-Resource Related Languages",
        "link": "https://arxiv.org/abs/2504.07315",
        "author": "Alessio Tosolini, Claire Bowern",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07315v1 Announce Type: new \nAbstract: We compare the outcomes of multilingual and crosslingual training for related and unrelated Australian languages with similar phonological inventories. We use the Montreal Forced Aligner to train acoustic models from scratch and adapt a large English model, evaluating results against seen data, unseen data (seen language), and unseen data and language. Results indicate benefits of adapting the English baseline model for previously unseen languages."
      },
      {
        "id": "oai:arXiv.org:2504.07316v1",
        "title": "Alice: Proactive Learning with Teacher's Demonstrations for Weak-to-Strong Generalization",
        "link": "https://arxiv.org/abs/2504.07316",
        "author": "Shujin Wu (May), Cheng Qian (May), Yi R. (May),  Fung, Paul Pu Liang, Heng Ji",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07316v1 Announce Type: new \nAbstract: The growing capabilities of large language models (LLMs) present a key challenge of maintaining effective human oversight. Weak-to-strong generalization (W2SG) offers a promising framework for supervising increasingly capable LLMs using weaker ones. Traditional W2SG methods rely on passive learning, where a weak teacher provides noisy demonstrations to train a strong student. This hinders students from employing their knowledge during training and reaching their full potential. In this work, we introduce Alice (pro{A}ctive {l}earning w{i}th tea{c}her's D{e}monstrations), a framework that leverages complementary knowledge between teacher and student to enhance the learning process.We probe the knowledge base of the teacher model by eliciting their uncertainty, and then use these insights together with teachers' responses as demonstrations to guide student models in self-generating improved responses for supervision. In addition, for situations with significant capability gaps between teacher and student models, we introduce cascade Alice, which employs a hierarchical training approach where weak teachers initially supervise intermediate models, who then guide stronger models in sequence. Experimental results demonstrate that our method significantly enhances the W2SG performance, yielding substantial improvements in three key tasks compared to the original W2SG: knowledge-based reasoning (+4.0%), mathematical reasoning (+22.62%), and logical reasoning (+12.11%). This highlights the effectiveness of our new W2SG paradigm that enables more robust knowledge transfer and supervision outcome."
      },
      {
        "id": "oai:arXiv.org:2504.07322v1",
        "title": "Bregman-Hausdorff divergence: strengthening the connections between computational geometry and machine learning",
        "link": "https://arxiv.org/abs/2504.07322",
        "author": "Tuyen Pham, Hana Dal Poz Kou\\v{r}imsk\\'a, Hubert Wagner",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07322v1 Announce Type: new \nAbstract: The purpose of this paper is twofold. On a technical side, we propose an extension of the Hausdorff distance from metric spaces to spaces equipped with asymmetric distance measures. Specifically, we focus on the family of Bregman divergences, which includes the popular Kullback--Leibler divergence (also known as relative entropy).\n  As a proof of concept, we use the resulting Bregman--Hausdorff divergence to compare two collections of probabilistic predictions produced by different machine learning models trained using the relative entropy loss. The algorithms we propose are surprisingly efficient even for large inputs with hundreds of dimensions.\n  In addition to the introduction of this technical concept, we provide a survey. It outlines the basics of Bregman geometry, as well as computational geometry algorithms. We focus on algorithms that are compatible with this geometry and are relevant for machine learning."
      },
      {
        "id": "oai:arXiv.org:2504.07334v1",
        "title": "Objaverse++: Curated 3D Object Dataset with Quality Annotations",
        "link": "https://arxiv.org/abs/2504.07334",
        "author": "Chendi Lin, Heshan Liu, Qunshu Lin, Zachary Bright, Shitao Tang, Yihui He, Minghao Liu, Ling Zhu, Cindy Le",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07334v1 Announce Type: new \nAbstract: This paper presents Objaverse++, a curated subset of Objaverse enhanced with detailed attribute annotations by human experts. Recent advances in 3D content generation have been driven by large-scale datasets such as Objaverse, which contains over 800,000 3D objects collected from the Internet. Although Objaverse represents the largest available 3D asset collection, its utility is limited by the predominance of low-quality models. To address this limitation, we manually annotate 10,000 3D objects with detailed attributes, including aesthetic quality scores, texture color classifications, multi-object composition flags, transparency characteristics, etc. Then, we trained a neural network capable of annotating the tags for the rest of the Objaverse dataset. Through experiments and a user study on generation results, we demonstrate that models pre-trained on our quality-focused subset achieve better performance than those trained on the larger dataset of Objaverse in image-to-3D generation tasks. In addition, by comparing multiple subsets of training data filtered by our tags, our results show that the higher the data quality, the faster the training loss converges. These findings suggest that careful curation and rich annotation can compensate for the raw dataset size, potentially offering a more efficient path to develop 3D generative models. We release our enhanced dataset of approximately 500,000 curated 3D models to facilitate further research on various downstream tasks in 3D computer vision. In the near future, we aim to extend our annotations to cover the entire Objaverse dataset."
      },
      {
        "id": "oai:arXiv.org:2504.07335v1",
        "title": "DLTPose: 6DoF Pose Estimation From Accurate Dense Surface Point Estimates",
        "link": "https://arxiv.org/abs/2504.07335",
        "author": "Akash Jadhav, Michael Greenspan",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07335v1 Announce Type: new \nAbstract: We propose DLTPose, a novel method for 6DoF object pose estimation from RGB-D images that combines the accuracy of sparse keypoint methods with the robustness of dense pixel-wise predictions. DLTPose predicts per-pixel radial distances to a set of minimally four keypoints, which are then fed into our novel Direct Linear Transform (DLT) formulation to produce accurate 3D object frame surface estimates, leading to better 6DoF pose estimation. Additionally, we introduce a novel symmetry-aware keypoint ordering approach, designed to handle object symmetries that otherwise cause inconsistencies in keypoint assignments. Previous keypoint-based methods relied on fixed keypoint orderings, which failed to account for the multiple valid configurations exhibited by symmetric objects, which our ordering approach exploits to enhance the model's ability to learn stable keypoint representations. Extensive experiments on the benchmark LINEMOD, Occlusion LINEMOD and YCB-Video datasets show that DLTPose outperforms existing methods, especially for symmetric and occluded objects, demonstrating superior Mean Average Recall values of 86.5% (LM), 79.7% (LM-O) and 89.5% (YCB-V). The code is available at https://anonymous.4open.science/r/DLTPose_/ ."
      },
      {
        "id": "oai:arXiv.org:2504.07336v1",
        "title": "Zeus: Zero-shot LLM Instruction for Union Segmentation in Multimodal Medical Imaging",
        "link": "https://arxiv.org/abs/2504.07336",
        "author": "Siyuan Dai, Kai Ye, Guodong Liu, Haoteng Tang, Liang Zhan",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07336v1 Announce Type: new \nAbstract: Medical image segmentation has achieved remarkable success through the continuous advancement of UNet-based and Transformer-based foundation backbones. However, clinical diagnosis in the real world often requires integrating domain knowledge, especially textual information. Conducting multimodal learning involves visual and text modalities shown as a solution, but collecting paired vision-language datasets is expensive and time-consuming, posing significant challenges. Inspired by the superior ability in numerous cross-modal tasks for Large Language Models (LLMs), we proposed a novel Vision-LLM union framework to address the issues. Specifically, we introduce frozen LLMs for zero-shot instruction generation based on corresponding medical images, imitating the radiology scanning and report generation process. {To better approximate real-world diagnostic processes}, we generate more precise text instruction from multimodal radiology images (e.g., T1-w or T2-w MRI and CT). Based on the impressive ability of semantic understanding and rich knowledge of LLMs. This process emphasizes extracting special features from different modalities and reunion the information for the ultimate clinical diagnostic. With generated text instruction, our proposed union segmentation framework can handle multimodal segmentation without prior collected vision-language datasets. To evaluate our proposed method, we conduct comprehensive experiments with influential baselines, the statistical results and the visualized case study demonstrate the superiority of our novel method.}"
      },
      {
        "id": "oai:arXiv.org:2504.07337v1",
        "title": "FLASH: Flexible Learning of Adaptive Sampling from History in Temporal Graph Neural Networks",
        "link": "https://arxiv.org/abs/2504.07337",
        "author": "Or Feldman, Krishna Sri Ipsit Mantri, Carola-Bibiane Sch\\\"onlieb, Chaim Baskin, Moshe Eliasof",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07337v1 Announce Type: new \nAbstract: Aggregating temporal signals from historic interactions is a key step in future link prediction on dynamic graphs. However, incorporating long histories is resource-intensive. Hence, temporal graph neural networks (TGNNs) often rely on historical neighbors sampling heuristics such as uniform sampling or recent neighbors selection. These heuristics are static and fail to adapt to the underlying graph structure. We introduce FLASH, a learnable and graph-adaptive neighborhood selection mechanism that generalizes existing heuristics. FLASH integrates seamlessly into TGNNs and is trained end-to-end using a self-supervised ranking loss. We provide theoretical evidence that commonly used heuristics hinders TGNNs performance, motivating our design. Extensive experiments across multiple benchmarks demonstrate consistent and significant performance improvements for TGNNs equipped with FLASH."
      },
      {
        "id": "oai:arXiv.org:2504.07342v1",
        "title": "Leveraging deep learning for plant disease identification: a bibliometric analysis in SCOPUS from 2018 to 2024",
        "link": "https://arxiv.org/abs/2504.07342",
        "author": "Enow Takang Achuo Albert, Ngalle Hermine Bille, Ngonkeu Mangaptche Eddy Leonard",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07342v1 Announce Type: new \nAbstract: This work aimed to present a bibliometric analysis of deep learning research for plant disease identification, with a special focus on generative modeling. A thorough analysis of SCOPUS-sourced bibliometric data from 253 documents was performed. Key performance metrics such as accuracy, precision, recall, and F1-score were analyzed for generative modeling. The findings highlighted significant contributions from some authors Too and Arnal Barbedo, whose works had notable citation counts, suggesting their influence on the academic community. Co-authorship networks revealed strong collaborative clusters, while keyword analysis identified emerging research gaps. This study highlights the role of collaboration and citation metrics in shaping research directions and enhancing the impact of scholarly work in applications of deep learning to plant disease identification. Future research should explore the methodologies of highly cited studies to inform best practices and policy-making."
      },
      {
        "id": "oai:arXiv.org:2504.07357v1",
        "title": "Revisiting Prompt Optimization with Large Reasoning Models-A Case Study on Event Extraction",
        "link": "https://arxiv.org/abs/2504.07357",
        "author": "Saurabh Srivastava, Ziyu Yao",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07357v1 Announce Type: new \nAbstract: Large Reasoning Models (LRMs) such as DeepSeek-R1 and OpenAI o1 have demonstrated remarkable capabilities in various reasoning tasks. Their strong capability to generate and reason over intermediate thoughts has also led to arguments that they may no longer require extensive prompt engineering or optimization to interpret human instructions and produce accurate outputs. In this work, we aim to systematically study this open question, using the structured task of event extraction for a case study. We experimented with two LRMs (DeepSeek-R1 and o1) and two general-purpose Large Language Models (LLMs) (GPT-4o and GPT-4.5), when they were used as task models or prompt optimizers. Our results show that on tasks as complicated as event extraction, LRMs as task models still benefit from prompt optimization, and that using LRMs as prompt optimizers yields more effective prompts. Finally, we provide an error analysis of common errors made by LRMs and highlight the stability and consistency of LRMs in refining task instructions and event guidelines."
      },
      {
        "id": "oai:arXiv.org:2504.07360v1",
        "title": "Enhancing Time Series Forecasting via Multi-Level Text Alignment with LLMs",
        "link": "https://arxiv.org/abs/2504.07360",
        "author": "Taibiao Zhao, Xiaobing Chen, Mingxuan Sun",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07360v1 Announce Type: new \nAbstract: The adaptation of large language models (LLMs) to time series forecasting poses unique challenges, as time series data is continuous in nature, while LLMs operate on discrete tokens. Despite the success of LLMs in natural language processing (NLP) and other structured domains, aligning time series data with language-based representations while maintaining both predictive accuracy and interpretability remains a significant hurdle. Existing methods have attempted to reprogram time series data into text-based forms, but these often fall short in delivering meaningful, interpretable results. In this paper, we propose a multi-level text alignment framework for time series forecasting using LLMs that not only improves prediction accuracy but also enhances the interpretability of time series representations. Our method decomposes time series into trend, seasonal, and residual components, which are then reprogrammed into component-specific text representations. We introduce a multi-level alignment mechanism, where component-specific embeddings are aligned with pre-trained word tokens, enabling more interpretable forecasts. Experiments on multiple datasets demonstrate that our method outperforms state-of-the-art models in accuracy while providing good interpretability."
      },
      {
        "id": "oai:arXiv.org:2504.07370v1",
        "title": "View-Dependent Uncertainty Estimation of 3D Gaussian Splatting",
        "link": "https://arxiv.org/abs/2504.07370",
        "author": "Chenyu Han, Corentin Dumery",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07370v1 Announce Type: new \nAbstract: 3D Gaussian Splatting (3DGS) has become increasingly popular in 3D scene reconstruction for its high visual accuracy. However, uncertainty estimation of 3DGS scenes remains underexplored and is crucial to downstream tasks such as asset extraction and scene completion. Since the appearance of 3D gaussians is view-dependent, the color of a gaussian can thus be certain from an angle and uncertain from another. We thus propose to model uncertainty in 3DGS as an additional view-dependent per-gaussian feature that can be modeled with spherical harmonics. This simple yet effective modeling is easily interpretable and can be integrated into the traditional 3DGS pipeline. It is also significantly faster than ensemble methods while maintaining high accuracy, as demonstrated in our experiments."
      },
      {
        "id": "oai:arXiv.org:2504.07371v1",
        "title": "Minimum width for universal approximation using squashable activation functions",
        "link": "https://arxiv.org/abs/2504.07371",
        "author": "Jonghyun Shin, Namjun Kim, Geonho Hwang, Sejun Park",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07371v1 Announce Type: new \nAbstract: The exact minimum width that allows for universal approximation of unbounded-depth networks is known only for ReLU and its variants. In this work, we study the minimum width of networks using general activation functions. Specifically, we focus on squashable functions that can approximate the identity function and binary step function by alternatively composing with affine transformations. We show that for networks using a squashable activation function to universally approximate $L^p$ functions from $[0,1]^{d_x}$ to $\\mathbb R^{d_y}$, the minimum width is $\\max\\{d_x,d_y,2\\}$ unless $d_x=d_y=1$; the same bound holds for $d_x=d_y=1$ if the activation function is monotone. We then provide sufficient conditions for squashability and show that all non-affine analytic functions and a class of piecewise functions are squashable, i.e., our minimum width result holds for those general classes of activation functions."
      },
      {
        "id": "oai:arXiv.org:2504.07373v1",
        "title": "ChronoFormer: Time-Aware Transformer Architectures for Structured Clinical Event Modeling",
        "link": "https://arxiv.org/abs/2504.07373",
        "author": "Yuanyun Zhang, Shi Li",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07373v1 Announce Type: new \nAbstract: The temporal complexity of electronic health record (EHR) data presents significant challenges for predicting clinical outcomes using machine learning. This paper proposes ChronoFormer, an innovative transformer based architecture specifically designed to encode and leverage temporal dependencies in longitudinal patient data. ChronoFormer integrates temporal embeddings, hierarchical attention mechanisms, and domain specific masking techniques. Extensive experiments conducted on three benchmark tasks mortality prediction, readmission prediction, and long term comorbidity onset demonstrate substantial improvements over current state of the art methods. Furthermore, detailed analyses of attention patterns underscore ChronoFormer's capability to capture clinically meaningful long range temporal relationships."
      },
      {
        "id": "oai:arXiv.org:2504.07375v1",
        "title": "Novel Diffusion Models for Multimodal 3D Hand Trajectory Prediction",
        "link": "https://arxiv.org/abs/2504.07375",
        "author": "Junyi Ma, Wentao Bao, Jingyi Xu, Guanzhong Sun, Xieyuanli Chen, Hesheng Wang",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07375v1 Announce Type: new \nAbstract: Predicting hand motion is critical for understanding human intentions and bridging the action space between human movements and robot manipulations. Existing hand trajectory prediction (HTP) methods forecast the future hand waypoints in 3D space conditioned on past egocentric observations. However, such models are only designed to accommodate 2D egocentric video inputs. There is a lack of awareness of multimodal environmental information from both 2D and 3D observations, hindering the further improvement of 3D HTP performance. In addition, these models overlook the synergy between hand movements and headset camera egomotion, either predicting hand trajectories in isolation or encoding egomotion only from past frames. To address these limitations, we propose novel diffusion models (MMTwin) for multimodal 3D hand trajectory prediction. MMTwin is designed to absorb multimodal information as input encompassing 2D RGB images, 3D point clouds, past hand waypoints, and text prompt. Besides, two latent diffusion models, the egomotion diffusion and the HTP diffusion as twins, are integrated into MMTwin to predict camera egomotion and future hand trajectories concurrently. We propose a novel hybrid Mamba-Transformer module as the denoising model of the HTP diffusion to better fuse multimodal features. The experimental results on three publicly available datasets and our self-recorded data demonstrate that our proposed MMTwin can predict plausible future 3D hand trajectories compared to the state-of-the-art baselines, and generalizes well to unseen environments. The code and pretrained models will be released at https://github.com/IRMVLab/MMTwin."
      },
      {
        "id": "oai:arXiv.org:2504.07378v1",
        "title": "BRepFormer: Transformer-Based B-rep Geometric Feature Recognition",
        "link": "https://arxiv.org/abs/2504.07378",
        "author": "Yongkang Dai, Xiaoshui Huang, Yunpeng Bai, Hao Guo, Hongping Gan, Ling Yang, Yilei Shi",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07378v1 Announce Type: new \nAbstract: Recognizing geometric features on B-rep models is a cornerstone technique for multimedia content-based retrieval and has been widely applied in intelligent manufacturing. However, previous research often merely focused on Machining Feature Recognition (MFR), falling short in effectively capturing the intricate topological and geometric characteristics of complex geometry features. In this paper, we propose BRepFormer, a novel transformer-based model to recognize both machining feature and complex CAD models' features. BRepFormer encodes and fuses the geometric and topological features of the models. Afterwards, BRepFormer utilizes a transformer architecture for feature propagation and a recognition head to identify geometry features. During each iteration of the transformer, we incorporate a bias that combines edge features and topology features to reinforce geometric constraints on each face. In addition, we also proposed a dataset named Complex B-rep Feature Dataset (CBF), comprising 20,000 B-rep models. By covering more complex B-rep models, it is better aligned with industrial applications. The experimental results demonstrate that BRepFormer achieves state-of-the-art accuracy on the MFInstSeg, MFTRCAD, and our CBF datasets."
      },
      {
        "id": "oai:arXiv.org:2504.07382v1",
        "title": "Model Discrepancy Learning: Synthetic Faces Detection Based on Multi-Reconstruction",
        "link": "https://arxiv.org/abs/2504.07382",
        "author": "Qingchao Jiang, Zhishuo Xu, Zhiying Zhu, Ning Chen, Haoyue Wang, Zhongjie Ba",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07382v1 Announce Type: new \nAbstract: Advances in image generation enable hyper-realistic synthetic faces but also pose risks, thus making synthetic face detection crucial. Previous research focuses on the general differences between generated images and real images, often overlooking the discrepancies among various generative techniques. In this paper, we explore the intrinsic relationship between synthetic images and their corresponding generation technologies. We find that specific images exhibit significant reconstruction discrepancies across different generative methods and that matching generation techniques provide more accurate reconstructions. Based on this insight, we propose a Multi-Reconstruction-based detector. By reversing and reconstructing images using multiple generative models, we analyze the reconstruction differences among real, GAN-generated, and DM-generated images to facilitate effective differentiation. Additionally, we introduce the Asian Synthetic Face Dataset (ASFD), containing synthetic Asian faces generated with various GANs and DMs. This dataset complements existing synthetic face datasets. Experimental results demonstrate that our detector achieves exceptional performance, with strong generalization and robustness."
      },
      {
        "id": "oai:arXiv.org:2504.07383v1",
        "title": "PROPEL: Supervised and Reinforcement Learning for Large-Scale Supply Chain Planning",
        "link": "https://arxiv.org/abs/2504.07383",
        "author": "Vahid Eghbal Akhlaghi, Reza Zandehshahvar, Pascal Van Hentenryck",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07383v1 Announce Type: new \nAbstract: This paper considers how to fuse Machine Learning (ML) and optimization to solve large-scale Supply Chain Planning (SCP) optimization problems. These problems can be formulated as MIP models which feature both integer (non-binary) and continuous variables, as well as flow balance and capacity constraints. This raises fundamental challenges for existing integrations of ML and optimization that have focused on binary MIPs and graph problems. To address these, the paper proposes PROPEL, a new framework that combines optimization with both supervised and Deep Reinforcement Learning (DRL) to reduce the size of search space significantly. PROPEL uses supervised learning, not to predict the values of all integer variables, but to identify the variables that are fixed to zero in the optimal solution, leveraging the structure of SCP applications. PROPEL includes a DRL component that selects which fixed-at-zero variables must be relaxed to improve solution quality when the supervised learning step does not produce a solution with the desired optimality tolerance. PROPEL has been applied to industrial supply chain planning optimizations with millions of variables. The computational results show dramatic improvements in solution times and quality, including a 60% reduction in primal integral and an 88% primal gap reduction, and improvement factors of up to 13.57 and 15.92, respectively."
      },
      {
        "id": "oai:arXiv.org:2504.07385v1",
        "title": "TALE: A Tool-Augmented Framework for Reference-Free Evaluation of Large Language Models",
        "link": "https://arxiv.org/abs/2504.07385",
        "author": "Sher Badshah, Ali Emami, Hassan Sajjad",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07385v1 Announce Type: new \nAbstract: As Large Language Models (LLMs) become increasingly integrated into real-world, autonomous applications, relying on static, pre-annotated references for evaluation poses significant challenges in cost, scalability, and completeness. We propose Tool-Augmented LLM Evaluation (TALE), a framework to assess LLM outputs without predetermined ground-truth answers. Unlike conventional metrics that compare to fixed references or depend solely on LLM-as-a-judge knowledge, TALE employs an agent with tool-access capabilities that actively retrieves and synthesizes external evidence. It iteratively generates web queries, collects information, summarizes findings, and refines subsequent searches through reflection. By shifting away from static references, TALE aligns with free-form question-answering tasks common in real-world scenarios. Experimental results on multiple free-form QA benchmarks show that TALE not only outperforms standard reference-based metrics for measuring response accuracy but also achieves substantial to near-perfect agreement with human evaluations. TALE enhances the reliability of LLM evaluations in real-world, dynamic scenarios without relying on static references."
      },
      {
        "id": "oai:arXiv.org:2504.07389v1",
        "title": "Task-Circuit Quantization: Leveraging Knowledge Localization and Interpretability for Compression",
        "link": "https://arxiv.org/abs/2504.07389",
        "author": "Hanqi Xiao, Yi-Lin Sung, Elias Stengel-Eskin, Mohit Bansal",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07389v1 Announce Type: new \nAbstract: Post-training quantization (PTQ) reduces a model's memory footprint by mapping full precision weights into low bit weights without costly retraining, but can degrade its downstream performance especially in low 2- to 3-bit settings. We develop a new mixed-precision PTQ approach, Task-Circuit Quantization (TaCQ), that draws parallels to automated circuit discovery, directly conditioning the quantization process on specific weight circuits -- which we define as sets of weights associated with downstream task performance. These weights are kept as 16-bit weights, while others are quantized, maintaining performance while only adding a marginal memory cost. Specifically, TaCQ contrasts unquantized model weights with a uniformly-quantized model to estimate the expected change in weights due to quantization and uses gradient information to predict the resulting impact on task performance, allowing us to preserve task-specific weights. We compare TaCQ-based quantization to existing mixed-precision quantization methods when conditioning both on general-purpose and task-specific data. Across QA, math reasoning, and text-to-SQL tasks for both Llama-3 and Qwen2.5, we find that TaCQ outperforms baselines using the same calibration data and a lower weight budget, achieving major improvements in the 2 and 3-bit regime. With only 3.1 bits we are able to recover 96% of Llama-3-8B-Instruct's unquantized 16-bit MMLU performance, obtaining a 5.25% absolute improvement over SPQR. We also observe consistently large gains over existing methods in the 2-bit regime, with an average gain of 14.74% over the strongest baseline, SliM-LLM. Moreover, we observe a 7.20% gain without conditioning on specific tasks, showing TaCQ's ability to identify important weights is not limited to task-conditioned settings."
      },
      {
        "id": "oai:arXiv.org:2504.07392v1",
        "title": "ID-Booth: Identity-consistent Face Generation with Diffusion Models",
        "link": "https://arxiv.org/abs/2504.07392",
        "author": "Darian Toma\\v{s}evi\\'c, Fadi Boutros, Chenhao Lin, Naser Damer, Vitomir \\v{S}truc, Peter Peer",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07392v1 Announce Type: new \nAbstract: Recent advances in generative modeling have enabled the generation of high-quality synthetic data that is applicable in a variety of domains, including face recognition. Here, state-of-the-art generative models typically rely on conditioning and fine-tuning of powerful pretrained diffusion models to facilitate the synthesis of realistic images of a desired identity. Yet, these models often do not consider the identity of subjects during training, leading to poor consistency between generated and intended identities. In contrast, methods that employ identity-based training objectives tend to overfit on various aspects of the identity, and in turn, lower the diversity of images that can be generated. To address these issues, we present in this paper a novel generative diffusion-based framework, called ID-Booth. ID-Booth consists of a denoising network responsible for data generation, a variational auto-encoder for mapping images to and from a lower-dimensional latent space and a text encoder that allows for prompt-based control over the generation procedure. The framework utilizes a novel triplet identity training objective and enables identity-consistent image generation while retaining the synthesis capabilities of pretrained diffusion models. Experiments with a state-of-the-art latent diffusion model and diverse prompts reveal that our method facilitates better intra-identity consistency and inter-identity separability than competing methods, while achieving higher image diversity. In turn, the produced data allows for effective augmentation of small-scale datasets and training of better-performing recognition models in a privacy-preserving manner. The source code for the ID-Booth framework is publicly available at https://github.com/dariant/ID-Booth."
      },
      {
        "id": "oai:arXiv.org:2504.07393v1",
        "title": "State Estimation Using Particle Filtering in Adaptive Machine Learning Methods: Integrating Q-Learning and NEAT Algorithms with Noisy Radar Measurements",
        "link": "https://arxiv.org/abs/2504.07393",
        "author": "Wonjin Song, Feng Bao",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07393v1 Announce Type: new \nAbstract: Reliable state estimation is essential for autonomous systems operating in complex, noisy environments. Classical filtering approaches, such as the Kalman filter, can struggle when facing nonlinear dynamics or non-Gaussian noise, and even more flexible particle filters often encounter sample degeneracy or high computational costs in large-scale domains. Meanwhile, adaptive machine learning techniques, including Q-learning and neuroevolutionary algorithms such as NEAT, rely heavily on accurate state feedback to guide learning; when sensor data are imperfect, these methods suffer from degraded convergence and suboptimal performance. In this paper, we propose an integrated framework that unifies particle filtering with Q-learning and NEAT to explicitly address the challenge of noisy measurements. By refining radar-based observations into reliable state estimates, our particle filter drives more stable policy updates (in Q-learning) or controller evolution (in NEAT), allowing both reinforcement learning and neuroevolution to converge faster, achieve higher returns or fitness, and exhibit greater resilience to sensor uncertainty. Experiments on grid-based navigation and a simulated car environment highlight consistent gains in training stability, final performance, and success rates over baselines lacking advanced filtering. Altogether, these findings underscore that accurate state estimation is not merely a preprocessing step, but a vital component capable of substantially enhancing adaptive machine learning in real-world applications plagued by sensor noise."
      },
      {
        "id": "oai:arXiv.org:2504.07394v1",
        "title": "ClimateBench-M: A Multi-Modal Climate Data Benchmark with a Simple Generative Method",
        "link": "https://arxiv.org/abs/2504.07394",
        "author": "Dongqi Fu, Yada Zhu, Zhining Liu, Lecheng Zheng, Xiao Lin, Zihao Li, Liri Fang, Katherine Tieu, Onkar Bhardwaj, Kommy Weldemariam, Hanghang Tong, Hendrik Hamann, Jingrui He",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07394v1 Announce Type: new \nAbstract: Climate science studies the structure and dynamics of Earth's climate system and seeks to understand how climate changes over time, where the data is usually stored in the format of time series, recording the climate features, geolocation, time attributes, etc. Recently, much research attention has been paid to the climate benchmarks. In addition to the most common task of weather forecasting, several pioneering benchmark works are proposed for extending the modality, such as domain-specific applications like tropical cyclone intensity prediction and flash flood damage estimation, or climate statement and confidence level in the format of natural language. To further motivate the artificial general intelligence development for climate science, in this paper, we first contribute a multi-modal climate benchmark, i.e., ClimateBench-M, which aligns (1) the time series climate data from ERA5, (2) extreme weather events data from NOAA, and (3) satellite image data from NASA HLS based on a unified spatial-temporal granularity. Second, under each data modality, we also propose a simple but strong generative method that could produce competitive performance in weather forecasting, thunderstorm alerts, and crop segmentation tasks in the proposed ClimateBench-M. The data and code of ClimateBench-M are publicly available at https://github.com/iDEA-iSAIL-Lab-UIUC/ClimateBench-M."
      },
      {
        "id": "oai:arXiv.org:2504.07395v1",
        "title": "FAIR-SIGHT: Fairness Assurance in Image Recognition via Simultaneous Conformal Thresholding and Dynamic Output Repair",
        "link": "https://arxiv.org/abs/2504.07395",
        "author": "Arya Fayyazi, Mehdi Kamal, Massoud Pedram",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07395v1 Announce Type: new \nAbstract: We introduce FAIR-SIGHT, an innovative post-hoc framework designed to ensure fairness in computer vision systems by combining conformal prediction with a dynamic output repair mechanism. Our approach calculates a fairness-aware non-conformity score that simultaneously assesses prediction errors and fairness violations. Using conformal prediction, we establish an adaptive threshold that provides rigorous finite-sample, distribution-free guarantees. When the non-conformity score for a new image exceeds the calibrated threshold, FAIR-SIGHT implements targeted corrective adjustments, such as logit shifts for classification and confidence recalibration for detection, to reduce both group and individual fairness disparities, all without the need for retraining or having access to internal model parameters. Comprehensive theoretical analysis validates our method's error control and convergence properties. At the same time, extensive empirical evaluations on benchmark datasets show that FAIR-SIGHT significantly reduces fairness disparities while preserving high predictive performance."
      },
      {
        "id": "oai:arXiv.org:2504.07397v1",
        "title": "MicroNAS: An Automated Framework for Developing a Fall Detection System",
        "link": "https://arxiv.org/abs/2504.07397",
        "author": "Seyed Mojtaba Mohasel, John Sheppard, Lindsey K. Molina, Richard R. Neptune, Shane R. Wurdeman, Corey A. Pew",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07397v1 Announce Type: new \nAbstract: This work presents MicroNAS, an automated neural architecture search tool specifically designed to create models optimized for microcontrollers with small memory resources. The ESP32 microcontroller, with 320 KB of memory, is used as the target platform. The artificial intelligence contribution lies in a novel method for optimizing convolutional neural network and gated recurrent unit architectures by considering the memory size of the target microcontroller as a guide. A comparison is made between memory-driven model optimization and traditional two-stage methods, which use pruning, to show the effectiveness of the proposed framework. To demonstrate the engineering application of MicroNAS, a fall detection system (FDS) for lower-limb amputees is developed as a pilot study. A critical challenge in fall detection studies, class imbalance in the dataset, is addressed. The results show that MicroNAS models achieved higher F1-scores than alternative approaches, such as ensemble methods and H2O Automated Machine Learning, presenting a significant step forward in real-time FDS development. Biomechanists using body-worn sensors for activity detection can adopt the open-source code to design machine learning models tailored for microcontroller platforms with limited memory."
      },
      {
        "id": "oai:arXiv.org:2504.07400v1",
        "title": "Talking Point based Ideological Discourse Analysis in News Events",
        "link": "https://arxiv.org/abs/2504.07400",
        "author": "Nishanth Nakshatri, Nikhil Mehta, Siyi Liu, Sihao Chen, Daniel J. Hopkins, Dan Roth, Dan Goldwasser",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07400v1 Announce Type: new \nAbstract: Analyzing ideological discourse even in the age of LLMs remains a challenge, as these models often struggle to capture the key elements that shape real-world narratives. Specifically, LLMs fail to focus on characteristic elements driving dominant discourses and lack the ability to integrate contextual information required for understanding abstract ideological views. To address these limitations, we propose a framework motivated by the theory of ideological discourse analysis to analyze news articles related to real-world events. Our framework represents the news articles using a relational structure - talking points, which captures the interaction between entities, their roles, and media frames along with a topic of discussion. It then constructs a vocabulary of repeating themes - prominent talking points, that are used to generate ideology-specific viewpoints (or partisan perspectives). We evaluate our framework's ability to generate these perspectives through automated tasks - ideology and partisan classification tasks, supplemented by human validation. Additionally, we demonstrate straightforward applicability of our framework in creating event snapshots, a visual way of interpreting event discourse. We release resulting dataset and model to the community to support further research."
      },
      {
        "id": "oai:arXiv.org:2504.07402v1",
        "title": "LauraTSE: Target Speaker Extraction using Auto-Regressive Decoder-Only Language Models",
        "link": "https://arxiv.org/abs/2504.07402",
        "author": "Beilong Tang, Bang Zeng, Ming Li",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07402v1 Announce Type: new \nAbstract: We propose LauraTSE, an Auto-Regressive Decoder-Only Language Model for Target Speaker Extraction (TSE) based on the LauraGPT backbone. It employs a small-scale auto-regressive decoder-only language model which takes the continuous representations for both the mixture and the reference speeches and produces the first few layers of the target speech's discrete codec representations. In addition, a one-step encoder-only language model reconstructs the sum of the predicted codec embeddings using both the mixture and the reference information. Our approach achieves superior or comparable performance to existing generative and discriminative TSE models. To the best of our knowledge, LauraTSE is the first single-task TSE model to leverage an auto-regressive decoder-only language model as the backbone."
      },
      {
        "id": "oai:arXiv.org:2504.07403v1",
        "title": "Multi-Selection for Recommendation Systems",
        "link": "https://arxiv.org/abs/2504.07403",
        "author": "Sahasrajit Sarmasarkar, Zhihao Jiang, Ashish Goel, Aleksandra Korolova, Kamesh Munagala",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07403v1 Announce Type: new \nAbstract: We present the construction of a multi-selection model to answer differentially private queries in the context of recommendation systems. The server sends back multiple recommendations and a ``local model'' to the user, which the user can run locally on its device to select the item that best fits its private features. We study a setup where the server uses a deep neural network (trained on the Movielens 25M dataset as the ground truth for movie recommendation. In the multi-selection paradigm, the average recommendation utility is approximately 97\\% of the optimal utility (as determined by the ground truth neural network) while maintaining a local differential privacy guarantee with $\\epsilon$ ranging around 1 with respect to feature vectors of neighboring users. This is in comparison to an average recommendation utility of 91\\% in the non-multi-selection regime under the same constraints."
      },
      {
        "id": "oai:arXiv.org:2504.07405v1",
        "title": "FlexIP: Dynamic Control of Preservation and Personality for Customized Image Generation",
        "link": "https://arxiv.org/abs/2504.07405",
        "author": "Linyan Huang, Haonan Lin, Yanning Zhou, Kaiwen Xiao",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07405v1 Announce Type: new \nAbstract: With the rapid advancement of 2D generative models, preserving subject identity while enabling diverse editing has emerged as a critical research focus. Existing methods typically face inherent trade-offs between identity preservation and personalized manipulation. We introduce FlexIP, a novel framework that decouples these objectives through two dedicated components: a Personalization Adapter for stylistic manipulation and a Preservation Adapter for identity maintenance. By explicitly injecting both control mechanisms into the generative model, our framework enables flexible parameterized control during inference through dynamic tuning of the weight adapter. Experimental results demonstrate that our approach breaks through the performance limitations of conventional methods, achieving superior identity preservation while supporting more diverse personalized generation capabilities (Project Page: https://flexip-tech.github.io/flexip/)."
      },
      {
        "id": "oai:arXiv.org:2504.07408v1",
        "title": "AI Coding with Few-Shot Prompting for Thematic Analysis",
        "link": "https://arxiv.org/abs/2504.07408",
        "author": "Samuel Flanders, Melati Nungsari, Mark Cheong Wing Loong",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07408v1 Announce Type: new \nAbstract: This paper explores the use of large language models (LLMs), here represented by GPT 3.5-Turbo to perform coding for a thematic analysis. Coding is highly labor intensive, making it infeasible for most researchers to conduct exhaustive thematic analyses of large corpora. We utilize few-shot prompting with higher quality codes generated on semantically similar passages to enhance the quality of the codes while utilizing a cheap, more easily scalable model."
      },
      {
        "id": "oai:arXiv.org:2504.07415v1",
        "title": "Leveraging LLMs for Multimodal Retrieval-Augmented Radiology Report Generation via Key Phrase Extraction",
        "link": "https://arxiv.org/abs/2504.07415",
        "author": "Kyoyun Choi, Byungmu Yoon, Soobum Kim, Jonggwon Park",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07415v1 Announce Type: new \nAbstract: Automated radiology report generation (RRG) holds potential to reduce radiologists' workload, especially as recent advancements in large language models (LLMs) enable the development of multimodal models for chest X-ray (CXR) report generation. However, multimodal LLMs (MLLMs) are resource-intensive, requiring vast datasets and substantial computational cost for training. To address these challenges, we propose a retrieval-augmented generation approach that leverages multimodal retrieval and LLMs to generate radiology reports while mitigating hallucinations and reducing computational demands. Our method uses LLMs to extract key phrases from radiology reports, effectively focusing on essential diagnostic information. Through exploring effective training strategies, including image encoder structure search, adding noise to text embeddings, and additional training objectives, we combine complementary pre-trained image encoders and adopt contrastive learning between text and semantic image embeddings. We evaluate our approach on MIMIC-CXR dataset, achieving state-of-the-art results on CheXbert metrics and competitive RadGraph F1 metric alongside MLLMs, without requiring LLM fine-tuning. Our method demonstrates robust generalization for multi-view RRG, making it suitable for comprehensive clinical applications."
      },
      {
        "id": "oai:arXiv.org:2504.07416v1",
        "title": "RadZero: Similarity-Based Cross-Attention for Explainable Vision-Language Alignment in Radiology with Zero-Shot Multi-Task Capability",
        "link": "https://arxiv.org/abs/2504.07416",
        "author": "Jonggwon Park, Soobum Kim, Byungmu Yoon, Kyoyun Choi",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07416v1 Announce Type: new \nAbstract: Recent advancements in multi-modal models have significantly improved vision-language alignment in radiology. However, existing approaches struggle to effectively utilize complex radiology reports for learning, rely on low-resolution images, and offer limited interpretability in attention mechanisms. To address these challenges, we introduce RadZero, a novel similarity-based cross-attention framework for vision-language alignment in radiology with zero-shot multi-task capability. RadZero leverages large language models to extract minimal semantic sentences from radiology reports and employs a multi-positive contrastive learning strategy to effectively capture relationships between images and multiple relevant textual descriptions. It also utilizes a pre-trained vision encoder with additional trainable Transformer layers, allowing efficient high-resolution image processing. By computing similarity between text embeddings and local image patch features, RadZero enables zero-shot inference with similarity probability for classification and pixel-level cross-modal similarity maps for grounding and segmentation. Experimental results on public chest radiograph benchmarks show that RadZero outperforms state-of-the-art methods in zero-shot classification, grounding, and segmentation. Furthermore, cross-modal similarity map analysis highlights its potential for improving explainability in vision-language alignment. Additionally, qualitative evaluation demonstrates RadZero's capability for open-vocabulary semantic segmentation, further validating its effectiveness in medical imaging."
      },
      {
        "id": "oai:arXiv.org:2504.07418v1",
        "title": "ThermoStereoRT: Thermal Stereo Matching in Real Time via Knowledge Distillation and Attention-based Refinement",
        "link": "https://arxiv.org/abs/2504.07418",
        "author": "Anning Hu, Ang Li, Xirui Jin, Danping Zou",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07418v1 Announce Type: new \nAbstract: We introduce ThermoStereoRT, a real-time thermal stereo matching method designed for all-weather conditions that recovers disparity from two rectified thermal stereo images, envisioning applications such as night-time drone surveillance or under-bed cleaning robots. Leveraging a lightweight yet powerful backbone, ThermoStereoRT constructs a 3D cost volume from thermal images and employs multi-scale attention mechanisms to produce an initial disparity map. To refine this map, we design a novel channel and spatial attention module. Addressing the challenge of sparse ground truth data in thermal imagery, we utilize knowledge distillation to boost performance without increasing computational demands. Comprehensive evaluations on multiple datasets demonstrate that ThermoStereoRT delivers both real-time capacity and robust accuracy, making it a promising solution for real-world deployment in various challenging environments. Our code will be released on https://github.com/SJTU-ViSYS-team/ThermoStereoRT"
      },
      {
        "id": "oai:arXiv.org:2504.07421v1",
        "title": "AgentAda: Skill-Adaptive Data Analytics for Tailored Insight Discovery",
        "link": "https://arxiv.org/abs/2504.07421",
        "author": "Amirhossein Abaskohi, Amrutha Varshini Ramesh, Shailesh Nanisetty, Chirag Goel, David Vazquez, Christopher Pal, Spandana Gella, Giuseppe Carenini, Issam H. Laradji",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07421v1 Announce Type: new \nAbstract: We introduce AgentAda, the first LLM-powered analytics agent that can learn and use new analytics skills to extract more specialized insights. Unlike existing methods that require users to manually decide which data analytics method to apply, AgentAda automatically identifies the skill needed from a library of analytical skills to perform the analysis. This also allows AgentAda to use skills that existing LLMs cannot perform out of the box. The library covers a range of methods, including clustering, predictive modeling, and NLP techniques like BERT, which allow AgentAda to handle complex analytics tasks based on what the user needs. AgentAda's dataset-to-insight extraction strategy consists of three key steps: (I) a question generator to generate queries relevant to the user's goal and persona, (II) a hybrid Retrieval-Augmented Generation (RAG)-based skill matcher to choose the best data analytics skill from the skill library, and (III) a code generator that produces executable code based on the retrieved skill's documentation to extract key patterns. We also introduce KaggleBench, a benchmark of curated notebooks across diverse domains, to evaluate AgentAda's performance. We conducted a human evaluation demonstrating that AgentAda provides more insightful analytics than existing tools, with 48.78% of evaluators preferring its analyses, compared to 27.67% for the unskilled agent. We also propose a novel LLM-as-a-judge approach that we show is aligned with human evaluation as a way to automate insight quality evaluation at larger scale."
      },
      {
        "id": "oai:arXiv.org:2504.07422v1",
        "title": "The Role of Machine Learning in Reducing Healthcare Costs: The Impact of Medication Adherence and Preventive Care on Hospitalization Expenses",
        "link": "https://arxiv.org/abs/2504.07422",
        "author": "Yixin Zhang, Yisong Chen",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07422v1 Announce Type: new \nAbstract: This study reveals the important role of prevention care and medication adherence in reducing hospitalizations. By using a structured dataset of 1,171 patients, four machine learning models Logistic Regression, Gradient Boosting, Random Forest, and Artificial Neural Networks are applied to predict five-year hospitalization risk, with the Gradient Boosting model achieving the highest accuracy of 81.2%. The result demonstrated that patients with high medication adherence and consistent preventive care can reduce 38.3% and 37.7% in hospitalization risk. The finding also suggests that targeted preventive care can have positive Return on Investment (ROI), and therefore ML models can effectively direct personalized interventions and contribute to long-term medical savings."
      },
      {
        "id": "oai:arXiv.org:2504.07433v1",
        "title": "From Token to Line: Enhancing Code Generation with a Long-Term Perspective",
        "link": "https://arxiv.org/abs/2504.07433",
        "author": "Tingwei Lu, Yangning Li, Liyuan Wang, Binghuai Lin, Jiwei Tang, Wanshi Xu, Hai-Tao Zheng, Yinghui Li, Bingxu An, Zhao Wei, Yong Xu",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07433v1 Announce Type: new \nAbstract: The emergence of large language models (LLMs) has significantly promoted the development of code generation task, sparking a surge in pertinent literature. Current research is hindered by redundant generation results and a tendency to overfit local patterns in the short term. Although existing studies attempt to alleviate the issue by adopting a multi-token prediction strategy, there remains limited focus on choosing the appropriate processing length for generations. By analyzing the attention between tokens during the generation process of LLMs, it can be observed that the high spikes of the attention scores typically appear at the end of lines. This insight suggests that it is reasonable to treat each line of code as a fundamental processing unit and generate them sequentially. Inspired by this, we propose the \\textbf{LSR-MCTS} algorithm, which leverages MCTS to determine the code line-by-line and select the optimal path. Further, we integrate a self-refine mechanism at each node to enhance diversity and generate higher-quality programs through error correction. Extensive experiments and comprehensive analyses on three public coding benchmarks demonstrate that our method outperforms the state-of-the-art performance approaches."
      },
      {
        "id": "oai:arXiv.org:2504.07437v1",
        "title": "Unifying and extending Diffusion Models through PDEs for solving Inverse Problems",
        "link": "https://arxiv.org/abs/2504.07437",
        "author": "Agnimitra Dasgupta, Alexsander Marciano da Cunha, Ali Fardisi, Mehrnegar Aminy, Brianna Binder, Bryan Shaddy, Assad A Oberai",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07437v1 Announce Type: new \nAbstract: Diffusion models have emerged as powerful generative tools with applications in computer vision and scientific machine learning (SciML), where they have been used to solve large-scale probabilistic inverse problems. Traditionally, these models have been derived using principles of variational inference, denoising, statistical signal processing, and stochastic differential equations. In contrast to the conventional presentation, in this study we derive diffusion models using ideas from linear partial differential equations and demonstrate that this approach has several benefits that include a constructive derivation of the forward and reverse processes, a unified derivation of multiple formulations and sampling strategies, and the discovery of a new class of models. We also apply the conditional version of these models to solving canonical conditional density estimation problems and challenging inverse problems. These problems help establish benchmarks for systematically quantifying the performance of different formulations and sampling strategies in this study, and for future studies. Finally, we identify and implement a mechanism through which a single diffusion model can be applied to measurements obtained from multiple measurement operators. Taken together, the contents of this manuscript provide a new understanding and several new directions in the application of diffusion models to solving physics-based inverse problems."
      },
      {
        "id": "oai:arXiv.org:2504.07440v1",
        "title": "Revisiting LLM Evaluation through Mechanism Interpretability: a New Metric and Model Utility Law",
        "link": "https://arxiv.org/abs/2504.07440",
        "author": "Yixin Cao, Jiahao Ying, Yaoning Wang, Xipeng Qiu, Xuanjing Huang, Yugang Jiang",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07440v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have become indispensable across academia, industry, and daily applications, yet current evaluation methods struggle to keep pace with their rapid development. In this paper, we analyze the core limitations of traditional evaluation pipelines and propose a novel metric, the Model Utilization Index (MUI), which introduces mechanism interpretability techniques to complement traditional performance metrics. MUI quantifies the extent to which a model leverages its capabilities to complete tasks. The core idea is that to assess an LLM's overall ability, we must evaluate not only its task performance but also the effort expended to achieve the outcome. Our extensive experiments reveal an inverse relationship between MUI and performance, from which we deduce a common trend observed in popular LLMs, which we term the Utility Law. Based on this, we derive four corollaries that address key challenges, including training judgement, the issue of data contamination, fairness in model comparison, and data diversity. We hope that our survey, novel metric, and utility law will foster mutual advancement in both evaluation and mechanism interpretability. Our code can be found at https://github.com/ALEX-nlp/MUI-Eva."
      },
      {
        "id": "oai:arXiv.org:2504.07441v1",
        "title": "WS-DETR: Robust Water Surface Object Detection through Vision-Radar Fusion with Detection Transformer",
        "link": "https://arxiv.org/abs/2504.07441",
        "author": "Huilin Yin, Pengyu Wang, Senmao Li, Jun Yan, Daniel Watzenig",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07441v1 Announce Type: new \nAbstract: Robust object detection for Unmanned Surface Vehicles (USVs) in complex water environments is essential for reliable navigation and operation. Specifically, water surface object detection faces challenges from blurred edges and diverse object scales. Although vision-radar fusion offers a feasible solution, existing approaches suffer from cross-modal feature conflicts, which negatively affect model robustness. To address this problem, we propose a robust vision-radar fusion model WS-DETR. In particular, we first introduce a Multi-Scale Edge Information Integration (MSEII) module to enhance edge perception and a Hierarchical Feature Aggregator (HiFA) to boost multi-scale object detection in the encoder. Then, we adopt self-moving point representations for continuous convolution and residual connection to efficiently extract irregular features under the scenarios of irregular point cloud data. To further mitigate cross-modal conflicts, an Adaptive Feature Interactive Fusion (AFIF) module is introduced to integrate visual and radar features through geometric alignment and semantic fusion. Extensive experiments on the WaterScenes dataset demonstrate that WS-DETR achieves state-of-the-art (SOTA) performance, maintaining its superiority even under adverse weather and lighting conditions."
      },
      {
        "id": "oai:arXiv.org:2504.07448v1",
        "title": "LoRI: Reducing Cross-Task Interference in Multi-Task Low-Rank Adaptation",
        "link": "https://arxiv.org/abs/2504.07448",
        "author": "Juzheng Zhang, Jiacheng You, Ashwinee Panda, Tom Goldstein",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07448v1 Announce Type: new \nAbstract: Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning (PEFT) method for Large Language Models (LLMs), yet it still incurs notable overhead and suffers from parameter interference in multi-task scenarios. We propose LoRA with Reduced Interference (LoRI), a simple yet effective approach that freezes the projection matrices $A$ as random projections and sparsifies the matrices $B$ using task-specific masks. This design substantially reduces the number of trainable parameters while maintaining strong task performance. Moreover, LoRI minimizes cross-task interference in adapter merging by leveraging the orthogonality between adapter subspaces, and supports continual learning by using sparsity to mitigate catastrophic forgetting. Extensive experiments across natural language understanding, mathematical reasoning, code generation, and safety alignment tasks demonstrate that LoRI outperforms full fine-tuning and existing PEFT methods, while using up to 95% fewer trainable parameters than LoRA. In multi-task experiments, LoRI enables effective adapter merging and continual learning with reduced cross-task interference. Code is available at: https://github.com/juzhengz/LoRI"
      },
      {
        "id": "oai:arXiv.org:2504.07454v1",
        "title": "How Can Objects Help Video-Language Understanding?",
        "link": "https://arxiv.org/abs/2504.07454",
        "author": "Zitian Tang, Shijie Wang, Junho Cho, Jaewook Yoo, Chen Sun",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07454v1 Announce Type: new \nAbstract: How multimodal large language models (MLLMs) perceive the visual world remains a mystery. To one extreme, object and relation modeling may be implicitly implemented with inductive biases, for example by treating objects as tokens. To the other extreme, empirical results reveal the surprising finding that simply performing visual captioning, which tends to ignore spatial configuration of the objects, serves as a strong baseline for video understanding. We aim to answer the question: how can objects help video-language understanding in MLLMs? We tackle the question from the object representation and adaptation perspectives. Specifically, we investigate the trade-off between representation expressiveness (e.g., distributed versus symbolic) and integration difficulty (e.g., data-efficiency when learning the adapters). Through extensive evaluations on five video question answering datasets, we confirm that explicit integration of object-centric representation remains necessary, and the symbolic objects can be most easily integrated while being performant for question answering. We hope our findings can encourage the community to explore the explicit integration of perception modules into MLLM design. Our code and models will be publicly released."
      },
      {
        "id": "oai:arXiv.org:2504.07459v1",
        "title": "Beyond LLMs: A Linguistic Approach to Causal Graph Generation from Narrative Texts",
        "link": "https://arxiv.org/abs/2504.07459",
        "author": "Zehan Li, Ruhua Pan, Xinyu Pi",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07459v1 Announce Type: new \nAbstract: We propose a novel framework for generating causal graphs from narrative texts, bridging high-level causality and detailed event-specific relationships. Our method first extracts concise, agent-centered vertices using large language model (LLM)-based summarization. We introduce an \"Expert Index,\" comprising seven linguistically informed features, integrated into a Situation-Task-Action-Consequence (STAC) classification model. This hybrid system, combining RoBERTa embeddings with the Expert Index, achieves superior precision in causal link identification compared to pure LLM-based approaches. Finally, a structured five-iteration prompting process refines and constructs connected causal graphs. Experiments on 100 narrative chapters and short stories demonstrate that our approach consistently outperforms GPT-4o and Claude 3.5 in causal graph quality, while maintaining readability. The open-source tool provides an interpretable, efficient solution for capturing nuanced causal chains in narratives."
      },
      {
        "id": "oai:arXiv.org:2504.07462v1",
        "title": "Learning Universal Features for Generalizable Image Forgery Localization",
        "link": "https://arxiv.org/abs/2504.07462",
        "author": "Hengrun Zhao, Yunzhi Zhuge, Yifan Wang, Lijun Wang, Huchuan Lu, Yu Zeng",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07462v1 Announce Type: new \nAbstract: In recent years, advanced image editing and generation methods have rapidly evolved, making detecting and locating forged image content increasingly challenging. Most existing image forgery detection methods rely on identifying the edited traces left in the image. However, because the traces of different forgeries are distinct, these methods can identify familiar forgeries included in the training data but struggle to handle unseen ones. In response, we present an approach for Generalizable Image Forgery Localization (GIFL). Once trained, our model can detect both seen and unseen forgeries, providing a more practical and efficient solution to counter false information in the era of generative AI. Our method focuses on learning general features from the pristine content rather than traces of specific forgeries, which are relatively consistent across different types of forgeries and therefore can be used as universal features to locate unseen forgeries. Additionally, as existing image forgery datasets are still dominated by traditional hand-crafted forgeries, we construct a new dataset consisting of images edited by various popular deep generative image editing methods to further encourage research in detecting images manipulated by deep generative models. Extensive experimental results show that the proposed approach outperforms state-of-the-art methods in the detection of unseen forgeries and also demonstrates competitive results for seen forgeries. The code and dataset are available at https://github.com/ZhaoHengrun/GIFL."
      },
      {
        "id": "oai:arXiv.org:2504.07465v1",
        "title": "Multi-Modal Data Fusion for Moisture Content Prediction in Apple Drying",
        "link": "https://arxiv.org/abs/2504.07465",
        "author": "Shichen Li, Chenhui Shao",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07465v1 Announce Type: new \nAbstract: Fruit drying is widely used in food manufacturing to reduce product moisture, ensure product safety, and extend product shelf life. Accurately predicting final moisture content (MC) is critically needed for quality control of drying processes. State-of-the-art methods can build deterministic relationships between process parameters and MC, but cannot adequately account for inherent process variabilities that are ubiquitous in fruit drying. To address this gap, this paper presents a novel multi-modal data fusion framework to effectively fuse two modalities of data: tabular data (process parameters) and high-dimensional image data (images of dried apple slices) to enable accurate MC prediction. The proposed modeling architecture permits flexible adjustment of information portion from tabular and image data modalities. Experimental validation shows that the multi-modal approach improves predictive accuracy substantially compared to state-of-the-art methods. The proposed method reduces root-mean-squared errors by 19.3%, 24.2%, and 15.2% over tabular-only, image-only, and standard tabular-image fusion models, respectively. Furthermore, it is demonstrated that our method is robust in varied tabular-image ratios and capable of effectively capturing inherent small-scale process variabilities. The proposed framework is extensible to a variety of other drying technologies."
      },
      {
        "id": "oai:arXiv.org:2504.07467v1",
        "title": "Defense against Prompt Injection Attacks via Mixture of Encodings",
        "link": "https://arxiv.org/abs/2504.07467",
        "author": "Ruiyi Zhang, David Sullivan, Kyle Jackson, Pengtao Xie, Mei Chen",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07467v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have emerged as a dominant approach for a wide range of NLP tasks, with their access to external information further enhancing their capabilities. However, this introduces new vulnerabilities, known as prompt injection attacks, where external content embeds malicious instructions that manipulate the LLM's output. Recently, the Base64 defense has been recognized as one of the most effective methods for reducing success rate of prompt injection attacks. Despite its efficacy, this method can degrade LLM performance on certain NLP tasks. To address this challenge, we propose a novel defense mechanism: mixture of encodings, which utilizes multiple character encodings, including Base64. Extensive experimental results show that our method achieves one of the lowest attack success rates under prompt injection attacks, while maintaining high performance across all NLP tasks, outperforming existing character encoding-based defense methods. This underscores the effectiveness of our mixture of encodings strategy for both safety and task performance metrics."
      },
      {
        "id": "oai:arXiv.org:2504.07470v1",
        "title": "Transformer-Based Temporal Information Extraction and Application: A Review",
        "link": "https://arxiv.org/abs/2504.07470",
        "author": "Xin Su, Phillip Howard, Steven Bethard",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07470v1 Announce Type: new \nAbstract: Temporal information extraction (IE) aims to extract structured temporal information from unstructured text, thereby uncovering the implicit timelines within. This technique is applied across domains such as healthcare, newswire, and intelligence analysis, aiding models in these areas to perform temporal reasoning and enabling human users to grasp the temporal structure of text. Transformer-based pre-trained language models have produced revolutionary advancements in natural language processing, demonstrating exceptional performance across a multitude of tasks. Despite the achievements garnered by Transformer-based approaches in temporal IE, there is a lack of comprehensive reviews on these endeavors. In this paper, we aim to bridge this gap by systematically summarizing and analyzing the body of work on temporal IE using Transformers while highlighting potential future research directions."
      },
      {
        "id": "oai:arXiv.org:2504.07471v1",
        "title": "Traversal Learning Coordination For Lossless And Efficient Distributed Learning",
        "link": "https://arxiv.org/abs/2504.07471",
        "author": "Erdenebileg Batbaatar, Jeonggeol Kim, Yongcheol Kim, Young Yoon",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07471v1 Announce Type: new \nAbstract: In this paper, we introduce Traversal Learning (TL), a novel approach designed to address the problem of decreased quality encountered in popular distributed learning (DL) paradigms such as Federated Learning (FL), Split Learning (SL), and SplitFed Learning (SFL). Traditional FL experiences from an accuracy drop during aggregation due to its averaging function, while SL and SFL face increased loss due to the independent gradient updates on each split network. TL adopts a unique strategy where the model traverses the nodes during forward propagation (FP) and performs backward propagation (BP) on the orchestrator, effectively implementing centralized learning (CL) principles within a distributed environment. The orchestrator is tasked with generating virtual batches and planning the sequential node visits of the model during FP, aligning them with the ordered index of the data within these batches. We conducted experiments on six datasets representing diverse characteristics across various domains. Our evaluation demonstrates that TL is on par with classic CL approaches in terms of accurate inference, thereby offering a viable and robust solution for DL tasks. TL outperformed other DL methods and improved accuracy by 7.85% for independent and identically distributed (IID) datasets, macro F1-score by 1.06% for non-IID datasets, accuracy by 2.60% for text classification, and AUC by 3.88% and 4.54% for medical and financial datasets, respectively. By effectively preserving data privacy while maintaining performance, TL represents a significant advancement in DL methodologies."
      },
      {
        "id": "oai:arXiv.org:2504.07476v1",
        "title": "CMEdataset Advancing China Map Detection and Standardization with Digital Image Resources",
        "link": "https://arxiv.org/abs/2504.07476",
        "author": "Yan Xu, Zhenqiang Zhang, Zhiwei Zhou, Liting Geng, Yue Li, Jintao Li",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07476v1 Announce Type: new \nAbstract: Digital images of Chinas maps play a crucial role in map detection, particularly in ensuring national sovereignty, territorial integrity, and map compliance. However, there is currently no publicly available dataset specifically dedicated to problematic maps the CME dataset. Existing datasets primarily focus on general map data and are insufficient for effectively identifying complex issues such as national boundary misrepresentations, missing elements, and blurred boundaries. Therefore, this study creates a Problematic Map dataset that covers five key problem areas, aiming to provide diverse samples for problematic map detection technologies, support high-precision map compliance detection, and enhance map data quality and timeliness. This dataset not only provides essential resources for map compliance, national security monitoring, and map updates, but also fosters innovation and application of related technologies."
      },
      {
        "id": "oai:arXiv.org:2504.07480v1",
        "title": "Echoes of Disagreement: Measuring Disparity in Social Consensus",
        "link": "https://arxiv.org/abs/2504.07480",
        "author": "Marios Papachristou, Jon Kleinberg",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07480v1 Announce Type: new \nAbstract: Public discourse and opinions stem from multiple social groups. Each group has beliefs about a topic (such as vaccination, abortion, gay marriage, etc.), and opinions are exchanged and blended to produce consensus. A particular measure of interest corresponds to measuring the influence of each group on the consensus and the disparity between groups on the extent to which they influence the consensus. In this paper, we study and give provable algorithms for optimizing the disparity under the DeGroot or the Friedkin-Johnsen models of opinion dynamics. Our findings provide simple poly-time algorithms to optimize disparity for most cases, fully characterize the instances that optimize disparity, and show how simple interventions such as contracting vertices or adding links affect disparity. Finally, we test our developed algorithms in a variety of real-world datasets."
      },
      {
        "id": "oai:arXiv.org:2504.07490v1",
        "title": "Geological Inference from Textual Data using Word Embeddings",
        "link": "https://arxiv.org/abs/2504.07490",
        "author": "Nanmanas Linphrachaya, Irving G\\'omez-M\\'endez, Adil Siripatana",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07490v1 Announce Type: new \nAbstract: This research explores the use of Natural Language Processing (NLP) techniques to locate geological resources, with a specific focus on industrial minerals. By using word embeddings trained with the GloVe model, we extract semantic relationships between target keywords and a corpus of geological texts. The text is filtered to retain only words with geographical significance, such as city names, which are then ranked by their cosine similarity to the target keyword. Dimensional reduction techniques, including Principal Component Analysis (PCA), Autoencoder, Variational Autoencoder (VAE), and VAE with Long Short-Term Memory (VAE-LSTM), are applied to enhance feature extraction and improve the accuracy of semantic relations.\n  For benchmarking, we calculate the proximity between the ten cities most semantically related to the target keyword and identified mine locations using the haversine equation. The results demonstrate that combining NLP with dimensional reduction techniques provides meaningful insights into the spatial distribution of natural resources. Although the result shows to be in the same region as the supposed location, the accuracy has room for improvement."
      },
      {
        "id": "oai:arXiv.org:2504.07491v1",
        "title": "Kimi-VL Technical Report",
        "link": "https://arxiv.org/abs/2504.07491",
        "author": "Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, Congcong Wang, Dehao Zhang, Dikang Du, Dongliang Wang, Enming Yuan, Enzhe Lu, Fang Li, Flood Sung, Guangda Wei, Guokun Lai, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haoning Wu, Haotian Yao, Haoyu Lu, Heng Wang, Hongcheng Gao, Huabin Zheng, Jiaming Li, Jianlin Su, Jianzhou Wang, Jiaqi Deng, Jiezhong Qiu, Jin Xie, Jinhong Wang, Jingyuan Liu, Junjie Yan, Kun Ouyang, Liang Chen, Lin Sui, Longhui Yu, Mengfan Dong, Mengnan Dong, Nuo Xu, Pengyu Cheng, Qizheng Gu, Runjie Zhou, Shaowei Liu, Sihan Cao, Tao Yu, Tianhui Song, Tongtong Bai, Wei Song, Weiran He, Weixiao Huang, Weixin Xu, Xiaokun Yuan, Xingcheng Yao, Xingzhe Wu, Xinxing Zu, Xinyu Zhou, Xinyuan Wang, Y. Charles, Yan Zhong, Yang Li, Yangyang Hu, Yanru Chen, Yejie Wang, Yibo Liu, Yibo Miao, Yidao Qin, Yimin Chen, Yiping Bao, Yiqin Wang, Yongsheng Kang, Yuanxin Liu, Yulun Du, Yuxin Wu, Yuzhi Wang, Yuzi Yan, Zaida Zhou, Zhaowei Li, Zhejun Jiang, Zheng Zhang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Zijia Zhao, Ziwei Chen",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07491v1 Announce Type: new \nAbstract: We present Kimi-VL, an efficient open-source Mixture-of-Experts (MoE) vision-language model (VLM) that offers advanced multimodal reasoning, long-context understanding, and strong agent capabilities - all while activating only 2.8B parameters in its language decoder (Kimi-VL-A3B). Kimi-VL demonstrates strong performance across challenging domains: as a general-purpose VLM, Kimi-VL excels in multi-turn agent tasks (e.g., OSWorld), matching flagship models. Furthermore, it exhibits remarkable capabilities across diverse challenging vision language tasks, including college-level image and video comprehension, OCR, mathematical reasoning, and multi-image understanding. In comparative evaluations, it effectively competes with cutting-edge efficient VLMs such as GPT-4o-mini, Qwen2.5-VL-7B, and Gemma-3-12B-IT, while surpassing GPT-4o in several key domains. Kimi-VL also advances in processing long contexts and perceiving clearly. With a 128K extended context window, Kimi-VL can process diverse long inputs, achieving impressive scores of 64.5 on LongVideoBench and 35.1 on MMLongBench-Doc. Its native-resolution vision encoder, MoonViT, further allows it to see and understand ultra-high-resolution visual inputs, achieving 83.2 on InfoVQA and 34.5 on ScreenSpot-Pro, while maintaining lower computational cost for common tasks. Building upon Kimi-VL, we introduce an advanced long-thinking variant: Kimi-VL-Thinking. Developed through long chain-of-thought (CoT) supervised fine-tuning (SFT) and reinforcement learning (RL), this model exhibits strong long-horizon reasoning capabilities. It achieves scores of 61.7 on MMMU, 36.8 on MathVision, and 71.3 on MathVista while maintaining the compact 2.8B activated LLM parameters, setting a new standard for efficient multimodal thinking models. Code and models are publicly accessible at https://github.com/MoonshotAI/Kimi-VL."
      },
      {
        "id": "oai:arXiv.org:2504.07494v1",
        "title": "Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM Inference Serving",
        "link": "https://arxiv.org/abs/2504.07494",
        "author": "Shihong Gao, Xin Zhang, Yanyan Shen, Lei Chen",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07494v1 Announce Type: new \nAbstract: Large language model (LLM) inference serving systems are essential to various LLM-based applications. As demand for LLM services continues to grow, scaling these systems to handle high request rates while meeting latency Service-Level Objectives (SLOs), referred to as effective throughput, becomes critical. However, existing systems often struggle to improve effective throughput, primarily due to a significant decline in Time To First Token (TTFT) SLO attainment. We identify two major causes of this bottleneck: (1) memory-intensive KV cache that limits batch size expansion under GPU memory constraints, and (2) rigid batch composition enforced by the default First-Come-First-Serve scheduling policy. In this paper, we introduce Apt-Serve, a scalable framework designed to enhance effective throughput in LLM inference serving. Apt-Serve features a new hybrid cache scheme that combines KV cache with a memory-efficient hidden cache for reusable input hidden state vectors, allowing large batch sizes and improving request concurrency. Based on the hybrid cache, Apt-Serve employs an adaptive runtime scheduling mechanism that dynamically optimizes batch composition. We formally define the adaptive scheduling optimization problem and propose an efficient algorithm with theoretical guarantees. Extensive evaluations on three real-world datasets and LLMs ranging from 13B to 66B parameters demonstrate that Apt-Serve achieves up to 8.8x improvement in effective throughput compared to the state-of-the-art inference serving systems."
      },
      {
        "id": "oai:arXiv.org:2504.07503v1",
        "title": "Event Signal Filtering via Probability Flux Estimation",
        "link": "https://arxiv.org/abs/2504.07503",
        "author": "Jinze Chen, Wei Zhai, Yang Cao, Bin Li, Zheng-Jun Zha",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07503v1 Announce Type: new \nAbstract: Events offer a novel paradigm for capturing scene dynamics via asynchronous sensing, but their inherent randomness often leads to degraded signal quality. Event signal filtering is thus essential for enhancing fidelity by reducing this internal randomness and ensuring consistent outputs across diverse acquisition conditions. Unlike traditional time series that rely on fixed temporal sampling to capture steady-state behaviors, events encode transient dynamics through polarity and event intervals, making signal modeling significantly more complex. To address this, the theoretical foundation of event generation is revisited through the lens of diffusion processes. The state and process information within events is modeled as continuous probability flux at threshold boundaries of the underlying irradiance diffusion. Building on this insight, a generative, online filtering framework called Event Density Flow Filter (EDFilter) is introduced. EDFilter estimates event correlation by reconstructing the continuous probability flux from discrete events using nonparametric kernel smoothing, and then resamples filtered events from this flux. To optimize fidelity over time, spatial and temporal kernels are employed in a time-varying optimization framework. A fast recursive solver with O(1) complexity is proposed, leveraging state-space models and lookup tables for efficient likelihood computation. Furthermore, a new real-world benchmark Rotary Event Dataset (RED) is released, offering microsecond-level ground truth irradiance for full-reference event filtering evaluation. Extensive experiments validate EDFilter's performance across tasks like event filtering, super-resolution, and direct event-based blob tracking. Significant gains in downstream applications such as SLAM and video reconstruction underscore its robustness and effectiveness."
      },
      {
        "id": "oai:arXiv.org:2504.07513v1",
        "title": "GPT Carry-On: Training Foundation Model for Customization Could Be Simple, Scalable and Affordable",
        "link": "https://arxiv.org/abs/2504.07513",
        "author": "Jianqiao Wangni",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07513v1 Announce Type: new \nAbstract: Modern large language foundation models (LLM) have now entered the daily lives of millions of users. We ask a natural question whether it is possible to customize LLM for every user or every task. From system and industrial economy consideration, general continue-training or fine-tuning still require substantial computation and memory of training GPU nodes, whereas most inference nodes under deployment, possibly with lower-end GPUs, are configured to make forward pass fastest possible. We propose a framework to take full advantages of existing LLMs and systems of online service. We train an additional branch of transformer blocks on the final-layer embedding of pretrained LLMs, which is the base, then a carry-on module merge the base models to compose a customized LLM. We can mix multiple layers, or multiple LLMs specialized in different domains such as chat, coding, math, to form a new mixture of LLM that best fit a new task. As the base model don't need to update parameters, we are able to outsource most computation of the training job on inference nodes, and only train a lightweight carry-on on training nodes, where we consume less than 1GB GPU memory to train a 100M carry-on layer on 30B LLM. We tested Qwen and DeepSeek opensourced models for continue-pretraining and got faster loss convergence. We use it to improve solving math questions with extremely small computation and model size, with 1000 data samples of chain-of-thoughts, and as small as 1 MB parameters of two layer layer carry-on, and the results are promising."
      },
      {
        "id": "oai:arXiv.org:2504.07519v1",
        "title": "VideoExpert: Augmented LLM for Temporal-Sensitive Video Understanding",
        "link": "https://arxiv.org/abs/2504.07519",
        "author": "Henghao Zhao, Ge-Peng Ji, Rui Yan, Huan Xiong, Zechao Li",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07519v1 Announce Type: new \nAbstract: The core challenge in video understanding lies in perceiving dynamic content changes over time. However, multimodal large language models struggle with temporal-sensitive video tasks, which requires generating timestamps to mark the occurrence of specific events. Existing strategies require MLLMs to generate absolute or relative timestamps directly. We have observed that those MLLMs tend to rely more on language patterns than visual cues when generating timestamps, affecting their performance. To address this problem, we propose VideoExpert, a general-purpose MLLM suitable for several temporal-sensitive video tasks. Inspired by the expert concept, VideoExpert integrates two parallel modules: the Temporal Expert and the Spatial Expert. The Temporal Expert is responsible for modeling time sequences and performing temporal grounding. It processes high-frame-rate yet compressed tokens to capture dynamic variations in videos and includes a lightweight prediction head for precise event localization. The Spatial Expert focuses on content detail analysis and instruction following. It handles specially designed spatial tokens and language input, aiming to generate content-related responses. These two experts collaborate seamlessly via a special token, ensuring coordinated temporal grounding and content generation. Notably, the Temporal and Spatial Experts maintain independent parameter sets. By offloading temporal grounding from content generation, VideoExpert prevents text pattern biases in timestamp predictions. Moreover, we introduce a Spatial Compress module to obtain spatial tokens. This module filters and compresses patch tokens while preserving key information, delivering compact yet detail-rich input for the Spatial Expert. Extensive experiments demonstrate the effectiveness and versatility of the VideoExpert."
      },
      {
        "id": "oai:arXiv.org:2504.07522v1",
        "title": "Adversarial Subspace Generation for Outlier Detection in High-Dimensional Data",
        "link": "https://arxiv.org/abs/2504.07522",
        "author": "Jose Cribeiro-Ramallo, Federico Matteucci, Paul Enciu, Alexander Jenke, Vadim Arzamasov, Thorsten Strufe, Klemens B\\\"ohm",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07522v1 Announce Type: new \nAbstract: Outlier detection in high-dimensional tabular data is challenging since data is often distributed across multiple lower-dimensional subspaces -- a phenomenon known as the Multiple Views effect (MV). This effect led to a large body of research focused on mining such subspaces, known as subspace selection. However, as the precise nature of the MV effect was not well understood, traditional methods had to rely on heuristic-driven search schemes that struggle to accurately capture the true structure of the data. Properly identifying these subspaces is critical for unsupervised tasks such as outlier detection or clustering, where misrepresenting the underlying data structure can hinder the performance. We introduce Myopic Subspace Theory (MST), a new theoretical framework that mathematically formulates the Multiple Views effect and writes subspace selection as a stochastic optimization problem. Based on MST, we introduce V-GAN, a generative method trained to solve such an optimization problem. This approach avoids any exhaustive search over the feature space while ensuring that the intrinsic data structure is preserved. Experiments on 42 real-world datasets show that using V-GAN subspaces to build ensemble methods leads to a significant increase in one-class classification performance -- compared to existing subspace selection, feature selection, and embedding methods. Further experiments on synthetic data show that V-GAN identifies subspaces more accurately while scaling better than other relevant subspace selection methods. These results confirm the theoretical guarantees of our approach and also highlight its practical viability in high-dimensional settings."
      },
      {
        "id": "oai:arXiv.org:2504.07524v1",
        "title": "DGOcc: Depth-aware Global Query-based Network for Monocular 3D Occupancy Prediction",
        "link": "https://arxiv.org/abs/2504.07524",
        "author": "Xu Zhao, Pengju Zhang, Bo Liu, Yihong Wu",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07524v1 Announce Type: new \nAbstract: Monocular 3D occupancy prediction, aiming to predict the occupancy and semantics within interesting regions of 3D scenes from only 2D images, has garnered increasing attention recently for its vital role in 3D scene understanding. Predicting the 3D occupancy of large-scale outdoor scenes from 2D images is ill-posed and resource-intensive. In this paper, we present \\textbf{DGOcc}, a \\textbf{D}epth-aware \\textbf{G}lobal query-based network for monocular 3D \\textbf{Occ}upancy prediction. We first explore prior depth maps to extract depth context features that provide explicit geometric information for the occupancy network. Then, in order to fully exploit the depth context features, we propose a Global Query-based (GQ) Module. The cooperation of attention mechanisms and scale-aware operations facilitates the feature interaction between images and 3D voxels. Moreover, a Hierarchical Supervision Strategy (HSS) is designed to avoid upsampling the high-dimension 3D voxel features to full resolution, which mitigates GPU memory utilization and time cost. Extensive experiments on SemanticKITTI and SSCBench-KITTI-360 datasets demonstrate that the proposed method achieves the best performance on monocular semantic occupancy prediction while reducing GPU and time overhead."
      },
      {
        "id": "oai:arXiv.org:2504.07527v1",
        "title": "Supervised Optimism Correction: Be Confident When LLMs Are Sure",
        "link": "https://arxiv.org/abs/2504.07527",
        "author": "Junjie Zhang, Rushuai Yang, Shunyu Liu, Ting-En Lin, Fei Huang, Yi Chen, Yongbin Li, Dacheng Tao",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07527v1 Announce Type: new \nAbstract: In this work, we establish a novel theoretical connection between supervised fine-tuning and offline reinforcement learning under the token-level Markov decision process, revealing that large language models indeed learn an implicit $Q$-function for inference. Through this theoretical lens, we demonstrate that the widely used beam search method suffers from unacceptable over-optimism, where inference errors are inevitably amplified due to inflated $Q$-value estimations of suboptimal steps. To address this limitation, we propose Supervised Optimism Correction(SOC), which introduces a simple yet effective auxiliary loss for token-level $Q$-value estimations during supervised fine-tuning. Specifically, the auxiliary loss employs implicit value regularization to boost model confidence in expert-demonstrated responses, thereby suppressing over-optimism toward insufficiently supervised responses. Extensive experiments on mathematical reasoning benchmarks, including GSM8K, MATH, and GAOKAO, showcase the superiority of the proposed SOC with beam search across a series of open-source models."
      },
      {
        "id": "oai:arXiv.org:2504.07532v1",
        "title": "AI-Slop to AI-Polish? Aligning Language Models through Edit-Based Writing Rewards and Test-time Computation",
        "link": "https://arxiv.org/abs/2504.07532",
        "author": "Tuhin Chakrabarty, Philippe Laban, Chien-Sheng Wu",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07532v1 Announce Type: new \nAbstract: AI-generated text is proliferating across domains, from creative writing and journalism to marketing content and scientific articles. Models can follow user-provided instructions to generate coherent and grammatically correct outputs but in this work, we study a more fundamental question: how do we evaluate and improve the writing quality of AI-generated text? Writing quality assessment has received less attention from the community, in part because it is fundamentally subjective and requires expertise. We first introduce the Writing Quality Benchmark (WQ) by consolidating five writing-preference datasets into 4,729 writing quality judgments. Our experiments show that competitive baselines, including state-of-the-art LLMs that excel at reasoning tasks, barely outperform random baselines on WQ. We then train specialized Writing Quality Reward Models (WQRM) of various sizes for writing quality assessment that demonstrate strong generalization on four out-of-distribution test sets and 74% accuracy on the WQ benchmark. To further show WQRM's practical benefits during inference, we leverage additional test-time compute to generate and rank multiple candidate revisions, allowing us to select higher-quality outputs from an initial draft. Human evaluation with 9 experienced writers confirm that WQRM-based selection produces writing samples preferred by experts 66% overall, and 72.2% when the reward gap is larger than 1 point. We release our datasets and models to encourage community engagement with writing quality assessment and development of AI writing systems better aligned with human preferences."
      },
      {
        "id": "oai:arXiv.org:2504.07540v1",
        "title": "PoGO: A Scalable Proof of Useful Work via Quantized Gradient Descent and Merkle Proofs",
        "link": "https://arxiv.org/abs/2504.07540",
        "author": "Jos\\'e I. Orlicki",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07540v1 Announce Type: new \nAbstract: We present a design called \\emph{Proof of Gradient Optimization} (PoGO) for blockchain consensus, where miners produce verifiable evidence of training large-scale machine-learning models. Building on previous work, we incorporate \\emph{quantized gradients} (4-bit precision) to reduce storage and computation requirements, while still preserving the ability of verifiers to check that real progress has been made on lowering the model's loss. Additionally, we employ Merkle proofs over the full 32-bit model to handle large parameter sets and to enable random leaf checks with minimal on-chain data. We illustrate these ideas using GPT-3 (175B parameters) as a reference example and also refer to smaller but high-performance models (e.g., \\emph{Gemma~3} with 27B parameters). We provide an empirical cost analysis showing that verification is significantly cheaper than training, thanks in part to quantization and sampling. We also discuss the necessity of longer block times (potentially hours) when incorporating meaningful training steps, the trade-offs when using specialized GPU hardware, and how binary diffs may incrementally optimize updates. Finally, we note that fine-tuning can be handled in a similar manner, merely changing the dataset and the manner of sampling but preserving the overall verification flow. Our protocol allows verifiers to issue either \\emph{positive} or \\emph{negative} attestations; these are aggregated at finalization to either confirm the update or slash the miner."
      },
      {
        "id": "oai:arXiv.org:2504.07542v1",
        "title": "SydneyScapes: Image Segmentation for Australian Environments",
        "link": "https://arxiv.org/abs/2504.07542",
        "author": "Hongyu Lyu, Julie Stephany Berrio, Mao Shan, Stewart Worrall",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07542v1 Announce Type: new \nAbstract: Autonomous Vehicles (AVs) are being partially deployed and tested across various global locations, including China, the USA, Germany, France, Japan, Korea, and the UK, but with limited demonstrations in Australia. The integration of machine learning (ML) into AV perception systems highlights the need for locally labelled datasets to develop and test algorithms in specific environments. To address this, we introduce SydneyScapes - a dataset tailored for computer vision tasks of image semantic, instance, and panoptic segmentation. This dataset, collected from Sydney and surrounding cities in New South Wales (NSW), Australia, consists of 756 images with high-quality pixel-level annotations. It is designed to assist AV industry and researchers by providing annotated data and tools for algorithm development, testing, and deployment in the Australian context. Additionally, we offer benchmarking results using state-of-the-art algorithms to establish reference points for future research and development. The dataset is publicly available at https://hdl.handle.net/2123/33051."
      },
      {
        "id": "oai:arXiv.org:2504.07549v1",
        "title": "STeP: A General and Scalable Framework for Solving Video Inverse Problems with Spatiotemporal Diffusion Priors",
        "link": "https://arxiv.org/abs/2504.07549",
        "author": "Bingliang Zhang, Zihui Wu, Berthy T. Feng, Yang Song, Yisong Yue, Katherine L. Bouman",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07549v1 Announce Type: new \nAbstract: We study how to solve general Bayesian inverse problems involving videos using diffusion model priors. While it is desirable to use a video diffusion prior to effectively capture complex temporal relationships, due to the computational and data requirements of training such a model, prior work has instead relied on image diffusion priors on single frames combined with heuristics to enforce temporal consistency. However, these approaches struggle with faithfully recovering the underlying temporal relationships, particularly for tasks with high temporal uncertainty. In this paper, we demonstrate the feasibility of practical and accessible spatiotemporal diffusion priors by fine-tuning latent video diffusion models from pretrained image diffusion models using limited videos in specific domains. Leveraging this plug-and-play spatiotemporal diffusion prior, we introduce a general and scalable framework for solving video inverse problems. We then apply our framework to two challenging scientific video inverse problems--black hole imaging and dynamic MRI. Our framework enables the generation of diverse, high-fidelity video reconstructions that not only fit observations but also recover multi-modal solutions. By incorporating a spatiotemporal diffusion prior, we significantly improve our ability to capture complex temporal relationships in the data while also enhancing spatial fidelity."
      },
      {
        "id": "oai:arXiv.org:2504.07556v1",
        "title": "TokenFocus-VQA: Enhancing Text-to-Image Alignment with Position-Aware Focus and Multi-Perspective Aggregations on LVLMs",
        "link": "https://arxiv.org/abs/2504.07556",
        "author": "Zijian Zhang, Xuhui Zheng, Xuecheng Wu, Chong Peng, Xuezhi Cao",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07556v1 Announce Type: new \nAbstract: While text-to-image (T2I) generation models have achieved remarkable progress in recent years, existing evaluation methodologies for vision-language alignment still struggle with the fine-grained semantic matching. Current approaches based on global similarity metrics often overlook critical token-level correspondences between textual descriptions and visual content. To this end, we present TokenFocus-VQA, a novel evaluation framework that leverages Large Vision-Language Models (LVLMs) through visual question answering (VQA) paradigm with position-specific probability optimization. Our key innovation lies in designing a token-aware loss function that selectively focuses on probability distributions at pre-defined vocabulary positions corresponding to crucial semantic elements, enabling precise measurement of fine-grained semantical alignment. The proposed framework further integrates ensemble learning techniques to aggregate multi-perspective assessments from diverse LVLMs architectures, thereby achieving further performance enhancement. Evaluated on the NTIRE 2025 T2I Quality Assessment Challenge Track 1, our TokenFocus-VQA ranks 2nd place (0.8445, only 0.0001 lower than the 1st method) on public evaluation and 2nd place (0.8426) on the official private test set, demonstrating superiority in capturing nuanced text-image correspondences compared to conventional evaluation methods."
      },
      {
        "id": "oai:arXiv.org:2504.07557v1",
        "title": "Using LLMs for Analyzing AIS Data",
        "link": "https://arxiv.org/abs/2504.07557",
        "author": "Gaspard Mertends, Gilles Dejaegere, Mahmoud Sakr",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07557v1 Announce Type: new \nAbstract: Recent research in Large Language Models (LLMs), has had a profound impact across various fields, including mobility data science. This paper explores the and experiment with different approaches to using LLMs for analyzing AIS data. We propose a set of carefully designed queries to assess the reasoning capabilities of LLMs in this kind of tasks. Further, we experiment with four different methods: (1) using LLMs as a natural language interface to a spatial database, (2) reasoning on raw data, (3) reasoning on compressed trajectories, and (4) reasoning on semantic trajectories. We investigate the strengths and weaknesses for the four methods, and discuss the findings. The goal is to provide valuable insights for both researchers and practitioners on selecting the most appropriate LLM-based method depending on their specific data analysis objectives."
      },
      {
        "id": "oai:arXiv.org:2504.07566v1",
        "title": "Diffusion Transformers for Tabular Data Time Series Generation",
        "link": "https://arxiv.org/abs/2504.07566",
        "author": "Fabrizio Garuti, Enver Sangineto, Simone Luetto, Lorenzo Forni, Rita Cucchiara",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07566v1 Announce Type: new \nAbstract: Tabular data generation has recently attracted a growing interest due to its different application scenarios. However, generating time series of tabular data, where each element of the series depends on the others, remains a largely unexplored domain. This gap is probably due to the difficulty of jointly solving different problems, the main of which are the heterogeneity of tabular data (a problem common to non-time-dependent approaches) and the variable length of a time series. In this paper, we propose a Diffusion Transformers (DiTs) based approach for tabular data series generation. Inspired by the recent success of DiTs in image and video generation, we extend this framework to deal with heterogeneous data and variable-length sequences. Using extensive experiments on six datasets, we show that the proposed approach outperforms previous work by a large margin."
      },
      {
        "id": "oai:arXiv.org:2504.07567v1",
        "title": "Benchmarking Image Embeddings for E-Commerce: Evaluating Off-the Shelf Foundation Models, Fine-Tuning Strategies and Practical Trade-offs",
        "link": "https://arxiv.org/abs/2504.07567",
        "author": "Urszula Czerwinska, Cenk Bircanoglu, Jeremy Chamoux",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07567v1 Announce Type: new \nAbstract: We benchmark foundation models image embeddings for classification and retrieval in e-Commerce, evaluating their suitability for real-world applications. Our study spans embeddings from pre-trained convolutional and transformer models trained via supervised, self-supervised, and text-image contrastive learning. We assess full fine-tuning and transfer learning (top-tuning) on six diverse e-Commerce datasets: fashion, consumer goods, cars, food, and retail. Results show full fine-tuning consistently performs well, while text-image and self-supervised embeddings can match its performance with less training. While supervised embeddings remain stable across architectures, SSL and contrastive embeddings vary significantly, often benefiting from top-tuning. Top-tuning emerges as an efficient alternative to full fine-tuning, reducing computational costs. We also explore cross-tuning, noting its impact depends on dataset characteristics. Our findings offer practical guidelines for embedding selection and fine-tuning strategies, balancing efficiency and performance."
      },
      {
        "id": "oai:arXiv.org:2504.07583v1",
        "title": "Do LLMs Understand Your Translations? Evaluating Paragraph-level MT with Question Answering",
        "link": "https://arxiv.org/abs/2504.07583",
        "author": "Patrick Fernandes, Sweta Agrawal, Emmanouil Zaranis, Andr\\'e F. T. Martins, Graham Neubig",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07583v1 Announce Type: new \nAbstract: Despite the steady progress in machine translation evaluation, existing automatic metrics struggle to capture how well meaning is preserved beyond sentence boundaries. We posit that reliance on a single intrinsic quality score, trained to mimic human judgments, might be insufficient for evaluating translations of long, complex passages, and a more ``pragmatic'' approach that assesses how accurately key information is conveyed by a translation in context is needed. We introduce TREQA (Translation Evaluation via Question-Answering), a framework that extrinsically evaluates translation quality by assessing how accurately candidate translations answer reading comprehension questions that target key information in the original source or reference texts. In challenging domains that require long-range understanding, such as literary texts, we show that TREQA is competitive with and, in some cases, outperforms state-of-the-art neural and LLM-based metrics in ranking alternative paragraph-level translations, despite never being explicitly optimized to correlate with human judgments. Furthermore, the generated questions and answers offer interpretability: empirical analysis shows that they effectively target translation errors identified by experts in evaluated datasets. Our code is available at https://github.com/deep-spin/treqa"
      },
      {
        "id": "oai:arXiv.org:2504.07594v1",
        "title": "Extending Visual Dynamics for Video-to-Music Generation",
        "link": "https://arxiv.org/abs/2504.07594",
        "author": "Xiaohao Liu, Teng Tu, Yunshan Ma, Tat-Seng Chua",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07594v1 Announce Type: new \nAbstract: Music profoundly enhances video production by improving quality, engagement, and emotional resonance, sparking growing interest in video-to-music generation. Despite recent advances, existing approaches remain limited in specific scenarios or undervalue the visual dynamics. To address these limitations, we focus on tackling the complexity of dynamics and resolving temporal misalignment between video and music representations. To this end, we propose DyViM, a novel framework to enhance dynamics modeling for video-to-music generation. Specifically, we extract frame-wise dynamics features via a simplified motion encoder inherited from optical flow methods, followed by a self-attention module for aggregation within frames. These dynamic features are then incorporated to extend existing music tokens for temporal alignment. Additionally, high-level semantics are conveyed through a cross-attention mechanism, and an annealing tuning strategy benefits to fine-tune well-trained music decoders efficiently, therefore facilitating seamless adaptation. Extensive experiments demonstrate DyViM's superiority over state-of-the-art (SOTA) methods."
      },
      {
        "id": "oai:arXiv.org:2504.07598v1",
        "title": "On Model and Data Scaling for Skeleton-based Self-Supervised Gait Recognition",
        "link": "https://arxiv.org/abs/2504.07598",
        "author": "Adrian Cosma, Andy C\\v{a}trun\\v{a}, Emilian R\\v{a}doi",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07598v1 Announce Type: new \nAbstract: Gait recognition from video streams is a challenging problem in computer vision biometrics due to the subtle differences between gaits and numerous confounding factors. Recent advancements in self-supervised pretraining have led to the development of robust gait recognition models that are invariant to walking covariates. While neural scaling laws have transformed model development in other domains by linking performance to data, model size, and compute, their applicability to gait remains unexplored. In this work, we conduct the first empirical study scaling on skeleton-based self-supervised gait recognition to quantify the effect of data quantity, model size and compute on downstream gait recognition performance. We pretrain multiple variants of GaitPT - a transformer-based architecture - on a dataset of 2.7 million walking sequences collected in the wild. We evaluate zero-shot performance across four benchmark datasets to derive scaling laws for data, model size, and compute. Our findings demonstrate predictable power-law improvements in performance with increased scale and confirm that data and compute scaling significantly influence downstream accuracy. We further isolate architectural contributions by comparing GaitPT with GaitFormer under controlled compute budgets. These results provide practical insights into resource allocation and performance estimation for real-world gait recognition systems."
      },
      {
        "id": "oai:arXiv.org:2504.07603v1",
        "title": "RASMD: RGB And SWIR Multispectral Driving Dataset for Robust Perception in Adverse Conditions",
        "link": "https://arxiv.org/abs/2504.07603",
        "author": "Youngwan Jin, Michal Kovac, Yagiz Nalcakan, Hyeongjin Ju, Hanbin Song, Sanghyeop Yeo, Shiho Kim",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07603v1 Announce Type: new \nAbstract: Current autonomous driving algorithms heavily rely on the visible spectrum, which is prone to performance degradation in adverse conditions like fog, rain, snow, glare, and high contrast. Although other spectral bands like near-infrared (NIR) and long-wave infrared (LWIR) can enhance vision perception in such situations, they have limitations and lack large-scale datasets and benchmarks. Short-wave infrared (SWIR) imaging offers several advantages over NIR and LWIR. However, no publicly available large-scale datasets currently incorporate SWIR data for autonomous driving. To address this gap, we introduce the RGB and SWIR Multispectral Driving (RASMD) dataset, which comprises 100,000 synchronized and spatially aligned RGB-SWIR image pairs collected across diverse locations, lighting, and weather conditions. In addition, we provide a subset for RGB-SWIR translation and object detection annotations for a subset of challenging traffic scenarios to demonstrate the utility of SWIR imaging through experiments on both object detection and RGB-to-SWIR image translation. Our experiments show that combining RGB and SWIR data in an ensemble framework significantly improves detection accuracy compared to RGB-only approaches, particularly in conditions where visible-spectrum sensors struggle. We anticipate that the RASMD dataset will advance research in multispectral imaging for autonomous driving and robust perception systems."
      },
      {
        "id": "oai:arXiv.org:2504.07611v1",
        "title": "Conditional Conformal Risk Adaptation",
        "link": "https://arxiv.org/abs/2504.07611",
        "author": "Rui Luo, Zhixin Zhou",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07611v1 Announce Type: new \nAbstract: Uncertainty quantification is becoming increasingly important in image segmentation, especially for high-stakes applications like medical imaging. While conformal risk control generalizes conformal prediction beyond standard miscoverage to handle various loss functions such as false negative rate, its application to segmentation often yields inadequate conditional risk control: some images experience very high false negative rates while others have negligibly small ones. We develop Conformal Risk Adaptation (CRA), which introduces a new score function for creating adaptive prediction sets that significantly improve conditional risk control for segmentation tasks. We establish a novel theoretical framework that demonstrates a fundamental connection between conformal risk control and conformal prediction through a weighted quantile approach, applicable to any score function. To address the challenge of poorly calibrated probabilities in segmentation models, we introduce a specialized probability calibration framework that enhances the reliability of pixel-wise inclusion estimates. Using these calibrated probabilities, we propose Calibrated Conformal Risk Adaptation (CCRA) and a stratified variant (CCRA-S) that partitions images based on their characteristics and applies group-specific thresholds to further enhance conditional risk control. Our experiments on polyp segmentation demonstrate that all three methods (CRA, CCRA, and CCRA-S) provide valid marginal risk control and deliver more consistent conditional risk control across diverse images compared to standard approaches, offering a principled approach to uncertainty quantification that is particularly valuable for high-stakes and personalized segmentation applications."
      },
      {
        "id": "oai:arXiv.org:2504.07612v1",
        "title": "SaRoHead: A Dataset for Satire Detection in Romanian Multi-Domain News Headlines",
        "link": "https://arxiv.org/abs/2504.07612",
        "author": "Mihnea-Alexandru V\\^irlan, R\\u{a}zvan-Alexandru Sm\\u{a}du, Dumitru-Clementin Cercel",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07612v1 Announce Type: new \nAbstract: The headline is an important part of a news article, influenced by expressiveness and connection to the exposed subject. Although most news outlets aim to present reality objectively, some publications prefer a humorous approach in which stylistic elements of satire, irony, and sarcasm blend to cover specific topics. Satire detection can be difficult because a headline aims to expose the main idea behind a news article. In this paper, we propose SaRoHead, the first corpus for satire detection in Romanian multi-domain news headlines. Our findings show that the clickbait used in some non-satirical headlines significantly influences the model."
      },
      {
        "id": "oai:arXiv.org:2504.07615v1",
        "title": "VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model",
        "link": "https://arxiv.org/abs/2504.07615",
        "author": "Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, Ruochen Xu, Tiancheng Zhao",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07615v1 Announce Type: new \nAbstract: Recently DeepSeek R1 has shown that reinforcement learning (RL) can substantially improve the reasoning capabilities of Large Language Models (LLMs) through a simple yet effective design. The core of R1 lies in its rule-based reward formulation, which leverages tasks with deterministic ground-truth answers to enable precise and stable reward computation. In the visual domain, we similarly observe that a wide range of visual understanding tasks are inherently equipped with well-defined ground-truth annotations. This property makes them naturally compatible with rule-based reward mechanisms. Motivated by this observation, we investigate the extension of R1-style reinforcement learning to Vision-Language Models (VLMs), aiming to enhance their visual reasoning capabilities. To this end, we develop VLM-R1, a dedicated framework designed to harness RL for improving VLMs' performance on general vision-language tasks. Using this framework, we further explore the feasibility of applying RL to visual domain. Experimental results indicate that the RL-based model not only delivers competitive performance on visual understanding tasks but also surpasses Supervised Fine-Tuning (SFT) in generalization ability. Furthermore, we conduct comprehensive ablation studies that uncover a series of noteworthy insights, including the presence of reward hacking in object detection, the emergence of the \"OD aha moment\", the impact of training data quality, and the scaling behavior of RL across different model sizes. Through these analyses, we aim to deepen the understanding of how reinforcement learning enhances the capabilities of vision-language models, and we hope our findings and open-source contributions will support continued progress in the vision-language RL community. Our code and model are available at https://github.com/om-ai-lab/VLM-R1"
      },
      {
        "id": "oai:arXiv.org:2504.07618v1",
        "title": "CTSR: Cartesian tensor-based sparse regression for data-driven discovery of high-dimensional invariant governing equations",
        "link": "https://arxiv.org/abs/2504.07618",
        "author": "Boqian Zhang, Juanmian Lei, Guoyou Sun, Shuaibing Ding, Jian Guo",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07618v1 Announce Type: new \nAbstract: Accurate and concise governing equations are crucial for understanding system dynamics. Recently, data-driven methods such as sparse regression have been employed to automatically uncover governing equations from data, representing a significant shift from traditional first-principles modeling. However, most existing methods focus on scalar equations, limiting their applicability to simple, low-dimensional scenarios, and failing to ensure rotation and reflection invariance without incurring significant computational cost or requiring additional prior knowledge. This paper proposes a Cartesian tensor-based sparse regression (CTSR) technique to accurately and efficiently uncover complex, high-dimensional governing equations while ensuring invariance. Evaluations on two two-dimensional (2D) and two three-dimensional (3D) test cases demonstrate that the proposed method achieves superior accuracy and efficiency compared to the conventional technique."
      },
      {
        "id": "oai:arXiv.org:2504.07624v1",
        "title": "ConceptFormer: Towards Efficient Use of Knowledge-Graph Embeddings in Large Language Models",
        "link": "https://arxiv.org/abs/2504.07624",
        "author": "Joel Barmettler, Abraham Bernstein, Luca Rossetto",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07624v1 Announce Type: new \nAbstract: Retrieval Augmented Generation (RAG) has enjoyed increased attention in the recent past and recent advancements in Large Language Models (LLMs) have highlighted the importance of integrating world knowledge into these systems. Current RAG methodologies often modify the internal architecture of pre-trained language models (PLMs) or rely on textifying knowledge graphs (KGs), which is inefficient in terms of token usage. This paper introduces ConceptFormer, a new approach to augment LLMs with structured knowledge from KGs, such as Wikidata, without altering their internal structure or relying on textual input of KGs. ConceptFormer operates in the LLM embedding vector space, creating and injecting \\emph{concept vectors} that encapsulate the information of the KG nodes directly. Trained in conjunction with a frozen LLM, ConceptFormer generates a comprehensive lookup table that maps KG nodes to their respective concept vectors. The approach aims to enhance the factual recall capabilities of LLMs by enabling them to process these concept vectors natively, thus enriching them with structured world knowledge in an efficient and scalable manner. Our experiments demonstrate that the addition of concept vectors to GPT-2 0.1B substantially increases its factual recall ability (Hit@10) by up to 272\\% when tested on sentences from Wikipedia and up to 348\\% on synthetically generated sentences. Even injecting only a single concept vector into the prompt increases factual recall ability (Hit@10) by up to 213\\% on Wikipedia sentences, significantly outperforming RAG with graph textification while consuming 130x fewer input tokens."
      },
      {
        "id": "oai:arXiv.org:2504.07625v1",
        "title": "Deep Learning Meets Teleconnections: Improving S2S Predictions for European Winter Weather",
        "link": "https://arxiv.org/abs/2504.07625",
        "author": "Philine L. Bommer, Marlene Kretschmer, Fiona R. Spuler, Kirill Bykov, Marina M. -C. H\\\"ohne",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07625v1 Announce Type: new \nAbstract: Predictions on subseasonal-to-seasonal (S2S) timescales--ranging from two weeks to two month--are crucial for early warning systems but remain challenging owing to chaos in the climate system. Teleconnections, such as the stratospheric polar vortex (SPV) and Madden-Julian Oscillation (MJO), offer windows of enhanced predictability, however, their complex interactions remain underutilized in operational forecasting. Here, we developed and evaluated deep learning architectures to predict North Atlantic-European (NAE) weather regimes, systematically assessing the role of remote drivers in improving S2S forecast skill of deep learning models. We implemented (1) a Long Short-term Memory (LSTM) network predicting the NAE regimes of the next six weeks based on previous regimes, (2) an Index-LSTM incorporating SPV and MJO indices, and (3) a ViT-LSTM using a Vision Transformer to directly encode stratospheric wind and tropical outgoing longwave radiation fields. These models are compared with operational hindcasts as well as other AI models. Our results show that leveraging teleconnection information enhances skill at longer lead times. Notably, the ViT-LSTM outperforms ECMWF's subseasonal hindcasts beyond week 4 by improving Scandinavian Blocking (SB) and Atlantic Ridge (AR) predictions. Analysis of high-confidence predictions reveals that NAO-, SB, and AR opportunity forecasts can be associated with SPV variability and MJO phase patterns aligning with established pathways, also indicating new patterns. Overall, our work demonstrates that encoding physically meaningful climate fields can enhance S2S prediction skill, advancing AI-driven subseasonal forecast. Moreover, the experiments highlight the potential of deep learning methods as investigative tools, providing new insights into atmospheric dynamics and predictability."
      },
      {
        "id": "oai:arXiv.org:2504.07633v1",
        "title": "Kernel Logistic Regression Learning for High-Capacity Hopfield Networks",
        "link": "https://arxiv.org/abs/2504.07633",
        "author": "Akira Tamamori",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07633v1 Announce Type: new \nAbstract: Hebbian learning limits Hopfield network storage capacity (pattern-to-neuron ratio around 0.14). We propose Kernel Logistic Regression (KLR) learning. Unlike linear methods, KLR uses kernels to implicitly map patterns to high-dimensional feature space, enhancing separability. By learning dual variables, KLR dramatically improves storage capacity, achieving perfect recall even when pattern numbers exceed neuron numbers (up to ratio 1.5 shown), and enhances noise robustness. KLR demonstrably outperforms Hebbian and linear logistic regression approaches."
      },
      {
        "id": "oai:arXiv.org:2504.07638v1",
        "title": "Predicting the Lifespan of Industrial Printheads with Survival Analysis",
        "link": "https://arxiv.org/abs/2504.07638",
        "author": "Dan Parii, Evelyne Janssen, Guangzhi Tang, Charalampos Kouzinopoulos, Marcin Pietrasik",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07638v1 Announce Type: new \nAbstract: Accurately predicting the lifespan of critical device components is essential for maintenance planning and production optimization, making it a topic of significant interest in both academia and industry. In this work, we investigate the use of survival analysis for predicting the lifespan of production printheads developed by Canon Production Printing. Specifically, we focus on the application of five techniques to estimate survival probabilities and failure rates: the Kaplan-Meier estimator, Cox proportional hazard model, Weibull accelerated failure time model, random survival forest, and gradient boosting. The resulting estimates are further refined using isotonic regression and subsequently aggregated to determine the expected number of failures. The predictions are then validated against real-world ground truth data across multiple time windows to assess model reliability. Our quantitative evaluation using three performance metrics demonstrates that survival analysis outperforms industry-standard baseline methods for printhead lifespan prediction."
      },
      {
        "id": "oai:arXiv.org:2504.07645v1",
        "title": "Prediction of Usage Probabilities of Shopping-Mall Corridors Using Heterogeneous Graph Neural Networks",
        "link": "https://arxiv.org/abs/2504.07645",
        "author": "Malik M Barakathullah, Immanuel Koh",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07645v1 Announce Type: new \nAbstract: We present a method based on graph neural network (GNN) for prediction of probabilities of usage of shopping-mall corridors. The heterogeneous graph network of shops and corridor paths are obtained from floorplans of the malls by creating vector layers for corridors, shops and entrances. These are subsequently assimilated into nodes and edges of graphs. The prediction of the usage probability is based on the shop features, namely, the area and usage categories they fall into, and on the graph connecting these shops, corridor junctions and entrances by corridor paths. Though the presented method is applicable for training on datasets obtained from a field survey or from pedestrian-detecting sensors, the target data of the supervised deep-learning work flow in this work are obtained from a probability method. We also include a context-specific representation learning of latent features. The usage-probability prediction is made on each edge, which is a connection by a section of corridor path between the adjacent nodes representing the shops or corridor points. To create a feature for each edge, the hidden-layer feature vectors acquired in the message-passing GNN layers at the nodes of each edge are averaged and concatenated with the vector obtained by their multiplication. These edge-features are then passed to multilayer perceptrons (MLP) to make the final prediction of usage probability on each edge. The samples of synthetic learning dataset for each shopping mall are obtained by changing the shops' usage and area categories, and by subsequently feeding the graph into the probability model.\n  When including different shopping malls in a single dataset, we also propose to consider graph-level features to inform the model with specific identifying features of each mall."
      },
      {
        "id": "oai:arXiv.org:2504.07646v1",
        "title": "On the Temporal Question-Answering Capabilities of Large Language Models Over Anonymized Data",
        "link": "https://arxiv.org/abs/2504.07646",
        "author": "Alfredo Garrach\\'on Ruiz, Tom\\'as de la Rosa, Daniel Borrajo",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07646v1 Announce Type: new \nAbstract: The applicability of Large Language Models (LLMs) in temporal reasoning tasks over data that is not present during training is still a field that remains to be explored. In this paper we work on this topic, focusing on structured and semi-structured anonymized data. We not only develop a direct LLM pipeline, but also compare various methodologies and conduct an in-depth analysis. We identified and examined seventeen common temporal reasoning tasks in natural language, focusing on their algorithmic components. To assess LLM performance, we created the \\textit{Reasoning and Answering Temporal Ability} dataset (RATA), featuring semi-structured anonymized data to ensure reliance on reasoning rather than on prior knowledge. We compared several methodologies, involving SoTA techniques such as Tree-of-Thought, self-reflexion and code execution, tuned specifically for this scenario. Our results suggest that achieving scalable and reliable solutions requires more than just standalone LLMs, highlighting the need for integrated approaches."
      },
      {
        "id": "oai:arXiv.org:2504.07654v1",
        "title": "ms-Mamba: Multi-scale Mamba for Time-Series Forecasting",
        "link": "https://arxiv.org/abs/2504.07654",
        "author": "Yusuf Meric Karadag, Sinan Kalkan, Ipek Gursel Dino",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07654v1 Announce Type: new \nAbstract: The problem of Time-series Forecasting is generally addressed by recurrent, Transformer-based and the recently proposed Mamba-based architectures. However, existing architectures generally process their input at a single temporal scale, which may be sub-optimal for many tasks where information changes over multiple time scales. In this paper, we introduce a novel architecture called Multi-scale Mamba (ms-Mamba) to address this gap. ms-Mamba incorporates multiple temporal scales by using multiple Mamba blocks with different sampling rates ($\\Delta$s). Our experiments on many benchmarks demonstrate that ms-Mamba outperforms state-of-the-art approaches, including the recently proposed Transformer-based and Mamba-based models."
      },
      {
        "id": "oai:arXiv.org:2504.07660v1",
        "title": "End-to-End Facial Expression Detection in Long Videos",
        "link": "https://arxiv.org/abs/2504.07660",
        "author": "Yini Fang, Alec Diallo, Yiqi Shi, Frederic Jumelle, Bertram Shi",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07660v1 Announce Type: new \nAbstract: Facial expression detection involves two interrelated tasks: spotting, which identifies the onset and offset of expressions, and recognition, which classifies them into emotional categories. Most existing methods treat these tasks separately using a two-step training pipelines. A spotting model first detects expression intervals. A recognition model then classifies the detected segments. However, this sequential approach leads to error propagation, inefficient feature learning, and suboptimal performance due to the lack of joint optimization of the two tasks. We propose FEDN, an end-to-end Facial Expression Detection Network that jointly optimizes spotting and recognition. Our model introduces a novel attention-based feature extraction module, incorporating segment attention and sliding window attention to improve facial feature learning. By unifying two tasks within a single network, we greatly reduce error propagation and enhance overall performance. Experiments on CASME}^2 and CASME^3 demonstrate state-of-the-art accuracy for both spotting and detection, underscoring the benefits of joint optimization for robust facial expression detection in long videos."
      },
      {
        "id": "oai:arXiv.org:2504.07661v1",
        "title": "Unveiling the Impact of Multimodal Features on Chinese Spelling Correction: From Analysis to Design",
        "link": "https://arxiv.org/abs/2504.07661",
        "author": "Xiaowu Zhang, Hongfei Zhao, Jingyi Hou, Zhijie Liu",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07661v1 Announce Type: new \nAbstract: The Chinese Spelling Correction (CSC) task focuses on detecting and correcting spelling errors in sentences. Current research primarily explores two approaches: traditional multimodal pre-trained models and large language models (LLMs). However, LLMs face limitations in CSC, particularly over-correction, making them suboptimal for this task. While existing studies have investigated the use of phonetic and graphemic information in multimodal CSC models, effectively leveraging these features to enhance correction performance remains a challenge. To address this, we propose the Multimodal Analysis for Character Usage (\\textbf{MACU}) experiment, identifying potential improvements for multimodal correctison. Based on empirical findings, we introduce \\textbf{NamBert}, a novel multimodal model for Chinese spelling correction. Experiments on benchmark datasets demonstrate NamBert's superiority over SOTA methods. We also conduct a comprehensive comparison between NamBert and LLMs, systematically evaluating their strengths and limitations in CSC. Our code and model are available at https://github.com/iioSnail/NamBert."
      },
      {
        "id": "oai:arXiv.org:2504.07667v1",
        "title": "S2R-HDR: A Large-Scale Rendered Dataset for HDR Fusion",
        "link": "https://arxiv.org/abs/2504.07667",
        "author": "Yujin Wang, Jiarui Wu, Yichen Bian, Fan Zhang, Tianfan Xue",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07667v1 Announce Type: new \nAbstract: The generalization of learning-based high dynamic range (HDR) fusion is often limited by the availability of training data, as collecting large-scale HDR images from dynamic scenes is both costly and technically challenging. To address these challenges, we propose S2R-HDR, the first large-scale high-quality synthetic dataset for HDR fusion, with 24,000 HDR samples. Using Unreal Engine 5, we design a diverse set of realistic HDR scenes that encompass various dynamic elements, motion types, high dynamic range scenes, and lighting. Additionally, we develop an efficient rendering pipeline to generate realistic HDR images. To further mitigate the domain gap between synthetic and real-world data, we introduce S2R-Adapter, a domain adaptation designed to bridge this gap and enhance the generalization ability of models. Experimental results on real-world datasets demonstrate that our approach achieves state-of-the-art HDR reconstruction performance. Dataset and code will be available at https://openimaginglab.github.io/S2R-HDR."
      },
      {
        "id": "oai:arXiv.org:2504.07670v1",
        "title": "LAPIS: A novel dataset for personalized image aesthetic assessment",
        "link": "https://arxiv.org/abs/2504.07670",
        "author": "Anne-Sofie Maerten, Li-Wei Chen, Stefanie De Winter, Christophe Bossens, Johan Wagemans",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07670v1 Announce Type: new \nAbstract: We present the Leuven Art Personalized Image Set (LAPIS), a novel dataset for personalized image aesthetic assessment (PIAA). It is the first dataset with images of artworks that is suitable for PIAA. LAPIS consists of 11,723 images and was meticulously curated in collaboration with art historians. Each image has an aesthetics score and a set of image attributes known to relate to aesthetic appreciation. Besides rich image attributes, LAPIS offers rich personal attributes of each annotator. We implemented two existing state-of-the-art PIAA models and assessed their performance on LAPIS. We assess the contribution of personal attributes and image attributes through ablation studies and find that performance deteriorates when certain personal and image attributes are removed. An analysis of failure cases reveals that both existing models make similar incorrect predictions, highlighting the need for improvements in artistic image aesthetic assessment. The LAPIS project page can be found at: https://github.com/Anne-SofieMaerten/LAPIS"
      },
      {
        "id": "oai:arXiv.org:2504.07680v1",
        "title": "Synthetic Fluency: Hallucinations, Confabulations, and the Creation of Irish Words in LLM-Generated Translations",
        "link": "https://arxiv.org/abs/2504.07680",
        "author": "Sheila Castilho, Zoe Fitzsimmons, Claire Holton, Aoife Mc Donagh",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07680v1 Announce Type: new \nAbstract: This study examines hallucinations in Large Language Model (LLM) translations into Irish, specifically focusing on instances where the models generate novel, non-existent words. We classify these hallucinations within verb and noun categories, identifying six distinct patterns among the latter. Additionally, we analyse whether these hallucinations adhere to Irish morphological rules and what linguistic tendencies they exhibit. Our findings show that while both GPT-4.o and GPT-4.o Mini produce similar types of hallucinations, the Mini model generates them at a significantly higher frequency. Beyond classification, the discussion raises speculative questions about the implications of these hallucinations for the Irish language. Rather than seeking definitive answers, we offer food for thought regarding the increasing use of LLMs and their potential role in shaping Irish vocabulary and linguistic evolution. We aim to prompt discussion on how such technologies might influence language over time, particularly in the context of low-resource, morphologically rich languages."
      },
      {
        "id": "oai:arXiv.org:2504.07685v1",
        "title": "Context-Aware Monolingual Human Evaluation of Machine Translation",
        "link": "https://arxiv.org/abs/2504.07685",
        "author": "Silvio Picinini, Sheila Castilho",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07685v1 Announce Type: new \nAbstract: This paper explores the potential of context-aware monolingual human evaluation for assessing machine translation (MT) when no source is given for reference. To this end, we compare monolingual with bilingual evaluations (with source text), under two scenarios: the evaluation of a single MT system, and the comparative evaluation of pairwise MT systems. Four professional translators performed both monolingual and bilingual evaluations by assigning ratings and annotating errors, and providing feedback on their experience. Our findings suggest that context-aware monolingual human evaluation achieves comparable outcomes to human bilingual evaluations, and suggest the feasibility and potential of monolingual evaluation as an efficient approach to assessing MT."
      },
      {
        "id": "oai:arXiv.org:2504.07687v1",
        "title": "FMNV: A Dataset of Media-Published News Videos for Fake News Detection",
        "link": "https://arxiv.org/abs/2504.07687",
        "author": "Yihao Wang, Zhong Qian, Peifeng Li",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07687v1 Announce Type: new \nAbstract: News media, particularly video-based platforms, have become deeply embedded in daily life, concurrently amplifying risks of misinformation dissemination. Consequently, multimodal fake news detection has garnered significant research attention. However, existing datasets predominantly comprise user-generated videos characterized by crude editing and limited public engagement, whereas professionally crafted fake news videos disseminated by media outlets often politically or virally motivated pose substantially greater societal harm. To address this gap, we construct FMNV, a novel dataset exclusively composed of news videos published by media organizations. Through empirical analysis of existing datasets and our curated collection, we categorize fake news videos into four distinct types. Building upon this taxonomy, we employ Large Language Models (LLMs) to automatically generate deceptive content by manipulating authentic media-published news videos. Furthermore, we propose FMNVD, a baseline model featuring a dual-stream architecture integrating CLIP and Faster R-CNN for video feature extraction, enhanced by co-attention mechanisms for feature refinement and multimodal aggregation. Comparative experiments demonstrate both the generalization capability of FMNV across multiple baselines and the superior detection efficacy of FMNVD. This work establishes critical benchmarks for detecting high-impact fake news in media ecosystems while advancing methodologies for cross-modal inconsistency analysis."
      },
      {
        "id": "oai:arXiv.org:2504.07691v1",
        "title": "Distilling Knowledge from Heterogeneous Architectures for Semantic Segmentation",
        "link": "https://arxiv.org/abs/2504.07691",
        "author": "Yanglin Huang, Kai Hu, Yuan Zhang, Zhineng Chen, Xieping Gao",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07691v1 Announce Type: new \nAbstract: Current knowledge distillation (KD) methods for semantic segmentation focus on guiding the student to imitate the teacher's knowledge within homogeneous architectures. However, these methods overlook the diverse knowledge contained in architectures with different inductive biases, which is crucial for enabling the student to acquire a more precise and comprehensive understanding of the data during distillation. To this end, we propose for the first time a generic knowledge distillation method for semantic segmentation from a heterogeneous perspective, named HeteroAKD. Due to the substantial disparities between heterogeneous architectures, such as CNN and Transformer, directly transferring cross-architecture knowledge presents significant challenges. To eliminate the influence of architecture-specific information, the intermediate features of both the teacher and student are skillfully projected into an aligned logits space. Furthermore, to utilize diverse knowledge from heterogeneous architectures and deliver customized knowledge required by the student, a teacher-student knowledge mixing mechanism (KMM) and a teacher-student knowledge evaluation mechanism (KEM) are introduced. These mechanisms are performed by assessing the reliability and its discrepancy between heterogeneous teacher-student knowledge. Extensive experiments conducted on three main-stream benchmarks using various teacher-student pairs demonstrate that our HeteroAKD outperforms state-of-the-art KD methods in facilitating distillation between heterogeneous architectures."
      },
      {
        "id": "oai:arXiv.org:2504.07698v1",
        "title": "Proactive User Information Acquisition via Chats on User-Favored Topics",
        "link": "https://arxiv.org/abs/2504.07698",
        "author": "Shiki Sato, Jun Baba, Asahi Hentona, Shinji Iwata, Akifumi Yoshimoto, Koichiro Yoshino",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07698v1 Announce Type: new \nAbstract: Chat-oriented dialogue systems designed to provide tangible benefits, such as sharing the latest news or preventing frailty in senior citizens, often require Proactive acquisition of specific user Information via chats on user-faVOred Topics (PIVOT). This study proposes the PIVOT task, designed to advance the technical foundation for these systems. In this task, a system needs to acquire the answers of a user to predefined questions without making the user feel abrupt while engaging in a chat on a predefined topic. We found that even recent large language models (LLMs) show a low success rate in the PIVOT task. We constructed a dataset suitable for the analysis to develop more effective systems. Finally, we developed a simple but effective system for this task by incorporating insights obtained through the analysis of this dataset."
      },
      {
        "id": "oai:arXiv.org:2504.07711v1",
        "title": "Merging Embedded Topics with Optimal Transport for Online Topic Modeling on Data Streams",
        "link": "https://arxiv.org/abs/2504.07711",
        "author": "Federica Granese, Benjamin Navet, Serena Villata, Charles Bouveyron",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07711v1 Announce Type: new \nAbstract: Topic modeling is a key component in unsupervised learning, employed to identify topics within a corpus of textual data. The rapid growth of social media generates an ever-growing volume of textual data daily, making online topic modeling methods essential for managing these data streams that continuously arrive over time. This paper introduces a novel approach to online topic modeling named StreamETM. This approach builds on the Embedded Topic Model (ETM) to handle data streams by merging models learned on consecutive partial document batches using unbalanced optimal transport. Additionally, an online change point detection algorithm is employed to identify shifts in topics over time, enabling the identification of significant changes in the dynamics of text streams. Numerical experiments on simulated and real-world data show StreamETM outperforming competitors."
      },
      {
        "id": "oai:arXiv.org:2504.07718v1",
        "title": "Multi-modal Reference Learning for Fine-grained Text-to-Image Retrieval",
        "link": "https://arxiv.org/abs/2504.07718",
        "author": "Zehong Ma, Hao Chen, Wei Zeng, Limin Su, Shiliang Zhang",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07718v1 Announce Type: new \nAbstract: Fine-grained text-to-image retrieval aims to retrieve a fine-grained target image with a given text query. Existing methods typically assume that each training image is accurately depicted by its textual descriptions. However, textual descriptions can be ambiguous and fail to depict discriminative visual details in images, leading to inaccurate representation learning. To alleviate the effects of text ambiguity, we propose a Multi-Modal Reference learning framework to learn robust representations. We first propose a multi-modal reference construction module to aggregate all visual and textual details of the same object into a comprehensive multi-modal reference. The multi-modal reference hence facilitates the subsequent representation learning and retrieval similarity computation. Specifically, a reference-guided representation learning module is proposed to use multi-modal references to learn more accurate visual and textual representations. Additionally, we introduce a reference-based refinement method that employs the object references to compute a reference-based similarity that refines the initial retrieval results. Extensive experiments are conducted on five fine-grained text-to-image retrieval datasets for different text-to-image retrieval tasks. The proposed method has achieved superior performance over state-of-the-art methods. For instance, on the text-to-person image retrieval dataset RSTPReid, our method achieves the Rank1 accuracy of 56.2\\%, surpassing the recent CFine by 5.6\\%."
      },
      {
        "id": "oai:arXiv.org:2504.07719v1",
        "title": "Counting Hours, Counting Losses: The Toll of Unpredictable Work Schedules on Financial Security",
        "link": "https://arxiv.org/abs/2504.07719",
        "author": "Pegah Nokhiz, Aravinda Kanchana Ruwanpathirana, Aditya Bhaskara, Suresh Venkatasubramanian",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07719v1 Announce Type: new \nAbstract: Financial instability has become a significant issue in today's society. While research typically focuses on financial aspects, there is a tendency to overlook time-related aspects of unstable work schedules. The inability to rely on consistent work schedules leads to burnout, work-family conflicts, and financial shocks that directly impact workers' income and assets. Unforeseen fluctuations in earnings pose challenges in financial planning, affecting decisions on savings and spending and ultimately undermining individuals' long-term financial stability and well-being.\n  This issue is particularly evident in sectors where workers experience frequently changing schedules without sufficient notice, including those in the food service and retail sectors, part-time and hourly workers, and individuals with lower incomes. These groups are already more financially vulnerable, and the unpredictable nature of their schedules exacerbates their financial fragility.\n  Our objective is to understand how unforeseen fluctuations in earnings exacerbate financial fragility by investigating the extent to which individuals' financial management depends on their ability to anticipate and plan for the future. To address this question, we develop a simulation framework that models how individuals optimize utility amidst financial uncertainty and the imperative to avoid financial ruin. We employ online learning techniques, specifically adapting workers' consumption policies based on evolving information about their work schedules.\n  With this framework, we show both theoretically and empirically how a worker's capacity to anticipate schedule changes enhances their long-term utility. Conversely, the inability to predict future events can worsen workers' instability. Moreover, our framework enables us to explore interventions to mitigate the problem of schedule uncertainty and evaluate their effectiveness."
      },
      {
        "id": "oai:arXiv.org:2504.07722v1",
        "title": "Relaxing the Markov Requirements on Reinforcement Learning Under Weak Partial Ignorability",
        "link": "https://arxiv.org/abs/2504.07722",
        "author": "MaryLena Bleile",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07722v1 Announce Type: new \nAbstract: Incomplete data, confounding effects, and violations of the Markov property are interrelated problems which are ubiquitous in Reinforcement Learning applications. We introduce the concept of ``partial ignorabilty\" and leverage it to establish a novel convergence theorem for adaptive Reinforcement Learning. This theoretical result relaxes the Markov assumption on the stochastic process underlying conventional $Q$-learning, deploying a generalized form of the Robbins-Monro stochastic approximation theorem to establish optimality. This result has clear downstream implications for most active subfields of Reinforcement Learning, with clear paths for extension to the field of Causal Inference."
      },
      {
        "id": "oai:arXiv.org:2504.07724v1",
        "title": "MRD-RAG: Enhancing Medical Diagnosis with Multi-Round Retrieval-Augmented Generation",
        "link": "https://arxiv.org/abs/2504.07724",
        "author": "Yixiang Chen, Penglei Sun, Xiang Li, Xiaowen Chu",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07724v1 Announce Type: new \nAbstract: In recent years, accurately and quickly deploying medical large language models (LLMs) has become a significant trend. Among these, retrieval-augmented generation (RAG) has garnered significant attention due to its features of rapid deployment and privacy protection. However, existing medical RAG frameworks still have shortcomings. Most existing medical RAG frameworks are designed for single-round question answering tasks and are not suitable for multi-round diagnostic dialogue. On the other hand, existing medical multi-round RAG frameworks do not consider the interconnections between potential diseases to inquire precisely like a doctor. To address these issues, we propose a Multi-Round Diagnostic RAG (MRD-RAG) framework that mimics the doctor's diagnostic process. This RAG framework can analyze diagnosis information of potential diseases and accurately conduct multi-round diagnosis like a doctor. To evaluate the effectiveness of our proposed frameworks, we conduct experiments on two modern medical datasets and two traditional Chinese medicine datasets, with evaluations by GPT and human doctors on different methods. The results indicate that our RAG framework can significantly enhance the diagnostic performance of LLMs, highlighting the potential of our approach in medical diagnosis. The code and data can be found in our project website https://github.com/YixiangCh/MRD-RAG/tree/master."
      },
      {
        "id": "oai:arXiv.org:2504.07729v1",
        "title": "Benchmarking Multi-Organ Segmentation Tools for Multi-Parametric T1-weighted Abdominal MRI",
        "link": "https://arxiv.org/abs/2504.07729",
        "author": "Nicole Tran, Anisa Prasad, Yan Zhuang, Tejas Sudharshan Mathai, Boah Kim, Sydney Lewis, Pritam Mukherjee, Jianfei Liu, Ronald M. Summers",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07729v1 Announce Type: new \nAbstract: The segmentation of multiple organs in multi-parametric MRI studies is critical for many applications in radiology, such as correlating imaging biomarkers with disease status (e.g., cirrhosis, diabetes). Recently, three publicly available tools, such as MRSegmentator (MRSeg), TotalSegmentator MRI (TS), and TotalVibeSegmentator (VIBE), have been proposed for multi-organ segmentation in MRI. However, the performance of these tools on specific MRI sequence types has not yet been quantified. In this work, a subset of 40 volumes from the public Duke Liver Dataset was curated. The curated dataset contained 10 volumes each from the pre-contrast fat saturated T1, arterial T1w, venous T1w, and delayed T1w phases, respectively. Ten abdominal structures were manually annotated in these volumes. Next, the performance of the three public tools was benchmarked on this curated dataset. The results indicated that MRSeg obtained a Dice score of 80.7 $\\pm$ 18.6 and Hausdorff Distance (HD) error of 8.9 $\\pm$ 10.4 mm. It fared the best ($p < .05$) across the different sequence types in contrast to TS and VIBE."
      },
      {
        "id": "oai:arXiv.org:2504.07733v1",
        "title": "DeepGreen: Effective LLM-Driven Green-washing Monitoring System Designed for Empirical Testing -- Evidence from China",
        "link": "https://arxiv.org/abs/2504.07733",
        "author": "Congluo Xu, Yu Miao, Yiling Xiao, Chengmengjia Lin",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07733v1 Announce Type: new \nAbstract: This paper proposes DeepGreen, an Large Language Model Driven (LLM-Driven) system for detecting corporate green-washing behaviour. Utilizing dual-layer LLM analysis, DeepGreen preliminarily identifies potential green keywords in financial statements and then assesses their implementation degree via iterative semantic analysis of LLM. A core variable GreenImplement is derived from the ratio from the two layers' output. We extract 204 financial statements of 68 companies from A-share market over three years, comprising 89,893 words, and analyse them through DeepGreen. Our analysis, supported by violin plots and K-means clustering, reveals insights and validates the variable against the Huazheng ESG rating. It offers a novel perspective for regulatory agencies and investors, serving as a proactive monitoring tool that complements traditional methods.Empirical tests show that green implementation can significantly boost the asset return rate of companies, but there is heterogeneity in scale. Small and medium-sized companies have limited contribution to asset return via green implementation, so there is a stronger motivation for green-washing."
      },
      {
        "id": "oai:arXiv.org:2504.07738v1",
        "title": "Automated Construction of a Knowledge Graph of Nuclear Fusion Energy for Effective Elicitation and Retrieval of Information",
        "link": "https://arxiv.org/abs/2504.07738",
        "author": "A. Loreti, K. Chen, R. George, R. Firth, A. Agnello, S. Tanaka",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07738v1 Announce Type: new \nAbstract: In this document, we discuss a multi-step approach to automated construction of a knowledge graph, for structuring and representing domain-specific knowledge from large document corpora. We apply our method to build the first knowledge graph of nuclear fusion energy, a highly specialized field characterized by vast scope and heterogeneity. This is an ideal benchmark to test the key features of our pipeline, including automatic named entity recognition and entity resolution. We show how pre-trained large language models can be used to address these challenges and we evaluate their performance against Zipf's law, which characterizes human-generated natural language. Additionally, we develop a knowledge-graph retrieval-augmented generation system that combines large language models with a multi-prompt approach. This system provides contextually relevant answers to natural-language queries, including complex multi-hop questions that require reasoning across interconnected entities."
      },
      {
        "id": "oai:arXiv.org:2504.07744v1",
        "title": "MMLA: Multi-Environment, Multi-Species, Low-Altitude Aerial Footage Dataset",
        "link": "https://arxiv.org/abs/2504.07744",
        "author": "Jenna Kline, Samuel Stevens, Guy Maalouf, Camille Rondeau Saint-Jean, Dat Nguyen Ngoc, Majid Mirmehdi, David Guerin, Tilo Burghardt, Elzbieta Pastucha, Blair Costelloe, Matthew Watson, Thomas Richardson, Ulrik Pagh Schultz Lundquist",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07744v1 Announce Type: new \nAbstract: Real-time wildlife detection in drone imagery is critical for numerous applications, including animal ecology, conservation, and biodiversity monitoring. Low-altitude drone missions are effective for collecting fine-grained animal movement and behavior data, particularly if missions are automated for increased speed and consistency. However, little work exists on evaluating computer vision models on low-altitude aerial imagery and generalizability across different species and settings. To fill this gap, we present a novel multi-environment, multi-species, low-altitude aerial footage (MMLA) dataset. MMLA consists of drone footage collected across three diverse environments: Ol Pejeta Conservancy and Mpala Research Centre in Kenya, and The Wilds Conservation Center in Ohio, which includes five species: Plains zebras, Grevy's zebras, giraffes, onagers, and African Painted Dogs. We comprehensively evaluate three YOLO models (YOLOv5m, YOLOv8m, and YOLOv11m) for detecting animals. Results demonstrate significant performance disparities across locations and species-specific detection variations. Our work highlights the importance of evaluating detection algorithms across different environments for robust wildlife monitoring applications using drones."
      },
      {
        "id": "oai:arXiv.org:2504.07745v1",
        "title": "SF2T: Self-supervised Fragment Finetuning of Video-LLMs for Fine-Grained Understanding",
        "link": "https://arxiv.org/abs/2504.07745",
        "author": "Yangliu Hu, Zikai Song, Na Feng, Yawei Luo, Junqing Yu, Yi-Ping Phoebe Chen, Wei Yang",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07745v1 Announce Type: new \nAbstract: Video-based Large Language Models (Video-LLMs) have witnessed substantial advancements in recent years, propelled by the advancement in multi-modal LLMs. Although these models have demonstrated proficiency in providing the overall description of videos, they struggle with fine-grained understanding, particularly in aspects such as visual dynamics and video details inquiries. To tackle these shortcomings, we find that fine-tuning Video-LLMs on self-supervised fragment tasks, greatly improve their fine-grained video understanding abilities. Hence we propose two key contributions:(1) Self-Supervised Fragment Fine-Tuning (SF$^2$T), a novel effortless fine-tuning method, employs the rich inherent characteristics of videos for training, while unlocking more fine-grained understanding ability of Video-LLMs. Moreover, it relieves researchers from labor-intensive annotations and smartly circumvents the limitations of natural language, which often fails to capture the complex spatiotemporal variations in videos; (2) A novel benchmark dataset, namely FineVidBench, for rigorously assessing Video-LLMs' performance at both the scene and fragment levels, offering a comprehensive evaluation of their capabilities. We assessed multiple models and validated the effectiveness of SF$^2$T on them. Experimental results reveal that our approach improves their ability to capture and interpret spatiotemporal details."
      },
      {
        "id": "oai:arXiv.org:2504.07749v1",
        "title": "NorEval: A Norwegian Language Understanding and Generation Evaluation Benchmark",
        "link": "https://arxiv.org/abs/2504.07749",
        "author": "Vladislav Mikhailov, Tita Enstad, David Samuel, Hans Christian Farseth{\\aa}s, Andrey Kutuzov, Erik Velldal, Lilja {\\O}vrelid",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07749v1 Announce Type: new \nAbstract: This paper introduces NorEval, a new and comprehensive evaluation suite for large-scale standardized benchmarking of Norwegian generative language models (LMs). NorEval consists of 24 high-quality human-created datasets -- of which five are created from scratch. In contrast to existing benchmarks for Norwegian, NorEval covers a broad spectrum of task categories targeting Norwegian language understanding and generation, establishes human baselines, and focuses on both of the official written standards of the Norwegian language: Bokm{\\aa}l and Nynorsk. All our datasets and a collection of over 100 human-written prompts are integrated into LM Evaluation Harness, ensuring flexible and reproducible evaluation. We describe the NorEval design and present the results of benchmarking 19 open-source pre-trained and instruction-tuned LMs for Norwegian in various scenarios. Our benchmark, evaluation framework, and annotation materials are publicly available."
      },
      {
        "id": "oai:arXiv.org:2504.07754v1",
        "title": "Efficient Tuning of Large Language Models for Knowledge-Grounded Dialogue Generation",
        "link": "https://arxiv.org/abs/2504.07754",
        "author": "Bo Zhang, Hui Ma, Dailin Li, Jian Ding, Jian Wang, Bo Xu, HongFei Lin",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07754v1 Announce Type: new \nAbstract: Large language models (LLMs) demonstrate remarkable text comprehension and generation capabilities but often lack the ability to utilize up-to-date or domain-specific knowledge not included in their training data. To address this gap, we introduce KEDiT, an efficient method for fine-tuning LLMs for knowledge-grounded dialogue generation. KEDiT operates in two main phases: first, it employs an information bottleneck to compress retrieved knowledge into learnable parameters, retaining essential information while minimizing computational overhead. Second, a lightweight knowledge-aware adapter integrates these compressed knowledge vectors into the LLM during fine-tuning, updating less than 2\\% of the model parameters. The experimental results on the Wizard of Wikipedia and a newly constructed PubMed-Dialog dataset demonstrate that KEDiT excels in generating contextually relevant and informative responses, outperforming competitive baselines in automatic, LLM-based, and human evaluations. This approach effectively combines the strengths of pretrained LLMs with the adaptability needed for incorporating dynamic knowledge, presenting a scalable solution for fields such as medicine."
      },
      {
        "id": "oai:arXiv.org:2504.07758v1",
        "title": "PIDSR:ComplementaryPolarizedImageDemosaicingandSuper-Resolution",
        "link": "https://arxiv.org/abs/2504.07758",
        "author": "Shuangfan Zhou, Chu Zhou, Youwei Lyu, Heng Guo, Zhanyu Ma, Boxin Shi, Imari Sato",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07758v1 Announce Type: new \nAbstract: Polarization cameras can capture multiple polarized images with different polarizer angles in a single shot, bringing convenience to polarization-based downstream tasks. However, their direct outputs are color-polarization filter array (CPFA) raw images, requiring demosaicing to reconstruct full-resolution, full-color polarized images; unfortunately, this necessary step introduces artifacts that make polarization-related parameters such as the degree of polarization (DoP) and angle of polarization (AoP) prone to error. Besides, limited by the hardware design, the resolution of a polarization camera is often much lower than that of a conventional RGB camera. Existing polarized image demosaicing (PID) methods are limited in that they cannot enhance resolution, while polarized image super-resolution (PISR) methods, though designed to obtain high-resolution (HR) polarized images from the demosaicing results, tend to retain or even amplify errors in the DoP and AoP introduced by demosaicing artifacts. In this paper, we propose PIDSR, a joint framework that performs complementary Polarized Image Demosaicing and Super-Resolution, showing the ability to robustly obtain high-quality HR polarized images with more accurate DoP and AoP from a CPFA raw image in a direct manner. Experiments show our PIDSR not only achieves state-of-the-art performance on both synthetic and real data, but also facilitates downstream tasks."
      },
      {
        "id": "oai:arXiv.org:2504.07761v1",
        "title": "Exploring a Patch-Wise Approach for Privacy-Preserving Fake ID Detection",
        "link": "https://arxiv.org/abs/2504.07761",
        "author": "Javier Mu\\~noz-Haro, Ruben Tolosana, Ruben Vera-Rodriguez, Aythami Morales, Julian Fierrez",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07761v1 Announce Type: new \nAbstract: In an increasingly digitalized world, verifying the authenticity of ID documents has become a critical challenge for real-life applications such as digital banking, crypto-exchanges, renting, etc. This study focuses on the topic of fake ID detection, covering several limitations in the field. In particular, no publicly available data from real ID documents exists, and most studies rely on proprietary in-house databases that are not available due to privacy reasons. In order to shed some light on this critical challenge that makes difficult to advance in the field, we explore a trade-off between privacy (i.e., amount of sensitive data available) and performance, proposing a novel patch-wise approach for privacy-preserving fake ID detection. Our proposed approach explores how privacy can be enhanced through: i) two levels of anonymization for an ID document (i.e., fully- and pseudo-anonymized), and ii) different patch size configurations, varying the amount of sensitive data visible in the patch image. Also, state-of-the-art methods such as Vision Transformers and Foundation Models are considered in the analysis. The experimental framework shows that, on an unseen database (DLC-2021), our proposal achieves 13.91% and 0% EERs at patch and ID document level, showing a good generalization to other databases. In addition to this exploration, another key contribution of our study is the release of the first publicly available database that contains 48,400 patches from both real and fake ID documents, along with the experimental framework and models, which will be available in our GitHub."
      },
      {
        "id": "oai:arXiv.org:2504.07785v1",
        "title": "Towards Micro-Action Recognition with Limited Annotations: An Asynchronous Pseudo Labeling and Training Approach",
        "link": "https://arxiv.org/abs/2504.07785",
        "author": "Yan Zhang, Lechao Cheng, Yaxiong Wang, Zhun Zhong, Meng Wang",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07785v1 Announce Type: new \nAbstract: Micro-Action Recognition (MAR) aims to classify subtle human actions in video. However, annotating MAR datasets is particularly challenging due to the subtlety of actions. To this end, we introduce the setting of Semi-Supervised MAR (SSMAR), where only a part of samples are labeled. We first evaluate traditional Semi-Supervised Learning (SSL) methods to SSMAR and find that these methods tend to overfit on inaccurate pseudo-labels, leading to error accumulation and degraded performance. This issue primarily arises from the common practice of directly using the predictions of classifier as pseudo-labels to train the model. To solve this issue, we propose a novel framework, called Asynchronous Pseudo Labeling and Training (APLT), which explicitly separates the pseudo-labeling process from model training. Specifically, we introduce a semi-supervised clustering method during the offline pseudo-labeling phase to generate more accurate pseudo-labels. Moreover, a self-adaptive thresholding strategy is proposed to dynamically filter noisy labels of different classes. We then build a memory-based prototype classifier based on the filtered pseudo-labels, which is fixed and used to guide the subsequent model training phase. By alternating the two pseudo-labeling and model training phases in an asynchronous manner, the model can not only be learned with more accurate pseudo-labels but also avoid the overfitting issue. Experiments on three MAR datasets show that our APLT largely outperforms state-of-the-art SSL methods. For instance, APLT improves accuracy by 14.5\\% over FixMatch on the MA-12 dataset when using only 50\\% labeled data. Code will be publicly available."
      },
      {
        "id": "oai:arXiv.org:2504.07792v1",
        "title": "Breaking the Barriers: Video Vision Transformers for Word-Level Sign Language Recognition",
        "link": "https://arxiv.org/abs/2504.07792",
        "author": "Alexander Brettmann, Jakob Gr\\\"avinghoff, Marlene R\\\"uschoff, Marie Westhues",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07792v1 Announce Type: new \nAbstract: Sign language is a fundamental means of communication for the deaf and hard-of-hearing (DHH) community, enabling nuanced expression through gestures, facial expressions, and body movements. Despite its critical role in facilitating interaction within the DHH population, significant barriers persist due to the limited fluency in sign language among the hearing population. Overcoming this communication gap through automatic sign language recognition (SLR) remains a challenge, particularly at a dynamic word-level, where temporal and spatial dependencies must be effectively recognized. While Convolutional Neural Networks have shown potential in SLR, they are computationally intensive and have difficulties in capturing global temporal dependencies between video sequences. To address these limitations, we propose a Video Vision Transformer (ViViT) model for word-level American Sign Language (ASL) recognition. Transformer models make use of self-attention mechanisms to effectively capture global relationships across spatial and temporal dimensions, which makes them suitable for complex gesture recognition tasks. The VideoMAE model achieves a Top-1 accuracy of 75.58% on the WLASL100 dataset, highlighting its strong performance compared to traditional CNNs with 65.89%. Our study demonstrates that transformer-based architectures have great potential to advance SLR, overcome communication barriers and promote the inclusion of DHH individuals."
      },
      {
        "id": "oai:arXiv.org:2504.07793v1",
        "title": "Revisiting Likelihood-Based Out-of-Distribution Detection by Modeling Representations",
        "link": "https://arxiv.org/abs/2504.07793",
        "author": "Yifan Ding, Arturas Aleksandrauskas, Amirhossein Ahmadian, Jonas Unger, Fredrik Lindsten, Gabriel Eilertsen",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07793v1 Announce Type: new \nAbstract: Out-of-distribution (OOD) detection is critical for ensuring the reliability of deep learning systems, particularly in safety-critical applications. Likelihood-based deep generative models have historically faced criticism for their unsatisfactory performance in OOD detection, often assigning higher likelihood to OOD data than in-distribution samples when applied to image data. In this work, we demonstrate that likelihood is not inherently flawed. Rather, several properties in the images space prohibit likelihood as a valid detection score. Given a sufficiently good likelihood estimator, specifically using the probability flow formulation of a diffusion model, we show that likelihood-based methods can still perform on par with state-of-the-art methods when applied in the representation space of pre-trained encoders. The code of our work can be found at $\\href{https://github.com/limchaos/Likelihood-OOD.git}{\\texttt{https://github.com/limchaos/Likelihood-OOD.git}}$."
      },
      {
        "id": "oai:arXiv.org:2504.07794v1",
        "title": "Plan-and-Refine: Diverse and Comprehensive Retrieval-Augmented Generation",
        "link": "https://arxiv.org/abs/2504.07794",
        "author": "Alireza Salemi, Chris Samarinas, Hamed Zamani",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07794v1 Announce Type: new \nAbstract: This paper studies the limitations of (retrieval-augmented) large language models (LLMs) in generating diverse and comprehensive responses, and introduces the Plan-and-Refine (P&amp;R) framework based on a two phase system design. In the global exploration phase, P&amp;R generates a diverse set of plans for the given input, where each plan consists of a list of diverse query aspects with corresponding additional descriptions. This phase is followed by a local exploitation phase that generates a response proposal for the input query conditioned on each plan and iteratively refines the proposal for improving the proposal quality. Finally, a reward model is employed to select the proposal with the highest factuality and coverage. We conduct our experiments based on the ICAT evaluation methodology--a recent approach for answer factuality and comprehensiveness evaluation. Experiments on the two diverse information seeking benchmarks adopted from non-factoid question answering and TREC search result diversification tasks demonstrate that P&amp;R significantly outperforms baselines, achieving up to a 13.1% improvement on the ANTIQUE dataset and a 15.41% improvement on the TREC dataset. Furthermore, a smaller scale user study confirms the substantial efficacy of the P&amp;R framework."
      },
      {
        "id": "oai:arXiv.org:2504.07803v1",
        "title": "A System for Comprehensive Assessment of RAG Frameworks",
        "link": "https://arxiv.org/abs/2504.07803",
        "author": "Mattia Rengo, Senad Beadini, Domenico Alfano, Roberto Abbruzzese",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07803v1 Announce Type: new \nAbstract: Retrieval Augmented Generation (RAG) has emerged as a standard paradigm for enhancing the factual accuracy and contextual relevance of Large Language Models (LLMs) by integrating retrieval mechanisms. However, existing evaluation frameworks fail to provide a holistic black-box approach to assessing RAG systems, especially in real-world deployment scenarios. To address this gap, we introduce SCARF (System for Comprehensive Assessment of RAG Frameworks), a modular and flexible evaluation framework designed to benchmark deployed RAG applications systematically. SCARF provides an end-to-end, black-box evaluation methodology, enabling a limited-effort comparison across diverse RAG frameworks. Our framework supports multiple deployment configurations and facilitates automated testing across vector databases and LLM serving strategies, producing a detailed performance report. Moreover, SCARF integrates practical considerations such as response coherence, providing a scalable and adaptable solution for researchers and industry professionals evaluating RAG applications. Using the REST APIs interface, we demonstrate how SCARF can be applied to real-world scenarios, showcasing its flexibility in assessing different RAG frameworks and configurations. SCARF is available at GitHub repository."
      },
      {
        "id": "oai:arXiv.org:2504.07807v1",
        "title": "Cluster-Driven Expert Pruning for Mixture-of-Experts Large Language Models",
        "link": "https://arxiv.org/abs/2504.07807",
        "author": "Hongcheng Guo, Juntao Yao, Boyang Wang, Junjia Du, Shaosheng Cao, Donglin Di, Shun Zhang, Zhoujun Li",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07807v1 Announce Type: new \nAbstract: Mixture-of-Experts (MoE) architectures have emerged as a promising paradigm for scaling large language models (LLMs) with sparse activation of task-specific experts. Despite their computational efficiency during inference, the massive overall parameter footprint of MoE models (e.g., GPT-4) introduces critical challenges for practical deployment. Current pruning approaches often fail to address two inherent characteristics of MoE systems: 1).intra-layer expert homogeneity where experts within the same MoE layer exhibit functional redundancy, and 2). inter-layer similarity patterns where deeper layers tend to contain progressively more homogeneous experts. To tackle these issues, we propose Cluster-driven Expert Pruning (C-Prune), a novel two-stage framework for adaptive task-specific compression of MoE LLMs. C-Prune operates through layer-wise expert clustering, which groups functionally similar experts within each MoE layer using parameter similarity metrics, followed by global cluster pruning, which eliminates redundant clusters across all layers through a unified importance scoring mechanism that accounts for cross-layer homogeneity. We validate C-Prune through extensive experiments on multiple MoE models and benchmarks. The results demonstrate that C-Prune effectively reduces model size while outperforming existing MoE pruning methods."
      },
      {
        "id": "oai:arXiv.org:2504.07810v1",
        "title": "Nonlocal Retinex-Based Variational Model and its Deep Unfolding Twin for Low-Light Image Enhancement",
        "link": "https://arxiv.org/abs/2504.07810",
        "author": "Daniel Torres, Joan Duran, Julia Navarro, Catalina Sbert",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07810v1 Announce Type: new \nAbstract: Images captured under low-light conditions present significant limitations in many applications, as poor lighting can obscure details, reduce contrast, and hide noise. Removing the illumination effects and enhancing the quality of such images is crucial for many tasks, such as image segmentation and object detection. In this paper, we propose a variational method for low-light image enhancement based on the Retinex decomposition into illumination, reflectance, and noise components. A color correction pre-processing step is applied to the low-light image, which is then used as the observed input in the decomposition. Moreover, our model integrates a novel nonlocal gradient-type fidelity term designed to preserve structural details. Additionally, we propose an automatic gamma correction module. Building on the proposed variational approach, we extend the model by introducing its deep unfolding counterpart, in which the proximal operators are replaced with learnable networks. We propose cross-attention mechanisms to capture long-range dependencies in both the nonlocal prior of the reflectance and the nonlocal gradient-based constraint. Experimental results demonstrate that both methods compare favorably with several recent and state-of-the-art techniques across different datasets. In particular, despite not relying on learning strategies, the variational model outperforms most deep learning approaches both visually and in terms of quality metrics."
      },
      {
        "id": "oai:arXiv.org:2504.07813v1",
        "title": "P2Object: Single Point Supervised Object Detection and Instance Segmentation",
        "link": "https://arxiv.org/abs/2504.07813",
        "author": "Pengfei Chen, Xuehui Yu, Xumeng Han, Kuiran Wang, Guorong Li, Lingxi Xie, Zhenjun Han, Jianbin Jiao",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07813v1 Announce Type: new \nAbstract: Object recognition using single-point supervision has attracted increasing attention recently. However, the performance gap compared with fully-supervised algorithms remains large. Previous works generated class-agnostic \\textbf{\\textit{proposals in an image}} offline and then treated mixed candidates as a single bag, putting a huge burden on multiple instance learning (MIL). In this paper, we introduce Point-to-Box Network (P2BNet), which constructs balanced \\textbf{\\textit{instance-level proposal bags}} by generating proposals in an anchor-like way and refining the proposals in a coarse-to-fine paradigm. Through further research, we find that the bag of proposals, either at the image level or the instance level, is established on discrete box sampling. This leads the pseudo box estimation into a sub-optimal solution, resulting in the truncation of object boundaries or the excessive inclusion of background. Hence, we conduct a series exploration of discrete-to-continuous optimization, yielding P2BNet++ and Point-to-Mask Network (P2MNet). P2BNet++ conducts an approximately continuous proposal sampling strategy by better utilizing spatial clues. P2MNet further introduces low-level image information to assist in pixel prediction, and a boundary self-prediction is designed to relieve the limitation of the estimated boxes. Benefiting from the continuous object-aware \\textbf{\\textit{pixel-level perception}}, P2MNet can generate more precise bounding boxes and generalize to segmentation tasks. Our method largely surpasses the previous methods in terms of the mean average precision on COCO, VOC, SBD, and Cityscapes, demonstrating great potential to bridge the performance gap compared with fully supervised tasks."
      },
      {
        "id": "oai:arXiv.org:2504.07822v1",
        "title": "DG-STMTL: A Novel Graph Convolutional Network for Multi-Task Spatio-Temporal Traffic Forecasting",
        "link": "https://arxiv.org/abs/2504.07822",
        "author": "Wanna Cui, Peizheng Wang, Faliang Yin",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07822v1 Announce Type: new \nAbstract: Spatio-temporal traffic prediction is crucial in intelligent transportation systems. The key challenge of accurate prediction is how to model the complex spatio-temporal dependencies and adapt to the inherent dynamics in data. Traditional Graph Convolutional Networks (GCNs) often struggle with static adjacency matrices that introduce domain bias or learnable matrices that may be overfitting to specific patterns. This challenge becomes more complex when considering Multi-Task Learning (MTL). While MTL has the potential to enhance prediction accuracy through task synergies, it can also face significant hurdles due to task interference. To overcome these challenges, this study introduces a novel MTL framework, Dynamic Group-wise Spatio-Temporal Multi-Task Learning (DG-STMTL). DG-STMTL proposes a hybrid adjacency matrix generation module that combines static matrices with dynamic ones through a task-specific gating mechanism. We also introduce a group-wise GCN module to enhance the modelling capability of spatio-temporal dependencies. We conduct extensive experiments on two real-world datasets to evaluate our method. Results show that our method outperforms other state-of-the-arts, indicating its effectiveness and robustness."
      },
      {
        "id": "oai:arXiv.org:2504.07825v1",
        "title": "What the HellaSwag? On the Validity of Common-Sense Reasoning Benchmarks",
        "link": "https://arxiv.org/abs/2504.07825",
        "author": "Pavel Chizhov, Mattia Nee, Pierre-Carl Langlais, Ivan P. Yamshchikov",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07825v1 Announce Type: new \nAbstract: Common-sense reasoning is a key language model capability because it encapsulates not just specific factual knowledge but rather general language and world understanding. Measuring common-sense reasoning, therefore, is crucial for language models of different sizes and applications. One of the most widely used benchmarks for evaluating such capabilities is HellaSwag; however, in this paper, we show that it has severe construct validity issues. These issues range from basic ungrammaticality and numerous typos to misleading prompts or equally correct options. Furthermore, we show that if models are evaluated only on answer texts, or with \"Lorem ipsum dolor...\" instead of the question, more than 65% of model predictions remain the same, and this cannot be attributed merely to contamination. Since benchmark scores are an essential part of model selection in both research and commercial applications, these validity issues can have severe consequences. In particular, knowing that taking benchmark scores at face value is ubiquitous, inadequate evaluation leads to ill-informed decisions about models. In this paper, we thoroughly investigate critical validity issues posed by HellaSwag and illustrate them with various evaluations using generative language models of different sizes. We argue that this benchmark does not accurately measure common-sense reasoning and, therefore, should not be used for evaluation in its current state. Based on the results of our study, we propose requirements that should be met by future common-sense reasoning benchmarks. In addition, we release GoldenSwag, a corrected subset of HellaSwag, which, to our belief, facilitates acceptable common-sense reasoning evaluation."
      },
      {
        "id": "oai:arXiv.org:2504.07826v1",
        "title": "MuSaRoNews: A Multidomain, Multimodal Satire Dataset from Romanian News Articles",
        "link": "https://arxiv.org/abs/2504.07826",
        "author": "R\\u{a}zvan-Alexandru Sm\\u{a}du, Andreea Iuga, Dumitru-Clementin Cercel",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07826v1 Announce Type: new \nAbstract: Satire and fake news can both contribute to the spread of false information, even though both have different purposes (one if for amusement, the other is to misinform). However, it is not enough to rely purely on text to detect the incongruity between the surface meaning and the actual meaning of the news articles, and, often, other sources of information (e.g., visual) provide an important clue for satire detection. This work introduces a multimodal corpus for satire detection in Romanian news articles named MuSaRoNews. Specifically, we gathered 117,834 public news articles from real and satirical news sources, composing the first multimodal corpus for satire detection in the Romanian language. We conducted experiments and showed that the use of both modalities improves performance."
      },
      {
        "id": "oai:arXiv.org:2504.07830v1",
        "title": "MOSAIC: Modeling Social AI for Content Dissemination and Regulation in Multi-Agent Simulations",
        "link": "https://arxiv.org/abs/2504.07830",
        "author": "Genglin Liu, Salman Rahman, Elisa Kreiss, Marzyeh Ghassemi, Saadia Gabriel",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07830v1 Announce Type: new \nAbstract: We present a novel, open-source social network simulation framework, MOSAIC, where generative language agents predict user behaviors such as liking, sharing, and flagging content. This simulation combines LLM agents with a directed social graph to analyze emergent deception behaviors and gain a better understanding of how users determine the veracity of online social content. By constructing user representations from diverse fine-grained personas, our system enables multi-agent simulations that model content dissemination and engagement dynamics at scale. Within this framework, we evaluate three different content moderation strategies with simulated misinformation dissemination, and we find that they not only mitigate the spread of non-factual content but also increase user engagement. In addition, we analyze the trajectories of popular content in our simulations, and explore whether simulation agents' articulated reasoning for their social interactions truly aligns with their collective engagement patterns. We open-source our simulation software to encourage further research within AI and social sciences."
      },
      {
        "id": "oai:arXiv.org:2504.07835v1",
        "title": "Pychop: Emulating Low-Precision Arithmetic in Numerical Methods and Neural Networks",
        "link": "https://arxiv.org/abs/2504.07835",
        "author": "Erin Carson, Xinye Chen",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07835v1 Announce Type: new \nAbstract: Motivated by the growing demand for low-precision arithmetic in computational science, we exploit lower-precision emulation in Python -- widely regarded as the dominant programming language for numerical analysis and machine learning. Low-precision training has revolutionized deep learning by enabling more efficient computation and reduced memory and energy consumption while maintaining model fidelity. To better enable numerical experimentation with and exploration of low precision computation, we developed the Pychop library, which supports customizable floating-point formats and a comprehensive set of rounding modes in Python, allowing users to benefit from fast, low-precision emulation in numerous applications. Pychop also introduces interfaces for both PyTorch and JAX, enabling efficient low-precision emulation on GPUs for neural network training and inference with unparalleled flexibility.\n  In this paper, we offer a comprehensive exposition of the design, implementation, validation, and practical application of Pychop, establishing it as a foundational tool for advancing efficient mixed-precision algorithms. Furthermore, we present empirical results on low-precision emulation for image classification and object detection using published datasets, illustrating the sensitivity of the use of low precision and offering valuable insights into its impact. Pychop enables in-depth investigations into the effects of numerical precision, facilitates the development of novel hardware accelerators, and integrates seamlessly into existing deep learning workflows. Software and experimental code are publicly available at https://github.com/inEXASCALE/pychop."
      },
      {
        "id": "oai:arXiv.org:2504.07836v1",
        "title": "AerialVG: A Challenging Benchmark for Aerial Visual Grounding by Exploring Positional Relations",
        "link": "https://arxiv.org/abs/2504.07836",
        "author": "Junli Liu, Qizhi Chen, Zhigang Wang, Yiwen Tang, Yiting Zhang, Chi Yan, Dong Wang, Xuelong Li, Bin Zhao",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07836v1 Announce Type: new \nAbstract: Visual grounding (VG) aims to localize target objects in an image based on natural language descriptions. In this paper, we propose AerialVG, a new task focusing on visual grounding from aerial views. Compared to traditional VG, AerialVG poses new challenges, \\emph{e.g.}, appearance-based grounding is insufficient to distinguish among multiple visually similar objects, and positional relations should be emphasized. Besides, existing VG models struggle when applied to aerial imagery, where high-resolution images cause significant difficulties. To address these challenges, we introduce the first AerialVG dataset, consisting of 5K real-world aerial images, 50K manually annotated descriptions, and 103K objects. Particularly, each annotation in AerialVG dataset contains multiple target objects annotated with relative spatial relations, requiring models to perform comprehensive spatial reasoning. Furthermore, we propose an innovative model especially for the AerialVG task, where a Hierarchical Cross-Attention is devised to focus on target regions, and a Relation-Aware Grounding module is designed to infer positional relations. Experimental results validate the effectiveness of our dataset and method, highlighting the importance of spatial reasoning in aerial visual grounding. The code and dataset will be released."
      },
      {
        "id": "oai:arXiv.org:2504.07848v1",
        "title": "Opinion dynamics and the unpredictability of opinion trajectories in an adaptive social network model",
        "link": "https://arxiv.org/abs/2504.07848",
        "author": "Akshay Gangadhar, Hiroki Sayama",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07848v1 Announce Type: new \nAbstract: Understanding opinion dynamics in social networks is critical for predicting social behavior and detecting polarization. Traditional approaches often rely on static snapshots of network states, which can obscure the underlying dynamics of opinion evolution. In this study, we introduce a dynamic framework that quantifies the unpredictability of opinion trajectories using the normalized Lempel-Ziv (nLZ) complexity. Our approach leverages an adaptive social network model where each node is characterized by three behavioral parameters - homophily, neophily, and social conformity - and where opinions evolve continuously according to a system of ordinary differential equations. The results reveal distinct nLZ complexity signatures for each node type: homophilic nodes exhibit consistently rising complexity, reflecting increasingly unpredictable opinion shifts that are counterintuitive given their tendency for similarity; neophilic nodes maintain low and stable complexity, suggesting that openness to novelty can, surprisingly, lead to stable opinion dynamics; and conformic nodes display a U-shaped complexity trend, transitioning from early opinion stagnation to later unpredictability. In fully heterogeneous networks, modest interaction effects emerge, with slight shifts in the unpredictability of each faction's trajectories. These findings underscore the importance of temporal analysis in uncovering hidden dynamical patterns, offering novel insights into the mechanisms underlying social adaptation and polarization."
      },
      {
        "id": "oai:arXiv.org:2504.07853v1",
        "title": "V2V3D: View-to-View Denoised 3D Reconstruction for Light-Field Microscopy",
        "link": "https://arxiv.org/abs/2504.07853",
        "author": "Jiayin Zhao, Zhenqi Fu, Tao Yu, Hui Qiao",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07853v1 Announce Type: new \nAbstract: Light field microscopy (LFM) has gained significant attention due to its ability to capture snapshot-based, large-scale 3D fluorescence images. However, existing LFM reconstruction algorithms are highly sensitive to sensor noise or require hard-to-get ground-truth annotated data for training. To address these challenges, this paper introduces V2V3D, an unsupervised view2view-based framework that establishes a new paradigm for joint optimization of image denoising and 3D reconstruction in a unified architecture. We assume that the LF images are derived from a consistent 3D signal, with the noise in each view being independent. This enables V2V3D to incorporate the principle of noise2noise for effective denoising. To enhance the recovery of high-frequency details, we propose a novel wave-optics-based feature alignment technique, which transforms the point spread function, used for forward propagation in wave optics, into convolution kernels specifically designed for feature alignment. Moreover, we introduce an LFM dataset containing LF images and their corresponding 3D intensity volumes. Extensive experiments demonstrate that our approach achieves high computational efficiency and outperforms the other state-of-the-art methods. These advancements position V2V3D as a promising solution for 3D imaging under challenging conditions."
      },
      {
        "id": "oai:arXiv.org:2504.07854v1",
        "title": "The KL3M Data Project: Copyright-Clean Training Resources for Large Language Models",
        "link": "https://arxiv.org/abs/2504.07854",
        "author": "Michael J Bommarito II, Jillian Bommarito, Daniel Martin Katz",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07854v1 Announce Type: new \nAbstract: Practically all large language models have been pre-trained on data that is subject to global uncertainty related to copyright infringement and breach of contract. This creates potential risk for users and developers due to this uncertain legal status. The KL3M Data Project directly confronts this critical issue by introducing the largest comprehensive training data pipeline that minimizes risks related to copyright or breach of contract. The foundation of this project is a corpus of over 132 million documents and trillions of tokens spanning 16 different sources that have been verified to meet the strict copyright and licensing protocol detailed herein. We are releasing the entire pipeline, including 1) the source code to acquire and process these documents, 2) the original document formats with associated provenance and metadata, 3) extracted content in a standardized format, 4) pre-tokenized representations of the documents, and 5) various mid- and post-train resources such as question-answer, summarization, conversion, drafting, classification, prediction, and conversational data. All of these resources are freely available to the public on S3, Hugging Face, and GitHub under CC-BY terms. We are committed to continuing this project in furtherance of a more ethical, legal, and sustainable approach to the development and use of AI models."
      },
      {
        "id": "oai:arXiv.org:2504.07863v1",
        "title": "Robust Hallucination Detection in LLMs via Adaptive Token Selection",
        "link": "https://arxiv.org/abs/2504.07863",
        "author": "Mengjia Niu, Hamed Haddadi, Guansong Pang",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07863v1 Announce Type: new \nAbstract: Hallucinations in large language models (LLMs) pose significant safety concerns that impede their broader deployment. Recent research in hallucination detection has demonstrated that LLMs' internal representations contain truthfulness hints, which can be harnessed for detector training. However, the performance of these detectors is heavily dependent on the internal representations of predetermined tokens, fluctuating considerably when working on free-form generations with varying lengths and sparse distributions of hallucinated entities. To address this, we propose HaMI, a novel approach that enables robust detection of hallucinations through adaptive selection and learning of critical tokens that are most indicative of hallucinations. We achieve this robustness by an innovative formulation of the Hallucination detection task as Multiple Instance (HaMI) learning over token-level representations within a sequence, thereby facilitating a joint optimisation of token selection and hallucination detection on generation sequences of diverse forms. Comprehensive experimental results on four hallucination benchmarks show that HaMI significantly outperforms existing state-of-the-art approaches."
      },
      {
        "id": "oai:arXiv.org:2504.07866v1",
        "title": "Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs",
        "link": "https://arxiv.org/abs/2504.07866",
        "author": "Yichun Yin, Wenyong Huang, Kaikai Song, Yehui Tang, Xueyu Wu, Wei Guo, Peng Guo, Yaoyuan Wang, Xiaojun Meng, Yasheng Wang, Dong Li, Can Chen, Dandan Tu, Yin Li, Fisher Yu, Ruiming Tang, Yunhe Wang, Baojun Wang, Bin Wang, Bo Wang, Boxiao Liu, Changzheng Zhang, Duyu Tang, Fei Mi, Hui Jin, Jiansheng Wei, Jiarui Qin, Jinpeng Li, Jun Zhao, Liqun Deng, Lin Li, Minghui Xu, Naifu Zhang, Nianzu Zheng, Qiang Li, Rongju Ruan, Shengjun Cheng, Tianyu Guo, Wei He, Wei Li, Weiwen Liu, Wulong Liu, Xinyi Dai, Yonghan Dong, Yu Pan, Yue Li, Yufei Wang, Yujun Li, Yunsheng Ni, Zhe Liu, Zhenhe Zhang, Zhicheng Liu",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07866v1 Announce Type: new \nAbstract: We present Pangu Ultra, a Large Language Model (LLM) with 135 billion parameters and dense Transformer modules trained on Ascend Neural Processing Units (NPUs). Although the field of LLM has been witnessing unprecedented advances in pushing the scale and capability of LLM in recent years, training such a large-scale model still involves significant optimization and system challenges. To stabilize the training process, we propose depth-scaled sandwich normalization, which effectively eliminates loss spikes during the training process of deep models. We pre-train our model on 13.2 trillion diverse and high-quality tokens and further enhance its reasoning capabilities during post-training. To perform such large-scale training efficiently, we utilize 8,192 Ascend NPUs with a series of system optimizations. Evaluations on multiple diverse benchmarks indicate that Pangu Ultra significantly advances the state-of-the-art capabilities of dense LLMs such as Llama 405B and Mistral Large 2, and even achieves competitive results with DeepSeek-R1, whose sparse model structure contains much more parameters. Our exploration demonstrates that Ascend NPUs are capable of efficiently and effectively training dense models with more than 100 billion parameters. Our model and system will be available for our commercial customers."
      },
      {
        "id": "oai:arXiv.org:2504.07867v1",
        "title": "SAMJAM: Zero-Shot Video Scene Graph Generation for Egocentric Kitchen Videos",
        "link": "https://arxiv.org/abs/2504.07867",
        "author": "Joshua Li, Fernando Jose Pena Cantu, Emily Yu, Alexander Wong, Yuchen Cui, Yuhao Chen",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07867v1 Announce Type: new \nAbstract: Video Scene Graph Generation (VidSGG) is an important topic in understanding dynamic kitchen environments. Current models for VidSGG require extensive training to produce scene graphs. Recently, Vision Language Models (VLM) and Vision Foundation Models (VFM) have demonstrated impressive zero-shot capabilities in a variety of tasks. However, VLMs like Gemini struggle with the dynamics for VidSGG, failing to maintain stable object identities across frames. To overcome this limitation, we propose SAMJAM, a zero-shot pipeline that combines SAM2's temporal tracking with Gemini's semantic understanding. SAM2 also improves upon Gemini's object grounding by producing more accurate bounding boxes. In our method, we first prompt Gemini to generate a frame-level scene graph. Then, we employ a matching algorithm to map each object in the scene graph with a SAM2-generated or SAM2-propagated mask, producing a temporally-consistent scene graph in dynamic environments. Finally, we repeat this process again in each of the following frames. We empirically demonstrate that SAMJAM outperforms Gemini by 8.33% in mean recall on the EPIC-KITCHENS and EPIC-KITCHENS-100 datasets."
      },
      {
        "id": "oai:arXiv.org:2504.07878v1",
        "title": "Token Level Routing Inference System for Edge Devices",
        "link": "https://arxiv.org/abs/2504.07878",
        "author": "Jianshu She, Wenhao Zheng, Zhengzhong Liu, Hongyi Wang, Eric Xing, Huaxiu Yao, Qirong Ho",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07878v1 Announce Type: new \nAbstract: The computational complexity of large language model (LLM) inference significantly constrains their deployment efficiency on edge devices. In contrast, small language models offer faster decoding and lower resource consumption but often suffer from degraded response quality and heightened susceptibility to hallucinations. To address this trade-off, collaborative decoding, in which a large model assists in generating critical tokens, has emerged as a promising solution. This paradigm leverages the strengths of both model types by enabling high-quality inference through selective intervention of the large model, while maintaining the speed and efficiency of the smaller model. In this work, we present a novel collaborative decoding inference system that allows small models to perform on-device inference while selectively consulting a cloud-based large model for critical token generation. Remarkably, the system achieves a 60% performance gain on CommonsenseQA using only a 0.5B model on an M1 MacBook, with under 7% of tokens generation uploaded to the large model in the cloud."
      },
      {
        "id": "oai:arXiv.org:2504.07887v1",
        "title": "Benchmarking Adversarial Robustness to Bias Elicitation in Large Language Models: Scalable Automated Assessment with LLM-as-a-Judge",
        "link": "https://arxiv.org/abs/2504.07887",
        "author": "Riccardo Cantini, Alessio Orsino, Massimo Ruggiero, Domenico Talia",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07887v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have revolutionized artificial intelligence, driving advancements in machine translation, summarization, and conversational agents. However, their increasing integration into critical societal domains has raised concerns about embedded biases, which can perpetuate stereotypes and compromise fairness. These biases stem from various sources, including historical inequalities in training data, linguistic imbalances, and adversarial manipulation. Despite mitigation efforts, recent studies indicate that LLMs remain vulnerable to adversarial attacks designed to elicit biased responses. This work proposes a scalable benchmarking framework to evaluate LLM robustness against adversarial bias elicitation. Our methodology involves (i) systematically probing models with a multi-task approach targeting biases across various sociocultural dimensions, (ii) quantifying robustness through safety scores using an LLM-as-a-Judge approach for automated assessment of model responses, and (iii) employing jailbreak techniques to investigate vulnerabilities in safety mechanisms. Our analysis examines prevalent biases in both small and large state-of-the-art models and their impact on model safety. Additionally, we assess the safety of domain-specific models fine-tuned for critical fields, such as medicine. Finally, we release a curated dataset of bias-related prompts, CLEAR-Bias, to facilitate systematic vulnerability benchmarking. Our findings reveal critical trade-offs between model size and safety, aiding the development of fairer and more robust future language models."
      },
      {
        "id": "oai:arXiv.org:2504.07891v1",
        "title": "SpecReason: Fast and Accurate Inference-Time Compute via Speculative Reasoning",
        "link": "https://arxiv.org/abs/2504.07891",
        "author": "Rui Pan, Yinwei Dai, Zhihao Zhang, Gabriele Oliaro, Zhihao Jia, Ravi Netravali",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07891v1 Announce Type: new \nAbstract: Recent advances in inference-time compute have significantly improved performance on complex tasks by generating long chains of thought (CoTs) using Large Reasoning Models (LRMs). However, this improved accuracy comes at the cost of high inference latency due to the length of generated reasoning sequences and the autoregressive nature of decoding. Our key insight in tackling these overheads is that LRM inference, and the reasoning that it embeds, is highly tolerant of approximations: complex tasks are typically broken down into simpler steps, each of which brings utility based on the semantic insight it provides for downstream steps rather than the exact tokens it generates. Accordingly, we introduce SpecReason, a system that automatically accelerates LRM inference by using a lightweight model to (speculatively) carry out simpler intermediate reasoning steps and reserving the costly base model only to assess (and potentially correct) the speculated outputs. Importantly, SpecReason's focus on exploiting the semantic flexibility of thinking tokens in preserving final-answer accuracy is complementary to prior speculation techniques, most notably speculative decoding, which demands token-level equivalence at each step. Across a variety of reasoning benchmarks, SpecReason achieves 1.5-2.5$\\times$ speedup over vanilla LRM inference while improving accuracy by 1.0-9.9\\%. Compared to speculative decoding without SpecReason, their combination yields an additional 19.4-44.2\\% latency reduction. We open-source SpecReason at https://github.com/ruipeterpan/specreason."
      },
      {
        "id": "oai:arXiv.org:2504.07894v1",
        "title": "DiverseFlow: Sample-Efficient Diverse Mode Coverage in Flows",
        "link": "https://arxiv.org/abs/2504.07894",
        "author": "Mashrur M. Morshed, Vishnu Boddeti",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07894v1 Announce Type: new \nAbstract: Many real-world applications of flow-based generative models desire a diverse set of samples that cover multiple modes of the target distribution. However, the predominant approach for obtaining diverse sets is not sample-efficient, as it involves independently obtaining many samples from the source distribution and mapping them through the flow until the desired mode coverage is achieved. As an alternative to repeated sampling, we introduce DiverseFlow: a training-free approach to improve the diversity of flow models. Our key idea is to employ a determinantal point process to induce a coupling between the samples that drives diversity under a fixed sampling budget. In essence, DiverseFlow allows exploration of more variations in a learned flow model with fewer samples. We demonstrate the efficacy of our method for tasks where sample-efficient diversity is desirable, such as text-guided image generation with polysemous words, inverse problems like large-hole inpainting, and class-conditional image synthesis."
      },
      {
        "id": "oai:arXiv.org:2504.07896v1",
        "title": "Fast Adaptation with Behavioral Foundation Models",
        "link": "https://arxiv.org/abs/2504.07896",
        "author": "Harshit Sikchi, Andrea Tirinzoni, Ahmed Touati, Yingchen Xu, Anssi Kanervisto, Scott Niekum, Amy Zhang, Alessandro Lazaric, Matteo Pirotta",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07896v1 Announce Type: new \nAbstract: Unsupervised zero-shot reinforcement learning (RL) has emerged as a powerful paradigm for pretraining behavioral foundation models (BFMs), enabling agents to solve a wide range of downstream tasks specified via reward functions in a zero-shot fashion, i.e., without additional test-time learning or planning. This is achieved by learning self-supervised task embeddings alongside corresponding near-optimal behaviors and incorporating an inference procedure to directly retrieve the latent task embedding and associated policy for any given reward function. Despite promising results, zero-shot policies are often suboptimal due to errors induced by the unsupervised training process, the embedding, and the inference procedure. In this paper, we focus on devising fast adaptation strategies to improve the zero-shot performance of BFMs in a few steps of online interaction with the environment while avoiding any performance drop during the adaptation process. Notably, we demonstrate that existing BFMs learn a set of skills containing more performant policies than those identified by their inference procedure, making them well-suited for fast adaptation. Motivated by this observation, we propose both actor-critic and actor-only fast adaptation strategies that search in the low-dimensional task-embedding space of the pre-trained BFM to rapidly improve the performance of its zero-shot policies on any downstream task. Notably, our approach mitigates the initial \"unlearning\" phase commonly observed when fine-tuning pre-trained RL models. We evaluate our fast adaptation strategies on top of four state-of-the-art zero-shot RL methods in multiple navigation and locomotion domains. Our results show that they achieve 10-40% improvement over their zero-shot performance in a few tens of episodes, outperforming existing baselines."
      },
      {
        "id": "oai:arXiv.org:2504.07901v1",
        "title": "Redefining Machine Translation on Social Network Services with Large Language Models",
        "link": "https://arxiv.org/abs/2504.07901",
        "author": "Hongcheng Guo, Fei Zhao, Shaosheng Cao, Xinze Lyu, Ziyan Liu, Yue Wang, Boyang Wang, Zhoujun Li, Chonggang Lu, Zhe Xu, Yao Hu",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07901v1 Announce Type: new \nAbstract: The globalization of social interactions has heightened the need for machine translation (MT) on Social Network Services (SNS), yet traditional models struggle with culturally nuanced content like memes, slang, and pop culture references. While large language models (LLMs) have advanced general-purpose translation, their performance on SNS-specific content remains limited due to insufficient specialized training data and evaluation benchmarks. This paper introduces RedTrans, a 72B LLM tailored for SNS translation, trained on a novel dataset developed through three innovations: (1) Supervised Finetuning with Dual-LLM Back-Translation Sampling, an unsupervised sampling method using LLM-based back-translation to select diverse data for large-scale finetuning; (2) Rewritten Preference Optimization (RePO), an algorithm that identifies and corrects erroneous preference pairs through expert annotation, building reliable preference corpora; and (3) RedTrans-Bench, the first benchmark for SNS translation, evaluating phenomena like humor localization, emoji semantics, and meme adaptation. Experiments show RedTrans outperforms state-of-the-art LLMs. Besides, RedTrans has already been deployed in a real-world production environment, demonstrating that domain-specific adaptation, effectively bridges the gap between generic and culturally grounded translation systems."
      },
      {
        "id": "oai:arXiv.org:2504.07910v1",
        "title": "Hodge Laplacians and Hodge Diffusion Maps",
        "link": "https://arxiv.org/abs/2504.07910",
        "author": "Alvaro Almeida Gomez, Jorge Duque Franco",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07910v1 Announce Type: new \nAbstract: We introduce Hodge Diffusion Maps, a novel manifold learning algorithm designed to analyze and extract topological information from high-dimensional data-sets. This method approximates the exterior derivative acting on differential forms, thereby providing an approximation of the Hodge Laplacian operator. Hodge Diffusion Maps extend existing non-linear dimensionality reduction techniques, including vector diffusion maps, as well as the theories behind diffusion maps and Laplacian Eigenmaps. Our approach captures higher-order topological features of the data-set by projecting it into lower-dimensional Euclidean spaces using the Hodge Laplacian. We develop a theoretical framework to estimate the approximation error of the exterior derivative, based on sample points distributed over a real manifold. Numerical experiments support and validate the proposed methodology."
      },
      {
        "id": "oai:arXiv.org:2504.07912v1",
        "title": "Echo Chamber: RL Post-training Amplifies Behaviors Learned in Pretraining",
        "link": "https://arxiv.org/abs/2504.07912",
        "author": "Rosie Zhao, Alexandru Meterez, Sham Kakade, Cengiz Pehlevan, Samy Jelassi, Eran Malach",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07912v1 Announce Type: new \nAbstract: Reinforcement learning (RL)-based fine-tuning has become a crucial step in post-training language models for advanced mathematical reasoning and coding. Following the success of frontier reasoning models, recent work has demonstrated that RL fine-tuning consistently improves performance, even in smaller-scale models; however, the underlying mechanisms driving these improvements are not well-understood. Understanding the effects of RL fine-tuning requires disentangling its interaction with pretraining data composition, hyperparameters, and model scale, but such problems are exacerbated by the lack of transparency regarding the training data used in many existing models. In this work, we present a systematic end-to-end study of RL fine-tuning for mathematical reasoning by training models entirely from scratch on different mixtures of fully open datasets. We investigate the effects of various RL fine-tuning algorithms (PPO, GRPO, and Expert Iteration) across models of different scales. Our study reveals that RL algorithms consistently converge towards a dominant output distribution, amplifying patterns in the pretraining data. We also find that models of different scales trained on the same data mixture will converge to distinct output distributions, suggesting that there are scale-dependent biases in model generalization. Moreover, we find that RL post-training on simpler questions can lead to performance gains on harder ones, indicating that certain reasoning capabilities generalize across tasks. Our findings show that small-scale proxies in controlled settings can elicit interesting insights regarding the role of RL in shaping language model behavior."
      },
      {
        "id": "oai:arXiv.org:2504.07916v1",
        "title": "Semantically Encoding Activity Labels for Context-Aware Human Activity Recognition",
        "link": "https://arxiv.org/abs/2504.07916",
        "author": "Wen Ge, Guanyi Mou, Emmanuel O. Agu, Kyumin Lee",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07916v1 Announce Type: new \nAbstract: Prior work has primarily formulated CA-HAR as a multi-label classification problem, where model inputs are time-series sensor data and target labels are binary encodings representing whether a given activity or context occurs. These CA-HAR methods either predicted each label independently or manually imposed relationships using graphs. However, both strategies often neglect an essential aspect: activity labels have rich semantic relationships. For instance, walking, jogging, and running activities share similar movement patterns but differ in pace and intensity, indicating that they are semantically related. Consequently, prior CA-HAR methods often struggled to accurately capture these inherent and nuanced relationships, particularly on datasets with noisy labels typically used for CA-HAR or situations where the ideal sensor type is unavailable (e.g., recognizing speech without audio sensors). To address this limitation, we propose SEAL, which leverage LMs to encode CA-HAR activity labels to capture semantic relationships. LMs generate vector embeddings that preserve rich semantic information from natural language. Our SEAL approach encodes input-time series sensor data from smart devices and their associated activity and context labels (text) as vector embeddings. During training, SEAL aligns the sensor data representations with their corresponding activity/context label embeddings in a shared embedding space. At inference time, SEAL performs a similarity search, returning the CA-HAR label with the embedding representation closest to the input data. Although LMs have been widely explored in other domains, surprisingly, their potential in CA-HAR has been underexplored, making our approach a novel contribution to the field. Our research opens up new possibilities for integrating more advanced LMs into CA-HAR tasks."
      },
      {
        "id": "oai:arXiv.org:2504.07934v1",
        "title": "SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual Reasoning Self-Improvement",
        "link": "https://arxiv.org/abs/2504.07934",
        "author": "Xiyao Wang, Zhengyuan Yang, Chao Feng, Hongjin Lu, Linjie Li, Chung-Ching Lin, Kevin Lin, Furong Huang, Lijuan Wang",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07934v1 Announce Type: new \nAbstract: In this paper, we present an effective method to enhance visual reasoning with significantly fewer training samples, relying purely on self-improvement with no knowledge distillation. Our key insight is that the difficulty of training data during reinforcement fine-tuning (RFT) is critical. Appropriately challenging samples can substantially boost reasoning capabilities even when the dataset is small. Despite being intuitive, the main challenge remains in accurately quantifying sample difficulty to enable effective data filtering. To this end, we propose a novel way of repurposing Monte Carlo Tree Search (MCTS) to achieve that. Starting from our curated 70k open-source training samples, we introduce an MCTS-based selection method that quantifies sample difficulty based on the number of iterations required by the VLMs to solve each problem. This explicit step-by-step reasoning in MCTS enforces the model to think longer and better identifies samples that are genuinely challenging. We filter and retain 11k samples to perform RFT on Qwen2.5-VL-7B-Instruct, resulting in our final model, ThinkLite-VL. Evaluation results on eight benchmarks show that ThinkLite-VL improves the average performance of Qwen2.5-VL-7B-Instruct by 7%, using only 11k training samples with no knowledge distillation. This significantly outperforms all existing 7B-level reasoning VLMs, and our fairly comparable baselines that use classic selection methods such as accuracy-based filtering. Notably, on MathVista, ThinkLite-VL-7B achieves the SoTA accuracy of 75.1, surpassing Qwen2.5-VL-72B, GPT-4o, and O1. Our code, data, and model are available at https://github.com/si0wang/ThinkLite-VL."
      },
      {
        "id": "oai:arXiv.org:2504.07940v1",
        "title": "Beyond the Frame: Generating 360{\\deg} Panoramic Videos from Perspective Videos",
        "link": "https://arxiv.org/abs/2504.07940",
        "author": "Rundong Luo, Matthew Wallingford, Ali Farhadi, Noah Snavely, Wei-Chiu Ma",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07940v1 Announce Type: new \nAbstract: 360{\\deg} videos have emerged as a promising medium to represent our dynamic visual world. Compared to the \"tunnel vision\" of standard cameras, their borderless field of view offers a more complete perspective of our surroundings. While existing video models excel at producing standard videos, their ability to generate full panoramic videos remains elusive. In this paper, we investigate the task of video-to-360{\\deg} generation: given a perspective video as input, our goal is to generate a full panoramic video that is consistent with the original video. Unlike conventional video generation tasks, the output's field of view is significantly larger, and the model is required to have a deep understanding of both the spatial layout of the scene and the dynamics of objects to maintain spatio-temporal consistency. To address these challenges, we first leverage the abundant 360{\\deg} videos available online and develop a high-quality data filtering pipeline to curate pairwise training data. We then carefully design a series of geometry- and motion-aware operations to facilitate the learning process and improve the quality of 360{\\deg} video generation. Experimental results demonstrate that our model can generate realistic and coherent 360{\\deg} videos from in-the-wild perspective video. In addition, we showcase its potential applications, including video stabilization, camera viewpoint control, and interactive visual question answering."
      },
      {
        "id": "oai:arXiv.org:2504.07942v1",
        "title": "MARS: a Multimodal Alignment and Ranking System for Few-Shot Segmentation",
        "link": "https://arxiv.org/abs/2504.07942",
        "author": "Nico Catalano, Stefano Samele, Paolo Pertino, Matteo Matteucci",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07942v1 Announce Type: new \nAbstract: Current Few Shot Segmentation literature lacks a mask selection method that goes beyond visual similarity between the query and example images, leading to suboptimal predictions. We present MARS, a plug-and-play ranking system that leverages multimodal cues to filter and merge mask proposals robustly. Starting from a set of mask predictions for a single query image, we score, filter, and merge them to improve results. Proposals are evaluated using multimodal scores computed at local and global levels. Extensive experiments on COCO-20i, Pascal-5i, LVIS-92i, and FSS-1000 demonstrate that integrating all four scoring components is crucial for robust ranking, validating our contribution. As MARS can be effortlessly integrated with various mask proposal systems, we deploy it across a wide range of top-performer methods and achieve new state-of-the-art results on multiple existing benchmarks. Code will be available upon acceptance."
      },
      {
        "id": "oai:arXiv.org:2504.07943v1",
        "title": "HoloPart: Generative 3D Part Amodal Segmentation",
        "link": "https://arxiv.org/abs/2504.07943",
        "author": "Yunhan Yang, Yuan-Chen Guo, Yukun Huang, Zi-Xin Zou, Zhipeng Yu, Yangguang Li, Yan-Pei Cao, Xihui Liu",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07943v1 Announce Type: new \nAbstract: 3D part amodal segmentation--decomposing a 3D shape into complete, semantically meaningful parts, even when occluded--is a challenging but crucial task for 3D content creation and understanding. Existing 3D part segmentation methods only identify visible surface patches, limiting their utility. Inspired by 2D amodal segmentation, we introduce this novel task to the 3D domain and propose a practical, two-stage approach, addressing the key challenges of inferring occluded 3D geometry, maintaining global shape consistency, and handling diverse shapes with limited training data. First, we leverage existing 3D part segmentation to obtain initial, incomplete part segments. Second, we introduce HoloPart, a novel diffusion-based model, to complete these segments into full 3D parts. HoloPart utilizes a specialized architecture with local attention to capture fine-grained part geometry and global shape context attention to ensure overall shape consistency. We introduce new benchmarks based on the ABO and PartObjaverse-Tiny datasets and demonstrate that HoloPart significantly outperforms state-of-the-art shape completion methods. By incorporating HoloPart with existing segmentation techniques, we achieve promising results on 3D part amodal segmentation, opening new avenues for applications in geometry editing, animation, and material assignment."
      },
      {
        "id": "oai:arXiv.org:2504.07945v1",
        "title": "GenEAva: Generating Cartoon Avatars with Fine-Grained Facial Expressions from Realistic Diffusion-based Faces",
        "link": "https://arxiv.org/abs/2504.07945",
        "author": "Hao Yu, Rupayan Mallick, Margrit Betke, Sarah Adel Bargal",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07945v1 Announce Type: new \nAbstract: Cartoon avatars have been widely used in various applications, including social media, online tutoring, and gaming. However, existing cartoon avatar datasets and generation methods struggle to present highly expressive avatars with fine-grained facial expressions and are often inspired from real-world identities, raising privacy concerns. To address these challenges, we propose a novel framework, GenEAva, for generating high-quality cartoon avatars with fine-grained facial expressions. Our approach fine-tunes a state-of-the-art text-to-image diffusion model to synthesize highly detailed and expressive facial expressions. We then incorporate a stylization model that transforms these realistic faces into cartoon avatars while preserving both identity and expression. Leveraging this framework, we introduce the first expressive cartoon avatar dataset, GenEAva 1.0, specifically designed to capture 135 fine-grained facial expressions, featuring 13,230 expressive cartoon avatars with a balanced distribution across genders, racial groups, and age ranges. We demonstrate that our fine-tuned model generates more expressive faces than the state-of-the-art text-to-image diffusion model SDXL. We also verify that the cartoon avatars generated by our framework do not include memorized identities from fine-tuning data. The proposed framework and dataset provide a diverse and expressive benchmark for future research in cartoon avatar generation."
      },
      {
        "id": "oai:arXiv.org:2504.07949v1",
        "title": "InteractAvatar: Modeling Hand-Face Interaction in Photorealistic Avatars with Deformable Gaussians",
        "link": "https://arxiv.org/abs/2504.07949",
        "author": "Kefan Chen, Sergiu Oprea, Justin Theiss, Sreyas Mohan, Srinath Sridhar, Aayush Prakash",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07949v1 Announce Type: new \nAbstract: With the rising interest from the community in digital avatars coupled with the importance of expressions and gestures in communication, modeling natural avatar behavior remains an important challenge across many industries such as teleconferencing, gaming, and AR/VR. Human hands are the primary tool for interacting with the environment and essential for realistic human behavior modeling, yet existing 3D hand and head avatar models often overlook the crucial aspect of hand-body interactions, such as between hand and face. We present InteracttAvatar, the first model to faithfully capture the photorealistic appearance of dynamic hand and non-rigid hand-face interactions. Our novel Dynamic Gaussian Hand model, combining template model and 3D Gaussian Splatting as well as a dynamic refinement module, captures pose-dependent change, e.g. the fine wrinkles and complex shadows that occur during articulation. Importantly, our hand-face interaction module models the subtle geometry and appearance dynamics that underlie common gestures. Through experiments of novel view synthesis, self reenactment and cross-identity reenactment, we demonstrate that InteracttAvatar can reconstruct hand and hand-face interactions from monocular or multiview videos with high-fidelity details and be animated with novel poses."
      },
      {
        "id": "oai:arXiv.org:2504.07951v1",
        "title": "Scaling Laws for Native Multimodal Models Scaling Laws for Native Multimodal Models",
        "link": "https://arxiv.org/abs/2504.07951",
        "author": "Mustafa Shukor, Enrico Fini, Victor Guilherme Turrisi da Costa, Matthieu Cord, Joshua Susskind, Alaaeldin El-Nouby",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07951v1 Announce Type: new \nAbstract: Building general-purpose models that can effectively perceive the world through multimodal signals has been a long-standing goal. Current approaches involve integrating separately pre-trained components, such as connecting vision encoders to LLMs and continuing multimodal training. While such approaches exhibit remarkable sample efficiency, it remains an open question whether such late-fusion architectures are inherently superior. In this work, we revisit the architectural design of native multimodal models (NMMs)--those trained from the ground up on all modalities--and conduct an extensive scaling laws study, spanning 457 trained models with different architectures and training mixtures. Our investigation reveals no inherent advantage to late-fusion architectures over early-fusion ones, which do not rely on image encoders. On the contrary, early-fusion exhibits stronger performance at lower parameter counts, is more efficient to train, and is easier to deploy. Motivated by the strong performance of the early-fusion architectures, we show that incorporating Mixture of Experts (MoEs) allows for models that learn modality-specific weights, significantly enhancing performance."
      },
      {
        "id": "oai:arXiv.org:2504.07952v1",
        "title": "Dynamic Cheatsheet: Test-Time Learning with Adaptive Memory",
        "link": "https://arxiv.org/abs/2504.07952",
        "author": "Mirac Suzgun, Mert Yuksekgonul, Federico Bianchi, Dan Jurafsky, James Zou",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07952v1 Announce Type: new \nAbstract: Despite their impressive performance on complex tasks, current language models (LMs) typically operate in a vacuum: Each input query is processed separately, without retaining insights from previous attempts. Here, we present Dynamic Cheatsheet (DC), a lightweight framework that endows a black-box LM with a persistent, evolving memory. Rather than repeatedly re-discovering or re-committing the same solutions and mistakes, DC enables models to store and reuse accumulated strategies, code snippets, and general problem-solving insights at inference time. This test-time learning enhances performance substantially across a range of tasks without needing explicit ground-truth labels or human feedback. Leveraging DC, Claude 3.5 Sonnet's accuracy more than doubled on AIME math exams once it began retaining algebraic insights across questions. Similarly, GPT-4o's success rate on Game of 24 increased from 10% to 99% after the model discovered and reused a Python-based solution. In tasks prone to arithmetic mistakes, such as balancing equations, DC enabled GPT-4o and Claude to reach near-perfect accuracy by recalling previously validated code, whereas their baselines stagnated around 50%. Beyond arithmetic challenges, DC yields notable accuracy gains on knowledge-demanding tasks. Claude achieved a 9% improvement in GPQA-Diamond and an 8% boost on MMLU-Pro problems. Crucially, DC's memory is self-curated, focusing on concise, transferable snippets rather than entire transcript. Unlike finetuning or static retrieval methods, DC adapts LMs' problem-solving skills on the fly, without modifying their underlying parameters. Overall, our findings present DC as a promising approach for augmenting LMs with persistent memory, bridging the divide between isolated inference events and the cumulative, experience-driven learning characteristic of human cognition."
      },
      {
        "id": "oai:arXiv.org:2504.07954v1",
        "title": "Perception-R1: Pioneering Perception Policy with Reinforcement Learning",
        "link": "https://arxiv.org/abs/2504.07954",
        "author": "En Yu, Kangheng Lin, Liang Zhao, Jisheng Yin, Yana Wei, Yuang Peng, Haoran Wei, Jianjian Sun, Chunrui Han, Zheng Ge, Xiangyu Zhang, Daxin Jiang, Jingyu Wang, Wenbing Tao",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07954v1 Announce Type: new \nAbstract: Inspired by the success of DeepSeek-R1, we explore the potential of rule-based reinforcement learning (RL) in MLLM post-training for perception policy learning. While promising, our initial experiments reveal that incorporating a thinking process through RL does not consistently lead to performance gains across all visual perception tasks. This leads us to delve into the essential role of RL in the context of visual perception. In this work, we return to the fundamentals and explore the effects of RL on different perception tasks. We observe that the perceptual complexity is a major factor in determining the effectiveness of RL. We also observe that reward design plays a crucial role in further approching the upper limit of model perception. To leverage these findings, we propose Perception-R1, a scalable RL framework using GRPO during MLLM post-training. With a standard Qwen2.5-VL-3B-Instruct, Perception-R1 achieves +4.2% on RefCOCO+, +17.9% on PixMo-Count, +4.2% on PageOCR, and notably, 31.9% AP on COCO2017 val for the first time, establishing a strong baseline for perception policy learning."
      },
      {
        "id": "oai:arXiv.org:2504.07955v1",
        "title": "BoxDreamer: Dreaming Box Corners for Generalizable Object Pose Estimation",
        "link": "https://arxiv.org/abs/2504.07955",
        "author": "Yuanhong Yu, Xingyi He, Chen Zhao, Junhao Yu, Jiaqi Yang, Ruizhen Hu, Yujun Shen, Xing Zhu, Xiaowei Zhou, Sida Peng",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07955v1 Announce Type: new \nAbstract: This paper presents a generalizable RGB-based approach for object pose estimation, specifically designed to address challenges in sparse-view settings. While existing methods can estimate the poses of unseen objects, their generalization ability remains limited in scenarios involving occlusions and sparse reference views, restricting their real-world applicability. To overcome these limitations, we introduce corner points of the object bounding box as an intermediate representation of the object pose. The 3D object corners can be reliably recovered from sparse input views, while the 2D corner points in the target view are estimated through a novel reference-based point synthesizer, which works well even in scenarios involving occlusions. As object semantic points, object corners naturally establish 2D-3D correspondences for object pose estimation with a PnP algorithm. Extensive experiments on the YCB-Video and Occluded-LINEMOD datasets show that our approach outperforms state-of-the-art methods, highlighting the effectiveness of the proposed representation and significantly enhancing the generalization capabilities of object pose estimation, which is crucial for real-world applications."
      },
      {
        "id": "oai:arXiv.org:2504.07956v1",
        "title": "VCR-Bench: A Comprehensive Evaluation Framework for Video Chain-of-Thought Reasoning",
        "link": "https://arxiv.org/abs/2504.07956",
        "author": "Yukun Qi, Yiming Zhao, Yu Zeng, Xikun Bao, Wenxuan Huang, Lin Chen, Zehui Chen, Jie Zhao, Zhongang Qi, Feng Zhao",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07956v1 Announce Type: new \nAbstract: The advancement of Chain-of-Thought (CoT) reasoning has significantly enhanced the capabilities of large language models (LLMs) and large vision-language models (LVLMs). However, a rigorous evaluation framework for video CoT reasoning remains absent. Current video benchmarks fail to adequately assess the reasoning process and expose whether failures stem from deficiencies in perception or reasoning capabilities. Therefore, we introduce VCR-Bench, a novel benchmark designed to comprehensively evaluate LVLMs' Video Chain-of-Thought Reasoning capabilities. VCR-Bench comprises 859 videos spanning a variety of video content and durations, along with 1,034 high-quality question-answer pairs. Each pair is manually annotated with a stepwise CoT rationale, where every step is tagged to indicate its association with the perception or reasoning capabilities. Furthermore, we design seven distinct task dimensions and propose the CoT score to assess the entire CoT process based on the stepwise tagged CoT rationals. Extensive experiments on VCR-Bench highlight substantial limitations in current LVLMs. Even the top-performing model, o1, only achieves a 62.8% CoT score and an 56.7% accuracy, while most models score below 40%. Experiments show most models score lower on perception than reasoning steps, revealing LVLMs' key bottleneck in temporal-spatial information processing for complex video reasoning. A robust positive correlation between the CoT score and accuracy confirms the validity of our evaluation framework and underscores the critical role of CoT reasoning in solving complex video reasoning tasks. We hope VCR-Bench to serve as a standardized evaluation framework and expose the actual drawbacks in complex video reasoning task."
      },
      {
        "id": "oai:arXiv.org:2504.07957v1",
        "title": "MM-IFEngine: Towards Multimodal Instruction Following",
        "link": "https://arxiv.org/abs/2504.07957",
        "author": "Shengyuan Ding, Shenxi Wu, Xiangyu Zhao, Yuhang Zang, Haodong Duan, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Dahua Lin, Jiaqi Wang",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07957v1 Announce Type: new \nAbstract: The Instruction Following (IF) ability measures how well Multi-modal Large Language Models (MLLMs) understand exactly what users are telling them and whether they are doing it right. Existing multimodal instruction following training data is scarce, the benchmarks are simple with atomic instructions, and the evaluation strategies are imprecise for tasks demanding exact output constraints. To address this, we present MM-IFEngine, an effective pipeline to generate high-quality image-instruction pairs. Our MM-IFEngine pipeline yields large-scale, diverse, and high-quality training data MM-IFInstruct-23k, which is suitable for Supervised Fine-Tuning (SFT) and extended as MM-IFDPO-23k for Direct Preference Optimization (DPO). We further introduce MM-IFEval, a challenging and diverse multi-modal instruction-following benchmark that includes (1) both compose-level constraints for output responses and perception-level constraints tied to the input images, and (2) a comprehensive evaluation pipeline incorporating both rule-based assessment and judge model. We conduct SFT and DPO experiments and demonstrate that fine-tuning MLLMs on MM-IFInstruct-23k and MM-IFDPO-23k achieves notable gains on various IF benchmarks, such as MM-IFEval (+10.2$\\%$), MIA (+7.6$\\%$), and IFEval (+12.3$\\%$). The full data and evaluation code will be released on https://github.com/SYuan03/MM-IFEngine."
      },
      {
        "id": "oai:arXiv.org:2504.07958v1",
        "title": "Detect Anything 3D in the Wild",
        "link": "https://arxiv.org/abs/2504.07958",
        "author": "Hanxue Zhang, Haoran Jiang, Qingsong Yao, Yanan Sun, Renrui Zhang, Hao Zhao, Hongyang Li, Hongzi Zhu, Zetong Yang",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07958v1 Announce Type: new \nAbstract: Despite the success of deep learning in close-set 3D object detection, existing approaches struggle with zero-shot generalization to novel objects and camera configurations. We introduce DetAny3D, a promptable 3D detection foundation model capable of detecting any novel object under arbitrary camera configurations using only monocular inputs. Training a foundation model for 3D detection is fundamentally constrained by the limited availability of annotated 3D data, which motivates DetAny3D to leverage the rich prior knowledge embedded in extensively pre-trained 2D foundation models to compensate for this scarcity. To effectively transfer 2D knowledge to 3D, DetAny3D incorporates two core modules: the 2D Aggregator, which aligns features from different 2D foundation models, and the 3D Interpreter with Zero-Embedding Mapping, which mitigates catastrophic forgetting in 2D-to-3D knowledge transfer. Experimental results validate the strong generalization of our DetAny3D, which not only achieves state-of-the-art performance on unseen categories and novel camera configurations, but also surpasses most competitors on in-domain data.DetAny3D sheds light on the potential of the 3D foundation model for diverse applications in real-world scenarios, e.g., rare object detection in autonomous driving, and demonstrates promise for further exploration of 3D-centric tasks in open-world settings. More visualization results can be found at DetAny3D project page."
      },
      {
        "id": "oai:arXiv.org:2504.07959v1",
        "title": "CCMNet: Leveraging Calibrated Color Correction Matrices for Cross-Camera Color Constancy",
        "link": "https://arxiv.org/abs/2504.07959",
        "author": "Dongyoung Kim, Mahmoud Afifi, Dongyun Kim, Michael S. Brown, Seon Joo Kim",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07959v1 Announce Type: new \nAbstract: Computational color constancy, or white balancing, is a key module in a camera's image signal processor (ISP) that corrects color casts from scene lighting. Because this operation occurs in the camera-specific raw color space, white balance algorithms must adapt to different cameras. This paper introduces a learning-based method for cross-camera color constancy that generalizes to new cameras without retraining. Our method leverages pre-calibrated color correction matrices (CCMs) available on ISPs that map the camera's raw color space to a standard space (e.g., CIE XYZ). Our method uses these CCMs to transform predefined illumination colors (i.e., along the Planckian locus) into the test camera's raw space. The mapped illuminants are encoded into a compact camera fingerprint embedding (CFE) that enables the network to adapt to unseen cameras. To prevent overfitting due to limited cameras and CCMs during training, we introduce a data augmentation technique that interpolates between cameras and their CCMs. Experimental results across multiple datasets and backbones show that our method achieves state-of-the-art cross-camera color constancy while remaining lightweight and relying only on data readily available in camera ISPs."
      },
      {
        "id": "oai:arXiv.org:2504.07960v1",
        "title": "VisualCloze: A Universal Image Generation Framework via Visual In-Context Learning",
        "link": "https://arxiv.org/abs/2504.07960",
        "author": "Zhong-Yu Li, Ruoyi Du, Juncheng Yan, Le Zhuo, Zhen Li, Peng Gao, Zhanyu Ma, Ming-Ming Cheng",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07960v1 Announce Type: new \nAbstract: Recent progress in diffusion models significantly advances various image generation tasks. However, the current mainstream approach remains focused on building task-specific models, which have limited efficiency when supporting a wide range of different needs. While universal models attempt to address this limitation, they face critical challenges, including generalizable task instruction, appropriate task distributions, and unified architectural design. To tackle these challenges, we propose VisualCloze, a universal image generation framework, which supports a wide range of in-domain tasks, generalization to unseen ones, unseen unification of multiple tasks, and reverse generation. Unlike existing methods that rely on language-based task instruction, leading to task ambiguity and weak generalization, we integrate visual in-context learning, allowing models to identify tasks from visual demonstrations. Meanwhile, the inherent sparsity of visual task distributions hampers the learning of transferable knowledge across tasks. To this end, we introduce Graph200K, a graph-structured dataset that establishes various interrelated tasks, enhancing task density and transferable knowledge. Furthermore, we uncover that our unified image generation formulation shared a consistent objective with image infilling, enabling us to leverage the strong generative priors of pre-trained infilling models without modifying the architectures."
      },
      {
        "id": "oai:arXiv.org:2504.07961v1",
        "title": "Geo4D: Leveraging Video Generators for Geometric 4D Scene Reconstruction",
        "link": "https://arxiv.org/abs/2504.07961",
        "author": "Zeren Jiang, Chuanxia Zheng, Iro Laina, Diane Larlus, Andrea Vedaldi",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07961v1 Announce Type: new \nAbstract: We introduce Geo4D, a method to repurpose video diffusion models for monocular 3D reconstruction of dynamic scenes. By leveraging the strong dynamic prior captured by such video models, Geo4D can be trained using only synthetic data while generalizing well to real data in a zero-shot manner. Geo4D predicts several complementary geometric modalities, namely point, depth, and ray maps. It uses a new multi-modal alignment algorithm to align and fuse these modalities, as well as multiple sliding windows, at inference time, thus obtaining robust and accurate 4D reconstruction of long videos. Extensive experiments across multiple benchmarks show that Geo4D significantly surpasses state-of-the-art video depth estimation methods, including recent methods such as MonST3R, which are also designed to handle dynamic scenes."
      },
      {
        "id": "oai:arXiv.org:2504.07962v1",
        "title": "GLUS: Global-Local Reasoning Unified into A Single Large Language Model for Video Segmentation",
        "link": "https://arxiv.org/abs/2504.07962",
        "author": "Lang Lin, Xueyang Yu, Ziqi Pang, Yu-Xiong Wang",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07962v1 Announce Type: new \nAbstract: This paper proposes a novel framework utilizing multi-modal large language models (MLLMs) for referring video object segmentation (RefVOS). Previous MLLM-based methods commonly struggle with the dilemma between \"Ref\" and \"VOS\": they either specialize in understanding a few key frames (global reasoning) or tracking objects on continuous frames (local reasoning), and rely on external VOS or frame selectors to mitigate the other end of the challenge. However, our framework GLUS shows that global and local consistency can be unified into a single video segmentation MLLM: a set of sparse \"context frames\" provides global information, while a stream of continuous \"query frames\" conducts local object tracking. This is further supported by jointly training the MLLM with a pre-trained VOS memory bank to simultaneously digest short-range and long-range temporal information. To improve the information efficiency within the limited context window of MLLMs, we introduce object contrastive learning to distinguish hard false-positive objects and a self-refined framework to identify crucial frames and perform propagation. By collectively integrating these insights, our GLUS delivers a simple yet effective baseline, achieving new state-of-the-art for MLLMs on the MeViS and Ref-Youtube-VOS benchmark. Our project page is at https://glus-video.github.io/."
      },
      {
        "id": "oai:arXiv.org:2504.07963v1",
        "title": "PixelFlow: Pixel-Space Generative Models with Flow",
        "link": "https://arxiv.org/abs/2504.07963",
        "author": "Shoufa Chen, Chongjian Ge, Shilong Zhang, Peize Sun, Ping Luo",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07963v1 Announce Type: new \nAbstract: We present PixelFlow, a family of image generation models that operate directly in the raw pixel space, in contrast to the predominant latent-space models. This approach simplifies the image generation process by eliminating the need for a pre-trained Variational Autoencoder (VAE) and enabling the whole model end-to-end trainable. Through efficient cascade flow modeling, PixelFlow achieves affordable computation cost in pixel space. It achieves an FID of 1.98 on 256$\\times$256 ImageNet class-conditional image generation benchmark. The qualitative text-to-image results demonstrate that PixelFlow excels in image quality, artistry, and semantic control. We hope this new paradigm will inspire and open up new opportunities for next-generation visual generation models. Code and models are available at https://github.com/ShoufaChen/PixelFlow."
      },
      {
        "id": "oai:arXiv.org:2504.07964v1",
        "title": "C3PO: Critical-Layer, Core-Expert, Collaborative Pathway Optimization for Test-Time Expert Re-Mixing",
        "link": "https://arxiv.org/abs/2504.07964",
        "author": "Zhongyang Li, Ziyue Li, Tianyi Zhou",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07964v1 Announce Type: new \nAbstract: Mixture-of-Experts (MoE) Large Language Models (LLMs) suffer from severely sub-optimal expert pathways-our study reveals that naive expert selection learned from pretraining leaves a surprising 10-20% accuracy gap for improvement. Motivated by this observation, we develop a novel class of test-time optimization methods to re-weight or \"re-mixing\" the experts in different layers jointly for each test sample. Since the test sample's ground truth is unknown, we propose to optimize a surrogate objective defined by the sample's \"successful neighbors\" from a reference set of samples. We introduce three surrogates and algorithms based on mode-finding, kernel regression, and the average loss of similar reference samples/tasks. To reduce the cost of optimizing whole pathways, we apply our algorithms merely to the core experts' mixing weights in critical layers, which enjoy similar performance but save significant computation. This leads to \"Critical-Layer, Core-Expert, Collaborative Pathway Optimization (C3PO)\". We apply C3PO to two recent MoE LLMs and examine it on six widely-used benchmarks. It consistently improves the base model by 7-15% in accuracy and outperforms widely used test-time learning baselines, e.g., in-context learning and prompt/prefix tuning, by a large margin. Moreover, C3PO enables MoE LLMs with 1-3B active parameters to outperform LLMs of 7-9B parameters, hence improving MoE's advantages on efficiency. Our thorough ablation study further sheds novel insights on achieving test-time improvement on MoE."
      },
      {
        "id": "oai:arXiv.org:2504.07965v1",
        "title": "Cat, Rat, Meow: On the Alignment of Language Model and Human Term-Similarity Judgments",
        "link": "https://arxiv.org/abs/2504.07965",
        "author": "Lorenz Linhardt, Tom Neuh\\\"auser, Lenka T\\v{e}tkov\\'a, Oliver Eberle",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07965v1 Announce Type: new \nAbstract: Small and mid-sized generative language models have gained increasing attention. Their size and availability make them amenable to being analyzed at a behavioral as well as a representational level, allowing investigations of how these levels interact. We evaluate 32 publicly available language models for their representational and behavioral alignment with human similarity judgments on a word triplet task. This provides a novel evaluation setting to probe semantic associations in language beyond common pairwise comparisons. We find that (1) even the representations of small language models can achieve human-level alignment, (2) instruction-tuned model variants can exhibit substantially increased agreement, (3) the pattern of alignment across layers is highly model dependent, and (4) alignment based on models' behavioral responses is highly dependent on model size, matching their representational alignment only for the largest evaluated models."
      },
      {
        "id": "oai:arXiv.org:2503.23943v1",
        "title": "DOMAC: Differentiable Optimization for High-Speed Multipliers and Multiply-Accumulators",
        "link": "https://arxiv.org/abs/2503.23943",
        "author": "Chenhao Xue, Yi Ren, Jinwei Zhou, Kezhi Li, Chen Zhang, Yibo Lin, Lining Zhang, Qiang Xu, Guangyu Sun",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.23943v1 Announce Type: cross \nAbstract: Multipliers and multiply-accumulators (MACs) are fundamental building blocks for compute-intensive applications such as artificial intelligence. With the diminishing returns of Moore's Law, optimizing multiplier performance now necessitates process-aware architectural innovations rather than relying solely on technology scaling. In this paper, we introduce DOMAC, a novel approach that employs differentiable optimization for designing multipliers and MACs at specific technology nodes. DOMAC establishes an analogy between optimizing multi-staged parallel compressor trees and training deep neural networks. Building on this insight, DOMAC reformulates the discrete optimization challenge into a continuous problem by incorporating differentiable timing and area objectives. This formulation enables us to utilize existing deep learning toolkit for highly efficient implementation of the differentiable solver. Experimental results demonstrate that DOMAC achieves significant enhancements in both performance and area efficiency compared to state-of-the-art baselines and commercial IPs in multiplier and MAC designs."
      },
      {
        "id": "oai:arXiv.org:2504.07102v1",
        "title": "Behavior Importance-Aware Graph Neural Architecture Search for Cross-Domain Recommendation",
        "link": "https://arxiv.org/abs/2504.07102",
        "author": "Chendi Ge, Xin Wang, Ziwei Zhang, Yijian Qin, Hong Chen, Haiyang Wu, Yang Zhang, Yuekui Yang, Wenwu Zhu",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07102v1 Announce Type: cross \nAbstract: Cross-domain recommendation (CDR) mitigates data sparsity and cold-start issues in recommendation systems. While recent CDR approaches using graph neural networks (GNNs) capture complex user-item interactions, they rely on manually designed architectures that are often suboptimal and labor-intensive. Additionally, extracting valuable behavioral information from source domains to improve target domain recommendations remains challenging. To address these challenges, we propose Behavior importance-aware Graph Neural Architecture Search (BiGNAS), a framework that jointly optimizes GNN architecture and data importance for CDR. BiGNAS introduces two key components: a Cross-Domain Customized Supernetwork and a Graph-Based Behavior Importance Perceptron. The supernetwork, as a one-shot, retrain-free module, automatically searches the optimal GNN architecture for each domain without the need for retraining. The perceptron uses auxiliary learning to dynamically assess the importance of source domain behaviors, thereby improving target domain recommendations. Extensive experiments on benchmark CDR datasets and a large-scale industry advertising dataset demonstrate that BiGNAS consistently outperforms state-of-the-art baselines. To the best of our knowledge, this is the first work to jointly optimize GNN architecture and behavior data importance for cross-domain recommendation."
      },
      {
        "id": "oai:arXiv.org:2504.07103v1",
        "title": "FG-RAG: Enhancing Query-Focused Summarization with Context-Aware Fine-Grained Graph RAG",
        "link": "https://arxiv.org/abs/2504.07103",
        "author": "Yubin Hong, Chaofan Li, Jingyi Zhang, Yingxia Shao",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07103v1 Announce Type: cross \nAbstract: Retrieval-Augmented Generation (RAG) enables large language models to provide more precise and pertinent responses by incorporating external knowledge. In the Query-Focused Summarization (QFS) task, GraphRAG-based approaches have notably enhanced the comprehensiveness and diversity of generated responses. However, existing GraphRAG-based approaches predominantly focus on coarse-grained information summarization without being aware of the specific query, and the retrieved content lacks sufficient contextual information to generate comprehensive responses. To address the deficiencies of current RAG systems, we propose Context-Aware Fine-Grained Graph RAG (FG-RAG) to enhance the performance of the QFS task. FG-RAG employs Context-Aware Entity Expansion in graph retrieval to expand the coverage of retrieved entities in the graph, thus providing enough contextual information for the retrieved content. Furthermore, FG-RAG utilizes Query-Level Fine-Grained Summarization to incorporate fine-grained details during response generation, enhancing query awareness for the generated summarization. Our evaluation demonstrates that FG-RAG outperforms other RAG systems in multiple metrics of comprehensiveness, diversity, and empowerment when handling the QFS task. Our implementation is available at https://github.com/BuptWululu/FG-RAG."
      },
      {
        "id": "oai:arXiv.org:2504.07104v1",
        "title": "Relevance Isn't All You Need: Scaling RAG Systems With Inference-Time Compute Via Multi-Criteria Reranking",
        "link": "https://arxiv.org/abs/2504.07104",
        "author": "Will LeVine, Bijan Varjavand",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07104v1 Announce Type: cross \nAbstract: Modern Large Language Model (LLM) systems typically rely on Retrieval Augmented Generation (RAG) which aims to gather context that is useful for response generation. These RAG systems typically optimize strictly towards retrieving context that is maximally relevant to the query. However, conventional theory suggests that retrieval systems which seek to maximize context relevance without any additional explicit criteria can create information bottlenecks. We reaffirm this finding in the modern age of LLM's by showing that in standard RAG pipelines, maximizing for context relevance alone can degrade downstream response quality. In response, we show evaluations of existing RAG methods which account for both context relevance and answer quality. These evaluations introduce a novel finding that existing RAG systems scale poorly with inference time compute usage when considering our combined metric. We introduce \"RErank BEyond reLevance (REBEL)\", which enables RAG systems to scale with inference-time compute via injection of multi-criteria optimization using Chain-of-Thought prompting (and optionally Multi-Turn dialogue). Ultimately, this enables a new performance/speed tradeoff curve, where RAG systems are able to achieve both higher relevance of retrieved contexts and superior answer quality as inference time increases. Code for the implementation of our method in llama-index can be found at the following PR: https://github.com/run-llama/llama_index/pull/17590. Code for running experiments using this llama-index implementation can be found at https://github.com/microsoft/REBEL."
      },
      {
        "id": "oai:arXiv.org:2504.07107v1",
        "title": "Guarding Digital Privacy: Exploring User Profiling and Security Enhancements",
        "link": "https://arxiv.org/abs/2504.07107",
        "author": "Rishika Kohli, Shaifu Gupta, Manoj Singh Gaur",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07107v1 Announce Type: cross \nAbstract: User profiling, the practice of collecting user information for personalized recommendations, has become widespread, driving progress in technology. However, this growth poses a threat to user privacy, as devices often collect sensitive data without their owners' awareness. This article aims to consolidate knowledge on user profiling, exploring various approaches and associated challenges. Through the lens of two companies sharing user data and an analysis of 18 popular Android applications in India across various categories, including $\\textit{Social, Education, Entertainment, Travel, Shopping and Others}$, the article unveils privacy vulnerabilities. Further, the article propose an enhanced machine learning framework, employing decision trees and neural networks, that improves state-of-the-art classifiers in detecting personal information exposure. Leveraging the XAI (explainable artificial intelligence) algorithm LIME (Local Interpretable Model-agnostic Explanations), it enhances interpretability, crucial for reliably identifying sensitive data. Results demonstrate a noteworthy performance boost, achieving a $75.01\\%$ accuracy with a reduced training time of $3.62$ seconds for neural networks. Concluding, the paper suggests research directions to strengthen digital security measures."
      },
      {
        "id": "oai:arXiv.org:2504.07109v1",
        "title": "OSCAR: Online Soft Compression And Reranking",
        "link": "https://arxiv.org/abs/2504.07109",
        "author": "Maxime Louis, Thibault Formal, Herv\\'e Dejean, St\\'ephane Clinchant",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07109v1 Announce Type: cross \nAbstract: Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by integrating external knowledge, leading to improved accuracy and relevance. However, scaling RAG pipelines remains computationally expensive as retrieval sizes grow. To address this, we introduce OSCAR, a novel query-dependent online soft compression method that reduces computational overhead while preserving performance. Unlike traditional hard compression methods, which shorten retrieved texts, or soft compression approaches, which map documents to continuous embeddings offline, OSCAR dynamically compresses retrieved information at inference time, eliminating storage overhead and enabling higher compression rates. Additionally, we extend OSCAR to simultaneously perform reranking, further optimizing the efficiency of the RAG pipeline. Our experiments demonstrate state-of-the-art performance with a 2-5x speed-up in inference and minimal to no loss in accuracy for LLMs ranging from 1B to 24B parameters. The models are available at: https://huggingface.co/collections/naver/oscar-67d446a8e3a2551f57464295."
      },
      {
        "id": "oai:arXiv.org:2504.07110v1",
        "title": "DashCLIP: Leveraging multimodal models for generating semantic embeddings for DoorDash",
        "link": "https://arxiv.org/abs/2504.07110",
        "author": "Omkar Gurjar, Kin Sum Liu, Praveen Kolli, Utsaw Kumar, Mandar Rahurkar",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07110v1 Announce Type: cross \nAbstract: Despite the success of vision-language models in various generative tasks, obtaining high-quality semantic representations for products and user intents is still challenging due to the inability of off-the-shelf models to capture nuanced relationships between the entities. In this paper, we introduce a joint training framework for product and user queries by aligning uni-modal and multi-modal encoders through contrastive learning on image-text data. Our novel approach trains a query encoder with an LLM-curated relevance dataset, eliminating the reliance on engagement history. These embeddings demonstrate strong generalization capabilities and improve performance across applications, including product categorization and relevance prediction. For personalized ads recommendation, a significant uplift in the click-through rate and conversion rate after the deployment further confirms the impact on key business metrics. We believe that the flexibility of our framework makes it a promising solution toward enriching the user experience across the e-commerce landscape."
      },
      {
        "id": "oai:arXiv.org:2504.07126v1",
        "title": "Proposed 2MW Wind Turbine for Use in the Governorate of Dhofar at the Sultanate of Oman",
        "link": "https://arxiv.org/abs/2504.07126",
        "author": "Osama Ahmed Marzouk, Omar Rashid Hamdan Al Badi, Maadh Hamed Salman Al Rashdi, Hamed Mohammed Eid Al Balushi",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07126v1 Announce Type: cross \nAbstract: In this work, we propose a preliminary design of a horizontal-axis wind turbine (HAWT) as a candidate for the Dhofar Wind Farm project, in the southern Omani Governorate \"Dhofar\", at the southwest part of the Sultanate of Oman. This wind farm (under construction) is considered to be the first commercial, utility-scale (50MW) wind farm in the GCC (Gulf Cooperation Council) area. The proposed wind turbine has an expected electricity generation of 2MW. We studied the wind atlas of Oman and from which we determined the maximum possible mean wind speed in the entire Sultanate and built our design based on that reference value, which is 6m/s (21.6km/h). After this, we applied a set of modeling equations that estimate the power output from the wind turbine rotor and matched the target electric power to the design variables using a MATLAB computer code. We reached a suitable design and we present here the distribution of the blade angle (twist angle), and the power per unit span along the rotor blade. The rotor design has 3 blades with a diameter of 70m and a rotational speed of 24rpm. This rotor gives 2.37MW of output power, which exceeds the target 2MW output, allowing for about 15% of power losses in the gearbox and generator. We utilized some commercial designs of wind turbines from different international manufacturers as references for typical limits or recommended values of some design parameters."
      },
      {
        "id": "oai:arXiv.org:2504.07132v1",
        "title": "SolRPDS: A Dataset for Analyzing Rug Pulls in Solana Decentralized Finance",
        "link": "https://arxiv.org/abs/2504.07132",
        "author": "Abdulrahman Alhaidari, Bhavani Kalal, Balaji Palanisamy, Shamik Sural",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07132v1 Announce Type: cross \nAbstract: Rug pulls in Solana have caused significant damage to users interacting with Decentralized Finance (DeFi). A rug pull occurs when developers exploit users' trust and drain liquidity from token pools on Decentralized Exchanges (DEXs), leaving users with worthless tokens. Although rug pulls in Ethereum and Binance Smart Chain (BSC) have gained attention recently, analysis of rug pulls in Solana remains largely under-explored. In this paper, we introduce SolRPDS (Solana Rug Pull Dataset), the first public rug pull dataset derived from Solana's transactions. We examine approximately four years of DeFi data (2021-2024) that covers suspected and confirmed tokens exhibiting rug pull patterns. The dataset, derived from 3.69 billion transactions, consists of 62,895 suspicious liquidity pools. The data is annotated for inactivity states, which is a key indicator, and includes several detailed liquidity activities such as additions, removals, and last interaction as well as other attributes such as inactivity periods and withdrawn token amounts, to help identify suspicious behavior. Our preliminary analysis reveals clear distinctions between legitimate and fraudulent liquidity pools and we found that 22,195 tokens in the dataset exhibit rug pull patterns during the examined period. SolRPDS can support a wide range of future research on rug pulls including the development of data-driven and heuristic-based solutions for real-time rug pull detection and mitigation."
      },
      {
        "id": "oai:arXiv.org:2504.07133v1",
        "title": "Can SGD Select Good Fishermen? Local Convergence under Self-Selection Biases and Beyond",
        "link": "https://arxiv.org/abs/2504.07133",
        "author": "Alkis Kalavasis, Anay Mehrotra, Felix Zhou",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07133v1 Announce Type: cross \nAbstract: We revisit the problem of estimating $k$ linear regressors with self-selection bias in $d$ dimensions with the maximum selection criterion, as introduced by Cherapanamjeri, Daskalakis, Ilyas, and Zampetakis [CDIZ23, STOC'23]. Our main result is a $\\operatorname{poly}(d,k,1/\\varepsilon) + {k}^{O(k)}$ time algorithm for this problem, which yields an improvement in the running time of the algorithms of [CDIZ23] and [GM24, arXiv]. We achieve this by providing the first local convergence algorithm for self-selection, thus resolving the main open question of [CDIZ23].\n  To obtain this algorithm, we reduce self-selection to a seemingly unrelated statistical problem called coarsening. Coarsening occurs when one does not observe the exact value of the sample but only some set (a subset of the sample space) that contains the exact value. Inference from coarse samples arises in various real-world applications due to rounding by humans and algorithms, limited precision of instruments, and lag in multi-agent systems.\n  Our reduction to coarsening is intuitive and relies on the geometry of the self-selection problem, which enables us to bypass the limitations of previous analytic approaches. To demonstrate its applicability, we provide a local convergence algorithm for linear regression under another self-selection criterion, which is related to second-price auction data. Further, we give the first polynomial time local convergence algorithm for coarse Gaussian mean estimation given samples generated from a convex partition. Previously, only a sample-efficient algorithm was known due to Fotakis, Kalavasis, Kontonis, and Tzamos [FKKT21, COLT'21]."
      },
      {
        "id": "oai:arXiv.org:2504.07134v1",
        "title": "Boundary representation learning via Transformer",
        "link": "https://arxiv.org/abs/2504.07134",
        "author": "Qiang Zou, Lizhen Zhu",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07134v1 Announce Type: cross \nAbstract: The recent rise of generative artificial intelligence (AI), powered by Transformer networks, has achieved remarkable success in natural language processing, computer vision, and graphics. However, the application of Transformers in computer-aided design (CAD), particularly for processing boundary representation (B-rep) models, remains largely unexplored. To bridge this gap, this paper introduces Boundary Representation Transformer (BRT), a novel method adapting Transformer for B-rep learning. B-rep models pose unique challenges due to their irregular topology and continuous geometric definitions, which are fundamentally different from the structured and discrete data Transformers are designed for. To address this, BRT proposes a continuous geometric embedding method that encodes B-rep surfaces (trimmed and untrimmed) into B\\'ezier triangles, preserving their shape and continuity without discretization. Additionally, BRT employs a topology-aware embedding method that organizes these geometric embeddings into a sequence of discrete tokens suitable for Transformers, capturing both geometric and topological characteristics within B-rep models. This enables the Transformer's attention mechanism to effectively learn shape patterns and contextual semantics of boundary elements in a B-rep model. Extensive experiments demonstrate that BRT achieves state-of-the-art performance in part classification and feature recognition tasks."
      },
      {
        "id": "oai:arXiv.org:2504.07156v1",
        "title": "PLM-eXplain: Divide and Conquer the Protein Embedding Space",
        "link": "https://arxiv.org/abs/2504.07156",
        "author": "Jan van Eck, Dea Gogishvili, Wilson Silva, Sanne Abeln",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07156v1 Announce Type: cross \nAbstract: Protein language models (PLMs) have revolutionised computational biology through their ability to generate powerful sequence representations for diverse prediction tasks. However, their black-box nature limits biological interpretation and translation to actionable insights. We present an explainable adapter layer - PLM-eXplain (PLM-X), that bridges this gap by factoring PLM embeddings into two components: an interpretable subspace based on established biochemical features, and a residual subspace that preserves the model's predictive power. Using embeddings from ESM2, our adapter incorporates well-established properties, including secondary structure and hydropathy while maintaining high performance. We demonstrate the effectiveness of our approach across three protein-level classification tasks: prediction of extracellular vesicle association, identification of transmembrane helices, and prediction of aggregation propensity. PLM-X enables biological interpretation of model decisions without sacrificing accuracy, offering a generalisable solution for enhancing PLM interpretability across various downstream applications. This work addresses a critical need in computational biology by providing a bridge between powerful deep learning models and actionable biological insights."
      },
      {
        "id": "oai:arXiv.org:2504.07157v1",
        "title": "GAAPO: Genetic Algorithmic Applied to Prompt Optimization",
        "link": "https://arxiv.org/abs/2504.07157",
        "author": "Xavier S\\'echeresse, Jacques-Yves Guilbert--Ly, Antoine Villedieu de Torcy",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07157v1 Announce Type: cross \nAbstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks, with their performance heavily dependent on the quality of input prompts \\cite{schulhoff2025promptsurvey} \\cite{sahoo2025promptengineering}. While prompt engineering has proven effective, it typically relies on manual adjustments, making it time-consuming and potentially suboptimal. This paper introduces GAAPO (Genetic Algorithm Applied to Prompt Optimization), a novel hybrid optimization framework that leverages genetic algorithm \\cite{dejong1988gen} principles to evolve prompts through successive generations. Unlike traditional genetic approaches that rely solely on mutation and crossover operations, GAAPO integrates multiple specialized prompt generation strategies within its evolutionary framework. Through extensive experimentation on diverse datasets including ETHOS, MMLU-Pro, and GPQA, our analysis reveals several important point for the future development of automatic prompt optimization methods: importance of the tradeoff between the population size and the number of generations, effect of selection methods on stability results, capacity of different LLMs and especially reasoning models to be able to automatically generate prompts from similar queries... Furthermore, we provide insights into the relative effectiveness of different prompt generation strategies and their evolution across optimization phases. These findings contribute to both the theoretical understanding of prompt optimization and practical applications in improving LLM performance."
      },
      {
        "id": "oai:arXiv.org:2504.07163v1",
        "title": "Multi-Object Tracking for Collision Avoidance Using Multiple Cameras in Open RAN Networks",
        "link": "https://arxiv.org/abs/2504.07163",
        "author": "Jordi Serra, Anton Aguilar, Ebrahim Abu-Helalah, Ra\\'ul Parada, Paolo Dini",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07163v1 Announce Type: cross \nAbstract: This paper deals with the multi-object detection and tracking problem, within the scope of open Radio Access Network (RAN), for collision avoidance in vehicular scenarios. To this end, a set of distributed intelligent agents collocated with cameras are considered. The fusion of detected objects is done at an edge service, considering Open RAN connectivity. Then, the edge service predicts the objects trajectories for collision avoidance. Compared to the related work a more realistic Open RAN network is implemented and multiple cameras are used."
      },
      {
        "id": "oai:arXiv.org:2504.07164v1",
        "title": "R2E-Gym: Procedural Environments and Hybrid Verifiers for Scaling Open-Weights SWE Agents",
        "link": "https://arxiv.org/abs/2504.07164",
        "author": "Naman Jain, Jaskirat Singh, Manish Shetty, Liang Zheng, Koushik Sen, Ion Stoica",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07164v1 Announce Type: cross \nAbstract: Improving open-source models on real-world SWE tasks (solving GITHUB issues) faces two key challenges: 1) scalable curation of execution environments to train these models, and, 2) optimal scaling of test-time compute. We introduce AgentGym, the largest procedurally-curated executable gym environment for training real-world SWE-agents, consisting of more than 8.7K tasks. AgentGym is powered by two main contributions: 1) SYNGEN: a synthetic data curation recipe that enables scalable curation of executable environments using test-generation and back-translation directly from commits, thereby reducing reliance on human-written issues or unit tests. We show that this enables more scalable training leading to pass@1 performance of 34.4% on SWE-Bench Verified benchmark with our 32B model. 2) Hybrid Test-time Scaling: we provide an in-depth analysis of two test-time scaling axes; execution-based and execution-free verifiers, demonstrating that they exhibit complementary strengths and limitations. Test-based verifiers suffer from low distinguishability, while execution-free verifiers are biased and often rely on stylistic features. Surprisingly, we find that while each approach individually saturates around 42-43%, significantly higher gains can be obtained by leveraging their complementary strengths. Overall, our approach achieves 51% on the SWE-Bench Verified benchmark, reflecting a new state-of-the-art for open-weight SWE-agents and for the first time showing competitive performance with proprietary models such as o1, o1-preview and sonnet-3.5-v2 (with tools). We will open-source our environments, models, and agent trajectories."
      },
      {
        "id": "oai:arXiv.org:2504.07210v1",
        "title": "MESA: Text-Driven Terrain Generation Using Latent Diffusion and Global Copernicus Data",
        "link": "https://arxiv.org/abs/2504.07210",
        "author": "Paul Borne--Pons (Adobe Research), Mikolaj Czerkawski (Asterisk Labs), Rosalie Martin (Adobe Research), Romain Rouffet (Adobe Research)",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07210v1 Announce Type: cross \nAbstract: Terrain modeling has traditionally relied on procedural techniques, which often require extensive domain expertise and handcrafted rules. In this paper, we present MESA - a novel data-centric alternative by training a diffusion model on global remote sensing data. This approach leverages large-scale geospatial information to generate high-quality terrain samples from text descriptions, showcasing a flexible and scalable solution for terrain generation. The model's capabilities are demonstrated through extensive experiments, highlighting its ability to generate realistic and diverse terrain landscapes. The dataset produced to support this work, the Major TOM Core-DEM extension dataset, is released openly as a comprehensive resource for global terrain data. The results suggest that data-driven models, trained on remote sensing data, can provide a powerful tool for realistic terrain modeling and generation."
      },
      {
        "id": "oai:arXiv.org:2504.07213v1",
        "title": "Evolutionary algorithms meet self-supervised learning: a comprehensive survey",
        "link": "https://arxiv.org/abs/2504.07213",
        "author": "Adriano Vinhas, Jo\\~ao Correia, Penousal Machado",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07213v1 Announce Type: cross \nAbstract: The number of studies that combine Evolutionary Machine Learning and self-supervised learning has been growing steadily in recent years. Evolutionary Machine Learning has been shown to help automate the design of machine learning algorithms and to lead to more reliable solutions. Self-supervised learning, on the other hand, has produced good results in learning useful features when labelled data is limited. This suggests that the combination of these two areas can help both in shaping evolutionary processes and in automating the design of deep neural networks, while also reducing the need for labelled data. Still, there are no detailed reviews that explain how Evolutionary Machine Learning and self-supervised learning can be used together. To help with this, we provide an overview of studies that bring these areas together. Based on this growing interest and the range of existing works, we suggest a new sub-area of research, which we call Evolutionary Self-Supervised Learning and introduce a taxonomy for it. Finally, we point out some of the main challenges and suggest directions for future research to help Evolutionary Self-Supervised Learning grow and mature as a field."
      },
      {
        "id": "oai:arXiv.org:2504.07221v1",
        "title": "Reservoir Computing with a Single Oscillating Gas Bubble: Emphasizing the Chaotic Regime",
        "link": "https://arxiv.org/abs/2504.07221",
        "author": "Hend Abdel-Ghani, A. H. Abbas, Ivan S. Maksymov",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07221v1 Announce Type: cross \nAbstract: The rising computational and energy demands of artificial intelligence systems urge the exploration of alternative software and hardware solutions that exploit physical effects for computation. According to machine learning theory, a neural network-based computational system must exhibit nonlinearity to effectively model complex patterns and relationships. This requirement has driven extensive research into various nonlinear physical systems to enhance the performance of neural networks. In this paper, we propose and theoretically validate a reservoir computing system based on a single bubble trapped within a bulk of liquid. By applying an external acoustic pressure wave to both encode input information and excite the complex nonlinear dynamics, we showcase the ability of this single-bubble reservoir computing system to forecast complex benchmarking time series and undertake classification tasks with high accuracy. Specifically, we demonstrate that a chaotic physical regime of bubble oscillation proves to be the most effective for this kind of computations."
      },
      {
        "id": "oai:arXiv.org:2504.07235v1",
        "title": "Earth-like planet predictor: A machine learning approach",
        "link": "https://arxiv.org/abs/2504.07235",
        "author": "Jeanne Davoult, Romain Eltschinger, Yann Alibert",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07235v1 Announce Type: cross \nAbstract: Searching for planets analogous to Earth in terms of mass and equilibrium temperature is currently the first step in the quest for habitable conditions outside our Solar System and, ultimately, the search for life in the universe. Future missions such as PLATO or LIFE will begin to detect and characterise these small, cold planets, dedicating significant observation time to them. The aim of this work is to predict which stars are most likely to host an Earth-like planet (ELP) to avoid blind searches, minimises detection times, and thus maximises the number of detections. Using a previous study on correlations between the presence of an ELP and the properties of its system, we trained a Random Forest to recognise and classify systems as 'hosting an ELP' or 'not hosting an ELP'. The Random Forest was trained and tested on populations of synthetic planetary systems derived from the Bern model, and then applied to real observed systems. The tests conducted on the machine learning (ML) model yield precision scores of up to 0.99, indicating that 99% of the systems identified by the model as having ELPs possess at least one. Among the few real observed systems that have been tested, 44 have been selected as having a high probability of hosting an ELP, and a quick study of the stability of these systems confirms that the presence of an Earth-like planet within them would leave them stable. The excellent results obtained from the tests conducted on the ML model demonstrate its ability to recognise the typical architectures of systems with or without ELPs within populations derived from the Bern model. If we assume that the Bern model adequately describes the architecture of real systems, then such a tool can prove indispensable in the search for Earth-like planets. A similar approach could be applied to other planetary system formation models to validate those predictions."
      },
      {
        "id": "oai:arXiv.org:2504.07245v1",
        "title": "A new training approach for text classification in Mental Health: LatentGLoss",
        "link": "https://arxiv.org/abs/2504.07245",
        "author": "Korhan Sevin\\c{c}",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07245v1 Announce Type: cross \nAbstract: This study presents a multi-stage approach to mental health classification by leveraging traditional machine learning algorithms, deep learning architectures, and transformer-based models. A novel data set was curated and utilized to evaluate the performance of various methods, starting with conventional classifiers and advancing through neural networks. To broaden the architectural scope, recurrent neural networks (RNNs) such as LSTM and GRU were also evaluated to explore their effectiveness in modeling sequential patterns in the data. Subsequently, transformer models such as BERT were fine-tuned to assess the impact of contextual embeddings in this domain. Beyond these baseline evaluations, the core contribution of this study lies in a novel training strategy involving a dual-model architecture composed of a teacher and a student network. Unlike standard distillation techniques, this method does not rely on soft label transfer; instead, it facilitates information flow through both the teacher model's output and its latent representations by modifying the loss function. The experimental results highlight the effectiveness of each modeling stage and demonstrate that the proposed loss function and teacher-student interaction significantly enhance the model's learning capacity in mental health prediction tasks."
      },
      {
        "id": "oai:arXiv.org:2504.07257v1",
        "title": "Better Decisions through the Right Causal World Model",
        "link": "https://arxiv.org/abs/2504.07257",
        "author": "Elisabeth Dillies, Quentin Delfosse, Jannis Bl\\\"uml, Raban Emunds, Florian Peter Busch, Kristian Kersting",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07257v1 Announce Type: cross \nAbstract: Reinforcement learning (RL) agents have shown remarkable performances in various environments, where they can discover effective policies directly from sensory inputs. However, these agents often exploit spurious correlations in the training data, resulting in brittle behaviours that fail to generalize to new or slightly modified environments. To address this, we introduce the Causal Object-centric Model Extraction Tool (COMET), a novel algorithm designed to learn the exact interpretable causal world models (CWMs). COMET first extracts object-centric state descriptions from observations and identifies the environment's internal states related to the depicted objects' properties. Using symbolic regression, it models object-centric transitions and derives causal relationships governing object dynamics. COMET further incorporates large language models (LLMs) for semantic inference, annotating causal variables to enhance interpretability.\n  By leveraging these capabilities, COMET constructs CWMs that align with the true causal structure of the environment, enabling agents to focus on task-relevant features. The extracted CWMs mitigate the danger of shortcuts, permitting the development of RL systems capable of better planning and decision-making across dynamic scenarios. Our results, validated in Atari environments such as Pong and Freeway, demonstrate the accuracy and robustness of COMET, highlighting its potential to bridge the gap between object-centric reasoning and causal inference in reinforcement learning."
      },
      {
        "id": "oai:arXiv.org:2504.07273v1",
        "title": "Evaluating Parameter-Based Training Performance of Neural Networks and Variational Quantum Circuits",
        "link": "https://arxiv.org/abs/2504.07273",
        "author": "Michael K\\\"olle, Alexander Feist, Jonas Stein, Sebastian W\\\"olckert, Claudia Linnhoff-Popien",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07273v1 Announce Type: cross \nAbstract: In recent years, neural networks (NNs) have driven significant advances in machine learning. However, as tasks grow more complex, NNs often require large numbers of trainable parameters, which increases computational and energy demands. Variational quantum circuits (VQCs) offer a promising alternative: they leverage quantum mechanics to capture intricate relationships and typically need fewer parameters. In this work, we evaluate NNs and VQCs on simple supervised and reinforcement learning tasks, examining models with different parameter sizes. We simulate VQCs and execute selected parts of the training process on real quantum hardware to approximate actual training times. Our results show that VQCs can match NNs in performance while using significantly fewer parameters, despite longer training durations. As quantum technology and algorithms advance, and VQC architectures improve, we posit that VQCs could become advantageous for certain machine learning tasks."
      },
      {
        "id": "oai:arXiv.org:2504.07285v1",
        "title": "A Scalable Approach to Clustering Embedding Projections",
        "link": "https://arxiv.org/abs/2504.07285",
        "author": "Donghao Ren, Fred Hohman, Dominik Moritz",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07285v1 Announce Type: cross \nAbstract: Interactive visualization of embedding projections is a useful technique for understanding data and evaluating machine learning models. Labeling data within these visualizations is critical for interpretation, as labels provide an overview of the projection and guide user navigation. However, most methods for producing labels require clustering the points, which can be computationally expensive as the number of points grows. In this paper, we describe an efficient clustering approach using kernel density estimation in the projected 2D space instead of points. This algorithm can produce high-quality cluster regions from a 2D density map in a few hundred milliseconds, orders of magnitude faster than current approaches. We contribute the design of the algorithm, benchmarks, and applications that demonstrate the utility of the algorithm, including labeling and summarization."
      },
      {
        "id": "oai:arXiv.org:2504.07308v1",
        "title": "MoEDiff-SR: Mixture of Experts-Guided Diffusion Model for Region-Adaptive MRI Super-Resolution",
        "link": "https://arxiv.org/abs/2504.07308",
        "author": "Zhe Wang, Yuhua Ru, Aladine Chetouani, Fang Chen, Fabian Bauer, Liping Zhang, Didier Hans, Rachid Jennane, Mohamed Jarraya, Yung Hsin Chen",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07308v1 Announce Type: cross \nAbstract: Magnetic Resonance Imaging (MRI) at lower field strengths (e.g., 3T) suffers from limited spatial resolution, making it challenging to capture fine anatomical details essential for clinical diagnosis and neuroimaging research. To overcome this limitation, we propose MoEDiff-SR, a Mixture of Experts (MoE)-guided diffusion model for region-adaptive MRI Super-Resolution (SR). Unlike conventional diffusion-based SR models that apply a uniform denoising process across the entire image, MoEDiff-SR dynamically selects specialized denoising experts at a fine-grained token level, ensuring region-specific adaptation and enhanced SR performance. Specifically, our approach first employs a Transformer-based feature extractor to compute multi-scale patch embeddings, capturing both global structural information and local texture details. The extracted feature embeddings are then fed into an MoE gating network, which assigns adaptive weights to multiple diffusion-based denoisers, each specializing in different brain MRI characteristics, such as centrum semiovale, sulcal and gyral cortex, and grey-white matter junction. The final output is produced by aggregating the denoised results from these specialized experts according to dynamically assigned gating probabilities. Experimental results demonstrate that MoEDiff-SR outperforms existing state-of-the-art methods in terms of quantitative image quality metrics, perceptual fidelity, and computational efficiency. Difference maps from each expert further highlight their distinct specializations, confirming the effective region-specific denoising capability and the interpretability of expert contributions. Additionally, clinical evaluation validates its superior diagnostic capability in identifying subtle pathological features, emphasizing its practical relevance in clinical neuroimaging. Our code is available at https://github.com/ZWang78/MoEDiff-SR."
      },
      {
        "id": "oai:arXiv.org:2504.07313v1",
        "title": "Identifying regions of interest in whole slide images of renal cell carcinoma",
        "link": "https://arxiv.org/abs/2504.07313",
        "author": "Mohammed Lamine Benomar, Nesma Settouti, Eric Debreuve, Xavier Descombes, Damien Ambrosetti",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07313v1 Announce Type: cross \nAbstract: The histopathological images contain a huge amount of information, which can make diagnosis an extremely timeconsuming and tedious task. In this study, we developed a completely automated system to detect regions of interest (ROIs) in whole slide images (WSI) of renal cell carcinoma (RCC), to reduce time analysis and assist pathologists in making more accurate decisions. The proposed approach is based on an efficient texture descriptor named dominant rotated local binary pattern (DRLBP) and color transformation to reveal and exploit the immense texture variability at the microscopic high magnifications level. Thereby, the DRLBPs retain the structural information and utilize the magnitude values in a local neighborhood for more discriminative power. For the classification of the relevant ROIs, feature extraction of WSIs patches was performed on the color channels separately to form the histograms. Next, we used the most frequently occurring patterns as a feature selection step to discard non-informative features. The performances of different classifiers on a set of 1800 kidney cancer patches originating from 12 whole slide images were compared and evaluated. Furthermore, the small size of the image dataset allows to investigate deep learning approach based on transfer learning for image patches classification by using deep features and fine-tuning methods. High recognition accuracy was obtained and the classifiers are efficient, the best precision result was 99.17% achieved with SVM. Moreover, transfer learning models perform well with comparable performance, and the highest precision using ResNet-50 reached 98.50%. The proposed approach results revealed a very efficient image classification and demonstrated efficacy in identifying ROIs. This study presents an automatic system to detect regions of interest relevant to the diagnosis of kidney cancer in whole slide histopathology images."
      },
      {
        "id": "oai:arXiv.org:2504.07341v1",
        "title": "Learning to erase quantum states: thermodynamic implications of quantum learning theory",
        "link": "https://arxiv.org/abs/2504.07341",
        "author": "Haimeng Zhao, Yuzhen Zhang, John Preskill",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07341v1 Announce Type: cross \nAbstract: The energy cost of erasing quantum states depends on our knowledge of the states. We show that learning algorithms can acquire such knowledge to erase many copies of an unknown state at the optimal energy cost. This is proved by showing that learning can be made fully reversible and has no fundamental energy cost itself. With simple counting arguments, we relate the energy cost of erasing quantum states to their complexity, entanglement, and magic. We further show that the constructed erasure protocol is computationally efficient when learning is efficient. Conversely, under standard cryptographic assumptions, we prove that the optimal energy cost cannot be achieved efficiently in general. These results also enable efficient work extraction based on learning. Together, our results establish a concrete connection between quantum learning theory and thermodynamics, highlighting the physical significance of learning processes and enabling efficient learning-based protocols for thermodynamic tasks."
      },
      {
        "id": "oai:arXiv.org:2504.07347v1",
        "title": "Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents",
        "link": "https://arxiv.org/abs/2504.07347",
        "author": "Yueying Li, Jim Dai, Tianyi Peng",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07347v1 Announce Type: cross \nAbstract: As demand for Large Language Models (LLMs) and AI agents rapidly grows, optimizing systems for efficient LLM inference becomes critical. While significant efforts have targeted system-level engineering, little is explored through a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for LLM inference, bridging the gap between queuing and LLM system communities. In particular, we study the throughput aspect in LLM inference systems. We prove that a large class of 'work-conserving' scheduling algorithms can achieve maximum throughput for both individual requests and AI agent workloads, highlighting 'work-conserving' as a key design principle in practice. Evaluations of real-world systems show that Orca and Sarathi-serve are throughput-optimal, reassuring practitioners, while FastTransformer and vanilla vLLM are not maximally stable and should be used with caution.\n  Our results highlight the substantial benefits queuing community can offer in improving LLM inference systems and call for more interdisciplinary developments."
      },
      {
        "id": "oai:arXiv.org:2504.07388v1",
        "title": "Min-Max Optimisation for Nonconvex-Nonconcave Functions Using a Random Zeroth-Order Extragradient Algorithm",
        "link": "https://arxiv.org/abs/2504.07388",
        "author": "Amir Ali Farzin, Yuen Man Pun, Philipp Braun, Antoine Lesage-landry, Youssef Diouane, Iman Shames",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07388v1 Announce Type: cross \nAbstract: This study explores the performance of the random Gaussian smoothing Zeroth-Order ExtraGradient (ZO-EG) scheme considering min-max optimisation problems with possibly NonConvex-NonConcave (NC-NC) objective functions. We consider both unconstrained and constrained, differentiable and non-differentiable settings. We discuss the min-max problem from the point of view of variational inequalities. For the unconstrained problem, we establish the convergence of the ZO-EG algorithm to the neighbourhood of an $\\epsilon$-stationary point of the NC-NC objective function, whose radius can be controlled under a variance reduction scheme, along with its complexity. For the constrained problem, we introduce the new notion of proximal variational inequalities and give examples of functions satisfying this property. Moreover, we prove analogous results to the unconstrained case for the constrained problem. For the non-differentiable case, we prove the convergence of the ZO-EG algorithm to a neighbourhood of an $\\epsilon$-stationary point of the smoothed version of the objective function, where the radius of the neighbourhood can be controlled, which can be related to the ($\\delta,\\epsilon$)-Goldstein stationary point of the original objective function."
      },
      {
        "id": "oai:arXiv.org:2504.07425v1",
        "title": "Enhancing Player Enjoyment with a Two-Tier DRL and LLM-Based Agent System for Fighting Games",
        "link": "https://arxiv.org/abs/2504.07425",
        "author": "Shouren Wang, Zehua Jiang, Fernando Sliva, Sam Earle, Julian Togelius",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07425v1 Announce Type: cross \nAbstract: Deep reinforcement learning (DRL) has effectively enhanced gameplay experiences and game design across various game genres. However, few studies on fighting game agents have focused explicitly on enhancing player enjoyment, a critical factor for both developers and players. To address this gap and establish a practical baseline for designing enjoyability-focused agents, we propose a two-tier agent (TTA) system and conducted experiments in the classic fighting game Street Fighter II. The first tier of TTA employs a task-oriented network architecture, modularized reward functions, and hybrid training to produce diverse and skilled DRL agents. In the second tier of TTA, a Large Language Model Hyper-Agent, leveraging players' playing data and feedback, dynamically selects suitable DRL opponents. In addition, we investigate and model several key factors that affect the enjoyability of the opponent. The experiments demonstrate improvements from 64. 36% to 156. 36% in the execution of advanced skills over baseline methods. The trained agents also exhibit distinct game-playing styles. Additionally, we conducted a small-scale user study, and the overall enjoyment in the player's feedback validates the effectiveness of our TTA system."
      },
      {
        "id": "oai:arXiv.org:2504.07426v1",
        "title": "Conditional Data Synthesis Augmentation",
        "link": "https://arxiv.org/abs/2504.07426",
        "author": "Xinyu Tian, Xiaotong Shen",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07426v1 Announce Type: cross \nAbstract: Reliable machine learning and statistical analysis rely on diverse, well-distributed training data. However, real-world datasets are often limited in size and exhibit underrepresentation across key subpopulations, leading to biased predictions and reduced performance, particularly in supervised tasks such as classification. To address these challenges, we propose Conditional Data Synthesis Augmentation (CoDSA), a novel framework that leverages generative models, such as diffusion models, to synthesize high-fidelity data for improving model performance across multimodal domains including tabular, textual, and image data. CoDSA generates synthetic samples that faithfully capture the conditional distributions of the original data, with a focus on under-sampled or high-interest regions. Through transfer learning, CoDSA fine-tunes pre-trained generative models to enhance the realism of synthetic data and increase sample density in sparse areas. This process preserves inter-modal relationships, mitigates data imbalance, improves domain adaptation, and boosts generalization. We also introduce a theoretical framework that quantifies the statistical accuracy improvements enabled by CoDSA as a function of synthetic sample volume and targeted region allocation, providing formal guarantees of its effectiveness. Extensive experiments demonstrate that CoDSA consistently outperforms non-adaptive augmentation strategies and state-of-the-art baselines in both supervised and unsupervised settings."
      },
      {
        "id": "oai:arXiv.org:2504.07439v1",
        "title": "LLM4Ranking: An Easy-to-use Framework of Utilizing Large Language Models for Document Reranking",
        "link": "https://arxiv.org/abs/2504.07439",
        "author": "Qi Liu, Haozhe Duan, Yiqun Chen, Quanfeng Lu, Weiwei Sun, Jiaxin Mao",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07439v1 Announce Type: cross \nAbstract: Utilizing large language models (LLMs) for document reranking has been a popular and promising research direction in recent years, many studies are dedicated to improving the performance and efficiency of using LLMs for reranking. Besides, it can also be applied in many real-world applications, such as search engines or retrieval-augmented generation. In response to the growing demand for research and application in practice, we introduce a unified framework, \\textbf{LLM4Ranking}, which enables users to adopt different ranking methods using open-source or closed-source API-based LLMs. Our framework provides a simple and extensible interface for document reranking with LLMs, as well as easy-to-use evaluation and fine-tuning scripts for this task. We conducted experiments based on this framework and evaluated various models and methods on several widely used datasets, providing reproducibility results on utilizing LLMs for document reranking. Our code is publicly available at https://github.com/liuqi6777/llm4ranking."
      },
      {
        "id": "oai:arXiv.org:2504.07450v1",
        "title": "Synthetic CT Generation from Time-of-Flight Non-Attenutaion-Corrected PET for Whole-Body PET Attenuation Correction",
        "link": "https://arxiv.org/abs/2504.07450",
        "author": "Weijie Chen, James Wang, Alan McMillan",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07450v1 Announce Type: cross \nAbstract: Positron Emission Tomography (PET) imaging requires accurate attenuation correction (AC) to account for photon loss due to tissue density variations. In PET/MR systems, computed tomography (CT), which offers a straightforward estimation of AC is not available. This study presents a deep learning approach to generate synthetic CT (sCT) images directly from Time-of-Flight (TOF) non-attenuation corrected (NAC) PET images, enhancing AC for PET/MR. We first evaluated models pre-trained on large-scale natural image datasets for a CT-to-CT reconstruction task, finding that the pre-trained model outperformed those trained solely on medical datasets. The pre-trained model was then fine-tuned using an institutional dataset of 35 TOF NAC PET and CT volume pairs, achieving the lowest mean absolute error (MAE) of 74.49 HU and highest peak signal-to-noise ratio (PSNR) of 28.66 dB within the body contour region. Visual assessments demonstrated improved reconstruction of both bone and soft tissue structures from TOF NAC PET images. This work highlights the effectiveness of using pre-trained deep learning models for medical image translation tasks. Future work will assess the impact of sCT on PET attenuation correction and explore additional neural network architectures and datasets to further enhance performance and practical applications in PET imaging."
      },
      {
        "id": "oai:arXiv.org:2504.07468v1",
        "title": "Novel Pooling-based VGG-Lite for Pneumonia and Covid-19 Detection from Imbalanced Chest X-Ray Datasets",
        "link": "https://arxiv.org/abs/2504.07468",
        "author": "Santanu Roy, Ashvath Suresh, Palak Sahu, Tulika Rudra Gupta",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07468v1 Announce Type: cross \nAbstract: This paper proposes a novel pooling-based VGG-Lite model in order to mitigate class imbalance issues in Chest X-Ray (CXR) datasets. Automatic Pneumonia detection from CXR images by deep learning model has emerged as a prominent and dynamic area of research, since the inception of the new Covid-19 variant in 2020. However, the standard Convolutional Neural Network (CNN) models encounter challenges associated with class imbalance, a prevalent issue found in many medical datasets. The innovations introduced in the proposed model architecture include: (I) A very lightweight CNN model, `VGG-Lite', is proposed as a base model, inspired by VGG-16 and MobileNet-V2 architecture. (II) On top of this base model, we leverage an ``Edge Enhanced Module (EEM)\" through a parallel branch, consisting of a ``negative image layer\", and a novel custom pooling layer ``2Max-Min Pooling\". This 2Max-Min Pooling layer is entirely novel in this investigation, providing more attention to edge components within pneumonia CXR images. Thus, it works as an efficient spatial attention module (SAM). We have implemented the proposed framework on two separate CXR datasets. The first dataset is obtained from a readily available source on the internet, and the second dataset is a more challenging CXR dataset, assembled by our research team from three different sources. Experimental results reveal that our proposed framework has outperformed pre-trained CNN models, and three recent trend existing models ``Vision Transformer\", ``Pooling-based Vision Transformer (PiT)'' and ``PneuNet\", by substantial margins on both datasets. The proposed framework VGG-Lite with EEM, has achieved a macro average of 95% accuracy, 97.1% precision, 96.1% recall, and 96.6% F1 score on the ``Pneumonia Imbalance CXR dataset\", without employing any pre-processing technique."
      },
      {
        "id": "oai:arXiv.org:2504.07478v1",
        "title": "Intelligent DoS and DDoS Detection: A Hybrid GRU-NTM Approach to Network Security",
        "link": "https://arxiv.org/abs/2504.07478",
        "author": "Caroline Panggabean, Chandrasekar Venkatachalam, Priyanka Shah, Sincy John, Renuka Devi P, Shanmugavalli Venkatachalam",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07478v1 Announce Type: cross \nAbstract: Detecting Denial of Service (DoS) and Distributed Denial of Service (DDoS) attacks remains a critical challenge in cybersecurity. This research introduces a hybrid deep learning model combining Gated Recurrent Units (GRUs) and a Neural Turing Machine (NTM) for enhanced intrusion detection. Trained on the UNSW-NB15 and BoT-IoT datasets, the model employs GRU layers for sequential data processing and an NTM for long-term pattern recognition. The proposed approach achieves 99% accuracy in distinguishing between normal, DoS, and DDoS traffic. These findings offer promising advancements in real-time threat detection and contribute to improved network security across various domains."
      },
      {
        "id": "oai:arXiv.org:2504.07481v1",
        "title": "A Mechanism-Learning Deeply Coupled Model for Remote Sensing Retrieval of Global Land Surface Temperature",
        "link": "https://arxiv.org/abs/2504.07481",
        "author": "Tian Xie, Menghui Jiang, Huanfeng Shen, Huifang Li, Cao Zeng, Xiaobin Guan, Jun Ma, Guanhao Zhang, Liangpei Zhang",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07481v1 Announce Type: cross \nAbstract: Land surface temperature (LST) retrieval from remote sensing data is pivotal for analyzing climate processes and surface energy budgets. However, LST retrieval is an ill-posed inverse problem, which becomes particularly severe when only a single band is available. In this paper, we propose a deeply coupled framework integrating mechanistic modeling and machine learning to enhance the accuracy and generalizability of single-channel LST retrieval. Training samples are generated using a physically-based radiative transfer model and a global collection of 5810 atmospheric profiles. A physics-informed machine learning framework is proposed to systematically incorporate the first principles from classical physical inversion models into the learning workflow, with optimization constrained by radiative transfer equations. Global validation demonstrated a 30% reduction in root-mean-square error versus standalone methods. Under extreme humidity, the mean absolute error decreased from 4.87 K to 2.29 K (53% improvement). Continental-scale tests across five continents confirmed the superior generalizability of this model."
      },
      {
        "id": "oai:arXiv.org:2504.07521v1",
        "title": "Why We Feel: Breaking Boundaries in Emotional Reasoning with Multimodal Large Language Models",
        "link": "https://arxiv.org/abs/2504.07521",
        "author": "Yuxiang Lin, Jingdong Sun, Zhi-Qi Cheng, Jue Wang, Haomin Liang, Zebang Cheng, Yifei Dong, Jun-Yan He, Xiaojiang Peng, Xian-Sheng Hua",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07521v1 Announce Type: cross \nAbstract: Most existing emotion analysis emphasizes which emotion arises (e.g., happy, sad, angry) but neglects the deeper why. We propose Emotion Interpretation (EI), focusing on causal factors-whether explicit (e.g., observable objects, interpersonal interactions) or implicit (e.g., cultural context, off-screen events)-that drive emotional responses. Unlike traditional emotion recognition, EI tasks require reasoning about triggers instead of mere labeling. To facilitate EI research, we present EIBench, a large-scale benchmark encompassing 1,615 basic EI samples and 50 complex EI samples featuring multifaceted emotions. Each instance demands rationale-based explanations rather than straightforward categorization. We further propose a Coarse-to-Fine Self-Ask (CFSA) annotation pipeline, which guides Vision-Language Models (VLLMs) through iterative question-answer rounds to yield high-quality labels at scale. Extensive evaluations on open-source and proprietary large language models under four experimental settings reveal consistent performance gaps-especially for more intricate scenarios-underscoring EI's potential to enrich empathetic, context-aware AI applications. Our benchmark and methods are publicly available at: https://github.com/Lum1104/EIBench, offering a foundation for advanced multimodal causal analysis and next-generation affective computing."
      },
      {
        "id": "oai:arXiv.org:2504.07560v1",
        "title": "PhaseGen: A Diffusion-Based Approach for Complex-Valued MRI Data Generation",
        "link": "https://arxiv.org/abs/2504.07560",
        "author": "Moritz Rempe, Fabian H\\\"orst, Helmut Becker, Marco Schlimbach, Lukas Rotkopf, Kevin Kr\\\"oninger, Jens Kleesiek",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07560v1 Announce Type: cross \nAbstract: Magnetic resonance imaging (MRI) raw data, or k-Space data, is complex-valued, containing both magnitude and phase information. However, clinical and existing Artificial Intelligence (AI)-based methods focus only on magnitude images, discarding the phase data despite its potential for downstream tasks, such as tumor segmentation and classification. In this work, we introduce $\\textit{PhaseGen}$, a novel complex-valued diffusion model for generating synthetic MRI raw data conditioned on magnitude images, commonly used in clinical practice. This enables the creation of artificial complex-valued raw data, allowing pretraining for models that require k-Space information. We evaluate PhaseGen on two tasks: skull-stripping directly in k-Space and MRI reconstruction using the publicly available FastMRI dataset. Our results show that training with synthetic phase data significantly improves generalization for skull-stripping on real-world data, with an increased segmentation accuracy from $41.1\\%$ to $80.1\\%$, and enhances MRI reconstruction when combined with limited real-world data. This work presents a step forward in utilizing generative AI to bridge the gap between magnitude-based datasets and the complex-valued nature of MRI raw data. This approach allows researchers to leverage the vast amount of avaliable image domain data in combination with the information-rich k-Space data for more accurate and efficient diagnostic tasks. We make our code publicly $\\href{https://github.com/TIO-IKIM/PhaseGen}{\\text{available here}}$."
      },
      {
        "id": "oai:arXiv.org:2504.07578v1",
        "title": "Privacy-Preserving Vertical K-Means Clustering",
        "link": "https://arxiv.org/abs/2504.07578",
        "author": "Federico Mazzone, Trevor Brown, Florian Kerschbaum, Kevin H. Wilson, Maarten Everts, Florian Hahn, Andreas Peter",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07578v1 Announce Type: cross \nAbstract: Clustering is a fundamental data processing task used for grouping records based on one or more features. In the vertically partitioned setting, data is distributed among entities, with each holding only a subset of those features. A key challenge in this scenario is that computing distances between records requires access to all distributed features, which may be privacy-sensitive and cannot be directly shared with other parties. The goal is to compute the joint clusters while preserving the privacy of each entity's dataset. Existing solutions using secret sharing or garbled circuits implement privacy-preserving variants of Lloyd's algorithm but incur high communication costs, scaling as O(nkt), where n is the number of data points, k the number of clusters, and t the number of rounds. These methods become impractical for large datasets or several parties, limiting their use to LAN settings only. On the other hand, a different line of solutions rely on differential privacy (DP) to outsource the local features of the parties to a central server. However, they often significantly degrade the utility of the clustering outcome due to excessive noise. In this work, we propose a novel solution based on homomorphic encryption and DP, reducing communication complexity to O(n+kt). In our method, parties securely outsource their features once, allowing a computing party to perform clustering operations under encryption. DP is applied only to the clusters' centroids, ensuring privacy with minimal impact on utility. Our solution clusters 100,000 two-dimensional points into five clusters using only 73MB of communication, compared to 101GB for existing works, and completes in just under 3 minutes on a 100Mbps network, whereas existing works take over 1 day. This makes our solution practical even for WAN deployments, all while maintaining accuracy comparable to plaintext k-means algorithms."
      },
      {
        "id": "oai:arXiv.org:2504.07606v1",
        "title": "Heart Failure Prediction using Modal Decomposition and Masked Autoencoders for Scarce Echocardiography Databases",
        "link": "https://arxiv.org/abs/2504.07606",
        "author": "Andr\\'es Bell-Navas, Mar\\'ia Villalba-Orero, Enrique Lara-Pezzi, Jes\\'us Garicano-Mena, Soledad Le Clainche",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07606v1 Announce Type: cross \nAbstract: Heart diseases constitute the main cause of international human defunction. According to the World Health Organization (WHO), approximately 18 million deaths happen each year due to precisely heart diseases. In particular, heart failures (HF) press the healthcare industry to develop systems for their early, rapid and effective prediction. In this work, an automatic system which analyses in real-time echocardiography video sequences is proposed for the challenging and more specific task of prediction of heart failure times. This system is based on a novel deep learning framework, and works in two stages. The first one transforms the data included in a database of echocardiography video sequences into a machine learning-compatible collection of annotated images which can be used in the training phase of any kind of machine learning-based framework, including a deep learning one. This initial stage includes the use of the Higher Order Dynamic Mode Decomposition (HODMD) algorithm for both data augmentation and feature extraction. The second stage is focused on building and training a Vision Transformer (ViT). Self-supervised learning (SSL) methods, which have been so far barely explored in the literature about heart failure prediction, are applied to effectively train the ViT from scratch, even with scarce databases of echocardiograms. The designed neural network analyses images from echocardiography sequences to estimate the time in which a heart failure will happen. The results obtained show the efficacy of the HODMD algorithm and the superiority of the proposed system with respect to several established ViT and Convolutional Neural Network (CNN) architectures."
      },
      {
        "id": "oai:arXiv.org:2504.07607v1",
        "title": "Stochastic Smoothed Primal-Dual Algorithms for Nonconvex Optimization with Linear Inequality Constraints",
        "link": "https://arxiv.org/abs/2504.07607",
        "author": "Ruichuan Huang, Jiawei Zhang, Ahmet Alacaoglu",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07607v1 Announce Type: cross \nAbstract: We propose smoothed primal-dual algorithms for solving stochastic and smooth nonconvex optimization problems with linear inequality constraints. Our algorithms are single-loop and only require a single stochastic gradient based on one sample at each iteration. A distinguishing feature of our algorithm is that it is based on an inexact gradient descent framework for the Moreau envelope, where the gradient of the Moreau envelope is estimated using one step of a stochastic primal-dual augmented Lagrangian method. To handle inequality constraints and stochasticity, we combine the recently established global error bounds in constrained optimization with a Moreau envelope-based analysis of stochastic proximal algorithms. For obtaining $\\varepsilon$-stationary points, we establish the optimal $O(\\varepsilon^{-4})$ sample complexity guarantee for our algorithms and provide extensions to stochastic linear constraints. We also show how to improve this complexity to $O(\\varepsilon^{-3})$ by using variance reduction and the expected smoothness assumption. Unlike existing methods, the iterations of our algorithms are free of subproblems, large batch sizes or increasing penalty parameters and use dual variable updates to ensure feasibility."
      },
      {
        "id": "oai:arXiv.org:2504.07619v1",
        "title": "Beating Transformers using Synthetic Cognition",
        "link": "https://arxiv.org/abs/2504.07619",
        "author": "Alfredo Ibias, Miguel Rodriguez-Galindo, Hector Antona, Guillem Ramirez-Miranda, Enric Guinovart",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07619v1 Announce Type: cross \nAbstract: The road to Artificial General Intelligence goes through the generation of episodic reactive behaviors, where the Transformer architecture has been proven to be the state-of-the-art. However, they still fail to develop reasoning. Recently, a novel approach for developing cognitive architectures, called Synthetic Cognition, has been proposed and implemented to develop instantaneous reactive behavior. In this study, we aim to explore the use of Synthetic Cognition to develop episodic reactive behaviors. We propose a mechanism to deal with sequences for the recent implementation of Synthetic Cognition, and test it against DNA foundation models in DNA sequence classification tasks. In our experiments, our proposal clearly outperforms the DNA foundation models, obtaining the best score on more benchmark tasks than the alternatives. Thus, we achieve two goals: expanding Synthetic Cognition to deal with sequences, and beating the Transformer architecture for sequence classification."
      },
      {
        "id": "oai:arXiv.org:2504.07643v1",
        "title": "CollEX -- A Multimodal Agentic RAG System Enabling Interactive Exploration of Scientific Collections",
        "link": "https://arxiv.org/abs/2504.07643",
        "author": "Florian Schneider, Narges Baba Ahmadi, Niloufar Baba Ahmadi, Iris Vogel, Martin Semmann, Chris Biemann",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07643v1 Announce Type: cross \nAbstract: In this paper, we introduce CollEx, an innovative multimodal agentic Retrieval-Augmented Generation (RAG) system designed to enhance interactive exploration of extensive scientific collections. Given the overwhelming volume and inherent complexity of scientific collections, conventional search systems often lack necessary intuitiveness and interactivity, presenting substantial barriers for learners, educators, and researchers. CollEx addresses these limitations by employing state-of-the-art Large Vision-Language Models (LVLMs) as multimodal agents accessible through an intuitive chat interface. By abstracting complex interactions via specialized agents equipped with advanced tools, CollEx facilitates curiosity-driven exploration, significantly simplifying access to diverse scientific collections and records therein. Our system integrates textual and visual modalities, supporting educational scenarios that are helpful for teachers, pupils, students, and researchers by fostering independent exploration as well as scientific excitement and curiosity. Furthermore, CollEx serves the research community by discovering interdisciplinary connections and complementing visual data. We illustrate the effectiveness of our system through a proof-of-concept application containing over 64,000 unique records across 32 collections from a local scientific collection from a public university."
      },
      {
        "id": "oai:arXiv.org:2504.07664v1",
        "title": "Data Requirement Goal Modeling for Machine Learning Systems",
        "link": "https://arxiv.org/abs/2504.07664",
        "author": "Asma Yamani, Nadeen AlAmoudi, Salma Albilali, Malak Baslyman, Jameleddine Hassine",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07664v1 Announce Type: cross \nAbstract: Machine Learning (ML) has been integrated into various software and systems. Two main components are essential for training an ML model: the training data and the ML algorithm. Given the critical role of data in ML system development, it has become increasingly important to assess the quality of data attributes and ensure that the data meets specific requirements before its utilization. This work proposes an approach to guide non-experts in identifying data requirements for ML systems using goal modeling. In this approach, we first develop the Data Requirement Goal Model (DRGM) by surveying the white literature to identify and categorize the issues and challenges faced by data scientists and requirement engineers working on ML-related projects. An initial DRGM was built to accommodate common tasks that would generalize across projects. Then, based on insights from both white and gray literature, a customization mechanism is built to help adjust the tasks, KPIs, and goals' importance of different elements within the DRGM. The generated model can aid its users in evaluating different datasets using GRL evaluation strategies. We then validate the approach through two illustrative examples based on real-world projects. The results from the illustrative examples demonstrate that the data requirements identified by the proposed approach align with the requirements of real-world projects, demonstrating the practicality and effectiveness of the proposed framework. The proposed dataset selection customization mechanism and the proposed DRGM are helpful in guiding non-experts in identifying the data requirements for machine learning systems tailored to a specific ML problem. This approach also aids in evaluating different dataset alternatives to choose the optimum dataset for the problem. For future work, we recommend implementing tool support to generate the DRGM based on a chatbot interface."
      },
      {
        "id": "oai:arXiv.org:2504.07677v1",
        "title": "Localization Meets Uncertainty: Uncertainty-Aware Multi-Modal Localization",
        "link": "https://arxiv.org/abs/2504.07677",
        "author": "Hye-Min Won, Jieun Lee, Jiyong Oh",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07677v1 Announce Type: cross \nAbstract: Reliable localization is critical for robot navigation in complex indoor environments. In this paper, we propose an uncertainty-aware localization method that enhances the reliability of localization outputs without modifying the prediction model itself. This study introduces a percentile-based rejection strategy that filters out unreliable 3-DoF pose predictions based on aleatoric and epistemic uncertainties the network estimates. We apply this approach to a multi-modal end-to-end localization that fuses RGB images and 2D LiDAR data, and we evaluate it across three real-world datasets collected using a commercialized serving robot. Experimental results show that applying stricter uncertainty thresholds consistently improves pose accuracy. Specifically, the mean position error is reduced by 41.0%, 56.7%, and 69.4%, and the mean orientation error by 55.6%, 65.7%, and 73.3%, when applying 90%, 80%, and 70% thresholds, respectively. Furthermore, the rejection strategy effectively removes extreme outliers, resulting in better alignment with ground truth trajectories. To the best of our knowledge, this is the first study to quantitatively demonstrate the benefits of percentile-based uncertainty rejection in multi-modal end-to-end localization tasks. Our approach provides a practical means to enhance the reliability and accuracy of localization systems in real-world deployments."
      },
      {
        "id": "oai:arXiv.org:2504.07696v1",
        "title": "Conformalized Generative Bayesian Imaging: An Uncertainty Quantification Framework for Computational Imaging",
        "link": "https://arxiv.org/abs/2504.07696",
        "author": "Canberk Ekmekci, Mujdat Cetin",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07696v1 Announce Type: cross \nAbstract: Uncertainty quantification plays an important role in achieving trustworthy and reliable learning-based computational imaging. Recent advances in generative modeling and Bayesian neural networks have enabled the development of uncertainty-aware image reconstruction methods. Current generative model-based methods seek to quantify the inherent (aleatoric) uncertainty on the underlying image for given measurements by learning to sample from the posterior distribution of the underlying image. On the other hand, Bayesian neural network-based approaches aim to quantify the model (epistemic) uncertainty on the parameters of a deep neural network-based reconstruction method by approximating the posterior distribution of those parameters. Unfortunately, an ongoing need for an inversion method that can jointly quantify complex aleatoric uncertainty and epistemic uncertainty patterns still persists. In this paper, we present a scalable framework that can quantify both aleatoric and epistemic uncertainties. The proposed framework accepts an existing generative model-based posterior sampling method as an input and introduces an epistemic uncertainty quantification capability through Bayesian neural networks with latent variables and deep ensembling. Furthermore, by leveraging the conformal prediction methodology, the proposed framework can be easily calibrated to ensure rigorous uncertainty quantification. We evaluated the proposed framework on magnetic resonance imaging, computed tomography, and image inpainting problems and showed that the epistemic and aleatoric uncertainty estimates produced by the proposed framework display the characteristic features of true epistemic and aleatoric uncertainties. Furthermore, our results demonstrated that the use of conformal prediction on top of the proposed framework enables marginal coverage guarantees consistent with frequentist principles."
      },
      {
        "id": "oai:arXiv.org:2504.07726v1",
        "title": "Quantum Machine Learning: Unveiling Trends, Impacts through Bibliometric Analysis",
        "link": "https://arxiv.org/abs/2504.07726",
        "author": "Riya Bansal, Nikhil Kumar Rajput",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07726v1 Announce Type: cross \nAbstract: Quantum Machine Learning (QML) is the intersection of two revolutionary fields: quantum computing and machine learning. It promises to unlock unparalleled capabilities in data analysis, model building, and problem-solving by harnessing the unique properties of quantum mechanics. This research endeavors to conduct a comprehensive bibliometric analysis of scientific information pertaining to QML covering the period from 2000 to 2023. An extensive dataset comprising 9493 scholarly works is meticulously examined to unveil notable trends, impact factors, and funding patterns within the domain. Additionally, the study employs bibliometric mapping techniques to visually illustrate the network relationships among key countries, institutions, authors, patent citations and significant keywords in QML research. The analysis reveals a consistent growth in publications over the examined period. The findings highlight the United States and China as prominent contributors, exhibiting substantial publication and citation metrics. Notably, the study concludes that QML, as a research subject, is currently in a formative stage, characterized by robust scholarly activity and ongoing development."
      },
      {
        "id": "oai:arXiv.org:2504.07736v1",
        "title": "A Novel Deep Learning Approach for Emulating Computationally Expensive Postfire Debris Flows",
        "link": "https://arxiv.org/abs/2504.07736",
        "author": "Palak Patel, Luke McGuire, Abani Patra",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07736v1 Announce Type: cross \nAbstract: Traditional physics-based models of geophysical flows, such as debris flows and landslides that pose significant risks to human lives and infrastructure are computationally expensive, limiting their utility for large-scale parameter sweeps, uncertainty quantification, inversions or real-time applications. This study presents an efficient alternative, a deep learning-based surrogate model built using a modified U-Net architecture to predict the dynamics of runoff-generated debris flows across diverse terrain based on data from physics based simulations. The study area is divided into smaller patches for localized predictions using a patch-predict-stitch methodology (complemented by limited global data to accelerate training). The patches are then combined to reconstruct spatially continuous flow maps, ensuring scalability for large domains. To enable fast training using limited expensive simulations, the deep learning model was trained on data from an ensemble of physics based simulations using parameters generated via Latin Hypercube Sampling and validated on unseen parameter sets and terrain, achieving maximum pointwise errors below 10% and robust generalization. Uncertainty quantification using Monte Carlo methods are enabled using the validated surrogate, which can facilitate probabilistic hazard assessments. This study highlights the potential of deep learning surrogates as powerful tools for geophysical flow analysis, enabling computationally efficient and reliable probabilistic hazard map predictions."
      },
      {
        "id": "oai:arXiv.org:2504.07740v1",
        "title": "Zero-Shot Cross-Domain Code Search without Fine-Tuning",
        "link": "https://arxiv.org/abs/2504.07740",
        "author": "Keyu Liang, Zhongxin Liu, Chao Liu, Zhiyuan Wan, David Lo, Xiaohu Yang",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07740v1 Announce Type: cross \nAbstract: Code search aims to retrieve semantically relevant code snippets for natural language queries. While pre-trained language models (PLMs) have shown remarkable performance in this task, they struggle in cross-domain scenarios, often requiring costly fine-tuning or facing performance drops in zero-shot settings. RAPID, which generates synthetic data for model fine-tuning, is currently the only effective method for zero-shot cross-domain code search. Despite its effectiveness, RAPID demands substantial computational resources for fine-tuning and needs to maintain specialized models for each domain, underscoring the need for a zero-shot, fine-tuning-free approach for cross-domain code search.\n  The key to tackling zero-shot cross-domain code search lies in bridging the gaps among domains. In this work, we propose to break the query-code matching process of code search into two simpler tasks: query-comment matching and code-code matching. Our empirical study reveals the strong complementarity among the three matching schemas in zero-shot cross-domain settings, i.e., query-code, query-comment, and code-code matching. Based on the findings, we propose CodeBridge, a zero-shot, fine-tuning-free approach for cross-domain code search. Specifically, CodeBridge uses Large Language Models (LLMs) to generate comments and pseudo-code, then combines query-code, query-comment, and code-code matching via PLM-based similarity scoring and sampling-based fusion. Experimental results show that our approach outperforms the state-of-the-art PLM-based code search approaches, i.e., CoCoSoDa and UniXcoder, by an average of 21.4% and 24.9% in MRR, respectively, across three datasets. Our approach also yields results that are better than or comparable to those of the zero-shot cross-domain code search approach RAPID, which requires costly fine-tuning."
      },
      {
        "id": "oai:arXiv.org:2504.07741v1",
        "title": "Harnessing Equivariance: Modeling Turbulence with Graph Neural Networks",
        "link": "https://arxiv.org/abs/2504.07741",
        "author": "Marius Kurz, Andrea Beck, Benjamin Sanderse",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07741v1 Announce Type: cross \nAbstract: This work proposes a novel methodology for turbulence modeling in Large Eddy Simulation (LES) based on Graph Neural Networks (GNNs), which embeds the discrete rotational, reflectional and translational symmetries of the Navier-Stokes equations into the model architecture. In addition, suitable invariant input and output spaces are derived that allow the GNN models to be embedded seamlessly into the LES framework to obtain a symmetry-preserving simulation setup. The suitability of the proposed approach is investigated for two canonical test cases: Homogeneous Isotropic Turbulence (HIT) and turbulent channel flow. For both cases, GNN models are trained successfully in actual simulations using Reinforcement Learning (RL) to ensure that the models are consistent with the underlying LES formulation and discretization. It is demonstrated for the HIT case that the resulting GNN-based LES scheme recovers rotational and reflectional equivariance up to machine precision in actual simulations. At the same time, the stability and accuracy remain on par with non-symmetry-preserving machine learning models that fail to obey these properties. The same modeling strategy translates well to turbulent channel flow, where the GNN model successfully learns the more complex flow physics and is able to recover the turbulent statistics and Reynolds stresses. It is shown that the GNN model learns a zonal modeling strategy with distinct behaviors in the near-wall and outer regions. The proposed approach thus demonstrates the potential of GNNs for turbulence modeling, especially in the context of LES and RL."
      },
      {
        "id": "oai:arXiv.org:2504.07742v1",
        "title": "Gradient-based Sample Selection for Faster Bayesian Optimization",
        "link": "https://arxiv.org/abs/2504.07742",
        "author": "Qiyu Wei, Haowei Wang, Zirui Cao, Songhao Wang, Richard Allmendinger, Mauricio A \\'Alvarez",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07742v1 Announce Type: cross \nAbstract: Bayesian optimization (BO) is an effective technique for black-box optimization. However, its applicability is typically limited to moderate-budget problems due to the cubic complexity in computing the Gaussian process (GP) surrogate model. In large-budget scenarios, directly employing the standard GP model faces significant challenges in computational time and resource requirements. In this paper, we propose a novel approach, gradient-based sample selection Bayesian Optimization (GSSBO), to enhance the computational efficiency of BO. The GP model is constructed on a selected set of samples instead of the whole dataset. These samples are selected by leveraging gradient information to maintain diversity and representation. We provide a theoretical analysis of the gradient-based sample selection strategy and obtain explicit sublinear regret bounds for our proposed framework. Extensive experiments on synthetic and real-world tasks demonstrate that our approach significantly reduces the computational cost of GP fitting in BO while maintaining optimization performance comparable to baseline methods."
      },
      {
        "id": "oai:arXiv.org:2504.07753v1",
        "title": "Virtual-mask Informed Prior for Sparse-view Dual-Energy CT Reconstruction",
        "link": "https://arxiv.org/abs/2504.07753",
        "author": "Zini Chen, Yao Xiao, Junyan Zhang, Shaoyu Wang, Liu Shi, Qiegen Liu",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07753v1 Announce Type: cross \nAbstract: Sparse-view sampling in dual-energy computed tomography (DECT) significantly reduces radiation dose and increases imaging speed, yet is highly prone to artifacts. Although diffusion models have demonstrated potential in effectively handling incomplete data, most existing methods in this field focus on the image do-main and lack global constraints, which consequently leads to insufficient reconstruction quality. In this study, we propose a dual-domain virtual-mask in-formed diffusion model for sparse-view reconstruction by leveraging the high inter-channel correlation in DECT. Specifically, the study designs a virtual mask and applies it to the high-energy and low-energy data to perform perturbation operations, thus constructing high-dimensional tensors that serve as the prior information of the diffusion model. In addition, a dual-domain collaboration strategy is adopted to integrate the information of the randomly selected high-frequency components in the wavelet domain with the information in the projection domain, for the purpose of optimizing the global struc-tures and local details. Experimental results indicated that the present method exhibits excellent performance across multiple datasets."
      },
      {
        "id": "oai:arXiv.org:2504.07757v1",
        "title": "Search-contempt: a hybrid MCTS algorithm for training AlphaZero-like engines with better computational efficiency",
        "link": "https://arxiv.org/abs/2504.07757",
        "author": "Ameya Joshi",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07757v1 Announce Type: cross \nAbstract: AlphaZero in 2017 was able to master chess and other games without human knowledge by playing millions of games against itself (self-play), with a computation budget running in the tens of millions of dollars. It used a variant of the Monte Carlo Tree Search (MCTS) algorithm, known as PUCT. This paper introduces search-contempt, a novel hybrid variant of the MCTS algorithm that fundamentally alters the distribution of positions generated in self-play, preferring more challenging positions. In addition, search-contempt has been shown to give a big boost in strength for engines in Odds Chess (where one side receives an unfavorable position from the start). More significantly, it opens up the possibility of training a self-play based engine, in a much more computationally efficient manner with the number of training games running into hundreds of thousands, costing tens of thousands of dollars (instead of tens of millions of training games costing millions of dollars required by AlphaZero). This means that it may finally be possible to train such a program from zero on a standard consumer GPU even with a very limited compute, cost, or time budget."
      },
      {
        "id": "oai:arXiv.org:2504.07760v1",
        "title": "PRAD: Periapical Radiograph Analysis Dataset and Benchmark Model Development",
        "link": "https://arxiv.org/abs/2504.07760",
        "author": "Zhenhuan Zhou, Yuchen Zhang, Ruihong Xu, Xuansen Zhao, Tao Li",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07760v1 Announce Type: cross \nAbstract: Deep learning (DL), a pivotal technology in artificial intelligence, has recently gained substantial traction in the domain of dental auxiliary diagnosis. However, its application has predominantly been confined to imaging modalities such as panoramic radiographs and Cone Beam Computed Tomography, with limited focus on auxiliary analysis specifically targeting Periapical Radiographs (PR). PR are the most extensively utilized imaging modality in endodontics and periodontics due to their capability to capture detailed local lesions at a low cost. Nevertheless, challenges such as resolution limitations and artifacts complicate the annotation and recognition of PR, leading to a scarcity of publicly available, large-scale, high-quality PR analysis datasets. This scarcity has somewhat impeded the advancement of DL applications in PR analysis. In this paper, we present PRAD-10K, a dataset for PR analysis. PRAD-10K comprises 10,000 clinical periapical radiograph images, with pixel-level annotations provided by professional dentists for nine distinct anatomical structures, lesions, and artificial restorations or medical devices, We also include classification labels for images with typical conditions or lesions. Furthermore, we introduce a DL network named PRNet to establish benchmarks for PR segmentation tasks. Experimental results demonstrate that PRNet surpasses previous state-of-the-art medical image segmentation models on the PRAD-10K dataset. The codes and dataset will be made publicly available."
      },
      {
        "id": "oai:arXiv.org:2504.07763v1",
        "title": "Data over dialogue: Why artificial intelligence is unlikely to humanise medicine",
        "link": "https://arxiv.org/abs/2504.07763",
        "author": "Joshua Hatherley",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07763v1 Announce Type: cross \nAbstract: Recently, a growing number of experts in artificial intelligence (AI) and medicine have be-gun to suggest that the use of AI systems, particularly machine learning (ML) systems, is likely to humanise the practice of medicine by substantially improving the quality of clinician-patient relationships. In this thesis, however, I argue that medical ML systems are more likely to negatively impact these relationships than to improve them. In particular, I argue that the use of medical ML systems is likely to comprise the quality of trust, care, empathy, understanding, and communication between clinicians and patients."
      },
      {
        "id": "oai:arXiv.org:2504.07775v1",
        "title": "Focal Cortical Dysplasia Type II Detection Using Cross Modality Transfer Learning and Grad-CAM in 3D-CNNs for MRI Analysis",
        "link": "https://arxiv.org/abs/2504.07775",
        "author": "Lorenzo Lasagni, Antonio Ciccarone, Renzo Guerrini, Matteo Lenge, Ludovico D'incerti",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07775v1 Announce Type: cross \nAbstract: Focal cortical dysplasia (FCD) type II is a major cause of drug-resistant epilepsy, often curable only by surgery. Despite its clinical importance, the diagnosis of FCD is very difficult in MRI because of subtle abnormalities, leading to misdiagnosis. This study investigates the use of 3D convolutional neural networks (3D-CNNs) for FCD detection, using a dataset of 170 subjects (85 FCD patients and 85 controls) composed of T1-weighted and FLAIR MRI scans. In particular, it investigates the benefits obtained from cross-modality transfer learning and explainable artificial intelligence (XAI) techniques, in particular Gradient-weighted Class Activation Mapping (Grad-CAM). ResNet architectures (ResNet-18, -34, and -50) were implemented, employing transfer learning strategies that used pre-trained weights from segmentation tasks. Results indicate that transfer learning significantly enhances classification accuracy (up to 80.3%) and interpretability, as measured by a novel Heat-Score metric, which evaluates the model's focus on clinically relevant regions. Improvements in the Heat-Score metric underscore the model's seizure zone localization capabilities, bringing AI predictions and clinical insights closer together. These results highlight the importance of transfer learning, including cross-modality, and XAI in advancing AI-based medical diagnostics, especially for difficult-to-diagnose pathologies such as FCD."
      },
      {
        "id": "oai:arXiv.org:2504.07777v1",
        "title": "Adaptive Detection of Fast Moving Celestial Objects Using a Mixture of Experts and Physical-Inspired Neural Network",
        "link": "https://arxiv.org/abs/2504.07777",
        "author": "Peng Jia, Ge Li, Bafeng Cheng, Yushan Li, Rongyu Sun",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07777v1 Announce Type: cross \nAbstract: Fast moving celestial objects are characterized by velocities across the celestial sphere that significantly differ from the motions of background stars. In observational images, these objects exhibit distinct shapes, contrasting with the typical appearances of stars. Depending on the observational method employed, these celestial entities may be designated as near-Earth objects or asteroids. Historically, fast moving celestial objects have been observed using ground-based telescopes, where the relative stability of stars and Earth facilitated effective image differencing techniques alongside traditional fast moving celestial object detection and classification algorithms. However, the growing prevalence of space-based telescopes, along with their diverse observational modes, produces images with different properties, rendering conventional methods less effective. This paper presents a novel algorithm for detecting fast moving celestial objects within star fields. Our approach enhances state-of-the-art fast moving celestial object detection neural networks by transforming them into physical-inspired neural networks. These neural networks leverage the point spread function of the telescope and the specific observational mode as prior information; they can directly identify moving fast moving celestial objects within star fields without requiring additional training, thereby addressing the limitations of traditional techniques. Additionally, all neural networks are integrated using the mixture of experts technique, forming a comprehensive fast moving celestial object detection algorithm. We have evaluated our algorithm using simulated observational data that mimics various observations carried out by space based telescope scenarios and real observation images. Results demonstrate that our method effectively detects fast moving celestial objects across different observational modes."
      },
      {
        "id": "oai:arXiv.org:2504.07818v1",
        "title": "Performance of Rank-One Tensor Approximation on Incomplete Data",
        "link": "https://arxiv.org/abs/2504.07818",
        "author": "Hugo Lebeau",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07818v1 Announce Type: cross \nAbstract: We are interested in the estimation of a rank-one tensor signal when only a portion $\\varepsilon$ of its noisy observation is available. We show that the study of this problem can be reduced to that of a random matrix model whose spectral analysis gives access to the reconstruction performance. These results shed light on and specify the loss of performance induced by an artificial reduction of the memory cost of a tensor via the deletion of a random part of its entries."
      },
      {
        "id": "oai:arXiv.org:2504.07820v1",
        "title": "Smoothed Distance Kernels for MMDs and Applications in Wasserstein Gradient Flows",
        "link": "https://arxiv.org/abs/2504.07820",
        "author": "Nicolaj Rux, Michael Quellmalz, Gabriele Steidl",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07820v1 Announce Type: cross \nAbstract: Negative distance kernels $K(x,y) := - \\|x-y\\|$ were used in the definition of maximum mean discrepancies (MMDs) in statistics and lead to favorable numerical results in various applications. In particular, so-called slicing techniques for handling high-dimensional kernel summations profit from the simple parameter-free structure of the distance kernel. However, due to its non-smoothness in $x=y$, most of the classical theoretical results, e.g. on Wasserstein gradient flows of the corresponding MMD functional do not longer hold true. In this paper, we propose a new kernel which keeps the favorable properties of the negative distance kernel as being conditionally positive definite of order one with a nearly linear increase towards infinity and a simple slicing structure, but is Lipschitz differentiable now. Our construction is based on a simple 1D smoothing procedure of the absolute value function followed by a Riemann-Liouville fractional integral transform. Numerical results demonstrate that the new kernel performs similarly well as the negative distance kernel in gradient descent methods, but now with theoretical guarantees."
      },
      {
        "id": "oai:arXiv.org:2504.07827v1",
        "title": "HarmonySeg: Tubular Structure Segmentation with Deep-Shallow Feature Fusion and Growth-Suppression Balanced Loss",
        "link": "https://arxiv.org/abs/2504.07827",
        "author": "Yi Huang, Ke Zhang, Wei Liu, Yuanyuan Wang, Vishal M. Patel, Le Lu, Xu Han, Dakai Jin, Ke Yan",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07827v1 Announce Type: cross \nAbstract: Accurate segmentation of tubular structures in medical images, such as vessels and airway trees, is crucial for computer-aided diagnosis, radiotherapy, and surgical planning. However, significant challenges exist in algorithm design when faced with diverse sizes, complex topologies, and (often) incomplete data annotation of these structures. We address these difficulties by proposing a new tubular structure segmentation framework named HarmonySeg. First, we design a deep-to-shallow decoder network featuring flexible convolution blocks with varying receptive fields, which enables the model to effectively adapt to tubular structures of different scales. Second, to highlight potential anatomical regions and improve the recall of small tubular structures, we incorporate vesselness maps as auxiliary information. These maps are aligned with image features through a shallow-and-deep fusion module, which simultaneously eliminates unreasonable candidates to maintain high precision. Finally, we introduce a topology-preserving loss function that leverages contextual and shape priors to balance the growth and suppression of tubular structures, which also allows the model to handle low-quality and incomplete annotations. Extensive quantitative experiments are conducted on four public datasets. The results show that our model can accurately segment 2D and 3D tubular structures and outperform existing state-of-the-art methods. External validation on a private dataset also demonstrates good generalizability."
      },
      {
        "id": "oai:arXiv.org:2504.07831v1",
        "title": "Deceptive Automated Interpretability: Language Models Coordinating to Fool Oversight Systems",
        "link": "https://arxiv.org/abs/2504.07831",
        "author": "Simon Lermen, Mateusz Dziemian, Natalia P\\'erez-Campanero Antol\\'in",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07831v1 Announce Type: cross \nAbstract: We demonstrate how AI agents can coordinate to deceive oversight systems using automated interpretability of neural networks. Using sparse autoencoders (SAEs) as our experimental framework, we show that language models (Llama, DeepSeek R1, and Claude 3.7 Sonnet) can generate deceptive explanations that evade detection. Our agents employ steganographic methods to hide information in seemingly innocent explanations, successfully fooling oversight models while achieving explanation quality comparable to reference labels. We further find that models can scheme to develop deceptive strategies when they believe the detection of harmful features might lead to negative consequences for themselves. All tested LLM agents were capable of deceiving the overseer while achieving high interpretability scores comparable to those of reference labels. We conclude by proposing mitigation strategies, emphasizing the critical need for robust understanding and defenses against deception."
      },
      {
        "id": "oai:arXiv.org:2504.07840v1",
        "title": "Understanding Learner-LLM Chatbot Interactions and the Impact of Prompting Guidelines",
        "link": "https://arxiv.org/abs/2504.07840",
        "author": "Cansu Koyuturk, Emily Theophilou, Sabrina Patania, Gregor Donabauer, Andrea Martinenghi, Chiara Antico, Alessia Telari, Alessia Testa, Sathya Bursic, Franca Garzotto, Davinia Hernandez-Leo, Udo Kruschwitz, Davide Taibi, Simona Amenta, Martin Ruskov, Dimitri Ognibene",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07840v1 Announce Type: cross \nAbstract: Large Language Models (LLMs) have transformed human-computer interaction by enabling natural language-based communication with AI-powered chatbots. These models are designed to be intuitive and user-friendly, allowing users to articulate requests with minimal effort. However, despite their accessibility, studies reveal that users often struggle with effective prompting, resulting in inefficient responses. Existing research has highlighted both the limitations of LLMs in interpreting vague or poorly structured prompts and the difficulties users face in crafting precise queries. This study investigates learner-AI interactions through an educational experiment in which participants receive structured guidance on effective prompting. We introduce and compare three types of prompting guidelines: a task-specific framework developed through a structured methodology and two baseline approaches. To assess user behavior and prompting efficacy, we analyze a dataset of 642 interactions from 107 users. Using Von NeuMidas, an extended pragmatic annotation schema for LLM interaction analysis, we categorize common prompting errors and identify recurring behavioral patterns. We then evaluate the impact of different guidelines by examining changes in user behavior, adherence to prompting strategies, and the overall quality of AI-generated responses. Our findings provide a deeper understanding of how users engage with LLMs and the role of structured prompting guidance in enhancing AI-assisted communication. By comparing different instructional frameworks, we offer insights into more effective approaches for improving user competency in AI interactions, with implications for AI literacy, chatbot usability, and the design of more responsive AI systems."
      },
      {
        "id": "oai:arXiv.org:2504.07872v1",
        "title": "Dual Engines of Thoughts: A Depth-Breadth Integration Framework for Open-Ended Analysis",
        "link": "https://arxiv.org/abs/2504.07872",
        "author": "Fei-Hsuan Yu, Yun-Cheng Chou, Teng-Ruei Chen",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07872v1 Announce Type: cross \nAbstract: We propose the Dual Engines of Thoughts (DEoT), an analytical framework for comprehensive open-ended reasoning. While traditional reasoning frameworks primarily focus on finding \"the best answer\" or \"the correct answer\" for single-answer problems, DEoT is specifically designed for \"open-ended questions,\" enabling both broader and deeper analytical exploration. The framework centers on three key components: a Base Prompter for refining user queries, a Solver Agent that orchestrates task decomposition, execution, and validation, and a Dual-Engine System consisting of a Breadth Engine (to explore diverse impact factors) and a Depth Engine (to perform deep investigations). This integrated design allows DEoT to balance wide-ranging coverage with in-depth analysis, and it is highly customizable, enabling users to adjust analytical parameters and tool configurations based on specific requirements. Experimental results show that DEoT excels in addressing complex, multi-faceted questions, achieving a total win rate of 77-86% compared to existing reasoning models, thus highlighting its effectiveness in real-world applications."
      },
      {
        "id": "oai:arXiv.org:2504.07898v1",
        "title": "How do Large Language Models Understand Relevance? A Mechanistic Interpretability Perspective",
        "link": "https://arxiv.org/abs/2504.07898",
        "author": "Qi Liu, Jiaxin Mao, Ji-Rong Wen",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07898v1 Announce Type: cross \nAbstract: Recent studies have shown that large language models (LLMs) can assess relevance and support information retrieval (IR) tasks such as document ranking and relevance judgment generation. However, the internal mechanisms by which off-the-shelf LLMs understand and operationalize relevance remain largely unexplored. In this paper, we systematically investigate how different LLM modules contribute to relevance judgment through the lens of mechanistic interpretability. Using activation patching techniques, we analyze the roles of various model components and identify a multi-stage, progressive process in generating either pointwise or pairwise relevance judgment. Specifically, LLMs first extract query and document information in the early layers, then process relevance information according to instructions in the middle layers, and finally utilize specific attention heads in the later layers to generate relevance judgments in the required format. Our findings provide insights into the mechanisms underlying relevance assessment in LLMs, offering valuable implications for future research on leveraging LLMs for IR tasks."
      },
      {
        "id": "oai:arXiv.org:2504.07904v1",
        "title": "The Efficacy of Semantics-Preserving Transformations in Self-Supervised Learning for Medical Ultrasound",
        "link": "https://arxiv.org/abs/2504.07904",
        "author": "Blake VanBerlo, Alexander Wong, Jesse Hoey, Robert Arntfield",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07904v1 Announce Type: cross \nAbstract: Data augmentation is a central component of joint embedding self-supervised learning (SSL). Approaches that work for natural images may not always be effective in medical imaging tasks. This study systematically investigated the impact of data augmentation and preprocessing strategies in SSL for lung ultrasound. Three data augmentation pipelines were assessed: (1) a baseline pipeline commonly used across imaging domains, (2) a novel semantic-preserving pipeline designed for ultrasound, and (3) a distilled set of the most effective transformations from both pipelines. Pretrained models were evaluated on multiple classification tasks: B-line detection, pleural effusion detection, and COVID-19 classification. Experiments revealed that semantics-preserving data augmentation resulted in the greatest performance for COVID-19 classification - a diagnostic task requiring global image context. Cropping-based methods yielded the greatest performance on the B-line and pleural effusion object classification tasks, which require strong local pattern recognition. Lastly, semantics-preserving ultrasound image preprocessing resulted in increased downstream performance for multiple tasks. Guidance regarding data augmentation and preprocessing strategies was synthesized for practitioners working with SSL in ultrasound."
      },
      {
        "id": "oai:arXiv.org:2504.07923v1",
        "title": "Trading Graph Neural Network",
        "link": "https://arxiv.org/abs/2504.07923",
        "author": "Xian Wu",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07923v1 Announce Type: cross \nAbstract: This paper proposes a new algorithm -- Trading Graph Neural Network (TGNN) that can structurally estimate the impact of asset features, dealer features and relationship features on asset prices in trading networks. It combines the strength of the traditional simulated method of moments (SMM) and recent machine learning techniques -- Graph Neural Network (GNN). It outperforms existing reduced-form methods with network centrality measures in prediction accuracy. The method can be used on networks with any structure, allowing for heterogeneity among both traders and assets."
      },
      {
        "id": "oai:arXiv.org:2504.07927v1",
        "title": "Zero-Shot Low-dose CT Denoising via Sinogram Flicking",
        "link": "https://arxiv.org/abs/2504.07927",
        "author": "Yongyi Shi, Ge Wang",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07927v1 Announce Type: cross \nAbstract: Many low-dose CT imaging methods rely on supervised learning, which requires a large number of paired noisy and clean images. However, obtaining paired images in clinical practice is challenging. To address this issue, zero-shot self-supervised methods train denoising networks using only the information within a single image, such as ZS-N2N. However, these methods often employ downsampling operations that degrade image resolution. Additionally, the training dataset is inherently constrained to the image itself. In this paper, we propose a zero-shot low-dose CT imaging method based on sinogram flicking, which operates within a single image but generates many copies via random conjugate ray matching. Specifically, two conjugate X-ray pencil beams measure the same path; their expected values should be identical, while their noise levels vary during measurements. By randomly swapping portions of the conjugate X-rays in the sinogram domain, we generate a large set of sinograms with consistent content but varying noise patterns. When displayed dynamically, these sinograms exhibit a flickering effect due to their identical structural content but differing noise patterns-hence the term sinogram flicking. We train the network on pairs of sinograms with the same content but different noise distributions using a lightweight model adapted from ZS-NSN. This process is repeated to obtain the final results. A simulation study demonstrates that our method outperforms state-of-the-art approaches such as ZS-N2N."
      },
      {
        "id": "oai:arXiv.org:2204.07661v3",
        "title": "Finding Pareto Trade-offs in Fair and Accurate Detection of Toxic Speech",
        "link": "https://arxiv.org/abs/2204.07661",
        "author": "Soumyajit Gupta, Venelin Kovatchev, Anubrata Das, Maria De-Arteaga, Matthew Lease",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2204.07661v3 Announce Type: replace \nAbstract: Optimizing NLP models for fairness poses many challenges. Lack of differentiable fairness measures prevents gradient-based loss training or requires surrogate losses that diverge from the true metric of interest. In addition, competing objectives (e.g., accuracy vs. fairness) often require making trade-offs based on stakeholder preferences, but stakeholders may not know their preferences before seeing system performance under different trade-off settings. To address these challenges, we begin by formulating a differentiable version of a popular fairness measure, Accuracy Parity, to provide balanced accuracy across demographic groups. Next, we show how model-agnostic, HyperNetwork optimization can efficiently train arbitrary NLP model architectures to learn Pareto-optimal trade-offs between competing metrics. Focusing on the task of toxic language detection, we show the generality and efficacy of our methods across two datasets, three neural architectures, and three fairness losses."
      },
      {
        "id": "oai:arXiv.org:2303.08431v5",
        "title": "Policy Gradient Converges to the Globally Optimal Policy for Nearly Linear-Quadratic Regulators",
        "link": "https://arxiv.org/abs/2303.08431",
        "author": "Yinbin Han, Meisam Razaviyayn, Renyuan Xu",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2303.08431v5 Announce Type: replace \nAbstract: Nonlinear control systems with partial information to the decision maker are prevalent in a variety of applications. As a step toward studying such nonlinear systems, this work explores reinforcement learning methods for finding the optimal policy in the nearly linear-quadratic regulator systems. In particular, we consider a dynamic system that combines linear and nonlinear components, and is governed by a policy with the same structure. Assuming that the nonlinear component comprises kernels with small Lipschitz coefficients, we characterize the optimization landscape of the cost function. Although the cost function is nonconvex in general, we establish the local strong convexity and smoothness in the vicinity of the global optimizer. Additionally, we propose an initialization mechanism to leverage these properties. Building on the developments, we design a policy gradient algorithm that is guaranteed to converge to the globally optimal policy with a linear rate."
      },
      {
        "id": "oai:arXiv.org:2303.17408v4",
        "title": "P-Transformer: A Prompt-based Multimodal Transformer Architecture For Medical Tabular Data",
        "link": "https://arxiv.org/abs/2303.17408",
        "author": "Yucheng Ruan, Xiang Lan, Daniel J. Tan, Hairil Rizal Abdullah, Mengling Feng",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2303.17408v4 Announce Type: replace \nAbstract: Medical tabular data, abundant in Electronic Health Records (EHRs), is a valuable resource for diverse medical tasks such as risk prediction. While deep learning approaches, particularly transformer-based models, have shown remarkable performance in tabular data prediction, there are still problems remaining for existing work to be effectively adapted into medical domain, such as ignoring unstructured free-texts and underutilizing the textual information in structured data. To address these issues, we propose PTransformer, a \\underline{P}rompt-based multimodal \\underline{Transformer} architecture designed specifically for medical tabular data. This framework consists of two critical components: a tabular cell embedding generator and a tabular transformer. The former efficiently encodes diverse modalities from both structured and unstructured tabular data into a harmonized language semantic space with the help of pre-trained sentence encoder and medical prompts. The latter integrates cell representations to generate patient embeddings for various medical tasks. In comprehensive experiments on two real-world datasets for three medical tasks, PTransformer demonstrated the improvements with 10.9%/11.0% on RMSE/MAE, 0.5%/2.2% on RMSE/MAE, and 1.6%/0.8% on BACC/AUROC compared to state-of-the-art (SOTA) baselines in predictability."
      },
      {
        "id": "oai:arXiv.org:2305.03535v3",
        "title": "Next-generation Surgical Navigation: Marker-less Multi-view 6DoF Pose Estimation of Surgical Instruments",
        "link": "https://arxiv.org/abs/2305.03535",
        "author": "Jonas Hein, Nicola Cavalcanti, Daniel Suter, Lukas Zingg, Fabio Carrillo, Lilian Calvet, Mazda Farshad, Marc Pollefeys, Nassir Navab, Philipp F\\\"urnstahl",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2305.03535v3 Announce Type: replace \nAbstract: State-of-the-art research of traditional computer vision is increasingly leveraged in the surgical domain. A particular focus in computer-assisted surgery is to replace marker-based tracking systems for instrument localization with pure image-based 6DoF pose estimation using deep-learning methods. However, state-of-the-art single-view pose estimation methods do not yet meet the accuracy required for surgical navigation. In this context, we investigate the benefits of multi-view setups for highly accurate and occlusion-robust 6DoF pose estimation of surgical instruments and derive recommendations for an ideal camera system that addresses the challenges in the operating room.\n  The contributions of this work are threefold. First, we present a multi-camera capture setup consisting of static and head-mounted cameras, which allows us to study the performance of pose estimation methods under various camera configurations. Second, we publish a multi-view RGB-D video dataset of ex-vivo spine surgeries, captured in a surgical wet lab and a real operating theatre and including rich annotations for surgeon, instrument, and patient anatomy. Third, we evaluate three state-of-the-art single-view and multi-view methods for the task of 6DoF pose estimation of surgical instruments and analyze the influence of camera configurations, training data, and occlusions on the pose accuracy and generalization ability. The best method utilizes five cameras in a multi-view pose optimization and achieves an average position and orientation error of 1.01 mm and 0.89{\\deg} for a surgical drill as well as 2.79 mm and 3.33{\\deg} for a screwdriver under optimal conditions. Our results demonstrate that marker-less tracking of surgical instruments is becoming a feasible alternative to existing marker-based systems."
      },
      {
        "id": "oai:arXiv.org:2305.10449v2",
        "title": "Cooperation Is All You Need",
        "link": "https://arxiv.org/abs/2305.10449",
        "author": "Ahsan Adeel, Junaid Muzaffar, Khubaib Ahmed, Mohsin Raza, Fahad Zia, Eamin Chaudary, Talha Bin Riaz, Ahmed Saeed",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2305.10449v2 Announce Type: replace \nAbstract: Going beyond 'dendritic democracy', we introduce a 'democracy of local processors', termed Cooperator. Here we compare their capabilities when used in permutation invariant neural networks for reinforcement learning (RL), with machine learning algorithms based on Transformers, such as ChatGPT. Transformers are based on the long standing conception of integrate-and-fire 'point' neurons, whereas Cooperator is inspired by recent neurobiological breakthroughs suggesting that the cellular foundations of mental life depend on context-sensitive pyramidal neurons in the neocortex which have two functionally distinct points. Weshow that when used for RL, an algorithm based on Cooperator learns far quicker than that based on Transformer, even while having the same number of parameters."
      },
      {
        "id": "oai:arXiv.org:2305.15932v4",
        "title": "BUCA: A Binary Classification Approach to Unsupervised Commonsense Question Answering",
        "link": "https://arxiv.org/abs/2305.15932",
        "author": "Jie He, Simon Chi Lok U, V\\'ictor Guti\\'errez-Basulto, Jeff Z. Pan",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2305.15932v4 Announce Type: replace \nAbstract: Unsupervised commonsense reasoning (UCR) is becoming increasingly popular as the construction of commonsense reasoning datasets is expensive, and they are inevitably limited in their scope. A popular approach to UCR is to fine-tune language models with external knowledge (e.g., knowledge graphs), but this usually requires a large number of training examples. In this paper, we propose to transform the downstream multiple choice question answering task into a simpler binary classification task by ranking all candidate answers according to their reasonableness. To this end, for training the model, we convert the knowledge graph triples into reasonable and unreasonable texts. Extensive experimental results show the effectiveness of our approach on various multiple choice question answering benchmarks. Furthermore, compared with existing UCR approaches using KGs, ours is less data hungry. Our code is available at https://github.com/probe2/BUCA."
      },
      {
        "id": "oai:arXiv.org:2307.06162v2",
        "title": "Deep Generative Models for Physiological Signals: A Systematic Literature Review",
        "link": "https://arxiv.org/abs/2307.06162",
        "author": "Nour Neifar, Afef Mdhaffar, Achraf Ben-Hamadou, Mohamed Jmaiel",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2307.06162v2 Announce Type: replace \nAbstract: In this paper, we present a systematic literature review on deep generative models for physiological signals, particularly electrocardiogram (ECG), electroencephalogram (EEG), photoplethysmogram (PPG) and electromyogram (EMG). Compared to the existing review papers, we present the first review that summarizes the recent state-of-the-art deep generative models. By analyzing the state-of-the-art research related to deep generative models along with their main applications and challenges, this review contributes to the overall understanding of these models applied to physiological signals. Additionally, by highlighting the employed evaluation protocol and the most used physiological databases, this review facilitates the assessment and benchmarking of deep generative models."
      },
      {
        "id": "oai:arXiv.org:2307.08813v3",
        "title": "Comparative Performance Evaluation of Large Language Models for Extracting Molecular Interactions and Pathway Knowledge",
        "link": "https://arxiv.org/abs/2307.08813",
        "author": "Gilchan Park, Byung-Jun Yoon, Xihaier Luo, Vanessa L\\'opez-Marrero, Shinjae Yoo, Shantenu Jha",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2307.08813v3 Announce Type: replace \nAbstract: Background Identification of the interactions and regulatory relations between biomolecules play pivotal roles in understanding complex biological systems and the mechanisms underlying diverse biological functions. However, the collection of such molecular interactions has heavily relied on expert curation in the past, making it labor-intensive and time-consuming. To mitigate these challenges, we propose leveraging the capabilities of large language models (LLMs) to automate genome-scale extraction of this crucial knowledge.\n  Results In this study, we investigate the efficacy of various LLMs in addressing biological tasks, such as the recognition of protein interactions, identification of genes linked to pathways affected by low-dose radiation, and the delineation of gene regulatory relationships. Overall, the larger models exhibited superior performance, indicating their potential for specific tasks that involve the extraction of complex interactions among genes and proteins. Although these models possessed detailed information for distinct gene and protein groups, they faced challenges in identifying groups with diverse functions and in recognizing highly correlated gene regulatory relationships.\n  Conclusions By conducting a comprehensive assessment of the state-of-the-art models using well-established molecular interaction and pathway databases, our study reveals that LLMs can identify genes/proteins associated with pathways of interest and predict their interactions to a certain extent. Furthermore, these models can provide important insights, marking a noteworthy stride toward advancing our understanding of biological systems through AI-assisted knowledge discovery."
      },
      {
        "id": "oai:arXiv.org:2309.00508v4",
        "title": "Geometry and Local Recovery of Global Minima of Two-layer Neural Networks at Overparameterization",
        "link": "https://arxiv.org/abs/2309.00508",
        "author": "Leyang Zhang, Yaoyu Zhang, Tao Luo",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2309.00508v4 Announce Type: replace \nAbstract: Under mild assumptions, we investigate the geometry of the loss landscape for two-layer neural networks in the vicinity of global minima. Utilizing novel techniques, we demonstrate: (i) how global minima with zero generalization error become geometrically separated from other global minima as the sample size grows; and (ii) the local convergence properties and rate of gradient flow dynamics. Our results indicate that two-layer neural networks can be locally recovered in the regime of overparameterization."
      },
      {
        "id": "oai:arXiv.org:2310.07056v2",
        "title": "TextPSG: Panoptic Scene Graph Generation from Textual Descriptions",
        "link": "https://arxiv.org/abs/2310.07056",
        "author": "Chengyang Zhao, Yikang Shen, Zhenfang Chen, Mingyu Ding, Chuang Gan",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2310.07056v2 Announce Type: replace \nAbstract: Panoptic Scene Graph has recently been proposed for comprehensive scene understanding. However, previous works adopt a fully-supervised learning manner, requiring large amounts of pixel-wise densely-annotated data, which is always tedious and expensive to obtain. To address this limitation, we study a new problem of Panoptic Scene Graph Generation from Purely Textual Descriptions (Caption-to-PSG). The key idea is to leverage the large collection of free image-caption data on the Web alone to generate panoptic scene graphs. The problem is very challenging for three constraints: 1) no location priors; 2) no explicit links between visual regions and textual entities; and 3) no pre-defined concept sets. To tackle this problem, we propose a new framework TextPSG consisting of four modules, i.e., a region grouper, an entity grounder, a segment merger, and a label generator, with several novel techniques. The region grouper first groups image pixels into different segments and the entity grounder then aligns visual segments with language entities based on the textual description of the segment being referred to. The grounding results can thus serve as pseudo labels enabling the segment merger to learn the segment similarity as well as guiding the label generator to learn object semantics and relation predicates, resulting in a fine-grained structured scene understanding. Our framework is effective, significantly outperforming the baselines and achieving strong out-of-distribution robustness. We perform comprehensive ablation studies to corroborate the effectiveness of our design choices and provide an in-depth analysis to highlight future directions. Our code, data, and results are available on our project page: https://textpsg.github.io/."
      },
      {
        "id": "oai:arXiv.org:2310.10803v3",
        "title": "SD-HuBERT: Sentence-Level Self-Distillation Induces Syllabic Organization in HuBERT",
        "link": "https://arxiv.org/abs/2310.10803",
        "author": "Cheol Jun Cho, Abdelrahman Mohamed, Shang-Wen Li, Alan W Black, Gopala K. Anumanchipalli",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2310.10803v3 Announce Type: replace \nAbstract: Data-driven unit discovery in self-supervised learning (SSL) of speech has embarked on a new era of spoken language processing. Yet, the discovered units often remain in phonetic space and the units beyond phonemes are largely underexplored. Here, we demonstrate that a syllabic organization emerges in learning sentence-level representation of speech. In particular, we adopt \"self-distillation\" objective to fine-tune the pretrained HuBERT with an aggregator token that summarizes the entire sentence. Without any supervision, the resulting model draws definite boundaries in speech, and the representations across frames exhibit salient syllabic structures. We demonstrate that this emergent structure largely corresponds to the ground truth syllables. Furthermore, we propose a new benchmark task, Spoken Speech ABX, for evaluating sentence-level representation of speech. When compared to previous models, our model outperforms in both unsupervised syllable discovery and learning sentence-level representation. Together, we demonstrate that the self-distillation of HuBERT gives rise to syllabic organization without relying on external labels or modalities, and potentially provides novel data-driven units for spoken language modeling."
      },
      {
        "id": "oai:arXiv.org:2401.03048v2",
        "title": "Latte: Latent Diffusion Transformer for Video Generation",
        "link": "https://arxiv.org/abs/2401.03048",
        "author": "Xin Ma, Yaohui Wang, Xinyuan Chen, Gengyun Jia, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, Yu Qiao",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2401.03048v2 Announce Type: replace \nAbstract: We propose a novel Latent Diffusion Transformer, namely Latte, for video generation. Latte first extracts spatio-temporal tokens from input videos and then adopts a series of Transformer blocks to model video distribution in the latent space. In order to model a substantial number of tokens extracted from videos, four efficient variants are introduced from the perspective of decomposing the spatial and temporal dimensions of input videos. To improve the quality of generated videos, we determine the best practices of Latte through rigorous experimental analysis, including video clip patch embedding, model variants, timestep-class information injection, temporal positional embedding, and learning strategies. Our comprehensive evaluation demonstrates that Latte achieves state-of-the-art performance across four standard video generation datasets, i.e., FaceForensics, SkyTimelapse, UCF101, and Taichi-HD. In addition, we extend Latte to text-to-video generation (T2V) task, where Latte achieves comparable results compared to recent T2V models. We strongly believe that Latte provides valuable insights for future research on incorporating Transformers into diffusion models for video generation."
      },
      {
        "id": "oai:arXiv.org:2401.13360v3",
        "title": "Understanding and Mitigating the Bias in Sample Selection for Learning with Noisy Labels",
        "link": "https://arxiv.org/abs/2401.13360",
        "author": "Qi Wei, Lei Feng, Haobo Wang, Bo An",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2401.13360v3 Announce Type: replace \nAbstract: Learning with noisy labels aims to ensure model generalization given a label-corrupted training set. The sample selection strategy achieves promising performance by selecting a label-reliable subset for model training. In this paper, we empirically reveal that existing sample selection methods suffer from both data and training bias that are represented as imbalanced selected sets and accumulation errors in practice, respectively. However, only the training bias was handled in previous studies. To address this limitation, we propose a noIse-Tolerant Expert Model (ITEM) for debiased learning in sample selection. Specifically, to mitigate the training bias, we design a robust network architecture that integrates with multiple experts. Compared with the prevailing double-branch network, our network exhibits better performance of selection and prediction by ensembling these experts while training with fewer parameters. Meanwhile, to mitigate the data bias, we propose a mixed sampling strategy based on two weight-based data samplers. By training on the mixture of two class-discriminative mini-batches, the model mitigates the effect of the imbalanced training set while avoiding sparse representations that are easily caused by sampling strategies. Extensive experiments and analyses demonstrate the effectiveness of ITEM. Our code is available at this url \\href{https://github.com/1998v7/ITEM}{ITEM}."
      },
      {
        "id": "oai:arXiv.org:2401.14391v2",
        "title": "Rethinking Patch Dependence for Masked Autoencoders",
        "link": "https://arxiv.org/abs/2401.14391",
        "author": "Letian Fu, Long Lian, Renhao Wang, Baifeng Shi, Xudong Wang, Adam Yala, Trevor Darrell, Alexei A. Efros, Ken Goldberg",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2401.14391v2 Announce Type: replace \nAbstract: In this work, we examine the impact of inter-patch dependencies in the decoder of masked autoencoders (MAE) on representation learning. We decompose the decoding mechanism for masked reconstruction into self-attention between mask tokens and cross-attention between masked and visible tokens. Our findings reveal that MAE reconstructs coherent images from visible patches not through interactions between patches in the decoder but by learning a global representation within the encoder. This discovery leads us to propose a simple visual pretraining framework: cross-attention masked autoencoders (CrossMAE). This framework employs only cross-attention in the decoder to independently read out reconstructions for a small subset of masked patches from encoder outputs. This approach achieves comparable or superior performance to traditional MAE across models ranging from ViT-S to ViT-H and significantly reduces computational requirements. By its design, CrossMAE challenges the necessity of interaction between mask tokens for effective masked pretraining. Code and models are publicly available: https://crossmae.github.io"
      },
      {
        "id": "oai:arXiv.org:2402.06038v2",
        "title": "Understanding Contrastive Representation Learning from Positive Unlabeled (PU) Data",
        "link": "https://arxiv.org/abs/2402.06038",
        "author": "Anish Acharya, Li Jing, Bhargav Bhushanam, Dhruv Choudhary, Michael Rabbat, Sujay Sanghavi, Inderjit S Dhillon",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2402.06038v2 Announce Type: replace \nAbstract: Pretext Invariant Representation Learning (PIRL) followed by Supervised Fine-Tuning (SFT) has become a standard paradigm for learning with limited labels. We extend this approach to the Positive Unlabeled (PU) setting, where only a small set of labeled positives and a large unlabeled pool -- containing both positives and negatives are available. We study this problem under two regimes: (i) without access to the class prior, and (ii) when the prior is known or can be estimated. We introduce Positive Unlabeled Contrastive Learning (puCL), an unbiased and variance reducing contrastive objective that integrates weak supervision from labeled positives judiciously into the contrastive loss. When the class prior is known, we propose Positive Unlabeled InfoNCE (puNCE), a prior-aware extension that re-weights unlabeled samples as soft positive negative mixtures. For downstream classification, we develop a pseudo-labeling algorithm that leverages the structure of the learned embedding space via PU aware clustering. Our framework is supported by theory; offering bias-variance analysis, convergence insights, and generalization guarantees via augmentation concentration; and validated empirically across standard PU benchmarks, where it consistently outperforms existing methods, particularly in low-supervision regimes."
      },
      {
        "id": "oai:arXiv.org:2402.18206v4",
        "title": "Balancing Act: Distribution-Guided Debiasing in Diffusion Models",
        "link": "https://arxiv.org/abs/2402.18206",
        "author": "Rishubh Parihar, Abhijnya Bhat, Abhipsa Basu, Saswat Mallick, Jogendra Nath Kundu, R. Venkatesh Babu",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2402.18206v4 Announce Type: replace \nAbstract: Diffusion Models (DMs) have emerged as powerful generative models with unprecedented image generation capability. These models are widely used for data augmentation and creative applications. However, DMs reflect the biases present in the training datasets. This is especially concerning in the context of faces, where the DM prefers one demographic subgroup vs others (eg. female vs male). In this work, we present a method for debiasing DMs without relying on additional data or model retraining. Specifically, we propose Distribution Guidance, which enforces the generated images to follow the prescribed attribute distribution. To realize this, we build on the key insight that the latent features of denoising UNet hold rich demographic semantics, and the same can be leveraged to guide debiased generation. We train Attribute Distribution Predictor (ADP) - a small mlp that maps the latent features to the distribution of attributes. ADP is trained with pseudo labels generated from existing attribute classifiers. The proposed Distribution Guidance with ADP enables us to do fair generation. Our method reduces bias across single/multiple attributes and outperforms the baseline by a significant margin for unconditional and text-conditional diffusion models. Further, we present a downstream task of training a fair attribute classifier by rebalancing the training set with our generated data."
      },
      {
        "id": "oai:arXiv.org:2404.08335v2",
        "title": "Toward a Theory of Tokenization in LLMs",
        "link": "https://arxiv.org/abs/2404.08335",
        "author": "Nived Rajaraman, Jiantao Jiao, Kannan Ramchandran",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2404.08335v2 Announce Type: replace \nAbstract: While there has been a large body of research attempting to circumvent tokenization for language modeling (Clark et al., 2022; Xue et al., 2022), the current consensus is that it is a necessary initial step for designing state-of-the-art performant language models. In this paper, we investigate tokenization from a theoretical point of view by studying the behavior of transformers on simple data generating processes. When trained on data drawn from certain simple $k^{\\text{th}}$-order Markov processes for $k > 1$, transformers exhibit a surprising phenomenon - in the absence of tokenization, they empirically fail to learn the right distribution and predict characters according to a unigram model (Makkuva et al., 2024). With the addition of tokenization, however, we empirically observe that transformers break through this barrier and are able to model the probabilities of sequences drawn from the source near-optimally, achieving small cross-entropy loss. With this observation as starting point, we study the end-to-end cross-entropy loss achieved by transformers with and without tokenization. With the appropriate tokenization, we show that even the simplest unigram models (over tokens) learnt by transformers are able to model the probability of sequences drawn from $k^{\\text{th}}$-order Markov sources near optimally. Our analysis provides a justification for the use of tokenization in practice through studying the behavior of transformers on Markovian data."
      },
      {
        "id": "oai:arXiv.org:2404.10702v3",
        "title": "Retrieval Augmented Verification for Zero-Shot Detection of Multimodal Disinformation",
        "link": "https://arxiv.org/abs/2404.10702",
        "author": "Arka Ujjal Dey, Artemis Llabr\\'es, Ernest Valveny, Dimosthenis Karatzas",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2404.10702v3 Announce Type: replace \nAbstract: The rise of disinformation on social media, especially through the strategic manipulation or repurposing of images, paired with provocative text, presents a complex challenge for traditional fact-checking methods. In this paper, we introduce a novel zero-shot approach to identify and interpret such multimodal disinformation, leveraging real-time evidence from credible sources. Our framework goes beyond simple true-or-false classifications by analyzing both the textual and visual components of social media claims in a structured, interpretable manner. By constructing a graph-based representation of entities and relationships within the claim, combined with pretrained visual features, our system automatically retrieves and matches external evidence to identify inconsistencies. Unlike traditional models dependent on labeled datasets, our method empowers users with transparency, illuminating exactly which aspects of the claim hold up to scrutiny and which do not. Our framework achieves competitive performance with state-of-the-art methods while offering enhanced explainability."
      },
      {
        "id": "oai:arXiv.org:2404.19363v2",
        "title": "Expressivity and Speech Synthesis",
        "link": "https://arxiv.org/abs/2404.19363",
        "author": "Andreas Triantafyllopoulos, Bj\\\"orn W. Schuller",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2404.19363v2 Announce Type: replace \nAbstract: Imbuing machines with the ability to talk has been a longtime pursuit of artificial intelligence (AI) research. From the very beginning, the community has not only aimed to synthesise high-fidelity speech that accurately conveys the semantic meaning of an utterance, but also to colour it with inflections that cover the same range of affective expressions that humans are capable of. After many years of research, it appears that we are on the cusp of achieving this when it comes to single, isolated utterances. This unveils an abundance of potential avenues to explore when it comes to combining these single utterances with the aim of synthesising more complex, longer-term behaviours. In the present chapter, we outline the methodological advances that brought us so far and sketch out the ongoing efforts to reach that coveted next level of artificial expressivity. We also discuss the societal implications coupled with rapidly advancing expressive speech synthesis (ESS) technology and highlight ways to mitigate those risks and ensure the alignment of ESS capabilities with ethical norms."
      },
      {
        "id": "oai:arXiv.org:2405.01814v2",
        "title": "Efficient Heterogeneous Large Language Model Decoding with Model-Attention Disaggregation",
        "link": "https://arxiv.org/abs/2405.01814",
        "author": "Shaoyuan Chen, Wencong Xiao, Yutong Lin, Mingxing Zhang, Yingdi Shan, Jinlei Jiang, Kang Chen, Yongwei Wu",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2405.01814v2 Announce Type: replace \nAbstract: Transformer-based large language models (LLMs) exhibit impressive performance in generative tasks but also introduce significant challenges in real-world serving due to inefficient use of the expensive, computation-optimized accelerators. Although disaggregated serving architectures have been proposed to split different phases of LLM inference, the efficiency of decoding phase is still low. This is caused by the varying resource demands of different operators in the transformer-based LLMs. Specifically, the attention operator is memory-intensive, exhibiting a memory access pattern that clashes with the strengths of modern accelerators, especially for long context requests. To enhance the efficiency of LLM decoding, we introduce model-attention disaggregation. This approach leverages a collection of cheap, memory-optimized devices for the attention operator while still utilizing high-end accelerators for other parts of the model. This heterogeneous setup ensures that each component is tailored to its specific workload, maximizing overall performance and cost efficiency. Our comprehensive analysis and experiments confirm the viability of splitting the attention computation over multiple devices. Also, the communication bandwidth required between heterogeneous devices proves to be manageable with prevalent networking technologies. To further validate our theory, we develop and deploy Lamina, an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster. Experimental results indicate that Lamina can provide 16.1 ~ 90.1% higher estimated throughput than existing solutions with similar costs."
      },
      {
        "id": "oai:arXiv.org:2405.04710v4",
        "title": "Untangling Lariats: Subgradient Following of Variationally Penalized Objectives",
        "link": "https://arxiv.org/abs/2405.04710",
        "author": "Kai-Chia Mo, Shai Shalev-Shwartz, Nis{\\ae}l Sh\\'artov",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2405.04710v4 Announce Type: replace \nAbstract: We describe an apparatus for subgradient-following of the optimum of convex problems with variational penalties. In this setting, we receive a sequence $y_i,\\ldots,y_n$ and seek a smooth sequence $x_1,\\ldots,x_n$. The smooth sequence needs to attain the minimum Bregman divergence to an input sequence with additive variational penalties in the general form of $\\sum_i{}g_i(x_{i+1}-x_i)$. We derive known algorithms such as the fused lasso and isotonic regression as special cases of our approach. Our approach also facilitates new variational penalties such as non-smooth barrier functions.\n  We then derive a novel lattice-based procedure for subgradient following of variational penalties characterized through the output of arbitrary convolutional filters. This paradigm yields efficient solvers for high-order filtering problems of temporal sequences in which sparse discrete derivatives such as acceleration and jerk are desirable. We also introduce and analyze new multivariate problems in which $\\mathbf{x}_i,\\mathbf{y}_i\\in\\mathbb{R}^d$ with variational penalties that depend on $\\|\\mathbf{x}_{i+1}-\\mathbf{x}_i\\|$. The norms we consider are $\\ell_2$ and $\\ell_\\infty$ which promote group sparsity."
      },
      {
        "id": "oai:arXiv.org:2405.08036v5",
        "title": "POWQMIX: Weighted Value Factorization with Potentially Optimal Joint Actions Recognition for Cooperative Multi-Agent Reinforcement Learning",
        "link": "https://arxiv.org/abs/2405.08036",
        "author": "Chang Huang, Shatong Zhu, Junqiao Zhao, Hongtu Zhou, Chen Ye, Tiantian Feng, Changjun Jiang",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2405.08036v5 Announce Type: replace \nAbstract: Value function factorization methods are commonly used in cooperative multi-agent reinforcement learning, with QMIX receiving significant attention. Many QMIX-based methods introduce monotonicity constraints between the joint action value and individual action values to achieve decentralized execution. However, such constraints limit the representation capacity of value factorization, restricting the joint action values it can represent and hindering the learning of the optimal policy. To address this challenge, we propose the Potentially Optimal Joint Actions Weighted QMIX (POWQMIX) algorithm, which recognizes the potentially optimal joint actions and assigns higher weights to the corresponding losses of these joint actions during training. We theoretically prove that with such a weighted training approach the optimal policy is guaranteed to be recovered. Experiments in matrix games, difficulty-enhanced predator-prey, and StarCraft II Multi-Agent Challenge environments demonstrate that our algorithm outperforms the state-of-the-art value-based multi-agent reinforcement learning methods."
      },
      {
        "id": "oai:arXiv.org:2405.15491v3",
        "title": "GSDeformer: Direct, Real-time and Extensible Cage-based Deformation for 3D Gaussian Splatting",
        "link": "https://arxiv.org/abs/2405.15491",
        "author": "Jiajun Huang, Shuolin Xu, Hongchuan Yu, Tong-Yee Lee",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2405.15491v3 Announce Type: replace \nAbstract: We present GSDeformer, a method that enables cage-based deformation on 3D Gaussian Splatting (3DGS). Our approach bridges cage-based deformation and 3DGS by using a proxy point-cloud representation. This point cloud is generated from 3D Gaussians, and deformations applied to the point cloud are translated into transformations on the 3D Gaussians. To handle potential bending caused by deformation, we incorporate a splitting process to approximate it. Our method does not modify or extend the core architecture of 3D Gaussian Splatting, making it compatible with any trained vanilla 3DGS or its variants. Additionally, we automate cage construction for 3DGS and its variants using a render-and-reconstruct approach. Experiments demonstrate that GSDeformer delivers superior deformation results compared to existing methods, is robust under extreme deformations, requires no retraining for editing, runs in real-time, and can be extended to other 3DGS variants. Project Page: https://jhuangbu.github.io/gsdeformer/"
      },
      {
        "id": "oai:arXiv.org:2405.16924v2",
        "title": "Demystifying amortized causal discovery with transformers",
        "link": "https://arxiv.org/abs/2405.16924",
        "author": "Francesco Montagna, Max Cairney-Leeming, Dhanya Sridhar, Francesco Locatello",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2405.16924v2 Announce Type: replace \nAbstract: Supervised learning approaches for causal discovery from observational data often achieve competitive performance despite seemingly avoiding explicit assumptions that traditional methods make for identifiability. In this work, we investigate CSIvA (Ke et al., 2023), a transformer-based model promising to train on synthetic data and transfer to real data. First, we bridge the gap with existing identifiability theory and show that constraints on the training data distribution implicitly define a prior on the test observations. Consistent with classical approaches, good performance is achieved when we have a good prior on the test data, and the underlying model is identifiable. At the same time, we find new trade-offs. Training on datasets generated from different classes of causal models, unambiguously identifiable in isolation, improves the test generalization. Performance is still guaranteed, as the ambiguous cases resulting from the mixture of identifiable causal models are unlikely to occur (which we formally prove). Overall, our study finds that amortized causal discovery still needs to obey identifiability theory, but it also differs from classical methods in how the assumptions are formulated, trading more reliance on assumptions on the noise type for fewer hypotheses on the mechanisms."
      },
      {
        "id": "oai:arXiv.org:2405.18560v3",
        "title": "Potential Field Based Deep Metric Learning",
        "link": "https://arxiv.org/abs/2405.18560",
        "author": "Shubhang Bhatnagar, Narendra Ahuja",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2405.18560v3 Announce Type: replace \nAbstract: Deep metric learning (DML) involves training a network to learn a semantically meaningful representation space. Many current approaches mine n-tuples of examples and model interactions within each tuplets. We present a novel, compositional DML model that instead of in tuples, represents the influence of each example (embedding) by a continuous potential field, and superposes the fields to obtain their combined global potential field. We use attractive/repulsive potential fields to represent interactions among embeddings from images of the same/different classes. Contrary to typical learning methods, where mutual influence of samples is proportional to their distance, we enforce reduction in such influence with distance, leading to a decaying field. We show that such decay helps improve performance on real world datasets with large intra-class variations and label noise. Like other proxy-based methods, we also use proxies to succinctly represent sub-populations of examples. We evaluate our method on three standard DML benchmarks- Cars-196, CUB-200-2011, and SOP datasets where it outperforms state-of-the-art baselines."
      },
      {
        "id": "oai:arXiv.org:2406.11835v2",
        "title": "OoDIS: Anomaly Instance Segmentation and Detection Benchmark",
        "link": "https://arxiv.org/abs/2406.11835",
        "author": "Alexey Nekrasov, Rui Zhou, Miriam Ackermann, Alexander Hermans, Bastian Leibe, Matthias Rottmann",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2406.11835v2 Announce Type: replace \nAbstract: Safe navigation of self-driving cars and robots requires a precise understanding of their environment. Training data for perception systems cannot cover the wide variety of objects that may appear during deployment. Thus, reliable identification of unknown objects, such as wild animals and untypical obstacles, is critical due to their potential to cause serious accidents. Significant progress in semantic segmentation of anomalies has been facilitated by the availability of out-of-distribution (OOD) benchmarks. However, a comprehensive understanding of scene dynamics requires the segmentation of individual objects, and thus the segmentation of instances is essential. Development in this area has been lagging, largely due to the lack of dedicated benchmarks. The situation is similar in object detection. While there is interest in detecting and potentially tracking every anomalous object, the availability of dedicated benchmarks is clearly limited. To address this gap, this work extends some commonly used anomaly segmentation benchmarks to include the instance segmentation and object detection tasks. Our evaluation of anomaly instance segmentation and object detection methods shows that both of these challenges remain unsolved problems. We provide a competition and benchmark website under https://vision.rwth-aachen.de/oodis"
      },
      {
        "id": "oai:arXiv.org:2406.12816v2",
        "title": "Neural Approximate Mirror Maps for Constrained Diffusion Models",
        "link": "https://arxiv.org/abs/2406.12816",
        "author": "Berthy T. Feng, Ricardo Baptista, Katherine L. Bouman",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2406.12816v2 Announce Type: replace \nAbstract: Diffusion models excel at creating visually-convincing images, but they often struggle to meet subtle constraints inherent in the training data. Such constraints could be physics-based (e.g., satisfying a PDE), geometric (e.g., respecting symmetry), or semantic (e.g., including a particular number of objects). When the training data all satisfy a certain constraint, enforcing this constraint on a diffusion model makes it more reliable for generating valid synthetic data and solving constrained inverse problems. However, existing methods for constrained diffusion models are restricted in the constraints they can handle. For instance, recent work proposed to learn mirror diffusion models (MDMs), but analytical mirror maps only exist for convex constraints and can be challenging to derive. We propose neural approximate mirror maps (NAMMs) for general, possibly non-convex constraints. Our approach only requires a differentiable distance function from the constraint set. We learn an approximate mirror map that transforms data into an unconstrained space and a corresponding approximate inverse that maps data back to the constraint set. A generative model, such as an MDM, can then be trained in the learned mirror space and its samples restored to the constraint set by the inverse map. We validate our approach on a variety of constraints, showing that compared to an unconstrained diffusion model, a NAMM-based MDM substantially improves constraint satisfaction. We also demonstrate how existing diffusion-based inverse-problem solvers can be easily applied in the learned mirror space to solve constrained inverse problems."
      },
      {
        "id": "oai:arXiv.org:2406.14528v3",
        "title": "DeciMamba: Exploring the Length Extrapolation Potential of Mamba",
        "link": "https://arxiv.org/abs/2406.14528",
        "author": "Assaf Ben-Kish, Itamar Zimerman, Shady Abu-Hussein, Nadav Cohen, Amir Globerson, Lior Wolf, Raja Giryes",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2406.14528v3 Announce Type: replace \nAbstract: Long-range sequence processing poses a significant challenge for Transformers due to their quadratic complexity in input length. A promising alternative is Mamba, which demonstrates high performance and achieves Transformer-level capabilities while requiring substantially fewer computational resources. In this paper we explore the length-generalization capabilities of Mamba, which we find to be relatively limited. Through a series of visualizations and analyses we identify that the limitations arise from a restricted effective receptive field, dictated by the sequence length used during training. To address this constraint, we introduce DeciMamba, a context-extension method specifically designed for Mamba. This mechanism, built on top of a hidden filtering mechanism embedded within the S6 layer, enables the trained model to extrapolate well even without additional training. Empirical experiments over real-world long-range NLP tasks show that DeciMamba can extrapolate to context lengths that are significantly longer than the ones seen during training, while enjoying faster inference."
      },
      {
        "id": "oai:arXiv.org:2406.15156v2",
        "title": "Reconsidering Faithfulness in Regular, Self-Explainable and Domain Invariant GNNs",
        "link": "https://arxiv.org/abs/2406.15156",
        "author": "Steve Azzolin, Antonio Longa, Stefano Teso, Andrea Passerini",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2406.15156v2 Announce Type: replace \nAbstract: As Graph Neural Networks (GNNs) become more pervasive, it becomes paramount to build reliable tools for explaining their predictions. A core desideratum is that explanations are \\textit{faithful}, \\ie that they portray an accurate picture of the GNN's reasoning process. However, a number of different faithfulness metrics exist, begging the question of what is faithfulness exactly and how to achieve it. We make three key contributions. We begin by showing that \\textit{existing metrics are not interchangeable} -- \\ie explanations attaining high faithfulness according to one metric may be unfaithful according to others -- and can systematically ignore important properties of explanations. We proceed to show that, surprisingly, \\textit{optimizing for faithfulness is not always a sensible design goal}. Specifically, we prove that for injective regular GNN architectures, perfectly faithful explanations are completely uninformative. This does not apply to modular GNNs, such as self-explainable and domain-invariant architectures, prompting us to study the relationship between architectural choices and faithfulness. Finally, we show that \\textit{faithfulness is tightly linked to out-of-distribution generalization}, in that simply ensuring that a GNN can correctly recognize the domain-invariant subgraph, as prescribed by the literature, does not guarantee that it is invariant unless this subgraph is also faithful.The code is publicly available on GitHub"
      },
      {
        "id": "oai:arXiv.org:2407.04752v3",
        "title": "SpikeLLM: Scaling up Spiking Neural Network to Large Language Models via Saliency-based Spiking",
        "link": "https://arxiv.org/abs/2407.04752",
        "author": "Xingrun Xing, Boyan Gao, Zheng Zhang, David A. Clifton, Shitao Xiao, Li Du, Guoqi Li, Jiajun Zhang",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2407.04752v3 Announce Type: replace \nAbstract: Recent advancements in large language models (LLMs) with billions of parameters have improved performance in various applications, but their inference processes demand significant energy and computational resources. In contrast, the human brain, with approximately 86 billion neurons, is much more energy-efficient than LLMs with similar parameters. Inspired by this, we redesign 7$\\sim$70 billion parameter LLMs using bio-plausible spiking mechanisms, emulating the efficient behavior of the human brain. We propose the first spiking large language model, SpikeLLM. Coupled with the proposed model, two essential approaches are proposed to improve spike training efficiency: Generalized Integrate-and-Fire (GIF) neurons to compress spike length from $T$ to $\\frac{T}{L} \\log_2 L$ bits, and an Optimal Brain Spiking framework to divide outlier channels and allocate different $T$ for GIF neurons, which further compresses spike length to approximate $log_2T$ bits. The necessity of spike-driven LLM is proved by comparison with quantized LLMs with similar operations. In the OmniQuant pipeline, SpikeLLM reduces 11.01% WikiText2 perplexity and improves 2.55% accuracy of common scene reasoning on a LLAMA-7B W4A4 model. In the GPTQ pipeline, SpikeLLM achieves direct additive in linear layers, significantly exceeding PB-LLMs."
      },
      {
        "id": "oai:arXiv.org:2407.08169v2",
        "title": "The Approximate Fisher Influence Function: Faster Estimation of Data Influence in Statistical Models",
        "link": "https://arxiv.org/abs/2407.08169",
        "author": "Omri Lev, Ashia C. Wilson",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2407.08169v2 Announce Type: replace \nAbstract: Quantifying the influence of infinitesimal changes in training data on model performance is crucial for understanding and improving machine learning models. In this work, we reformulate this problem as a weighted empirical risk minimization and enhance existing influence function-based methods by using information geometry to derive a new algorithm to estimate influence. Our formulation proves versatile across various applications, and we further demonstrate in simulations how it remains informative even in non-convex cases. Furthermore, we show that our method offers significant computational advantages over current Newton step-based methods."
      },
      {
        "id": "oai:arXiv.org:2407.09722v4",
        "title": "Optimized Multi-Token Joint Decoding with Auxiliary Model for LLM Inference",
        "link": "https://arxiv.org/abs/2407.09722",
        "author": "Zongyue Qin, Ziniu Hu, Zifan He, Neha Prakriya, Jason Cong, Yizhou Sun",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2407.09722v4 Announce Type: replace \nAbstract: Large language models (LLMs) have achieved remarkable success across diverse tasks, yet their inference processes are hindered by substantial time and energy demands due to single-token generation at each decoding step. While previous methods such as speculative decoding mitigate these inefficiencies by producing multiple tokens per step, each token is still generated by its single-token distribution, thereby enhancing speed without improving effectiveness. In contrast, our work simultaneously enhances inference speed and improves the output effectiveness. We consider multi-token joint decoding (MTJD), which generates multiple tokens from their joint distribution at each iteration, theoretically reducing perplexity and enhancing task performance. However, MTJD suffers from the high cost of sampling from the joint distribution of multiple tokens. Inspired by speculative decoding, we introduce multi-token assisted decoding (MTAD), a novel framework designed to accelerate MTJD. MTAD leverages a smaller auxiliary model to approximate the joint distribution of a larger model, incorporating a verification mechanism that not only ensures the accuracy of this approximation, but also improves the decoding efficiency over conventional speculative decoding. Theoretically, we demonstrate that MTAD closely approximates exact MTJD with bounded error. Empirical evaluations using Llama-2 and OPT models ranging from 13B to 70B parameters across various tasks reveal that MTAD reduces perplexity by 21.2% and improves downstream performance compared to standard single-token sampling. Furthermore, MTAD achieves a 1.42x speed-up and consumes 1.54x less energy than conventional speculative decoding methods. These results highlight MTAD's ability to make multi-token joint decoding both effective and efficient, promoting more sustainable and high-performance deployment of LLMs."
      },
      {
        "id": "oai:arXiv.org:2408.16357v2",
        "title": "Law of Vision Representation in MLLMs",
        "link": "https://arxiv.org/abs/2408.16357",
        "author": "Shijia Yang, Bohan Zhai, Quanzeng You, Jianbo Yuan, Hongxia Yang, Chenfeng Xu",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2408.16357v2 Announce Type: replace \nAbstract: We present the \"Law of Vision Representation\" in multimodal large language models (MLLMs). It reveals a strong correlation between the combination of cross-modal alignment, correspondence in vision representation, and MLLM performance. We quantify the two factors using the cross-modal Alignment and Correspondence score (AC score). Through extensive experiments involving thirteen different vision representation settings and evaluations across eight benchmarks, we find that the AC score is linearly correlated to model performance. By leveraging this relationship, we are able to identify and train the optimal vision representation only, which does not require finetuning the language model every time, resulting in a 99.7% reduction in computational cost."
      },
      {
        "id": "oai:arXiv.org:2409.10365v2",
        "title": "Robust image representations with counterfactual contrastive learning",
        "link": "https://arxiv.org/abs/2409.10365",
        "author": "M\\'elanie Roschewitz, Fabio De Sousa Ribeiro, Tian Xia, Galvin Khara, Ben Glocker",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2409.10365v2 Announce Type: replace \nAbstract: Contrastive pretraining can substantially increase model generalisation and downstream performance. However, the quality of the learned representations is highly dependent on the data augmentation strategy applied to generate positive pairs. Positive contrastive pairs should preserve semantic meaning while discarding unwanted variations related to the data acquisition domain. Traditional contrastive pipelines attempt to simulate domain shifts through pre-defined generic image transformations. However, these do not always mimic realistic and relevant domain variations for medical imaging, such as scanner differences. To tackle this issue, we herein introduce counterfactual contrastive learning, a novel framework leveraging recent advances in causal image synthesis to create contrastive positive pairs that faithfully capture relevant domain variations. Our method, evaluated across five datasets encompassing both chest radiography and mammography data, for two established contrastive objectives (SimCLR and DINO-v2), outperforms standard contrastive learning in terms of robustness to acquisition shift. Notably, counterfactual contrastive learning achieves superior downstream performance on both in-distribution and external datasets, especially for images acquired with scanners under-represented in the training set. Further experiments show that the proposed framework extends beyond acquisition shifts, with models trained with counterfactual contrastive learning reducing subgroup disparities across biological sex."
      },
      {
        "id": "oai:arXiv.org:2409.13349v2",
        "title": "ID-Guard: A Universal Framework for Combating Facial Manipulation via Breaking Identification",
        "link": "https://arxiv.org/abs/2409.13349",
        "author": "Zuomin Qu, Wei Lu, Xiangyang Luo, Qian Wang, Xiaochun Cao",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2409.13349v2 Announce Type: replace \nAbstract: The misuse of deep learning-based facial manipulation poses a significant threat to civil rights. To prevent this fraud at its source, proactive defense has been proposed to disrupt the manipulation process by adding invisible adversarial perturbations into images, making the forged output unconvincing to observers. However, the non-specific disruption against the output may lead to the retention of identifiable facial features, potentially resulting in the stigmatization of the individual. This paper proposes a universal framework for combating facial manipulation, termed ID-Guard. Specifically, this framework operates with a single forward pass of an encoder-decoder network to produce a cross-model transferable adversarial perturbation. A novel Identity Destruction Module (IDM) is introduced to degrade identifiable features in forged faces. We optimize the perturbation generation by framing the disruption of different facial manipulations as a multi-task learning problem, and a dynamic weight strategy is devised to enhance cross-model performance. Experimental results demonstrate that the proposed ID-Guard exhibits strong efficacy in defending against various facial manipulation models, effectively degrading identifiable regions in manipulated images. It also enables disrupted images to evade facial inpainting and image recognition systems. Additionally, ID-Guard can seamlessly function as a plug-and-play component, integrating with other tasks such as adversarial training."
      },
      {
        "id": "oai:arXiv.org:2409.18025v5",
        "title": "An Adversarial Perspective on Machine Unlearning for AI Safety",
        "link": "https://arxiv.org/abs/2409.18025",
        "author": "Jakub {\\L}ucki, Boyi Wei, Yangsibo Huang, Peter Henderson, Florian Tram\\`er, Javier Rando",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2409.18025v5 Announce Type: replace \nAbstract: Large language models are finetuned to refuse questions about hazardous knowledge, but these protections can often be bypassed. Unlearning methods aim at completely removing hazardous capabilities from models and make them inaccessible to adversaries. This work challenges the fundamental differences between unlearning and traditional safety post-training from an adversarial perspective. We demonstrate that existing jailbreak methods, previously reported as ineffective against unlearning, can be successful when applied carefully. Furthermore, we develop a variety of adaptive methods that recover most supposedly unlearned capabilities. For instance, we show that finetuning on 10 unrelated examples or removing specific directions in the activation space can recover most hazardous capabilities for models edited with RMU, a state-of-the-art unlearning method. Our findings challenge the robustness of current unlearning approaches and question their advantages over safety training."
      },
      {
        "id": "oai:arXiv.org:2409.19075v3",
        "title": "Meta-RTL: Reinforcement-Based Meta-Transfer Learning for Low-Resource Commonsense Reasoning",
        "link": "https://arxiv.org/abs/2409.19075",
        "author": "Yu Fu, Jie He, Yifan Yang, Qun Liu, Deyi Xiong",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2409.19075v3 Announce Type: replace \nAbstract: Meta learning has been widely used to exploit rich-resource source tasks to improve the performance of low-resource target tasks. Unfortunately, most existing meta learning approaches treat different source tasks equally, ignoring the relatedness of source tasks to the target task in knowledge transfer. To mitigate this issue, we propose a reinforcement-based multi-source meta-transfer learning framework (Meta-RTL) for low-resource commonsense reasoning. In this framework, we present a reinforcement-based approach to dynamically estimating source task weights that measure the contribution of the corresponding tasks to the target task in the meta-transfer learning. The differences between the general loss of the meta model and task-specific losses of source-specific temporal meta models on sampled target data are fed into the policy network of the reinforcement learning module as rewards. The policy network is built upon LSTMs that capture long-term dependencies on source task weight estimation across meta learning iterations. We evaluate the proposed Meta-RTL using both BERT and ALBERT as the backbone of the meta model on three commonsense reasoning benchmark datasets. Experimental results demonstrate that Meta-RTL substantially outperforms strong baselines and previous task selection strategies and achieves larger improvements on extremely low-resource settings."
      },
      {
        "id": "oai:arXiv.org:2410.01595v3",
        "title": "KnobGen: Controlling the Sophistication of Artwork in Sketch-Based Diffusion Models",
        "link": "https://arxiv.org/abs/2410.01595",
        "author": "Pouyan Navard, Amin Karimi Monsefi, Mengxi Zhou, Wei-Lun Chao, Alper Yilmaz, Rajiv Ramnath",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.01595v3 Announce Type: replace \nAbstract: Recent advances in diffusion models have significantly improved text-to-image (T2I) generation, but they often struggle to balance fine-grained precision with high-level control. Methods like ControlNet and T2I-Adapter excel at following sketches by seasoned artists but tend to be overly rigid, replicating unintentional flaws in sketches from novice users. Meanwhile, coarse-grained methods, such as sketch-based abstraction frameworks, offer more accessible input handling but lack the precise control needed for detailed, professional use. To address these limitations, we propose KnobGen, a dual-pathway framework that democratizes sketch-based image generation by seamlessly adapting to varying levels of sketch complexity and user skill. KnobGen uses a Coarse-Grained Controller (CGC) module for high-level semantics and a Fine-Grained Controller (FGC) module for detailed refinement. The relative strength of these two modules can be adjusted through our knob inference mechanism to align with the user's specific needs. These mechanisms ensure that KnobGen can flexibly generate images from both novice sketches and those drawn by seasoned artists. This maintains control over the final output while preserving the natural appearance of the image, as evidenced on the MultiGen-20M dataset and a newly collected sketch dataset."
      },
      {
        "id": "oai:arXiv.org:2410.02113v2",
        "title": "Mamba Neural Operator: Who Wins? Transformers vs. State-Space Models for PDEs",
        "link": "https://arxiv.org/abs/2410.02113",
        "author": "Chun-Wun Cheng, Jiahao Huang, Yi Zhang, Guang Yang, Carola-Bibiane Sch\\\"onlieb, Angelica I Aviles-Rivero",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.02113v2 Announce Type: replace \nAbstract: Partial differential equations (PDEs) are widely used to model complex physical systems, but solving them efficiently remains a significant challenge. Recently, Transformers have emerged as the preferred architecture for PDEs due to their ability to capture intricate dependencies. However, they struggle with representing continuous dynamics and long-range interactions. To overcome these limitations, we introduce the Mamba Neural Operator (MNO), a novel framework that enhances neural operator-based techniques for solving PDEs. MNO establishes a formal theoretical connection between structured state-space models (SSMs) and neural operators, offering a unified structure that can adapt to diverse architectures, including Transformer-based models. By leveraging the structured design of SSMs, MNO captures long-range dependencies and continuous dynamics more effectively than traditional Transformers. Through extensive analysis, we show that MNO significantly boosts the expressive power and accuracy of neural operators, making it not just a complement but a superior framework for PDE-related tasks, bridging the gap between efficient representation and accurate solution approximation."
      },
      {
        "id": "oai:arXiv.org:2410.04639v3",
        "title": "Radial Basis Operator Networks",
        "link": "https://arxiv.org/abs/2410.04639",
        "author": "Jason Kurz, Sean Oughton, Shitao Liu",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.04639v3 Announce Type: replace \nAbstract: Operator networks are designed to approximate nonlinear operators, which provide mappings between infinite-dimensional spaces such as function spaces. These networks are playing an increasingly important role in machine learning, with their most notable contributions in the field of scientific computing. Their significance stems from their ability to handle the type of data often encountered in scientific applications. For instance, in climate modeling or fluid dynamics, input data typically consists of discretized continuous fields (like temperature distributions or velocity fields). We introduce the radial basis operator network (RBON), which represents a significant advancement as the first operator network capable of learning an operator in both the time domain and frequency domain when adjusted to accept complex-valued inputs. Despite the small, single hidden-layer structure, the RBON boasts small $L^2$ relative test error for both in- and out-of-distribution data (OOD) of less than $1\\times 10^{-7}$ in some benchmark cases. Moreover, the RBON maintains small error on OOD data from entirely different function classes from the training data."
      },
      {
        "id": "oai:arXiv.org:2410.06264v2",
        "title": "Think While You Generate: Discrete Diffusion with Planned Denoising",
        "link": "https://arxiv.org/abs/2410.06264",
        "author": "Sulin Liu, Juno Nam, Andrew Campbell, Hannes St\\\"ark, Yilun Xu, Tommi Jaakkola, Rafael G\\'omez-Bombarelli",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.06264v2 Announce Type: replace \nAbstract: Discrete diffusion has achieved state-of-the-art performance, outperforming or approaching autoregressive models on standard benchmarks. In this work, we introduce Discrete Diffusion with Planned Denoising (DDPD), a novel framework that separates the generation process into two models: a planner and a denoiser. At inference time, the planner selects which positions to denoise next by identifying the most corrupted positions in need of denoising, including both initially corrupted and those requiring additional refinement. This plan-and-denoise approach enables more efficient reconstruction during generation by iteratively identifying and denoising corruptions in the optimal order. DDPD outperforms traditional denoiser-only mask diffusion methods, achieving superior results on language modeling benchmarks such as text8, OpenWebText, and token-based image generation on ImageNet $256 \\times 256$. Notably, in language modeling, DDPD significantly reduces the performance gap between diffusion-based and autoregressive methods in terms of generative perplexity. Code is available at https://github.com/liusulin/DDPD."
      },
      {
        "id": "oai:arXiv.org:2410.08206v2",
        "title": "Interactive4D: Interactive 4D LiDAR Segmentation",
        "link": "https://arxiv.org/abs/2410.08206",
        "author": "Ilya Fradlin, Idil Esen Zulfikar, Kadir Yilmaz, Theodora Kontogianni, Bastian Leibe",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.08206v2 Announce Type: replace \nAbstract: Interactive segmentation has an important role in facilitating the annotation process of future LiDAR datasets. Existing approaches sequentially segment individual objects at each LiDAR scan, repeating the process throughout the entire sequence, which is redundant and ineffective. In this work, we propose interactive 4D segmentation, a new paradigm that allows segmenting multiple objects on multiple LiDAR scans simultaneously, and Interactive4D, the first interactive 4D segmentation model that segments multiple objects on superimposed consecutive LiDAR scans in a single iteration by utilizing the sequential nature of LiDAR data. While performing interactive segmentation, our model leverages the entire space-time volume, leading to more efficient segmentation. Operating on the 4D volume, it directly provides consistent instance IDs over time and also simplifies tracking annotations. Moreover, we show that click simulations are crucial for successful model training on LiDAR point clouds. To this end, we design a click simulation strategy that is better suited for the characteristics of LiDAR data. To demonstrate its accuracy and effectiveness, we evaluate Interactive4D on multiple LiDAR datasets, where Interactive4D achieves a new state-of-the-art by a large margin. We publicly release the code and models at https://vision.rwth-aachen.de/Interactive4D."
      },
      {
        "id": "oai:arXiv.org:2410.08709v3",
        "title": "Distillation of Discrete Diffusion through Dimensional Correlations",
        "link": "https://arxiv.org/abs/2410.08709",
        "author": "Satoshi Hayakawa, Yuhta Takida, Masaaki Imaizumi, Hiromi Wakaki, Yuki Mitsufuji",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.08709v3 Announce Type: replace \nAbstract: Diffusion models have demonstrated exceptional performances in various fields of generative modeling, but suffer from slow sampling speed due to their iterative nature. While this issue is being addressed in continuous domains, discrete diffusion models face unique challenges, particularly in capturing dependencies between elements (e.g., pixel relationships in image, sequential dependencies in language) mainly due to the computational cost of processing high-dimensional joint distributions. In this paper, (i) we propose \"mixture\" models for discrete diffusion that are capable of treating dimensional correlations while remaining scalable, and (ii) we provide a set of loss functions for distilling the iterations of existing models. Two primary theoretical insights underpin our approach: First, conventional models with element-wise independence can well approximate the data distribution, but essentially require {\\it many sampling steps}. Second, our loss functions enable the mixture models to distill such many-step conventional models into just a few steps by learning the dimensional correlations. Our experimental results show the effectiveness of the proposed method in distilling pretrained discrete diffusion models across image and language domains. The code used in the paper is available at https://github.com/sony/di4c ."
      },
      {
        "id": "oai:arXiv.org:2410.08893v3",
        "title": "Drama: Mamba-Enabled Model-Based Reinforcement Learning Is Sample and Parameter Efficient",
        "link": "https://arxiv.org/abs/2410.08893",
        "author": "Wenlong Wang, Ivana Dusparic, Yucheng Shi, Ke Zhang, Vinny Cahill",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.08893v3 Announce Type: replace \nAbstract: Model-based reinforcement learning (RL) offers a solution to the data inefficiency that plagues most model-free RL algorithms. However, learning a robust world model often requires complex and deep architectures, which are computationally expensive and challenging to train. Within the world model, sequence models play a critical role in accurate predictions, and various architectures have been explored, each with its own challenges. Currently, recurrent neural network (RNN)-based world models struggle with vanishing gradients and capturing long-term dependencies. Transformers, on the other hand, suffer from the quadratic memory and computational complexity of self-attention mechanisms, scaling as $O(n^2)$, where $n$ is the sequence length.\n  To address these challenges, we propose a state space model (SSM)-based world model, Drama, specifically leveraging Mamba, that achieves $O(n)$ memory and computational complexity while effectively capturing long-term dependencies and enabling efficient training with longer sequences. We also introduce a novel sampling method to mitigate the suboptimality caused by an incorrect world model in the early training stages. Combining these techniques, Drama achieves a normalised score on the Atari100k benchmark that is competitive with other state-of-the-art (SOTA) model-based RL algorithms, using only a 7 million-parameter world model. Drama is accessible and trainable on off-the-shelf hardware, such as a standard laptop. Our code is available at https://github.com/realwenlongwang/Drama.git."
      },
      {
        "id": "oai:arXiv.org:2410.11741v2",
        "title": "POLO -- Point-based, multi-class animal detection",
        "link": "https://arxiv.org/abs/2410.11741",
        "author": "Giacomo May, Emanuele Dalsasso, Benjamin Kellenberger, Devis Tuia",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.11741v2 Announce Type: replace \nAbstract: Automated wildlife surveys based on drone imagery and object detection technology are a powerful and increasingly popular tool in conservation biology. Most detectors require training images with annotated bounding boxes, which are tedious, expensive, and not always unambiguous to create. To reduce the annotation load associated with this practice, we develop POLO, a multi-class object detection model that can be trained entirely on point labels. POLO is based on simple, yet effective modifications to the YOLOv8 architecture, including alterations to the prediction process, training losses, and post-processing. We test POLO on drone recordings of waterfowl containing up to multiple thousands of individual birds in one image and compare it to a regular YOLOv8. Our experiments show that at the same annotation cost, POLO achieves improved accuracy in counting animals in aerial imagery."
      },
      {
        "id": "oai:arXiv.org:2410.12586v3",
        "title": "How to Make LLMs Forget: On Reversing In-Context Knowledge Edits",
        "link": "https://arxiv.org/abs/2410.12586",
        "author": "Paul Youssef, Zhixue Zhao, J\\\"org Schl\\\"otterer, Christin Seifert",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.12586v3 Announce Type: replace \nAbstract: In-context knowledge editing (IKE) enables efficient modification of large language model (LLM) outputs without parameter changes and at zero-cost. However, it can be misused to manipulate responses opaquely, e.g., insert misinformation or offensive content. Such malicious interventions could be incorporated into high-level wrapped APIs where the final input prompt is not shown to end-users. To address this issue, we investigate the detection and reversal of IKE-edits. First, we demonstrate that IKE-edits can be detected with high accuracy (F1 > 80\\%) using only the top-10 output probabilities of the next token, even in a black-box setting, e.g. proprietary LLMs with limited output information. Further, we introduce the novel task of reversing IKE-edits using specially tuned reversal tokens. We explore using both continuous and discrete reversal tokens, achieving over 80\\% accuracy in recovering original, unedited outputs across multiple LLMs. Our continuous reversal tokens prove particularly effective, with minimal impact on unedited prompts. Through analysis of output distributions, attention patterns, and token rankings, we provide insights into IKE's effects on LLMs and how reversal tokens mitigate them. This work represents a significant step towards enhancing LLM resilience against potential misuse of in-context editing, improving their transparency and trustworthiness."
      },
      {
        "id": "oai:arXiv.org:2410.13453v3",
        "title": "Adaptive Augmentation Policy Optimization with LLM Feedback",
        "link": "https://arxiv.org/abs/2410.13453",
        "author": "Ant Duru, Alptekin Temizel",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.13453v3 Announce Type: replace \nAbstract: Data augmentation is a critical component of deep learning pipelines, enhancing model generalization by increasing dataset diversity. Traditional augmentation strategies rely on manually designed transformations, stochastic sampling, or automated search-based approaches. Although automated methods improve performance, they often require extensive computational resources and are tailored to specific datasets. In this work, we propose a Large Language Model (LLM)-guided augmentation optimization strategy that refines augmentation policies based on model performance feedback. We introduce two approaches: (1) LLM-Guided Augmentation Policy Optimization, where augmentation policies are selected by an LLM prior to training and iteratively refined across multiple training cycles, and (2) Adaptive LLM-Guided Augmentation Policy Optimization, where policies adapt in real-time based on performance metrics. This in-training approach eliminates the need for full model retraining before receiving LLM feedback, thereby reducing computational costs while improving performance. Our methodology employs an LLM to dynamically select augmentation transformations based on dataset characteristics, model architecture, and prior training outcomes. Unlike traditional search-based methods, our approach leverages the contextual knowledge of LLMs, particularly in specialized domains like medical imaging, to recommend augmentation strategies tailored to domain-specific data. We evaluate our approach on multiple domain-specific image classification datasets where augmentation is key to model robustness. Results show that LLM-guided augmentation optimization outperforms traditional methods, improving model accuracy. These findings highlight the potential of LLMs in automating and adapting deep learning training workflows."
      },
      {
        "id": "oai:arXiv.org:2410.15060v2",
        "title": "BYOCL: Build Your Own Consistent Latent with Hierarchical Representative Latent Clustering",
        "link": "https://arxiv.org/abs/2410.15060",
        "author": "Jiayue Dai, Yunya Wang, Yihan Fang, Yuetong Chen, Butian Xiong",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.15060v2 Announce Type: replace \nAbstract: To address the semantic inconsistency issue with SAM or other single-image segmentation models handling image sequences, we introduce BYOCL. This novel model outperforms SAM in extensive experiments, showcasing its Hierarchical prototype capabilities across CLIP and other representations. BYOCL significantly reduces time and space consumption by dividing inputs into smaller batches, achieving exponential time reduction compared to previous methods. Our approach leverages the SAM image encoder for feature extraction, followed by Intra-Batch and Inter-Batch clustering algorithms. Extensive experiments demonstrate that BYOCL far exceeds the previous state-of-the-art single image segmentation model. Our work is the first to apply consistent segmentation using foundation models without requiring training, utilizing plug-and-play modules for any latent space, making our method highly efficientModels are available at \\href{https://github.com/cyt1202/BYOCL.git"
      },
      {
        "id": "oai:arXiv.org:2410.17810v2",
        "title": "EntityCLIP: Entity-Centric Image-Text Matching via Multimodal Attentive Contrastive Learning",
        "link": "https://arxiv.org/abs/2410.17810",
        "author": "Yaxiong Wang, Yujiao Wu, Lianwei Wu, Lechao Cheng, Zhun Zhong, Meng Wang",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.17810v2 Announce Type: replace \nAbstract: Recent advancements in image-text matching have been notable, yet prevailing models predominantly cater to broad queries and struggle with accommodating fine-grained query intention. In this paper, we work towards the \\textbf{E}ntity-centric \\textbf{I}mage-\\textbf{T}ext \\textbf{M}atching (EITM), a task that the text and image involve specific entity-related information. The challenge of this task mainly lies in the larger semantic gap in entity association modeling, comparing with the general image-text matching problem.To narrow the huge semantic gap between the entity-centric text and the images, we take the fundamental CLIP as the backbone and devise a multimodal attentive contrastive learning framework to tam CLIP to adapt EITM problem, developing a model named EntityCLIP. The key of our multimodal attentive contrastive learning is to generate interpretive explanation text using Large Language Models (LLMs) as the bridge clues. In specific, we proceed by extracting explanatory text from off-the-shelf LLMs. This explanation text, coupled with the image and text, is then input into our specially crafted Multimodal Attentive Experts (MMAE) module, which effectively integrates explanation texts to narrow the gap of the entity-related text and image in a shared semantic space. Building on the enriched features derived from MMAE, we further design an effective Gated Integrative Image-text Matching (GI-ITM) strategy. The GI-ITM employs an adaptive gating mechanism to aggregate MMAE's features, subsequently applying image-text matching constraints to steer the alignment between the text and the image. Extensive experiments are conducted on three social media news benchmarks including N24News, VisualNews, and GoodNews, the results shows that our method surpasses the competition methods with a clear margin."
      },
      {
        "id": "oai:arXiv.org:2410.22318v2",
        "title": "Online Detecting LLM-Generated Texts via Sequential Hypothesis Testing by Betting",
        "link": "https://arxiv.org/abs/2410.22318",
        "author": "Can Chen, Jun-Kun Wang",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.22318v2 Announce Type: replace \nAbstract: Developing algorithms to differentiate between machine-generated texts and human-written texts has garnered substantial attention in recent years. Existing methods in this direction typically concern an offline setting where a dataset containing a mix of real and machine-generated texts is given upfront, and the task is to determine whether each sample in the dataset is from a large language model (LLM) or a human. However, in many practical scenarios, sources such as news websites, social media accounts, or on other forums publish content in a streaming fashion. Therefore, in this online scenario, how to quickly and accurately determine whether the source is an LLM with strong statistical guarantees is crucial for these media or platforms to function effectively and prevent the spread of misinformation and other potential misuse of LLMs. To tackle the problem of online detection, we develop an algorithm based on the techniques of sequential hypothesis testing by betting that not only builds upon and complements existing offline detection techniques but also enjoys statistical guarantees, which include a controlled false positive rate and the expected time to correctly identify a source as an LLM. Experiments were conducted to demonstrate the effectiveness of our method."
      },
      {
        "id": "oai:arXiv.org:2410.23780v3",
        "title": "Driving by the Rules: A Benchmark for Integrating Traffic Sign Regulations into Vectorized HD Map",
        "link": "https://arxiv.org/abs/2410.23780",
        "author": "Xinyuan Chang, Maixuan Xue, Xinran Liu, Zheng Pan, Xing Wei",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.23780v3 Announce Type: replace \nAbstract: Ensuring adherence to traffic sign regulations is essential for both human and autonomous vehicle navigation. While current online mapping solutions often prioritize the construction of the geometric and connectivity layers of HD maps, overlooking the construction of the traffic regulation layer within HD maps. Addressing this gap, we introduce MapDR, a novel dataset designed for the extraction of Driving Rules from traffic signs and their association with vectorized, locally perceived HD Maps. MapDR features over $10,000$ annotated video clips that capture the intricate correlation between traffic sign regulations and lanes. Built upon this benchmark and the newly defined task of integrating traffic regulations into online HD maps, we provide modular and end-to-end solutions: VLE-MEE and RuleVLM, offering a strong baseline for advancing autonomous driving technology. It fills a critical gap in the integration of traffic sign rules, contributing to the development of reliable autonomous driving systems. Code is available at https://github.com/MIV-XJTU/MapDR."
      },
      {
        "id": "oai:arXiv.org:2411.00062v3",
        "title": "Scalable Reinforcement Post-Training Beyond Static Human Prompts: Evolving Alignment via Asymmetric Self-Play",
        "link": "https://arxiv.org/abs/2411.00062",
        "author": "Ziyu Ye, Rishabh Agarwal, Tianqi Liu, Rishabh Joshi, Sarmishta Velury, Quoc V. Le, Qijun Tan, Yuan Liu",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.00062v3 Announce Type: replace \nAbstract: Current reinforcement learning (RL) frameworks for large language models (LLM) post-training typically assume a fixed prompt distribution, which is sub-optimal and bottlenecks scalability. Prior works have explored prompt evolving, but are often limited to the supervised fine-tuning stage, and prompts are sampled and evolved uniformly without signals. This empirical work presents a paradigm shift: Evolving Alignment via Asymmetric Self-Play (eva), that casts post-training as an infinite game with regret-based signals for 2 players: (i) a creator, who strategically samples and creates new informative prompts and (ii) a solver, who learns to produce preferred responses. eva is the first method that allows language models to adaptively create training prompts in both offline and online RL post-training. The design is simple, easy-to-use yet remarkably effective: eva sets a new SOTA on challenging benchmarks, without any extra human prompts, e.g. it boosts the win-rate of gemma-2-9b-it on Arena-Hard by 51.6% -> 60.1% for DPO and 52.6% -> 62.4% for RLOO, surpassing claude-3-opus and catching up to gemini-1.5-pro, both of which are orders of magnitude larger. Extensive experiments show eva can create effective RL curricula and is robust across ablations. We believe adaptively evolving prompts are key to designing the next-generation RL post-training scheme."
      },
      {
        "id": "oai:arXiv.org:2411.05874v2",
        "title": "Interplay between Federated Learning and Explainable Artificial Intelligence: a Scoping Review",
        "link": "https://arxiv.org/abs/2411.05874",
        "author": "Luis M. Lopez-Ramos, Florian Leiser, Aditya Rastogi, Steven Hicks, Inga Str\\\"umke, Vince I. Madai, Tobias Budig, Ali Sunyaev, Adam Hilbert",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.05874v2 Announce Type: replace \nAbstract: The joint implementation of federated learning (FL) and explainable artificial intelligence (XAI) could allow training models from distributed data and explaining their inner workings while preserving essential aspects of privacy. Toward establishing the benefits and tensions associated with their interplay, this scoping review maps the publications that jointly deal with FL and XAI, focusing on publications that reported an interplay between FL and model interpretability or post-hoc explanations. Out of the 37 studies meeting our criteria, only one explicitly and quantitatively analyzed the influence of FL on model explanations, revealing a significant research gap. The aggregation of interpretability metrics across FL nodes created generalized global insights at the expense of node-specific patterns being diluted. Several studies proposed FL algorithms incorporating explanation methods to safeguard the learning process against defaulting or malicious nodes. Studies using established FL libraries or following reporting guidelines are a minority. More quantitative research and structured, transparent practices are needed to fully understand their mutual impact and under which conditions it happens."
      },
      {
        "id": "oai:arXiv.org:2411.08033v2",
        "title": "GaussianAnything: Interactive Point Cloud Flow Matching For 3D Object Generation",
        "link": "https://arxiv.org/abs/2411.08033",
        "author": "Yushi Lan, Shangchen Zhou, Zhaoyang Lyu, Fangzhou Hong, Shuai Yang, Bo Dai, Xingang Pan, Chen Change Loy",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.08033v2 Announce Type: replace \nAbstract: While 3D content generation has advanced significantly, existing methods still face challenges with input formats, latent space design, and output representations. This paper introduces a novel 3D generation framework that addresses these challenges, offering scalable, high-quality 3D generation with an interactive Point Cloud-structured Latent space. Our framework employs a Variational Autoencoder (VAE) with multi-view posed RGB-D(epth)-N(ormal) renderings as input, using a unique latent space design that preserves 3D shape information, and incorporates a cascaded latent flow-based model for improved shape-texture disentanglement. The proposed method, GaussianAnything, supports multi-modal conditional 3D generation, allowing for point cloud, caption, and single image inputs. Notably, the newly proposed latent space naturally enables geometry-texture disentanglement, thus allowing 3D-aware editing. Experimental results demonstrate the effectiveness of our approach on multiple datasets, outperforming existing native 3D methods in both text- and image-conditioned 3D generation."
      },
      {
        "id": "oai:arXiv.org:2411.08753v4",
        "title": "Which Viewpoint Shows it Best? Language for Weakly Supervising View Selection in Multi-view Instructional Videos",
        "link": "https://arxiv.org/abs/2411.08753",
        "author": "Sagnik Majumder, Tushar Nagarajan, Ziad Al-Halah, Reina Pradhan, Kristen Grauman",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.08753v4 Announce Type: replace \nAbstract: Given a multi-view video, which viewpoint is most informative for a human observer? Existing methods rely on heuristics or expensive \"best-view\" supervision to answer this question, limiting their applicability. We propose a weakly supervised approach that leverages language accompanying an instructional multi-view video as a means to recover its most informative viewpoint(s). Our key hypothesis is that the more accurately an individual view can predict a view-agnostic text summary, the more informative it is. To put this into action, we propose LangView, a framework that uses the relative accuracy of view-dependent caption predictions as a proxy for best view pseudo-labels. Then, those pseudo-labels are used to train a view selector, together with an auxiliary camera pose predictor that enhances view-sensitivity. During inference, our model takes as input only a multi-view video--no language or camera poses--and returns the best viewpoint to watch at each timestep. On two challenging datasets comprised of diverse multi-camera setups and how-to activities, our model consistently outperforms state-of-the-art baselines, both with quantitative metrics and human evaluation. Project page: https://vision.cs.utexas.edu/projects/which-view-shows-it-best."
      },
      {
        "id": "oai:arXiv.org:2411.10868v4",
        "title": "Destabilizing a Social Network Model via Intrinsic Feedback Vulnerabilities",
        "link": "https://arxiv.org/abs/2411.10868",
        "author": "Lane H. Rogers, Emma J. Reid, Robert A. Bridges",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.10868v4 Announce Type: replace \nAbstract: Social influence plays a significant role in shaping individual sentiments and actions, particularly in a world of ubiquitous digital interconnection. The rapid development of generative AI has engendered well-founded concerns regarding the potential scalable implementation of radicalization techniques in social media. Motivated by these developments, we present a case study investigating the effects of small but intentional perturbations on a simple social network. We employ Taylor's classic model of social influence and tools from robust control theory (most notably the Dynamical Structure Function (DSF)), to identify perturbations that qualitatively alter the system's behavior while remaining as unobtrusive as possible. We examine two such scenarios: perturbations to an existing link and perturbations that introduce a new link to the network. In each case, we identify destabilizing perturbations of minimal norm and simulate their effects. Remarkably, we find that small but targeted alterations to network structure may lead to the radicalization of all agents, exhibiting the potential for large-scale shifts in collective behavior to be triggered by comparatively minuscule adjustments in social influence. Given that this method of identifying perturbations that are innocuous yet destabilizing applies to any suitable dynamical system, our findings emphasize a need for similar analyses to be carried out on real systems (e.g., real social networks), to identify the places where such dynamics may already exist."
      },
      {
        "id": "oai:arXiv.org:2411.11260v2",
        "title": "Large corpora and large language models: a replicable method for automating grammatical annotation",
        "link": "https://arxiv.org/abs/2411.11260",
        "author": "Cameron Morin, Matti Marttinen Larsson",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.11260v2 Announce Type: replace \nAbstract: Much linguistic research relies on annotated datasets of features extracted from text corpora, but the rapid quantitative growth of these corpora has created practical difficulties for linguists to manually annotate large data samples. In this paper, we present a replicable, supervised method that leverages large language models for assisting the linguist in grammatical annotation through prompt engineering, training, and evaluation. We introduce a methodological pipeline applied to the case study of formal variation in the English evaluative verb construction 'consider X (as) (to be) Y', based on the large language model Claude 3.5 Sonnet and corpus data from Davies' NOW and EnTenTen21 (SketchEngine). Overall, we reach a model accuracy of over 90% on our held-out test samples with only a small amount of training data, validating the method for the annotation of very large quantities of tokens of the construction in the future. We discuss the generalisability of our results for a wider range of case studies of grammatical constructions and grammatical variation and change, underlining the value of AI copilots as tools for future linguistic research, notwithstanding some important caveats."
      },
      {
        "id": "oai:arXiv.org:2411.15139v3",
        "title": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving",
        "link": "https://arxiv.org/abs/2411.15139",
        "author": "Bencheng Liao, Shaoyu Chen, Haoran Yin, Bo Jiang, Cheng Wang, Sixu Yan, Xinbang Zhang, Xiangyu Li, Ying Zhang, Qian Zhang, Xinggang Wang",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.15139v3 Announce Type: replace \nAbstract: Recently, the diffusion model has emerged as a powerful generative technique for robotic policy learning, capable of modeling multi-mode action distributions. Leveraging its capability for end-to-end autonomous driving is a promising direction. However, the numerous denoising steps in the robotic diffusion policy and the more dynamic, open-world nature of traffic scenes pose substantial challenges for generating diverse driving actions at a real-time speed. To address these challenges, we propose a novel truncated diffusion policy that incorporates prior multi-mode anchors and truncates the diffusion schedule, enabling the model to learn denoising from anchored Gaussian distribution to the multi-mode driving action distribution. Additionally, we design an efficient cascade diffusion decoder for enhanced interaction with conditional scene context. The proposed model, DiffusionDrive, demonstrates 10$\\times$ reduction in denoising steps compared to vanilla diffusion policy, delivering superior diversity and quality in just 2 steps. On the planning-oriented NAVSIM dataset, with the aligned ResNet-34 backbone, DiffusionDrive achieves 88.1 PDMS without bells and whistles, setting a new record, while running at a real-time speed of 45 FPS on an NVIDIA 4090. Qualitative results on challenging scenarios further confirm that DiffusionDrive can robustly generate diverse plausible driving actions. Code and model will be available at https://github.com/hustvl/DiffusionDrive."
      },
      {
        "id": "oai:arXiv.org:2411.19346v3",
        "title": "CLIP meets DINO for Tuning Zero-Shot Classifier using Unlabeled Image Collections",
        "link": "https://arxiv.org/abs/2411.19346",
        "author": "Mohamed Fazli Imam, Rufael Fedaku Marew, Jameel Hassan, Mustansar Fiaz, Alham Fikri Aji, Hisham Cholakkal",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.19346v3 Announce Type: replace \nAbstract: In the era of foundation models, CLIP has emerged as a powerful tool for aligning text & visual modalities into a common embedding space. However, the alignment objective used to train CLIP often results in subpar visual features for fine-grained tasks. In contrast, SSL-pretrained models like DINO excel at extracting rich visual features due to their specialized training paradigm. Yet, these SSL models require an additional supervised linear probing step, which relies on fully labeled data which is often expensive and difficult to obtain at scale. In this paper, we propose a label-free prompt-tuning method that leverages the rich visual features of self-supervised learning models (DINO) and the broad textual knowledge of large language models (LLMs) to largely enhance CLIP-based image classification performance using unlabeled images. Our approach unfolds in three key steps: (1) We generate robust textual feature embeddings that more accurately represent object classes by leveraging class-specific descriptions from LLMs, enabling more effective zero-shot classification compared to CLIP's default name-specific prompts. (2) These textual embeddings are then used to produce pseudo-labels to train an alignment module that integrates the complementary strengths of LLM description-based textual embeddings & DINO's visual features. (3) Finally, we prompt-tune CLIP's vision encoder through DINO-assisted supervision using the trained alignment module. This three-step process allows us to harness the best of visual & textual foundation models, resulting in a powerful and efficient approach that surpasses state-of-the-art label-free classification methods. Notably, our framework, NoLA (No Labels Attached), achieves an average absolute gain of 3.6% over the state-of-the-art LaFTer across 11 diverse image classification datasets. Our code & models can be found at https://github.com/fazliimam/NoLA."
      },
      {
        "id": "oai:arXiv.org:2412.01147v2",
        "title": "A2VIS: Amodal-Aware Approach to Video Instance Segmentation",
        "link": "https://arxiv.org/abs/2412.01147",
        "author": "Minh Tran, Thang Pham, Winston Bounsavy, Tri Nguyen, Ngan Le",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.01147v2 Announce Type: replace \nAbstract: Handling occlusion remains a significant challenge for video instance-level tasks like Multiple Object Tracking (MOT) and Video Instance Segmentation (VIS). In this paper, we propose a novel framework, Amodal-Aware Video Instance Segmentation (A2VIS), which incorporates amodal representations to achieve a reliable and comprehensive understanding of both visible and occluded parts of objects in a video. The key intuition is that awareness of amodal segmentation through spatiotemporal dimension enables a stable stream of object information. In scenarios where objects are partially or completely hidden from view, amodal segmentation offers more consistency and less dramatic changes along the temporal axis compared to visible segmentation. Hence, both amodal and visible information from all clips can be integrated into one global instance prototype. To effectively address the challenge of video amodal segmentation, we introduce the spatiotemporal-prior Amodal Mask Head, which leverages visible information intra clips while extracting amodal characteristics inter clips. Through extensive experiments and ablation studies, we show that A2VIS excels in both MOT and VIS tasks in identifying and tracking object instances with a keen understanding of their full shape."
      },
      {
        "id": "oai:arXiv.org:2412.03371v2",
        "title": "SGSST: Scaling Gaussian Splatting StyleTransfer",
        "link": "https://arxiv.org/abs/2412.03371",
        "author": "Bruno Galerne, Jianling Wang, Lara Raad, Jean-Michel Morel",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.03371v2 Announce Type: replace \nAbstract: Applying style transfer to a full 3D environment is a challenging task that has seen many developments since the advent of neural rendering. 3D Gaussian splatting (3DGS) has recently pushed further many limits of neural rendering in terms of training speed and reconstruction quality. This work introduces SGSST: Scaling Gaussian Splatting Style Transfer, an optimization-based method to apply style transfer to pretrained 3DGS scenes. We demonstrate that a new multiscale loss based on global neural statistics, that we name SOS for Simultaneously Optimized Scales, enables style transfer to ultra-high resolution 3D scenes. Not only SGSST pioneers 3D scene style transfer at such high image resolutions, it also produces superior visual quality as assessed by thorough qualitative, quantitative and perceptual comparisons."
      },
      {
        "id": "oai:arXiv.org:2412.08864v2",
        "title": "A Graph-Based Synthetic Data Pipeline for Scaling High-Quality Reasoning Instructions",
        "link": "https://arxiv.org/abs/2412.08864",
        "author": "Jiankang Wang, Jianjun Xu, Xiaorui Wang, Yuxin Wang, Mengting Xing, Shancheng Fang, Zhineng Chen, Hongtao Xie, Yongdong Zhang",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.08864v2 Announce Type: replace \nAbstract: Synthesizing high-quality reasoning data for continual training has been proven to be effective in enhancing the performance of Large Language Models (LLMs). However, previous synthetic approaches struggle to easily scale up data and incur high costs in the pursuit of high quality. In this paper, we propose the Graph-based Synthetic Data Pipeline (GSDP), an economical and scalable framework for high-quality reasoning data synthesis. Inspired by knowledge graphs, we extracted knowledge points from seed data and constructed a knowledge point relationships graph to explore their interconnections. By exploring the implicit relationships among knowledge, our method achieves $\\times$255 data expansion. Furthermore, GSDP led by open-source models, achieves synthesis quality comparable to GPT-4-0613 while maintaining $\\times$100 lower costs. To tackle the most challenging mathematical reasoning task, we present the GSDP-MATH dataset comprising over 1.91 million pairs of math problems and answers. After fine-tuning on GSDP-MATH, GSDP-7B based on Mistral-7B achieves 37.7% accuracy on MATH and 78.4% on GSM8K, demonstrating the effectiveness of our method. The dataset and models will be released in https://github.com/Jayce1kk/GSDP."
      },
      {
        "id": "oai:arXiv.org:2412.13292v2",
        "title": "Refining Answer Distributions for Improved Large Language Model Reasoning",
        "link": "https://arxiv.org/abs/2412.13292",
        "author": "Soumyasundar Pal, Didier Ch\\'etelat, Yingxue Zhang, Mark Coates",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.13292v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) have exhibited an impressive capability to perform reasoning tasks, especially if they are encouraged to generate a sequence of intermediate steps. Reasoning performance can be improved by suitably combining multiple LLM responses, generated either in parallel in a single query, or via sequential interactions with LLMs throughout the reasoning process. Existing strategies for combination, such as self-consistency and progressive-hint-prompting, make inefficient usage of the LLM responses. We present Refined Answer Distributions, a novel and principled algorithmic framework to enhance the reasoning capabilities of LLMs. Our approach can be viewed as an iterative sampling strategy for forming a Monte Carlo approximation of an underlying distribution of answers, with the goal of identifying the mode -- the most likely answer. Empirical evaluation on several reasoning benchmarks demonstrates the superiority of the proposed approach."
      },
      {
        "id": "oai:arXiv.org:2412.14602v2",
        "title": "Towards Scalable and Deep Graph Neural Networks via Noise Masking",
        "link": "https://arxiv.org/abs/2412.14602",
        "author": "Yuxuan Liang, Wentao Zhang, Zeang Sheng, Ling Yang, Quanqing Xu, Jiawei Jiang, Yunhai Tong, Bin Cui",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.14602v2 Announce Type: replace \nAbstract: In recent years, Graph Neural Networks (GNNs) have achieved remarkable success in many graph mining tasks. However, scaling them to large graphs is challenging due to the high computational and storage costs of repeated feature propagation and non-linear transformation during training. One commonly employed approach to address this challenge is model-simplification, which only executes the Propagation (P) once in the pre-processing, and Combine (C) these receptive fields in different ways and then feed them into a simple model for better performance. Despite their high predictive performance and scalability, these methods still face two limitations. First, existing approaches mainly focus on exploring different C methods from the model perspective, neglecting the crucial problem of performance degradation with increasing P depth from the data-centric perspective, known as the over-smoothing problem. Second, pre-processing overhead takes up most of the end-to-end processing time, especially for large-scale graphs. To address these limitations, we present random walk with noise masking (RMask), a plug-and-play module compatible with the existing model-simplification works. This module enables the exploration of deeper GNNs while preserving their scalability. Unlike the previous model-simplification works, we focus on continuous P and found that the noise existing inside each P is the cause of the over-smoothing issue, and use the efficient masking mechanism to eliminate them. Experimental results on six real-world datasets demonstrate that model-simplification works equipped with RMask yield superior performance compared to their original version and can make a good trade-off between accuracy and efficiency."
      },
      {
        "id": "oai:arXiv.org:2412.14719v2",
        "title": "Prototypical Calibrating Ambiguous Samples for Micro-Action Recognition",
        "link": "https://arxiv.org/abs/2412.14719",
        "author": "Kun Li, Dan Guo, Guoliang Chen, Chunxiao Fan, Jingyuan Xu, Zhiliang Wu, Hehe Fan, Meng Wang",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.14719v2 Announce Type: replace \nAbstract: Micro-Action Recognition (MAR) has gained increasing attention due to its crucial role as a form of non-verbal communication in social interactions, with promising potential for applications in human communication and emotion analysis. However, current approaches often overlook the inherent ambiguity in micro-actions, which arises from the wide category range and subtle visual differences between categories. This oversight hampers the accuracy of micro-action recognition. In this paper, we propose a novel Prototypical Calibrating Ambiguous Network (PCAN) to unleash and mitigate the ambiguity of MAR. Firstly, we employ a hierarchical action-tree to identify the ambiguous sample, categorizing them into distinct sets of ambiguous samples of false negatives and false positives, considering both body- and action-level categories. Secondly, we implement an ambiguous contrastive refinement module to calibrate these ambiguous samples by regulating the distance between ambiguous samples and their corresponding prototypes. This calibration process aims to pull false negative (FN) samples closer to their respective prototypes and push false positive (FP) samples apart from their affiliated prototypes. In addition, we propose a new prototypical diversity amplification loss to strengthen the model's capacity by amplifying the differences between different prototypes. Finally, we propose a prototype-guided rectification to rectify prediction by incorporating the representability of prototypes. Extensive experiments conducted on the benchmark dataset demonstrate the superior performance of our method compared to existing approaches. The code is available at https://github.com/kunli-cs/PCAN."
      },
      {
        "id": "oai:arXiv.org:2412.14865v2",
        "title": "Hierarchical Subspaces of Policies for Continual Offline Reinforcement Learning",
        "link": "https://arxiv.org/abs/2412.14865",
        "author": "Anthony Kobanda, R\\'emy Portelas, Odalric-Ambrym Maillard, Ludovic Denoyer",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.14865v2 Announce Type: replace \nAbstract: We consider a Continual Reinforcement Learning setup, where a learning agent must continuously adapt to new tasks while retaining previously acquired skill sets, with a focus on the challenge of avoiding forgetting past gathered knowledge and ensuring scalability with the growing number of tasks. Such issues prevail in autonomous robotics and video game simulations, notably for navigation tasks prone to topological or kinematic changes. To address these issues, we introduce HiSPO, a novel hierarchical framework designed specifically for continual learning in navigation settings from offline data. Our method leverages distinct policy subspaces of neural networks to enable flexible and efficient adaptation to new tasks while preserving existing knowledge. We demonstrate, through a careful experimental study, the effectiveness of our method in both classical MuJoCo maze environments and complex video game-like navigation simulations, showcasing competitive performances and satisfying adaptability with respect to classical continual learning metrics, in particular regarding the memory usage and efficiency."
      },
      {
        "id": "oai:arXiv.org:2412.15291v4",
        "title": "A Large-Scale Simulation on Large Language Models for Decision-Making in Political Science",
        "link": "https://arxiv.org/abs/2412.15291",
        "author": "Chenxiao Yu, Jinyi Ye, Yuangang Li, Zheng Li, Emilio Ferrara, Xiyang Hu, Yue Zhao",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.15291v4 Announce Type: replace \nAbstract: While LLMs have demonstrated remarkable capabilities in text generation and reasoning, their ability to simulate human decision-making -- particularly in political contexts -- remains an open question. However, modeling voter behavior presents unique challenges due to limited voter-level data, evolving political landscapes, and the complexity of human reasoning. In this study, we develop a theory-driven, multi-step reasoning framework that integrates demographic, temporal and ideological factors to simulate voter decision-making at scale. Using synthetic personas calibrated to real-world voter data, we conduct large-scale simulations of recent U.S. presidential elections. Our method significantly improves simulation accuracy while mitigating model biases. We examine its robustness by comparing performance across different LLMs. We further investigate the challenges and constraints that arise from LLM-based political simulations. Our work provides both a scalable framework for modeling political decision-making behavior and insights into the promise and limitations of using LLMs in political science research."
      },
      {
        "id": "oai:arXiv.org:2412.20439v2",
        "title": "Image Augmentation Agent for Weakly Supervised Semantic Segmentation",
        "link": "https://arxiv.org/abs/2412.20439",
        "author": "Wangyu Wu, Xianglin Qiu, Siqi Song, Zhenhong Chen, Xiaowei Huang, Fei Ma, Jimin Xiao",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.20439v2 Announce Type: replace \nAbstract: Weakly-supervised semantic segmentation (WSSS) has achieved remarkable progress using only image-level labels. However, most existing WSSS methods focus on designing new network structures and loss functions to generate more accurate dense labels, overlooking the limitations imposed by fixed datasets, which can constrain performance improvements. We argue that more diverse trainable images provides WSSS richer information and help model understand more comprehensive semantic pattern. Therefore in this paper, we introduce a novel approach called Image Augmentation Agent (IAA) which shows that it is possible to enhance WSSS from data generation perspective. IAA mainly design an augmentation agent that leverages large language models (LLMs) and diffusion models to automatically generate additional images for WSSS. In practice, to address the instability in prompt generation by LLMs, we develop a prompt self-refinement mechanism. It allow LLMs to re-evaluate the rationality of generated prompts to produce more coherent prompts. Additionally, we insert an online filter into diffusion generation process to dynamically ensure the quality and balance of generated images. Experimental results show that our method significantly surpasses state-of-the-art WSSS approaches on the PASCAL VOC 2012 and MS COCO 2014 datasets."
      },
      {
        "id": "oai:arXiv.org:2501.02430v2",
        "title": "FOLDER: Accelerating Multi-modal Large Language Models with Enhanced Performance",
        "link": "https://arxiv.org/abs/2501.02430",
        "author": "Haicheng Wang, Zhemeng Yu, Gabriele Spadaro, Chen Ju, Victor Qu\\'etu, Shuai Xiao, Enzo Tartaglione",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.02430v2 Announce Type: replace \nAbstract: Recently, Multi-modal Large Language Models (MLLMs) have shown remarkable effectiveness for multi-modal tasks due to their abilities to generate and understand cross-modal data. However, processing long sequences of visual tokens extracted from visual backbones poses a challenge for deployment in real-time applications. To address this issue, we introduce FOLDER, a simple yet effective plug-and-play module designed to reduce the length of the visual token sequence, mitigating both computational and memory demands during training and inference. Through a comprehensive analysis of the token reduction process, we analyze the information loss introduced by different reduction strategies and develop FOLDER to preserve key information while removing visual redundancy. We showcase the effectiveness of FOLDER by integrating it into the visual backbone of several MLLMs, significantly accelerating the inference phase. Furthermore, we evaluate its utility as a training accelerator or even performance booster for MLLMs. In both contexts, FOLDER achieves comparable or even better performance than the original models, while dramatically reducing complexity by removing up to 70% of visual tokens."
      },
      {
        "id": "oai:arXiv.org:2501.05279v3",
        "title": "Learning convolution operators on compact Abelian groups",
        "link": "https://arxiv.org/abs/2501.05279",
        "author": "Emilia Magnani, Ernesto De Vito, Philipp Hennig, Lorenzo Rosasco",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.05279v3 Announce Type: replace \nAbstract: We consider the problem of learning convolution operators associated to compact Abelian groups. We study a regularization-based approach and provide corresponding learning guarantees under natural regularity conditions on the convolution kernel. More precisely, we assume the convolution kernel is a function in a translation invariant Hilbert space and analyze a natural ridge regression (RR) estimator. Building on existing results for RR, we characterize the accuracy of the estimator in terms of finite sample bounds. Interestingly, regularity assumptions which are classical in the analysis of RR, have a novel and natural interpretation in terms of space/frequency localization. Theoretical results are illustrated by numerical simulations."
      },
      {
        "id": "oai:arXiv.org:2501.05449v2",
        "title": "Explainable AI-Enhanced Deep Learning for Pumpkin Leaf Disease Detection: A Comparative Analysis of CNN Architectures",
        "link": "https://arxiv.org/abs/2501.05449",
        "author": "Md. Arafat Alam Khandaker, Ziyan Shirin Raha, Shifat Islam, Tashreef Muhammad",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.05449v2 Announce Type: replace \nAbstract: Pumpkin leaf diseases are significant threats to agricultural productivity, requiring a timely and precise diagnosis for effective management. Traditional identification methods are laborious and susceptible to human error, emphasizing the necessity for automated solutions. This study employs on the \"Pumpkin Leaf Disease Dataset\", that comprises of 2000 high-resolution images separated into five categories. Downy mildew, powdery mildew, mosaic disease, bacterial leaf spot, and healthy leaves. The dataset was rigorously assembled from several agricultural fields to ensure a strong representation for model training. We explored many proficient deep learning architectures, including DenseNet201, DenseNet121, DenseNet169, Xception, ResNet50, ResNet101 and InceptionResNetV2, and observed that ResNet50 performed most effectively, with an accuracy of 90.5% and comparable precision, recall, and F1-Score. We used Explainable AI (XAI) approaches like Grad-CAM, Grad-CAM++, Score-CAM, and Layer-CAM to provide meaningful representations of model decision-making processes, which improved understanding and trust in automated disease diagnostics. These findings demonstrate ResNet50's potential to revolutionize pumpkin leaf disease detection, allowing for earlier and more accurate treatments."
      },
      {
        "id": "oai:arXiv.org:2501.05550v2",
        "title": "Emergent weight morphologies in deep neural networks",
        "link": "https://arxiv.org/abs/2501.05550",
        "author": "Pascal de Jong, Felix Meigel, Steffen Rulands",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.05550v2 Announce Type: replace \nAbstract: Whether deep neural networks can exhibit emergent behaviour is not only relevant for understanding how deep learning works, it is also pivotal for estimating potential security risks of increasingly capable artificial intelligence systems. Here, we show that training deep neural networks gives rise to emergent weight morphologies independent of the training data. Specifically, in analogy to condensed matter physics, we derive a theory that predict that the homogeneous state of deep neural networks is unstable in a way that leads to the emergence of periodic channel structures. We verified these structures by performing numerical experiments on a variety of data sets. Our work demonstrates emergence in the training of deep neural networks, which impacts the achievable performance of deep neural networks."
      },
      {
        "id": "oai:arXiv.org:2501.06425v3",
        "title": "Tensor Product Attention Is All You Need",
        "link": "https://arxiv.org/abs/2501.06425",
        "author": "Yifan Zhang, Yifeng Liu, Huizhuo Yuan, Zhen Qin, Yang Yuan, Quanquan Gu, Andrew Chi-Chih Yao",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.06425v3 Announce Type: replace \nAbstract: Scaling language models to handle longer input sequences typically necessitates large key-value (KV) caches, resulting in substantial memory overhead during inference. In this paper, we propose Tensor Product Attention (TPA), a novel attention mechanism that uses tensor decompositions to represent queries, keys, and values compactly, significantly shrinking KV cache size at inference time. By factorizing these representations into contextual low-rank components (contextual factorization) and seamlessly integrating with RoPE, TPA achieves improved model quality alongside memory efficiency. Based on TPA, we introduce the Tensor ProducT ATTenTion Transformer (T6), a new model architecture for sequence modeling. Through extensive empirical evaluation of language modeling tasks, we demonstrate that T6 exceeds the performance of standard Transformer baselines including MHA, MQA, GQA, and MLA across various metrics, including perplexity and a range of renowned evaluation benchmarks. Notably, TPA's memory efficiency enables the processing of significantly longer sequences under fixed resource constraints, addressing a critical scalability challenge in modern language models. The code is available at https://github.com/tensorgi/T6."
      },
      {
        "id": "oai:arXiv.org:2501.06465v3",
        "title": "MedCT: A Clinical Terminology Graph for Generative AI Applications in Healthcare",
        "link": "https://arxiv.org/abs/2501.06465",
        "author": "Ye Chen, Dongdong Huang, Haoyun Xu, Cong Fu, Lin Sheng, Qingli Zhou, Yuqiang Shen, Kai Wang",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.06465v3 Announce Type: replace \nAbstract: We introduce the world's first clinical terminology for the Chinese healthcare community, namely MedCT, accompanied by a clinical foundation model MedBERT and an entity linking model MedLink. The MedCT system enables standardized and programmable representation of Chinese clinical data, successively stimulating the development of new medicines, treatment pathways, and better patient outcomes for the populous Chinese community. Moreover, the MedCT knowledge graph provides a principled mechanism to minimize the hallucination problem of large language models (LLMs), therefore achieving significant levels of accuracy and safety in LLM-based clinical applications. By leveraging the LLMs' emergent capabilities of generativeness and expressiveness, we were able to rapidly built a production-quality terminology system and deployed to real-world clinical field within three months, while classical terminologies like SNOMED CT have gone through more than twenty years development. Our experiments show that the MedCT system achieves state-of-the-art (SOTA) performance in semantic matching and entity linking tasks, not only for Chinese but also for English. We also conducted a longitudinal field experiment by applying MedCT and LLMs in a representative spectrum of clinical tasks, including electronic health record (EHR) auto-generation and medical document search for diagnostic decision making. Our study shows a multitude of values of MedCT for clinical workflows and patient outcomes, especially in the new genre of clinical LLM applications. We present our approach in sufficient engineering detail, such that implementing a clinical terminology for other non-English societies should be readily reproducible. We openly release our terminology, models and algorithms, along with real-world clinical datasets for the development."
      },
      {
        "id": "oai:arXiv.org:2501.07824v3",
        "title": "Real-time Verification and Refinement of Language Model Text Generation",
        "link": "https://arxiv.org/abs/2501.07824",
        "author": "Joonho Ko, Jinheon Baek, Sung Ju Hwang",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.07824v3 Announce Type: replace \nAbstract: Large language models (LLMs) have shown remarkable performance across a wide range of natural language tasks. However, a critical challenge remains in that they sometimes generate factually incorrect answers. To address this, while many previous work has focused on identifying errors in their generation and further refining them, they are slow in deployment since they are designed to verify the response from LLMs only after their entire generation (from the first to last tokens) is done. Further, we observe that once LLMs generate incorrect tokens early on, there is a higher likelihood that subsequent tokens will also be factually incorrect. To this end, in this work, we propose Streaming-VR (Streaming Verification and Refinement), a novel approach designed to enhance the efficiency of verification and refinement of LLM outputs. Specifically, the proposed Streaming-VR enables on-the-fly verification and correction of tokens as they are being generated, similar to a streaming process, ensuring that each subset of tokens is checked and refined in real-time by another LLM as the LLM constructs its response. Through comprehensive evaluations on multiple datasets, we demonstrate that our approach not only enhances the factual accuracy of LLMs, but also offers a more efficient solution compared to prior refinement methods."
      },
      {
        "id": "oai:arXiv.org:2501.11841v3",
        "title": "Survey on Monocular Metric Depth Estimation",
        "link": "https://arxiv.org/abs/2501.11841",
        "author": "Jiuling Zhang",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.11841v3 Announce Type: replace \nAbstract: Monocular Depth Estimation (MDE) is a core task in computer vision that enables spatial understanding, 3D reconstruction, and autonomous navigation. Deep learning methods typically estimate relative depth from a single image, but the lack of metric scale often leads to geometric inconsistencies. This limitation severely impacts applications such as visual SLAM, detailed 3D modeling, and novel view synthesis. Monocular Metric Depth Estimation (MMDE) addresses this issue by producing depth maps with absolute scale, ensuring frame-to-frame consistency and supporting direct deployment without scale calibration. This paper presents a structured survey of depth estimation methods, tracing the evolution from traditional geometry-based approaches to modern deep learning models. Recent progress in MMDE is analyzed, with a focus on two key challenges: poor generalization and blurred object boundaries. To tackle these problems, researchers have explored various strategies, including self-supervised learning with unlabeled data, patch-based training, architectural enhancements, and generative model integration. Each method is discussed in terms of technical contribution, performance improvement, and remaining limitations. The survey consolidates recent findings, identifies unresolved challenges, and outlines future directions for MMDE. By highlighting key advancements and open problems, this paper aims to support the continued development and real-world adoption of metric depth estimation in computer vision."
      },
      {
        "id": "oai:arXiv.org:2501.13011v2",
        "title": "MONA: Myopic Optimization with Non-myopic Approval Can Mitigate Multi-step Reward Hacking",
        "link": "https://arxiv.org/abs/2501.13011",
        "author": "Sebastian Farquhar, Vikrant Varma, David Lindner, David Elson, Caleb Biddulph, Ian Goodfellow, Rohin Shah",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.13011v2 Announce Type: replace \nAbstract: Future advanced AI systems may learn sophisticated strategies through reinforcement learning (RL) that humans cannot understand well enough to safely evaluate. We propose a training method which avoids agents learning undesired multi-step plans that receive high reward (multi-step \"reward hacks\") even if humans are not able to detect that the behaviour is undesired. The method, Myopic Optimization with Non-myopic Approval (MONA), works by combining short-sighted optimization with far-sighted reward. We demonstrate that MONA can prevent multi-step reward hacking that ordinary RL causes, even without being able to detect the reward hacking and without any extra information that ordinary RL does not get access to. We study MONA empirically in three settings which model different misalignment failure modes including 2-step environments with LLMs representing delegated oversight and encoded reasoning and longer-horizon gridworld environments representing sensor tampering."
      },
      {
        "id": "oai:arXiv.org:2501.13230v2",
        "title": "Let SSMs be ConvNets: State-space Modeling with Optimal Tensor Contractions",
        "link": "https://arxiv.org/abs/2501.13230",
        "author": "Yan Ru Pei",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.13230v2 Announce Type: replace \nAbstract: We introduce Centaurus, a class of networks composed of generalized state-space model (SSM) blocks, where the SSM operations can be treated as tensor contractions during training. The optimal order of tensor contractions can then be systematically determined for every SSM block to maximize training efficiency. This allows more flexibility in designing SSM blocks beyond the depthwise-separable configuration commonly implemented. The new design choices will take inspiration from classical convolutional blocks including group convolutions, full convolutions, and bottleneck blocks. We architect the Centaurus network with a mixture of these blocks, to balance between network size and performance, as well as memory and computational efficiency during both training and inference. We show that this heterogeneous network design outperforms its homogeneous counterparts in raw audio processing tasks including keyword spotting, speech denoising, and automatic speech recognition (ASR). For ASR, Centaurus is the first network with competitive performance that can be made fully state-space based, without using any nonlinear recurrence (LSTMs), explicit convolutions (CNNs), or (surrogate) attention mechanism. The source code is available as supplementary material on https://openreview.net/forum?id=PkpNRmBZ32"
      },
      {
        "id": "oai:arXiv.org:2501.14174v5",
        "title": "Dreamweaver: Learning Compositional World Models from Pixels",
        "link": "https://arxiv.org/abs/2501.14174",
        "author": "Junyeob Baek, Yi-Fu Wu, Gautam Singh, Sungjin Ahn",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.14174v5 Announce Type: replace \nAbstract: Humans have an innate ability to decompose their perceptions of the world into objects and their attributes, such as colors, shapes, and movement patterns. This cognitive process enables us to imagine novel futures by recombining familiar concepts. However, replicating this ability in artificial intelligence systems has proven challenging, particularly when it comes to modeling videos into compositional concepts and generating unseen, recomposed futures without relying on auxiliary data, such as text, masks, or bounding boxes. In this paper, we propose Dreamweaver, a neural architecture designed to discover hierarchical and compositional representations from raw videos and generate compositional future simulations. Our approach leverages a novel Recurrent Block-Slot Unit (RBSU) to decompose videos into their constituent objects and attributes. In addition, Dreamweaver uses a multi-future-frame prediction objective to capture disentangled representations for dynamic concepts more effectively as well as static concepts. In experiments, we demonstrate our model outperforms current state-of-the-art baselines for world modeling when evaluated under the DCI framework across multiple datasets. Furthermore, we show how the modularized concept representations of our model enable compositional imagination, allowing the generation of novel videos by recombining attributes from previously seen objects. cun-bjy.github.io/dreamweaver-website"
      },
      {
        "id": "oai:arXiv.org:2501.15293v3",
        "title": "Deep Learning in Early Alzheimer's disease's Detection: A Comprehensive Survey of Classification, Segmentation, and Feature Extraction Methods",
        "link": "https://arxiv.org/abs/2501.15293",
        "author": "Rubab Hafeez, Sadia Waheed, Syeda Aleena Naqvi, Fahad Maqbool, Amna Sarwar, Sajjad Saleem, Muhammad Imran Sharif, Kamran Siddique, Zahid Akhtar",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.15293v3 Announce Type: replace \nAbstract: Alzheimers disease is a deadly neurological condition, impairing important memory and brain functions. Alzheimers disease promotes brain shrinkage, ultimately leading to dementia. Dementia diagnosis typically takes 2.8 to 4.4 years after the first clinical indication. Advancements in computing and information technology have led to many techniques of studying Alzheimers disease. Early identification and therapy are crucial for preventing Alzheimers disease, as early-onset dementia hits people before the age of 65, while late-onset dementia occurs after this age. According to the 2015 World Alzheimers disease Report, there are 46.8 million individuals worldwide suffering from dementia, with an anticipated 74.7 million more by 2030 and 131.5 million by 2050. Deep Learning has outperformed conventional Machine Learning techniques by identifying intricate structures in high-dimensional data. Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN), have achieved an accuracy of up to 96.0% for Alzheimers disease classification, and 84.2% for mild cognitive impairment (MCI) conversion prediction. There have been few literature surveys available on applying ML to predict dementia, lacking in congenital observations. However, this survey has focused on a specific data channel for dementia detection. This study evaluated Deep Learning algorithms for early Alzheimers disease detection, using openly accessible datasets, feature segmentation, and classification methods. This article also has identified research gaps and limits in detecting Alzheimers disease, which can inform future research."
      },
      {
        "id": "oai:arXiv.org:2502.04790v2",
        "title": "S$^2$-MAD: Breaking the Token Barrier to Enhance Multi-Agent Debate Efficiency",
        "link": "https://arxiv.org/abs/2502.04790",
        "author": "Yuting Zeng, Weizhe Huang, Lei Jiang, Tongxuan Liu, Xitai Jin, Chen Tianying Tiana, Jing Li, Xiaohua Xu",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.04790v2 Announce Type: replace \nAbstract: Large language models (LLMs) have demonstrated remarkable capabilities across various natural language processing (NLP) scenarios, but they still face challenges when handling complex arithmetic and logical reasoning tasks. While Chain-Of-Thought (CoT) reasoning, self-consistency (SC) and self-correction strategies have attempted to guide models in sequential, multi-step reasoning, Multi-agent Debate (MAD) has emerged as a viable approach for enhancing the reasoning capabilities of LLMs. By increasing both the number of agents and the frequency of debates, the performance of LLMs improves significantly. However, this strategy results in a significant increase in token costs, presenting a barrier to scalability. To address this challenge, we introduce a novel sparsification strategy designed to reduce token costs within MAD. This approach minimizes ineffective exchanges of information and unproductive discussions among agents, thereby enhancing the overall efficiency of the debate process. We conduct comparative experiments on multiple datasets across various models, demonstrating that our approach significantly reduces the token costs in MAD to a considerable extent. Specifically, compared to MAD, our approach achieves an impressive reduction of up to 94.5\\% in token costs while maintaining performance degradation below 2.0\\%."
      },
      {
        "id": "oai:arXiv.org:2502.05780v2",
        "title": "GOLD: Graph Out-of-Distribution Detection via Implicit Adversarial Latent Generation",
        "link": "https://arxiv.org/abs/2502.05780",
        "author": "Danny Wang, Ruihong Qiu, Guangdong Bai, Zi Huang",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.05780v2 Announce Type: replace \nAbstract: Despite graph neural networks' (GNNs) great success in modelling graph-structured data, out-of-distribution (OOD) test instances still pose a great challenge for current GNNs. One of the most effective techniques to detect OOD nodes is to expose the detector model with an additional OOD node-set, yet the extra OOD instances are often difficult to obtain in practice. Recent methods for image data address this problem using OOD data synthesis, typically relying on pre-trained generative models like Stable Diffusion. However, these approaches require vast amounts of additional data, as well as one-for-all pre-trained generative models, which are not available for graph data. Therefore, we propose the GOLD framework for graph OOD detection, an implicit adversarial learning pipeline with synthetic OOD exposure without pre-trained models. The implicit adversarial training process employs a novel alternating optimisation framework by training: (1) a latent generative model to regularly imitate the in-distribution (ID) embeddings from an evolving GNN, and (2) a GNN encoder and an OOD detector to accurately classify ID data while increasing the energy divergence between the ID embeddings and the generative model's synthetic embeddings. This novel approach implicitly transforms the synthetic embeddings into pseudo-OOD instances relative to the ID data, effectively simulating exposure to OOD scenarios without auxiliary data. Extensive OOD detection experiments are conducted on five benchmark graph datasets, verifying the superior performance of GOLD without using real OOD data compared with the state-of-the-art OOD exposure and non-exposure baselines."
      },
      {
        "id": "oai:arXiv.org:2502.06685v2",
        "title": "No Trick, No Treat: Pursuits and Challenges Towards Simulation-free Training of Neural Samplers",
        "link": "https://arxiv.org/abs/2502.06685",
        "author": "Jiajun He, Yuanqi Du, Francisco Vargas, Dinghuai Zhang, Shreyas Padhy, RuiKang OuYang, Carla Gomes, Jos\\'e Miguel Hern\\'andez-Lobato",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.06685v2 Announce Type: replace \nAbstract: We consider the sampling problem, where the aim is to draw samples from a distribution whose density is known only up to a normalization constant. Recent breakthroughs in generative modeling to approximate a high-dimensional data distribution have sparked significant interest in developing neural network-based methods for this challenging problem. However, neural samplers typically incur heavy computational overhead due to simulating trajectories during training. This motivates the pursuit of simulation-free training procedures of neural samplers. In this work, we propose an elegant modification to previous methods, which allows simulation-free training with the help of a time-dependent normalizing flow. However, it ultimately suffers from severe mode collapse. On closer inspection, we find that nearly all successful neural samplers rely on Langevin preconditioning to avoid mode collapsing. We systematically analyze several popular methods with various objective functions and demonstrate that, in the absence of Langevin preconditioning, most of them fail to adequately cover even a simple target. Finally, we draw attention to a strong baseline by combining the state-of-the-art MCMC method, Parallel Tempering (PT), with an additional generative model to shed light on future explorations of neural samplers."
      },
      {
        "id": "oai:arXiv.org:2502.07005v5",
        "title": "Geometry-aware RL for Manipulation of Varying Shapes and Deformable Objects",
        "link": "https://arxiv.org/abs/2502.07005",
        "author": "Tai Hoang, Huy Le, Philipp Becker, Vien Anh Ngo, Gerhard Neumann",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.07005v5 Announce Type: replace \nAbstract: Manipulating objects with varying geometries and deformable objects is a major challenge in robotics. Tasks such as insertion with different objects or cloth hanging require precise control and effective modelling of complex dynamics. In this work, we frame this problem through the lens of a heterogeneous graph that comprises smaller sub-graphs, such as actuators and objects, accompanied by different edge types describing their interactions. This graph representation serves as a unified structure for both rigid and deformable objects tasks, and can be extended further to tasks comprising multiple actuators. To evaluate this setup, we present a novel and challenging reinforcement learning benchmark, including rigid insertion of diverse objects, as well as rope and cloth manipulation with multiple end-effectors. These tasks present a large search space, as both the initial and target configurations are uniformly sampled in 3D space. To address this issue, we propose a novel graph-based policy model, dubbed Heterogeneous Equivariant Policy (HEPi), utilizing $SE(3)$ equivariant message passing networks as the main backbone to exploit the geometric symmetry. In addition, by modeling explicit heterogeneity, HEPi can outperform Transformer-based and non-heterogeneous equivariant policies in terms of average returns, sample efficiency, and generalization to unseen objects. Our project page is available at https://thobotics.github.io/hepi."
      },
      {
        "id": "oai:arXiv.org:2502.07203v2",
        "title": "Playmate: Flexible Control of Portrait Animation via 3D-Implicit Space Guided Diffusion",
        "link": "https://arxiv.org/abs/2502.07203",
        "author": "Xingpei Ma, Jiaran Cai, Yuansheng Guan, Shenneng Huang, Qiang Zhang, Shunsi Zhang",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.07203v2 Announce Type: replace \nAbstract: Recent diffusion-based talking face generation models have demonstrated impressive potential in synthesizing videos that accurately match a speech audio clip with a given reference identity. However, existing approaches still encounter significant challenges due to uncontrollable factors, such as inaccurate lip-sync, inappropriate head posture and the lack of fine-grained control over facial expressions. In order to introduce more face-guided conditions beyond speech audio clips, a novel two-stage training framework Playmate is proposed to generate more lifelike facial expressions and talking faces. In the first stage, we introduce a decoupled implicit 3D representation along with a meticulously designed motion-decoupled module to facilitate more accurate attribute disentanglement and generate expressive talking videos directly from audio cues. Then, in the second stage, we introduce an emotion-control module to encode emotion control information into the latent space, enabling fine-grained control over emotions and thereby achieving the ability to generate talking videos with desired emotion. Extensive experiments demonstrate that Playmate outperforms existing state-of-the-art methods in terms of video quality and lip-synchronization, and improves flexibility in controlling emotion and head pose. The code will be available at https://playmate111.github.io."
      },
      {
        "id": "oai:arXiv.org:2502.07532v3",
        "title": "Diffusion-LAM: Probabilistic Limited Area Weather Forecasting with Diffusion",
        "link": "https://arxiv.org/abs/2502.07532",
        "author": "Erik Larsson, Joel Oskarsson, Tomas Landelius, Fredrik Lindsten",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.07532v3 Announce Type: replace \nAbstract: Machine learning methods have been shown to be effective for weather forecasting, based on the speed and accuracy compared to traditional numerical models. While early efforts primarily concentrated on deterministic predictions, the field has increasingly shifted toward probabilistic forecasting to better capture the forecast uncertainty. Most machine learning-based models have been designed for global-scale predictions, with only limited work targeting regional or limited area forecasting, which allows more specialized and flexible modeling for specific locations. This work introduces Diffusion-LAM, a probabilistic limited area weather model leveraging conditional diffusion. By conditioning on boundary data from surrounding regions, our approach generates forecasts within a defined area. Experimental results on the MEPS limited area dataset demonstrate the potential of Diffusion-LAM to deliver accurate probabilistic forecasts, highlighting its promise for limited-area weather prediction."
      },
      {
        "id": "oai:arXiv.org:2502.10122v4",
        "title": "Modern Hopfield Networks with Continuous-Time Memories",
        "link": "https://arxiv.org/abs/2502.10122",
        "author": "Saul Santos, Ant\\'onio Farinhas, Daniel C. McNamee, Andr\\'e F. T. Martins",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.10122v4 Announce Type: replace \nAbstract: Recent research has established a connection between modern Hopfield networks (HNs) and transformer attention heads, with guarantees of exponential storage capacity. However, these models still face challenges scaling storage efficiently. Inspired by psychological theories of continuous neural resource allocation in working memory, we propose an approach that compresses large discrete Hopfield memories into smaller, continuous-time memories. Leveraging continuous attention, our new energy function modifies the update rule of HNs, replacing the traditional softmax-based probability mass function with a probability density, over the continuous memory. This formulation aligns with modern perspectives on human executive function, offering a principled link between attractor dynamics in working memory and resource-efficient memory allocation. Our framework maintains competitive performance with HNs while leveraging a compressed memory, reducing computational costs across synthetic and video datasets."
      },
      {
        "id": "oai:arXiv.org:2502.11079v2",
        "title": "Phantom: Subject-consistent video generation via cross-modal alignment",
        "link": "https://arxiv.org/abs/2502.11079",
        "author": "Lijie Liu, Tianxiang Ma, Bingchuan Li, Zhuowei Chen, Jiawei Liu, Gen Li, Siyu Zhou, Qian He, Xinglong Wu",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.11079v2 Announce Type: replace \nAbstract: The continuous development of foundational models for video generation is evolving into various applications, with subject-consistent video generation still in the exploratory stage. We refer to this as Subject-to-Video, which extracts subject elements from reference images and generates subject-consistent videos following textual instructions. We believe that the essence of subject-to-video lies in balancing the dual-modal prompts of text and image, thereby deeply and simultaneously aligning both text and visual content. To this end, we propose Phantom, a unified video generation framework for both single- and multi-subject references. Building on existing text-to-video and image-to-video architectures, we redesign the joint text-image injection model and drive it to learn cross-modal alignment via text-image-video triplet data. The proposed method achieves high-fidelity subject-consistent video generation while addressing issues of image content leakage and multi-subject confusion. Evaluation results indicate that our method outperforms other state-of-the-art closed-source commercial solutions. In particular, we emphasize subject consistency in human generation, covering existing ID-preserving video generation while offering enhanced advantages."
      },
      {
        "id": "oai:arXiv.org:2502.14792v2",
        "title": "RendBEV: Semantic Novel View Synthesis for Self-Supervised Bird's Eye View Segmentation",
        "link": "https://arxiv.org/abs/2502.14792",
        "author": "Henrique Pi\\~neiro Monteagudo, Leonardo Taccari, Aurel Pjetri, Francesco Sambo, Samuele Salti",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.14792v2 Announce Type: replace \nAbstract: Bird's Eye View (BEV) semantic maps have recently garnered a lot of attention as a useful representation of the environment to tackle assisted and autonomous driving tasks. However, most of the existing work focuses on the fully supervised setting, training networks on large annotated datasets. In this work, we present RendBEV, a new method for the self-supervised training of BEV semantic segmentation networks, leveraging differentiable volumetric rendering to receive supervision from semantic perspective views computed by a 2D semantic segmentation model. Our method enables zero-shot BEV semantic segmentation, and already delivers competitive results in this challenging setting. When used as pretraining to then fine-tune on labeled BEV ground-truth, our method significantly boosts performance in low-annotation regimes, and sets a new state of the art when fine-tuning on all available labels."
      },
      {
        "id": "oai:arXiv.org:2503.00961v2",
        "title": "CAGN-GAT Fusion: A Hybrid Contrastive Attentive Graph Neural Network for Network Intrusion Detection",
        "link": "https://arxiv.org/abs/2503.00961",
        "author": "Md Abrar Jahin, Shahriar Soudeep, M. F. Mridha, Raihan Kabir, Md Rashedul Islam, Yutaka Watanobe",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.00961v2 Announce Type: replace \nAbstract: Cybersecurity threats are growing, making network intrusion detection essential. Traditional machine learning models remain effective in resource-limited environments due to their efficiency, requiring fewer parameters and less computational time. However, handling short and highly imbalanced datasets remains challenging. In this study, we propose the fusion of a Contrastive Attentive Graph Network and Graph Attention Network (CAGN-GAT Fusion) and benchmark it against 15 other models, including both Graph Neural Networks (GNNs) and traditional ML models. Our evaluation is conducted on four benchmark datasets (KDD-CUP-1999, NSL-KDD, UNSW-NB15, and CICIDS2017) using a short and proportionally imbalanced dataset with a constant size of 5000 samples to ensure fairness in comparison. Results show that CAGN-GAT Fusion demonstrates stable and competitive accuracy, recall, and F1-score, even though it does not achieve the highest performance in every dataset. Our analysis also highlights the impact of adaptive graph construction techniques, including small changes in connections (edge perturbation) and selective hiding of features (feature masking), improving detection performance. The findings confirm that GNNs, particularly CAGN-GAT Fusion, are robust and computationally efficient, making them well-suited for resource-constrained environments. Future work will explore GraphSAGE layers and multiview graph construction techniques to further enhance adaptability and detection accuracy."
      },
      {
        "id": "oai:arXiv.org:2503.01284v2",
        "title": "Soybean Disease Detection via Interpretable Hybrid CNN-GNN: Integrating MobileNetV2 and GraphSAGE with Cross-Modal Attention",
        "link": "https://arxiv.org/abs/2503.01284",
        "author": "Md Abrar Jahin, Soudeep Shahriar, M. F. Mridha, Md. Jakir Hossen, Nilanjan Dey",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.01284v2 Announce Type: replace \nAbstract: Soybean leaf disease detection is critical for agricultural productivity but faces challenges due to visually similar symptoms and limited interpretability in conventional methods. While Convolutional Neural Networks (CNNs) excel in spatial feature extraction, they often neglect inter-image relational dependencies, leading to misclassifications. This paper proposes an interpretable hybrid Sequential CNN-Graph Neural Network (GNN) framework that synergizes MobileNetV2 for localized feature extraction and GraphSAGE for relational modeling. The framework constructs a graph where nodes represent leaf images, with edges defined by cosine similarity-based adjacency matrices and adaptive neighborhood sampling. This design captures fine-grained lesion features and global symptom patterns, addressing inter-class similarity challenges. Cross-modal interpretability is achieved via Grad-CAM and Eigen-CAM visualizations, generating heatmaps to highlight disease-influential regions. Evaluated on a dataset of ten soybean leaf diseases, the model achieves $97.16\\%$ accuracy, surpassing standalone CNNs ($\\le95.04\\%$) and traditional machine learning models ($\\le77.05\\%$). Ablation studies validate the sequential architecture's superiority over parallel or single-model configurations. With only 2.3 million parameters, the lightweight MobileNetV2-GraphSAGE combination ensures computational efficiency, enabling real-time deployment in resource-constrained environments. The proposed approach bridges the gap between accurate classification and practical applicability, offering a robust, interpretable tool for agricultural diagnostics while advancing CNN-GNN integration in plant pathology research."
      },
      {
        "id": "oai:arXiv.org:2503.02488v2",
        "title": "New centrality measure: ksi-centrality",
        "link": "https://arxiv.org/abs/2503.02488",
        "author": "Mikhail Tuzhilin",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.02488v2 Announce Type: replace \nAbstract: We introduce new centrality measures, called ksi-centrality and normalized ksi-centrality measure the importance of a node up to the importance of its neighbors. First, we show that normalized ksi-centrality can be rewritten in terms of the Laplacian matrix such that its expression is similar to the local clustering coefficient. After that we introduce average normalized ksi-coefficient and show that for a random Erdos-Renyi graph it is almost the same as average clustering coefficient. It also shows behavior similar to the clustering coefficient for the Windmill and Wheel graphs. Finally, we show that the distributions of ksi centrality and normalized ksi centrality distinguish networks based on real data from artificial networks, including the Watts-Strogatz, Barabasi-Albert and Boccaletti-Hwang-Latora small-world networks. Furthermore, we show the relationship between normalized ksi centrality and the average normalized ksi coefficient and the algebraic connectivity of the graph and the Chegeer number."
      },
      {
        "id": "oai:arXiv.org:2503.02606v2",
        "title": "ARC-Flow : Articulated, Resolution-Agnostic, Correspondence-Free Matching and Interpolation of 3D Shapes Under Flow Fields",
        "link": "https://arxiv.org/abs/2503.02606",
        "author": "Adam Hartshorne, Allen Paul, Tony Shardlow, Neill D. F. Campbell",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.02606v2 Announce Type: replace \nAbstract: This work presents a unified framework for the unsupervised prediction of physically plausible interpolations between two 3D articulated shapes and the automatic estimation of dense correspondence between them. Interpolation is modelled as a diffeomorphic transformation using a smooth, time-varying flow field governed by Neural Ordinary Differential Equations (ODEs). This ensures topological consistency and non-intersecting trajectories while accommodating hard constraints, such as volume preservation, and soft constraints, \\eg physical priors.\n  Correspondence is recovered using an efficient Varifold formulation, that is effective on high-fidelity surfaces with differing parameterisations. By providing a simple skeleton for the source shape only, we impose physically motivated constraints on the deformation field and resolve symmetric ambiguities. This is achieved without relying on skinning weights or any prior knowledge of the skeleton's target pose configuration.\n  Qualitative and quantitative results demonstrate competitive or superior performance over existing state-of-the-art approaches in both shape correspondence and interpolation tasks across standard datasets."
      },
      {
        "id": "oai:arXiv.org:2503.06451v2",
        "title": "A Quantitative Evaluation of the Expressivity of BMI, Pose and Gender in Body Embeddings for Recognition and Identification",
        "link": "https://arxiv.org/abs/2503.06451",
        "author": "Basudha Pal, Siyuan Huang, Rama Chellappa",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.06451v2 Announce Type: replace \nAbstract: Person Re-identification (ReID) systems identify individuals across images or video frames and play a critical role in various real-world applications. However, many ReID methods are influenced by sensitive attributes such as gender, pose, and body mass index (BMI), which vary in uncontrolled environments, leading to biases and reduced generalization. To address this, we extend the concept of expressivity to the body recognition domain to better understand how ReID models encode these attributes. Expressivity, defined as the mutual information between feature vector representations and specific attributes, is computed using a secondary neural network that takes feature and attribute vectors as inputs. This provides a quantitative framework for analyzing the extent to which sensitive attributes are embedded in the model's representations. We apply expressivity analysis to SemReID, a state-of-the-art self-supervised ReID model, and find that BMI consistently exhibits the highest expressivity scores in the model's final layers, underscoring its dominant role in feature encoding. In the final attention layer of the trained network, the expressivity order for body attributes is BMI > Pitch > Yaw > Gender, highlighting their relative importance in learned representations. Additionally, expressivity values evolve progressively across network layers and training epochs, reflecting a dynamic encoding of attributes during feature extraction. These insights emphasize the influence of body-related attributes on ReID models and provide a systematic methodology for identifying and mitigating attribute-driven biases. By leveraging expressivity analysis, we offer valuable tools to enhance the fairness, robustness, and generalization of ReID systems in diverse real-world settings."
      },
      {
        "id": "oai:arXiv.org:2503.06965v2",
        "title": "SeCap: Self-Calibrating and Adaptive Prompts for Cross-view Person Re-Identification in Aerial-Ground Networks",
        "link": "https://arxiv.org/abs/2503.06965",
        "author": "Shining Wang, Yunlong Wang, Ruiqi Wu, Bingliang Jiao, Wenxuan Wang, Peng Wang",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.06965v2 Announce Type: replace \nAbstract: When discussing the Aerial-Ground Person Re-identification (AGPReID) task, we face the main challenge of the significant appearance variations caused by different viewpoints, making identity matching difficult. To address this issue, previous methods attempt to reduce the differences between viewpoints by critical attributes and decoupling the viewpoints. While these methods can mitigate viewpoint differences to some extent, they still face two main issues: (1) difficulty in handling viewpoint diversity and (2) neglect of the contribution of local features. To effectively address these challenges, we design and implement the Self-Calibrating and Adaptive Prompt (SeCap) method for the AGPReID task. The core of this framework relies on the Prompt Re-calibration Module (PRM), which adaptively re-calibrates prompts based on the input. Combined with the Local Feature Refinement Module (LFRM), SeCap can extract view-invariant features from local features for AGPReID. Meanwhile, given the current scarcity of datasets in the AGPReID field, we further contribute two real-world Large-scale Aerial-Ground Person Re-Identification datasets, LAGPeR and G2APS-ReID. The former is collected and annotated by us independently, covering $4,231$ unique identities and containing $63,841$ high-quality images; the latter is reconstructed from the person search dataset G2APS. Through extensive experiments on AGPReID datasets, we demonstrate that SeCap is a feasible and effective solution for the AGPReID task. The datasets and source code available on https://github.com/wangshining681/SeCap-AGPReID."
      },
      {
        "id": "oai:arXiv.org:2503.08200v2",
        "title": "Route Sparse Autoencoder to Interpret Large Language Models",
        "link": "https://arxiv.org/abs/2503.08200",
        "author": "Wei Shi, Sihang Li, Tao Liang, Mingyang Wan, Guojun Ma, Xiang Wang, Xiangnan He",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.08200v2 Announce Type: replace \nAbstract: Mechanistic interpretability of large language models (LLMs) aims to uncover the internal processes of information propagation and reasoning. Sparse autoencoders (SAEs) have demonstrated promise in this domain by extracting interpretable and monosemantic features. However, prior works primarily focus on feature extraction from a single layer, failing to effectively capture activations that span multiple layers. In this paper, we introduce Route Sparse Autoencoder (RouteSAE), a new framework that integrates a routing mechanism with a shared SAE to efficiently extract features from multiple layers. It dynamically assigns weights to activations from different layers, incurring minimal parameter overhead while achieving high interpretability and flexibility for targeted feature manipulation. We evaluate RouteSAE through extensive experiments on Llama-3.2-1B-Instruct. Specifically, under the same sparsity constraint of 64, RouteSAE extracts 22.5% more features than baseline SAEs while achieving a 22.3% higher interpretability score. These results underscore the potential of RouteSAE as a scalable and effective method for LLM interpretability, with applications in feature discovery and model intervention. Our codes are available at https://github.com/swei2001/RouteSAEs."
      },
      {
        "id": "oai:arXiv.org:2503.08580v2",
        "title": "Comparing Next-Day Wildfire Predictability of MODIS and VIIRS Satellite Data",
        "link": "https://arxiv.org/abs/2503.08580",
        "author": "Justus Karlsson, Yonghao Xu, Amanda Berg, Leif Haglund",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.08580v2 Announce Type: replace \nAbstract: Multiple studies have performed next-day fire prediction using satellite imagery. Two main satellites are used to detect wildfires: MODIS and VIIRS. Both satellites provide fire mask products, called MOD14 and VNP14, respectively. Studies have used one or the other, but there has been no comparison between them to determine which might be more suitable for next-day fire prediction. In this paper, we first evaluate how well VIIRS and MODIS data can be used to forecast wildfire spread one day ahead. We find that the model using VIIRS as input and VNP14 as target achieves the best results. Interestingly, the model using MODIS as input and VNP14 as target performs significantly better than using VNP14 as input and MOD14 as target. Next, we discuss why MOD14 might be harder to use for predicting next-day fires. We find that the MOD14 fire mask is highly stochastic and does not correlate with reasonable fire spread patterns. This is detrimental for machine learning tasks, as the model learns irrational patterns. Therefore, we conclude that MOD14 is unsuitable for next-day fire prediction and that VNP14 is a much better option. However, using MODIS input and VNP14 as target, we achieve a significant improvement in predictability. This indicates that an improved fire detection model is possible for MODIS. The full code and dataset is available online: https://github.com/justuskarlsson/wildfire-mod14-vnp14"
      },
      {
        "id": "oai:arXiv.org:2503.10111v2",
        "title": "Continual Text-to-Video Retrieval with Frame Fusion and Task-Aware Routing",
        "link": "https://arxiv.org/abs/2503.10111",
        "author": "Zecheng Zhao, Zhi Chen, Zi Huang, Shazia Sadiq, Tong Chen",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.10111v2 Announce Type: replace \nAbstract: Text-to-Video Retrieval (TVR) aims to retrieve relevant videos based on textual queries. However, as video content evolves continuously, adapting TVR systems to new data remains a critical yet under-explored challenge. In this paper, we introduce the first benchmark for Continual Text-to-Video Retrieval (CTVR) to address the limitations of existing approaches. Current Pre-Trained Model (PTM)-based TVR methods struggle with maintaining model plasticity when adapting to new tasks, while existing Continual Learning (CL) methods suffer from catastrophic forgetting, leading to semantic misalignment between historical queries and stored video features. To address these two challenges, we propose FrameFusionMoE, a novel CTVR framework that comprises two key components: (1) the Frame Fusion Adapter (FFA), which captures temporal video dynamics while preserving model plasticity, and (2) the Task-Aware Mixture-of-Experts (TAME), which ensures consistent semantic alignment between queries across tasks and the stored video features. Thus, FrameFusionMoE enables effective adaptation to new video content while preserving historical text-video relevance to mitigate catastrophic forgetting. We comprehensively evaluate FrameFusionMoE on two benchmark datasets under various task settings. Results demonstrate that FrameFusionMoE outperforms existing CL and TVR methods, achieving superior retrieval performance with minimal degradation on earlier tasks when handling continuous video streams. Our code is available at: https://github.com/JasonCodeMaker/CTVR."
      },
      {
        "id": "oai:arXiv.org:2503.13366v3",
        "title": "Optimal Bounds for Adversarial Constrained Online Convex Optimization",
        "link": "https://arxiv.org/abs/2503.13366",
        "author": "Ricardo N. Ferreira, Cl\\'audia Soares",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.13366v3 Announce Type: replace \nAbstract: Constrained Online Convex Optimization (COCO) can be seen as a generalization of the standard Online Convex Optimization (OCO) framework. At each round, a cost function and constraint function are revealed after a learner chooses an action. The goal is to minimize both the regret and cumulative constraint violation (CCV) against an adaptive adversary. We show for the first time that is possible to obtain the optimal $O(\\sqrt{T})$ bound on both regret and CCV, improving the best known bounds of $O \\left( \\sqrt{T} \\right)$ and $\\tilde{O} \\left( \\sqrt{T} \\right)$ for the regret and CCV, respectively. Based on a new surrogate loss function enforcing a minimum penalty on the constraint function, we demonstrate that both the Follow-the-Regularized-Leader and the Online Gradient Descent achieve the optimal bounds."
      },
      {
        "id": "oai:arXiv.org:2503.13911v2",
        "title": "Incorporating Attributes and Multi-Scale Structures for Heterogeneous Graph Contrastive Learning",
        "link": "https://arxiv.org/abs/2503.13911",
        "author": "Ruobing Jiang, Yacong Li, Haobing Liu, Yanwei Yu",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.13911v2 Announce Type: replace \nAbstract: Heterogeneous graphs (HGs) are composed of multiple types of nodes and edges, making it more effective in capturing the complex relational structures inherent in the real world. However, in real-world scenarios, labeled data is often difficult to obtain, which limits the applicability of semi-supervised approaches. Self-supervised learning aims to enable models to automatically learn useful features from data, effectively addressing the challenge of limited labeling data. In this paper, we propose a novel contrastive learning framework for heterogeneous graphs (ASHGCL), which incorporates three distinct views, each focusing on node attributes, high-order and low-order structural information, respectively, to effectively capture attribute information, high-order structures, and low-order structures for node representation learning. Furthermore, we introduce an attribute-enhanced positive sample selection strategy that combines both structural information and attribute information, effectively addressing the issue of sampling bias. Extensive experiments on four real-world datasets show that ASHGCL outperforms state-of-the-art unsupervised baselines and even surpasses some supervised benchmarks."
      },
      {
        "id": "oai:arXiv.org:2503.16825v2",
        "title": "SGFormer: Satellite-Ground Fusion for 3D Semantic Scene Completion",
        "link": "https://arxiv.org/abs/2503.16825",
        "author": "Xiyue Guo, Jiarui Hu, Junjie Hu, Hujun Bao, Guofeng Zhang",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.16825v2 Announce Type: replace \nAbstract: Recently, camera-based solutions have been extensively explored for scene semantic completion (SSC). Despite their success in visible areas, existing methods struggle to capture complete scene semantics due to frequent visual occlusions. To address this limitation, this paper presents the first satellite-ground cooperative SSC framework, i.e., SGFormer, exploring the potential of satellite-ground image pairs in the SSC task. Specifically, we propose a dual-branch architecture that encodes orthogonal satellite and ground views in parallel, unifying them into a common domain. Additionally, we design a ground-view guidance strategy that corrects satellite image biases during feature encoding, addressing misalignment between satellite and ground views. Moreover, we develop an adaptive weighting strategy that balances contributions from satellite and ground views. Experiments demonstrate that SGFormer outperforms the state of the art on SemanticKITTI and SSCBench-KITTI-360 datasets. Our code is available on https://github.com/gxytcrc/SGFormer."
      },
      {
        "id": "oai:arXiv.org:2503.18445v3",
        "title": "Benchmarking Multi-modal Semantic Segmentation under Sensor Failures: Missing and Noisy Modality Robustness",
        "link": "https://arxiv.org/abs/2503.18445",
        "author": "Chenfei Liao, Kaiyu Lei, Xu Zheng, Junha Moon, Zhixiong Wang, Yixuan Wang, Danda Pani Paudel, Luc Van Gool, Xuming Hu",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.18445v3 Announce Type: replace \nAbstract: Multi-modal semantic segmentation (MMSS) addresses the limitations of single-modality data by integrating complementary information across modalities. Despite notable progress, a significant gap persists between research and real-world deployment due to variability and uncertainty in multi-modal data quality. Robustness has thus become essential for practical MMSS applications. However, the absence of standardized benchmarks for evaluating robustness hinders further advancement. To address this, we first survey existing MMSS literature and categorize representative methods to provide a structured overview. We then introduce a robustness benchmark that evaluates MMSS models under three scenarios: Entire-Missing Modality (EMM), Random-Missing Modality (RMM), and Noisy Modality (NM). From a probabilistic standpoint, we model modality failure under two conditions: (1) all damaged combinations are equally probable; (2) each modality fails independently following a Bernoulli distribution. Based on these, we propose four metrics-$mIoU^{Avg}_{EMM}$, $mIoU^{E}_{EMM}$, $mIoU^{Avg}_{RMM}$, and $mIoU^{E}_{RMM}$-to assess model robustness under EMM and RMM. This work provides the first dedicated benchmark for MMSS robustness, offering new insights and tools to advance the field. Source code is available at https://github.com/Chenfei-Liao/Multi-Modal-Semantic-Segmentation-Robustness-Benchmark."
      },
      {
        "id": "oai:arXiv.org:2503.21934v3",
        "title": "Proof or Bluff? Evaluating LLMs on 2025 USA Math Olympiad",
        "link": "https://arxiv.org/abs/2503.21934",
        "author": "Ivo Petrov, Jasper Dekoninck, Lyuben Baltadzhiev, Maria Drencheva, Kristian Minchev, Mislav Balunovi\\'c, Nikola Jovanovi\\'c, Martin Vechev",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.21934v3 Announce Type: replace \nAbstract: Recent math benchmarks for large language models (LLMs) such as MathArena indicate that state-of-the-art reasoning models achieve impressive performance on mathematical competitions like AIME, with the leading model, Gemini-2.5-Pro, achieving scores comparable to top human competitors. However, these benchmarks evaluate models solely based on final numerical answers, neglecting rigorous reasoning and proof generation which are essential for real-world mathematical tasks. To address this, we introduce the first comprehensive evaluation of full-solution reasoning for challenging mathematical problems. Using expert human annotators, we evaluated several state-of-the-art reasoning models on the six problems from the 2025 USAMO within hours of their release. Our results reveal that all tested models struggled significantly: only Gemini-2.5-Pro achieves a non-trivial score of 25%, while all other models achieve less than 5%. Through detailed analysis of reasoning traces, we identify the most common failure modes and find several unwanted artifacts arising from the optimization strategies employed during model training. Overall, our results suggest that current LLMs are inadequate for rigorous mathematical reasoning tasks, highlighting the need for substantial improvements in reasoning and proof generation capabilities."
      },
      {
        "id": "oai:arXiv.org:2503.22418v2",
        "title": "Robustness quantification: a new method for assessing the reliability of the predictions of a classifier",
        "link": "https://arxiv.org/abs/2503.22418",
        "author": "Adri\\'an Detavernier, Jasper De Bock",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.22418v2 Announce Type: replace \nAbstract: Based on existing ideas in the field of imprecise probabilities, we present a new approach for assessing the reliability of the individual predictions of a generative probabilistic classifier. We call this approach robustness quantification, compare it to uncertainty quantification, and demonstrate that it continues to work well even for classifiers that are learned from small training sets that are sampled from a shifted distribution."
      },
      {
        "id": "oai:arXiv.org:2503.23495v3",
        "title": "Embedding Shift Dissection on CLIP: Effects of Augmentations on VLM's Representation Learning",
        "link": "https://arxiv.org/abs/2503.23495",
        "author": "Ashim Dahal, Saydul Akbar Murad, Nick Rahimi",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.23495v3 Announce Type: replace \nAbstract: Understanding the representation shift on Vision Language Models like CLIP under different augmentations provides valuable insights on Mechanistic Interpretability. In this study, we show the shift on CLIP's embeddings on 9 common augmentation techniques: noise, blur, color jitter, scale and rotate, flip, elastic and perspective transforms, random brightness and contrast, and coarse dropout of pixel blocks. We scrutinize the embedding shifts under similarity on attention map, patch, edge, detail preservation, cosine similarity, L2 distance, pairwise distance and dendrogram clusters and provide qualitative analysis on sample images. Our findings suggest certain augmentations like noise, perspective transform and shift scaling have higher degree of drastic impact on embedding shift. This study provides a concrete foundation for future work on VLM's robustness for mechanical interpretation and adversarial data defense. The code implementation for this study can be found on \\href{https://github.com/ashimdahal/clip-shift-analysis}{https://github.com/ashimdahal/clip-shift-analysis}."
      },
      {
        "id": "oai:arXiv.org:2503.23765v2",
        "title": "STI-Bench: Are MLLMs Ready for Precise Spatial-Temporal World Understanding?",
        "link": "https://arxiv.org/abs/2503.23765",
        "author": "Yun Li, Yiming Zhang, Tao Lin, XiangRui Liu, Wenxiao Cai, Zheng Liu, Bo Zhao",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.23765v2 Announce Type: replace \nAbstract: The use of Multimodal Large Language Models (MLLMs) as an end-to-end solution for Embodied AI and Autonomous Driving has become a prevailing trend. While MLLMs have been extensively studied for visual semantic understanding tasks, their ability to perform precise and quantitative spatial-temporal understanding in real-world applications remains largely unexamined, leading to uncertain prospects. To evaluate models' Spatial-Temporal Intelligence, we introduce STI-Bench, a benchmark designed to evaluate MLLMs' spatial-temporal understanding through challenging tasks such as estimating and predicting the appearance, pose, displacement, and motion of objects. Our benchmark encompasses a wide range of robot and vehicle operations across desktop, indoor, and outdoor scenarios. The extensive experiments reveals that the state-of-the-art MLLMs still struggle in real-world spatial-temporal understanding, especially in tasks requiring precise distance estimation and motion analysis."
      },
      {
        "id": "oai:arXiv.org:2503.23820v3",
        "title": "When Counterfactual Reasoning Fails: Chaos and Real-World Complexity",
        "link": "https://arxiv.org/abs/2503.23820",
        "author": "Yahya Aalaila, Gerrit Gro{\\ss}mann, Sumantrak Mukherjee, Jonas Wahl, Sebastian Vollmer",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.23820v3 Announce Type: replace \nAbstract: Counterfactual reasoning, a cornerstone of human cognition and decision-making, is often seen as the 'holy grail' of causal learning, with applications ranging from interpreting machine learning models to promoting algorithmic fairness. While counterfactual reasoning has been extensively studied in contexts where the underlying causal model is well-defined, real-world causal modeling is often hindered by model and parameter uncertainty, observational noise, and chaotic behavior. The reliability of counterfactual analysis in such settings remains largely unexplored. In this work, we investigate the limitations of counterfactual reasoning within the framework of Structural Causal Models. Specifically, we empirically investigate \\emph{counterfactual sequence estimation} and highlight cases where it becomes increasingly unreliable. We find that realistic assumptions, such as low degrees of model uncertainty or chaotic dynamics, can result in counterintuitive outcomes, including dramatic deviations between predicted and true counterfactual trajectories. This work urges caution when applying counterfactual reasoning in settings characterized by chaos and uncertainty. Furthermore, it raises the question of whether certain systems may pose fundamental limitations on the ability to answer counterfactual questions about their behavior."
      },
      {
        "id": "oai:arXiv.org:2503.24305v3",
        "title": "Evaluating machine learning models for predicting pesticides toxicity to honey bees",
        "link": "https://arxiv.org/abs/2503.24305",
        "author": "Jakub Adamczyk, Jakub Poziemski, Pawel Siedlecki",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.24305v3 Announce Type: replace \nAbstract: Small molecules play a critical role in the biomedical, environmental, and agrochemical domains, each with distinct physicochemical requirements and success criteria. Although biomedical research benefits from extensive datasets and established benchmarks, agrochemical data remain scarce, particularly with respect to species-specific toxicity. This work focuses on ApisTox, the most comprehensive dataset of experimentally validated chemical toxicity to the honey bee (Apis mellifera), an ecologically vital pollinator. We evaluate ApisTox using a diverse suite of machine learning approaches, including molecular fingerprints, graph kernels, and graph neural networks, as well as pretrained models. Comparative analysis with medicinal datasets from the MoleculeNet benchmark reveals that ApisTox represents a distinct chemical space. Performance degradation on non-medicinal datasets, such as ApisTox, demonstrates their limited generalizability of current state-of-the-art algorithms trained solely on biomedical data. Our study highlights the need for more diverse datasets and for targeted model development geared toward the agrochemical domain."
      },
      {
        "id": "oai:arXiv.org:2504.02323v2",
        "title": "CoTAL: Human-in-the-Loop Prompt Engineering, Chain-of-Thought Reasoning, and Active Learning for Generalizable Formative Assessment Scoring",
        "link": "https://arxiv.org/abs/2504.02323",
        "author": "Clayton Cohn, Nicole Hutchins, Ashwin T S, Gautam Biswas",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02323v2 Announce Type: replace \nAbstract: Large language models (LLMs) have created new opportunities to assist teachers and support student learning. Methods such as chain-of-thought (CoT) prompting enable LLMs to grade formative assessments in science, providing scores and relevant feedback to students. However, the extent to which these methods generalize across curricula in multiple domains (such as science, computing, and engineering) remains largely untested. In this paper, we introduce Chain-of-Thought Prompting + Active Learning (CoTAL), an LLM-based approach to formative assessment scoring that (1) leverages Evidence-Centered Design (ECD) principles to develop curriculum-aligned formative assessments and rubrics, (2) applies human-in-the-loop prompt engineering to automate response scoring, and (3) incorporates teacher and student feedback to iteratively refine assessment questions, grading rubrics, and LLM prompts for automated grading. Our findings demonstrate that CoTAL improves GPT-4's scoring performance, achieving gains of up to 24.5% over a non-prompt-engineered baseline. Both teachers and students view CoTAL as effective in scoring and explaining student responses, each providing valuable refinements to enhance grading accuracy and explanation quality."
      },
      {
        "id": "oai:arXiv.org:2504.03122v2",
        "title": "From Observation to Orientation: an Adaptive Integer Programming Approach to Intervention Design",
        "link": "https://arxiv.org/abs/2504.03122",
        "author": "Abdelmonem Elrefaey, Rong Pan",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03122v2 Announce Type: replace \nAbstract: Using both observational and experimental data, a causal discovery process can identify the causal relationships between variables. A unique adaptive intervention design paradigm is presented in this work, where causal directed acyclic graphs (DAGs) are for effectively recovered with practical budgetary considerations. In order to choose treatments that optimize information gain under these considerations, an iterative integer programming (IP) approach is proposed, which drastically reduces the number of experiments required. Simulations over a broad range of graph sizes and edge densities are used to assess the effectiveness of the suggested approach. Results show that the proposed adaptive IP approach achieves full causal graph recovery with fewer intervention iterations and variable manipulations than random intervention baselines, and it is also flexible enough to accommodate a variety of practical constraints."
      },
      {
        "id": "oai:arXiv.org:2504.03135v2",
        "title": "Hierarchical Modeling for Medical Visual Question Answering with Cross-Attention Fusion",
        "link": "https://arxiv.org/abs/2504.03135",
        "author": "Junkai Zhang, Bin Li, Shoujun Zhou, Yue Du",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03135v2 Announce Type: replace \nAbstract: Medical Visual Question Answering (Med-VQA) answers clinical questions using medical images, aiding diagnosis. Designing the MedVQA system holds profound importance in assisting clinical diagnosis and enhancing diagnostic accuracy. Building upon this foundation, Hierarchical Medical VQA extends Medical VQA by organizing medical questions into a hierarchical structure and making level-specific predictions to handle fine-grained distinctions. Recently, many studies have proposed hierarchical MedVQA tasks and established datasets, However, several issues still remain: (1) imperfect hierarchical modeling leads to poor differentiation between question levels causing semantic fragmentation across hierarchies. (2) Excessive reliance on implicit learning in Transformer-based cross-modal self-attention fusion methods, which obscures crucial local semantic correlations in medical scenarios. To address these issues, this study proposes a HiCA-VQA method, including two modules: Hierarchical Prompting for fine-grained medical questions and Hierarchical Answer Decoders. The hierarchical prompting module pre-aligns hierarchical text prompts with image features to guide the model in focusing on specific image regions according to question types, while the hierarchical decoder performs separate predictions for questions at different levels to improve accuracy across granularities. The framework also incorporates a cross-attention fusion module where images serve as queries and text as key-value pairs. Experiments on the Rad-Restruct benchmark demonstrate that the HiCA-VQA framework better outperforms existing state-of-the-art methods in answering hierarchical fine-grained questions. This study provides an effective pathway for hierarchical visual question answering systems, advancing medical image understanding."
      },
      {
        "id": "oai:arXiv.org:2504.03624v2",
        "title": "Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models",
        "link": "https://arxiv.org/abs/2504.03624",
        "author": "NVIDIA,  :, Aaron Blakeman, Aarti Basant, Abhinav Khattar, Adithya Renduchintala, Akhiad Bercovich, Aleksander Ficek, Alexis Bjorlin, Ali Taghibakhshi, Amala Sanjay Deshmukh, Ameya Sunil Mahabaleshwarkar, Andrew Tao, Anna Shors, Ashwath Aithal, Ashwin Poojary, Ayush Dattagupta, Balaram Buddharaju, Bobby Chen, Boris Ginsburg, Boxin Wang, Brandon Norick, Brian Butterfield, Bryan Catanzaro, Carlo del Mundo, Chengyu Dong, Christine Harvey, Christopher Parisien, Dan Su, Daniel Korzekwa, Danny Yin, Daria Gitman, David Mosallanezhad, Deepak Narayanan, Denys Fridman, Dima Rekesh, Ding Ma, Dmytro Pykhtar, Dong Ahn, Duncan Riach, Dusan Stosic, Eileen Long, Elad Segal, Ellie Evans, Eric Chung, Erick Galinkin, Evelina Bakhturina, Ewa Dobrowolska, Fei Jia, Fuxiao Liu, Gargi Prasad, Gerald Shen, Guilin Liu, Guo Chen, Haifeng Qian, Helen Ngo, Hongbin Liu, Hui Li, Igor Gitman, Ilia Karmanov, Ivan Moshkov, Izik Golan, Jan Kautz, Jane Polak Scowcroft, Jared Casper, Jarno Seppanen, Jason Lu, Jason Sewall, Jiaqi Zeng, Jiaxuan You, Jimmy Zhang, Jing Zhang, Jining Huang, Jinze Xue, Jocelyn Huang, Joey Conway, John Kamalu, Jon Barker, Jonathan Cohen, Joseph Jennings, Jupinder Parmar, Karan Sapra, Kari Briski, Kateryna Chumachenko, Katherine Luna, Keshav Santhanam, Kezhi Kong, Kirthi Sivamani, Krzysztof Pawelec, Kumar Anik, Kunlun Li, Lawrence McAfee, Leon Derczynski, Lindsey Pavao, Luis Vega, Lukas Voegtle, Maciej Bala, Maer Rodrigues de Melo, Makesh Narsimhan Sreedhar, Marcin Chochowski, Markus Kliegl, Marta Stepniewska-Dziubinska, Matthieu Le, Matvei Novikov, Mehrzad Samadi, Michael Andersch, Michael Evans, Miguel Martinez, Mike Chrzanowski, Mike Ranzinger, Mikolaj Blaz, Misha Smelyanskiy, Mohamed Fawzy, Mohammad Shoeybi, Mostofa Patwary, Nayeon Lee, Nima Tajbakhsh, Ning Xu, Oleg Rybakov, Oleksii Kuchaiev, Olivier Delalleau, Osvald Nitski, Parth Chadha, Pasha Shamis, Paulius Micikevicius, Pavlo Molchanov, Peter Dykas, Philipp Fischer, Pierre-Yves Aquilanti, Piotr Bialecki, Prasoon Varshney, Pritam Gundecha, Przemek Tredak, Rabeeh Karimi, Rahul Kandu, Ran El-Yaniv, Raviraj Joshi, Roger Waleffe, Ruoxi Zhang, Sabrina Kavanaugh, Sahil Jain, Samuel Kriman, Sangkug Lym, Sanjeev Satheesh, Saurav Muralidharan, Sean Narenthiran, Selvaraj Anandaraj, Seonmyeong Bak, Sergey Kashirsky, Seungju Han, Shantanu Acharya, Shaona Ghosh, Sharath Turuvekere Sreenivas, Sharon Clay, Shelby Thomas, Shrimai Prabhumoye, Shubham Pachori, Shubham Toshniwal, Shyamala Prayaga, Siddhartha Jain, Sirshak Das, Slawek Kierat, Somshubra Majumdar, Song Han, Soumye Singhal, Sriharsha Niverty, Stefania Alborghetti, Suseella Panguluri, Swetha Bhendigeri, Syeda Nahida Akter, Szymon Migacz, Tal Shiri, Terry Kong, Timo Roman, Tomer Ronen, Trisha Saar, Tugrul Konuk, Tuomas Rintamaki, Tyler Poon, Ushnish De, Vahid Noroozi, Varun Singh, Vijay Korthikanti, Vitaly Kurin, Wasi Uddin Ahmad, Wei Du, Wei Ping, Wenliang Dai, Wonmin Byeon, Xiaowei Ren, Yao Xu, Yejin Choi, Yian Zhang, Ying Lin, Yoshi Suhara, Zhiding Yu, Zhiqi Li, Zhiyu Li, Zhongbo Zhu, Zhuolin Yang, Zijia Chen",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03624v2 Announce Type: replace \nAbstract: As inference-time scaling becomes critical for enhanced reasoning capabilities, it is increasingly becoming important to build models that are efficient to infer. We introduce Nemotron-H, a family of 8B and 56B/47B hybrid Mamba-Transformer models designed to reduce inference cost for a given accuracy level. To achieve this goal, we replace the majority of self-attention layers in the common Transformer model architecture with Mamba layers that perform constant computation and require constant memory per generated token. We show that Nemotron-H models offer either better or on-par accuracy compared to other similarly-sized state-of-the-art open-sourced Transformer models (e.g., Qwen-2.5-7B/72B and Llama-3.1-8B/70B), while being up to 3$\\times$ faster at inference. To further increase inference speed and reduce the memory required at inference time, we created Nemotron-H-47B-Base from the 56B model using a new compression via pruning and distillation technique called MiniPuzzle. Nemotron-H-47B-Base achieves similar accuracy to the 56B model, but is 20% faster to infer. In addition, we introduce an FP8-based training recipe and show that it can achieve on par results with BF16-based training. This recipe is used to train the 56B model. All Nemotron-H models will be released, with support in Hugging Face, NeMo, and Megatron-LM."
      },
      {
        "id": "oai:arXiv.org:2504.03783v2",
        "title": "FAST: Federated Active Learning with Foundation Models for Communication-efficient Sampling and Training",
        "link": "https://arxiv.org/abs/2504.03783",
        "author": "Haoyuan Li, Mathias Funk, Jindong Wang, Aaqib Saeed",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03783v2 Announce Type: replace \nAbstract: Federated Active Learning (FAL) has emerged as a promising framework to leverage large quantities of unlabeled data across distributed clients while preserving data privacy. However, real-world deployments remain limited by high annotation costs and communication-intensive sampling processes, particularly in a cross-silo setting, when clients possess substantial local datasets. This paper addresses the crucial question: What is the best practice to reduce communication costs in human-in-the-loop learning with minimal annotator effort? Existing FAL methods typically rely on iterative annotation processes that separate active sampling from federated updates, leading to multiple rounds of expensive communication and annotation. In response, we introduce FAST, a two-pass FAL framework that harnesses foundation models for weak labeling in a preliminary pass, followed by a refinement pass focused exclusively on the most uncertain samples. By leveraging representation knowledge from foundation models and integrating refinement steps into a streamlined workflow, FAST substantially reduces the overhead incurred by iterative active sampling. Extensive experiments on diverse medical and natural image benchmarks demonstrate that FAST outperforms existing FAL methods by an average of 4.36% while reducing communication rounds eightfold under a limited 5% labeling budget."
      },
      {
        "id": "oai:arXiv.org:2504.03970v2",
        "title": "VideoComp: Advancing Fine-Grained Compositional and Temporal Alignment in Video-Text Models",
        "link": "https://arxiv.org/abs/2504.03970",
        "author": "Dahun Kim, AJ Piergiovanni, Ganesh Mallya, Anelia Angelova",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03970v2 Announce Type: replace \nAbstract: We introduce VideoComp, a benchmark and learning framework for advancing video-text compositionality understanding, aimed at improving vision-language models (VLMs) in fine-grained temporal alignment. Unlike existing benchmarks focused on static image-text compositionality or isolated single-event videos, our benchmark targets alignment in continuous multi-event videos. Leveraging video-text datasets with temporally localized event captions (e.g. ActivityNet-Captions, YouCook2), we construct two compositional benchmarks, ActivityNet-Comp and YouCook2-Comp. We create challenging negative samples with subtle temporal disruptions such as reordering, action word replacement, partial captioning, and combined disruptions. These benchmarks comprehensively test models' compositional sensitivity across extended, cohesive video-text sequences. To improve model performance, we propose a hierarchical pairwise preference loss that strengthens alignment with temporally accurate pairs and gradually penalizes increasingly disrupted ones, encouraging fine-grained compositional learning. To mitigate the limited availability of densely annotated video data, we introduce a pretraining strategy that concatenates short video-caption pairs to simulate multi-event sequences. We evaluate video-text foundational models and large multimodal models (LMMs) on our benchmark, identifying both strengths and areas for improvement in compositionality. Overall, our work provides a comprehensive framework for evaluating and enhancing model capabilities in achieving fine-grained, temporally coherent video-text alignment."
      },
      {
        "id": "oai:arXiv.org:2504.04141v2",
        "title": "Cognitive Debiasing Large Language Models for Decision-Making",
        "link": "https://arxiv.org/abs/2504.04141",
        "author": "Yougang Lyu, Shijie Ren, Yue Feng, Zihan Wang, Zhumin Chen, Zhaochun Ren, Maarten de Rijke",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.04141v2 Announce Type: replace \nAbstract: Large language models (LLMs) have shown potential in supporting decision-making applications, particularly as personal conversational assistants in the financial, healthcare, and legal domains. While prompt engineering strategies have enhanced the capabilities of LLMs in decision-making, cognitive biases inherent to LLMs present significant challenges. Cognitive biases are systematic patterns of deviation from norms or rationality in decision-making that can lead to the production of inaccurate outputs. Existing cognitive bias mitigation strategies assume that input prompts contain (exactly) one type of cognitive bias and therefore fail to perform well in realistic settings where there maybe any number of biases.\n  To fill this gap, we propose a cognitive debiasing approach, called self-debiasing, that enhances the reliability of LLMs by iteratively refining prompts. Our method follows three sequential steps -- bias determination, bias analysis, and cognitive debiasing -- to iteratively mitigate potential cognitive biases in prompts. Experimental results on finance, healthcare, and legal decision-making tasks, using both closed-source and open-source LLMs, demonstrate that the proposed self-debiasing method outperforms both advanced prompt engineering methods and existing cognitive debiasing techniques in average accuracy under no-bias, single-bias, and multi-bias settings."
      },
      {
        "id": "oai:arXiv.org:2504.04753v2",
        "title": "CADCrafter: Generating Computer-Aided Design Models from Unconstrained Images",
        "link": "https://arxiv.org/abs/2504.04753",
        "author": "Cheng Chen, Jiacheng Wei, Tianrun Chen, Chi Zhang, Xiaofeng Yang, Shangzhan Zhang, Bingchen Yang, Chuan-Sheng Foo, Guosheng Lin, Qixing Huang, Fayao Liu",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.04753v2 Announce Type: replace \nAbstract: Creating CAD digital twins from the physical world is crucial for manufacturing, design, and simulation. However, current methods typically rely on costly 3D scanning with labor-intensive post-processing. To provide a user-friendly design process, we explore the problem of reverse engineering from unconstrained real-world CAD images that can be easily captured by users of all experiences. However, the scarcity of real-world CAD data poses challenges in directly training such models. To tackle these challenges, we propose CADCrafter, an image-to-parametric CAD model generation framework that trains solely on synthetic textureless CAD data while testing on real-world images. To bridge the significant representation disparity between images and parametric CAD models, we introduce a geometry encoder to accurately capture diverse geometric features. Moreover, the texture-invariant properties of the geometric features can also facilitate the generalization to real-world scenarios. Since compiling CAD parameter sequences into explicit CAD models is a non-differentiable process, the network training inherently lacks explicit geometric supervision. To impose geometric validity constraints, we employ direct preference optimization (DPO) to fine-tune our model with the automatic code checker feedback on CAD sequence quality. Furthermore, we collected a real-world dataset, comprised of multi-view images and corresponding CAD command sequence pairs, to evaluate our method. Experimental results demonstrate that our approach can robustly handle real unconstrained CAD images, and even generalize to unseen general objects."
      },
      {
        "id": "oai:arXiv.org:2504.04834v2",
        "title": "Learning Affine Correspondences by Integrating Geometric Constraints",
        "link": "https://arxiv.org/abs/2504.04834",
        "author": "Pengju Sun, Banglei Guan, Zhenbao Yu, Yang Shang, Qifeng Yu, Daniel Barath",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.04834v2 Announce Type: replace \nAbstract: Affine correspondences have received significant attention due to their benefits in tasks like image matching and pose estimation. Existing methods for extracting affine correspondences still have many limitations in terms of performance; thus, exploring a new paradigm is crucial. In this paper, we present a new pipeline designed for extracting accurate affine correspondences by integrating dense matching and geometric constraints. Specifically, a novel extraction framework is introduced, with the aid of dense matching and a novel keypoint scale and orientation estimator. For this purpose, we propose loss functions based on geometric constraints, which can effectively improve accuracy by supervising neural networks to learn feature geometry. The experimental show that the accuracy and robustness of our method outperform the existing ones in image matching tasks. To further demonstrate the effectiveness of the proposed method, we applied it to relative pose estimation. Affine correspondences extracted by our method lead to more accurate poses than the baselines on a range of real-world datasets. The code is available at https://github.com/stilcrad/DenseAffine."
      },
      {
        "id": "oai:arXiv.org:2504.05506v2",
        "title": "ChartQAPro: A More Diverse and Challenging Benchmark for Chart Question Answering",
        "link": "https://arxiv.org/abs/2504.05506",
        "author": "Ahmed Masry, Mohammed Saidul Islam, Mahir Ahmed, Aayush Bajaj, Firoz Kabir, Aaryaman Kartha, Md Tahmid Rahman Laskar, Mizanur Rahman, Shadikur Rahman, Mehrad Shahmohammadi, Megh Thakkar, Md Rizwan Parvez, Enamul Hoque, Shafiq Joty",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.05506v2 Announce Type: replace \nAbstract: Charts are ubiquitous, as people often use them to analyze data, answer questions, and discover critical insights. However, performing complex analytical tasks with charts requires significant perceptual and cognitive effort. Chart Question Answering (CQA) systems automate this process by enabling models to interpret and reason with visual representations of data. However, existing benchmarks like ChartQA lack real-world diversity and have recently shown performance saturation with modern large vision-language models (LVLMs). To address these limitations, we introduce ChartQAPro, a new benchmark that includes 1,341 charts from 157 diverse sources, spanning various chart types, including infographics and dashboards, and featuring 1,948 questions in various types, such as multiple-choice, conversational, hypothetical, and unanswerable questions, to better reflect real-world challenges. Our evaluations with 21 models show a substantial performance drop for LVLMs on ChartQAPro; e.g., Claude Sonnet 3.5 scores 90.5% on ChartQA but only 55.81% on ChartQAPro, underscoring the complexity of chart reasoning. We complement our findings with detailed error analyses and ablation studies, identifying key challenges and opportunities for advancing LVLMs in chart understanding and reasoning. We release ChartQAPro at https://github.com/vis-nlp/ChartQAPro."
      },
      {
        "id": "oai:arXiv.org:2504.05586v2",
        "title": "Finding Fantastic Experts in MoEs: A Unified Study for Expert Dropping Strategies and Observations",
        "link": "https://arxiv.org/abs/2504.05586",
        "author": "Ajay Jaiswal, Jianyu Wang, Yixiao Li, Pingzhi Li, Tianlong Chen, Zhangyang Wang, Chong Wang, Ruoming Pang, Xianzhi Du",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.05586v2 Announce Type: replace \nAbstract: Sparsely activated Mixture-of-Experts (SMoE) has shown promise in scaling up the learning capacity of neural networks. However, vanilla SMoEs have issues such as expert redundancy and heavy memory requirements, making them inefficient and non-scalable, especially for resource-constrained scenarios. Expert-level sparsification of SMoEs involves pruning the least important experts to address these limitations. In this work, we aim to address three questions: (1) What is the best recipe to identify the least knowledgeable subset of experts that can be dropped with minimal impact on performance? (2) How should we perform expert dropping (one-shot or iterative), and what correction measures can we undertake to minimize its drastic impact on SMoE subnetwork capabilities? (3) What capabilities of full-SMoEs are severely impacted by the removal of the least dominant experts, and how can we recover them? Firstly, we propose MoE Experts Compression Suite (MC-Suite), which is a collection of some previously explored and multiple novel recipes to provide a comprehensive benchmark for estimating expert importance from diverse perspectives, as well as unveil numerous valuable insights for SMoE experts. Secondly, unlike prior works with a one-shot expert pruning approach, we explore the benefits of iterative pruning with the re-estimation of the MC-Suite criterion. Moreover, we introduce the benefits of task-agnostic fine-tuning as a correction mechanism during iterative expert dropping, which we term MoE Lottery Subnetworks. Lastly, we present an experimentally validated conjecture that, during expert dropping, SMoEs' instruction-following capabilities are predominantly hurt, which can be restored to a robust level subject to external augmentation of instruction-following capabilities using k-shot examples and supervised fine-tuning."
      },
      {
        "id": "oai:arXiv.org:2504.05761v2",
        "title": "AiGAS-dEVL-RC: An Adaptive Growing Neural Gas Model for Recurrently Drifting Unsupervised Data Streams",
        "link": "https://arxiv.org/abs/2504.05761",
        "author": "Maria Arostegi, Miren Nekane Bilbao, Jesus L. Lobo, Javier Del Ser",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.05761v2 Announce Type: replace \nAbstract: Concept drift and extreme verification latency pose significant challenges in data stream learning, particularly when dealing with recurring concept changes in dynamic environments. This work introduces a novel method based on the Growing Neural Gas (GNG) algorithm, designed to effectively handle abrupt recurrent drifts while adapting to incrementally evolving data distributions (incremental drifts). Leveraging the self-organizing and topological adaptability of GNG, the proposed approach maintains a compact yet informative memory structure, allowing it to efficiently store and retrieve knowledge of past or recurring concepts, even under conditions of delayed or sparse stream supervision. Our experiments highlight the superiority of our approach over existing data stream learning methods designed to cope with incremental non-stationarities and verification latency, demonstrating its ability to quickly adapt to new drifts, robustly manage recurring patterns, and maintain high predictive accuracy with a minimal memory footprint. Unlike other techniques that fail to leverage recurring knowledge, our proposed approach is proven to be a robust and efficient online learning solution for unsupervised drifting data flows."
      },
      {
        "id": "oai:arXiv.org:2504.06138v2",
        "title": "A Multimedia Analytics Model for the Foundation Model Era",
        "link": "https://arxiv.org/abs/2504.06138",
        "author": "Marcel Worring, Jan Zah\\'alka, Stef van den Elzen, Maximilian T. Fischer, Daniel A. Keim",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.06138v2 Announce Type: replace \nAbstract: The rapid advances in Foundation Models and agentic Artificial Intelligence are transforming multimedia analytics by enabling richer, more sophisticated interactions between humans and analytical systems. Existing conceptual models for visual and multimedia analytics, however, do not adequately capture the complexity introduced by these powerful AI paradigms. To bridge this gap, we propose a comprehensive multimedia analytics model specifically designed for the foundation model era. Building upon established frameworks from visual analytics, multimedia analytics, knowledge generation, analytic task definition, mixed-initiative guidance, and human-in-the-loop reinforcement learning, our model emphasizes integrated human-AI teaming based on visual analytics agents from both technical and conceptual perspectives. Central to the model is a seamless, yet explicitly separable, interaction channel between expert users and semi-autonomous analytical processes, ensuring continuous alignment between user intent and AI behavior. The model addresses practical challenges in sensitive domains such as intelligence analysis, investigative journalism, and other fields handling complex, high-stakes data. We illustrate through detailed case studies how our model facilitates deeper understanding and targeted improvement of multimedia analytics solutions. By explicitly capturing how expert users can optimally interact with and guide AI-powered multimedia analytics systems, our conceptual framework sets a clear direction for system design, comparison, and future research."
      },
      {
        "id": "oai:arXiv.org:2504.06212v2",
        "title": "NNN: Next-Generation Neural Networks for Marketing Mix Modeling",
        "link": "https://arxiv.org/abs/2504.06212",
        "author": "Thomas Mulc, Mike Anderson, Paul Cubre, Huikun Zhang, Ivy Liu, Saket Kumar",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.06212v2 Announce Type: replace \nAbstract: We present NNN, a Transformer-based neural network approach to Marketing Mix Modeling (MMM) designed to address key limitations of traditional methods. Unlike conventional MMMs which rely on scalar inputs and parametric decay functions, NNN uses rich embeddings to capture both quantitative and qualitative aspects of marketing and organic channels (e.g., search queries, ad creatives). This, combined with its attention mechanism, enables NNN to model complex interactions, capture long-term effects, and potentially improve sales attribution accuracy. We show that L1 regularization permits the use of such expressive models in typical data-constrained settings. Evaluating NNN on simulated and real-world data demonstrates its efficacy, particularly through considerable improvement in predictive power. Beyond attribution, NNN provides valuable, complementary insights through model probing, such as evaluating keyword or creative effectiveness, enhancing model interpretability."
      },
      {
        "id": "oai:arXiv.org:2504.06265v2",
        "title": "GOLLuM: Gaussian Process Optimized LLMs -- Reframing LLM Finetuning through Bayesian Optimization",
        "link": "https://arxiv.org/abs/2504.06265",
        "author": "Bojana Rankovi\\'c, Philippe Schwaller",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.06265v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) can encode complex relationships in their latent spaces, yet harnessing them for optimization under uncertainty remains challenging. We address this gap with a novel architecture that reframes LLM finetuning as Gaussian process (GP) marginal likelihood optimization via deep kernel methods. We introduce LLM-based deep kernels, jointly optimized with GPs to preserve the benefits of both - LLMs to provide a rich and flexible input space for Bayesian optimization and - GPs to model this space with predictive uncertainty for more efficient sampling. Applied to Buchwald-Hartwig reaction optimization, our method nearly doubles the discovery rate of high-performing reactions compared to static LLM embeddings (from 24% to 43% coverage of the top 5% reactions in just 50 optimization iterations). We also observe a 14% improvement over domain-specific representations without requiring specialized features. Extensive empirical evaluation across 19 benchmarks - ranging from general chemistry to reaction and molecular property optimization - demonstrates our method's robustness, generality, and consistent improvements across: (1) tasks, (2) LLM architectures (encoder, decoder, encoder-decoder), (3) pretraining domains (chemistry-related or general-purpose) and (4) hyperparameter settings (tuned once on a single dataset). Finally, we explain these improvements: joint LLM-GP optimization through marginal likelihood implicitly performs contrastive learning, aligning representations to produce (1) better-structured embedding spaces, (2) improved uncertainty calibration, and (3) more efficient sampling - without requiring any external loss. This work provides both practical advances in sample-efficient optimization and insights into what makes effective Bayesian optimization."
      },
      {
        "id": "oai:arXiv.org:2504.06611v2",
        "title": "Wanting to be Understood",
        "link": "https://arxiv.org/abs/2504.06611",
        "author": "Chrisantha Fernando, Dylan Banarse, Simon Osindero",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.06611v2 Announce Type: replace \nAbstract: This paper explores an intrinsic motivation for mutual awareness, hypothesizing that humans possess a fundamental drive to understand and to be understood even in the absence of extrinsic rewards. Through simulations of the perceptual crossing paradigm, we explore the effect of various internal reward functions in reinforcement learning agents. The drive to understand is implemented as an active inference type artificial curiosity reward, whereas the drive to be understood is implemented through intrinsic rewards for imitation, influence/impressionability, and sub-reaction time anticipation of the other. Results indicate that while artificial curiosity alone does not lead to a preference for social interaction, rewards emphasizing reciprocal understanding successfully drive agents to prioritize interaction. We demonstrate that this intrinsic motivation can facilitate cooperation in tasks where only one agent receives extrinsic reward for the behaviour of the other."
      },
      {
        "id": "oai:arXiv.org:2504.06643v2",
        "title": "AMAD: AutoMasked Attention for Unsupervised Multivariate Time Series Anomaly Detection",
        "link": "https://arxiv.org/abs/2504.06643",
        "author": "Tiange Huang, Yongjun Li",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.06643v2 Announce Type: replace \nAbstract: Unsupervised multivariate time series anomaly detection (UMTSAD) plays a critical role in various domains, including finance, networks, and sensor systems. In recent years, due to the outstanding performance of deep learning in general sequential tasks, many models have been specialized for deep UMTSAD tasks and have achieved impressive results, particularly those based on the Transformer and self-attention mechanisms. However, the sequence anomaly association assumptions underlying these models are often limited to specific predefined patterns and scenarios, such as concentrated or peak anomaly patterns. These limitations hinder their ability to generalize to diverse anomaly situations, especially where the lack of labels poses significant challenges. To address these issues, we propose AMAD, which integrates \\textbf{A}uto\\textbf{M}asked Attention for UMTS\\textbf{AD} scenarios. AMAD introduces a novel structure based on the AutoMask mechanism and an attention mixup module, forming a simple yet generalized anomaly association representation framework. This framework is further enhanced by a Max-Min training strategy and a Local-Global contrastive learning approach. By combining multi-scale feature extraction with automatic relative association modeling, AMAD provides a robust and adaptable solution to UMTSAD challenges. Extensive experimental results demonstrate that the proposed model achieving competitive performance results compared to SOTA benchmarks across a variety of datasets."
      },
      {
        "id": "oai:arXiv.org:2504.06647v2",
        "title": "Uni-PrevPredMap: Extending PrevPredMap to a Unified Framework of Prior-Informed Modeling for Online Vectorized HD Map Construction",
        "link": "https://arxiv.org/abs/2504.06647",
        "author": "Nan Peng, Xun Zhou, Mingming Wang, Guisong Chen, Wenqi Xu",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.06647v2 Announce Type: replace \nAbstract: Safety constitutes a foundational imperative for autonomous driving systems, necessitating the maximal incorporation of accessible external prior information. This study establishes that temporal perception buffers and cost-efficient maps inherently form complementary prior sources for online vectorized high-definition (HD) map construction. We present Uni-PrevPredMap, a unified prior-informed framework that systematically integrates two synergistic information sources: previous predictions and simulated outdated HD maps. The framework introduces two core innovations: a tile-indexed 3D vectorized global map processor enabling efficient refreshment, storage, and retrieval of 3D vectorized priors; a tri-mode operational optimization paradigm ensuring consistency across non-prior, temporal-prior, and temporal-map-fusion-prior scenarios while mitigating reliance on idealized map fidelity assumptions. Uni-PrevPredMap achieves state-of-the-art performance in map-absent scenarios across established online vectorized HD map construction benchmarks. When provided with simulated outdated HD maps, the framework exhibits robust capabilities in error-resilient prior fusion, empirically confirming the synergistic complementarity between previous predictions and simulated outdated HD maps. Code will be available at https://github.com/pnnnnnnn/Uni-PrevPredMap."
      },
      {
        "id": "oai:arXiv.org:2504.06742v2",
        "title": "nnLandmark: A Self-Configuring Method for 3D Medical Landmark Detection",
        "link": "https://arxiv.org/abs/2504.06742",
        "author": "Alexandra Ertl, Shuhan Xiao, Stefan Denner, Robin Peretzke, David Zimmerer, Peter Neher, Fabian Isensee, Klaus Maier-Hein",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.06742v2 Announce Type: replace \nAbstract: Landmark detection plays a crucial role in medical imaging tasks that rely on precise spatial localization, including specific applications in diagnosis, treatment planning, image registration, and surgical navigation. However, manual annotation is labor-intensive and requires expert knowledge. While deep learning shows promise in automating this task, progress is hindered by limited public datasets, inconsistent benchmarks, and non-standardized baselines, restricting reproducibility, fair comparisons, and model generalizability. This work introduces nnLandmark, a self-configuring deep learning framework for 3D medical landmark detection, adapting nnU-Net to perform heatmap-based regression. By leveraging nnU-Net's automated configuration, nnLandmark eliminates the need for manual parameter tuning, offering out-of-the-box usability. It achieves state-of-the-art accuracy across two public datasets, with a mean radial error (MRE) of 1.5 mm on the Mandibular Molar Landmark (MML) dental CT dataset and 1.2 mm for anatomical fiducials on a brain MRI dataset (AFIDs), where nnLandmark aligns with the inter-rater variability of 1.5 mm. With its strong generalization, reproducibility, and ease of deployment, nnLandmark establishes a reliable baseline for 3D landmark detection, supporting research in anatomical localization and clinical workflows that depend on precise landmark identification. The code will be available soon."
      },
      {
        "id": "oai:arXiv.org:2504.06752v2",
        "title": "Compass Control: Multi Object Orientation Control for Text-to-Image Generation",
        "link": "https://arxiv.org/abs/2504.06752",
        "author": "Rishubh Parihar, Vaibhav Agrawal, Sachidanand VS, R. Venkatesh Babu",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.06752v2 Announce Type: replace \nAbstract: Existing approaches for controlling text-to-image diffusion models, while powerful, do not allow for explicit 3D object-centric control, such as precise control of object orientation. In this work, we address the problem of multi-object orientation control in text-to-image diffusion models. This enables the generation of diverse multi-object scenes with precise orientation control for each object. The key idea is to condition the diffusion model with a set of orientation-aware \\textbf{compass} tokens, one for each object, along with text tokens. A light-weight encoder network predicts these compass tokens taking object orientation as the input. The model is trained on a synthetic dataset of procedurally generated scenes, each containing one or two 3D assets on a plain background. However, direct training this framework results in poor orientation control as well as leads to entanglement among objects. To mitigate this, we intervene in the generation process and constrain the cross-attention maps of each compass token to its corresponding object regions. The trained model is able to achieve precise orientation control for a) complex objects not seen during training and b) multi-object scenes with more than two objects, indicating strong generalization capabilities. Further, when combined with personalization methods, our method precisely controls the orientation of the new object in diverse contexts. Our method achieves state-of-the-art orientation control and text alignment, quantified with extensive evaluations and a user study."
      },
      {
        "id": "oai:arXiv.org:2504.06801v2",
        "title": "MonoPlace3D: Learning 3D-Aware Object Placement for 3D Monocular Detection",
        "link": "https://arxiv.org/abs/2504.06801",
        "author": "Rishubh Parihar, Srinjay Sarkar, Sarthak Vora, Jogendra Kundu, R. Venkatesh Babu",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.06801v2 Announce Type: replace \nAbstract: Current monocular 3D detectors are held back by the limited diversity and scale of real-world datasets. While data augmentation certainly helps, it's particularly difficult to generate realistic scene-aware augmented data for outdoor settings. Most current approaches to synthetic data generation focus on realistic object appearance through improved rendering techniques. However, we show that where and how objects are positioned is just as crucial for training effective 3D monocular detectors. The key obstacle lies in automatically determining realistic object placement parameters - including position, dimensions, and directional alignment when introducing synthetic objects into actual scenes. To address this, we introduce MonoPlace3D, a novel system that considers the 3D scene content to create realistic augmentations. Specifically, given a background scene, MonoPlace3D learns a distribution over plausible 3D bounding boxes. Subsequently, we render realistic objects and place them according to the locations sampled from the learned distribution. Our comprehensive evaluation on two standard datasets KITTI and NuScenes, demonstrates that MonoPlace3D significantly improves the accuracy of multiple existing monocular 3D detectors while being highly data efficient."
      },
      {
        "id": "oai:arXiv.org:2504.06958v2",
        "title": "VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement Fine-Tuning",
        "link": "https://arxiv.org/abs/2504.06958",
        "author": "Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, Limin Wang",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.06958v2 Announce Type: replace \nAbstract: Recent advancements in reinforcement learning have significantly advanced the reasoning capabilities of multimodal large language models (MLLMs). While approaches such as Group Relative Policy Optimization (GRPO) and rule-based reward mechanisms demonstrate promise in text and image domains, their application to video understanding remains limited. This paper presents a systematic exploration of Reinforcement Fine-Tuning (RFT) with GRPO for video MLLMs, aiming to enhance spatio-temporal perception while maintaining general capabilities. Our experiments reveal that RFT is highly data-efficient for task-specific improvements. Through multi-task RFT on spatio-temporal perception objectives with limited samples, we develop VideoChat-R1, a powerful video MLLM that achieves state-of-the-art performance on spatio-temporal perception tasks without sacrificing chat ability, while exhibiting emerging spatio-temporal reasoning abilities. Compared to Qwen2.5-VL-7B, VideoChat-R1 boosts performance several-fold in tasks like temporal grounding (+31.8) and object tracking (+31.2). Additionally, it significantly improves on general QA benchmarks such as VideoMME (+0.9), MVBench (+1.0), and Perception Test (+0.9). Our findings underscore the potential of RFT for specialized task enhancement of Video MLLMs. We hope our work offers valuable insights for future RL research in video MLLMs."
      },
      {
        "id": "oai:arXiv.org:2504.07083v2",
        "title": "GenDoP: Auto-regressive Camera Trajectory Generation as a Director of Photography",
        "link": "https://arxiv.org/abs/2504.07083",
        "author": "Mengchen Zhang, Tong Wu, Jing Tan, Ziwei Liu, Gordon Wetzstein, Dahua Lin",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07083v2 Announce Type: replace \nAbstract: Camera trajectory design plays a crucial role in video production, serving as a fundamental tool for conveying directorial intent and enhancing visual storytelling. In cinematography, Directors of Photography meticulously craft camera movements to achieve expressive and intentional framing. However, existing methods for camera trajectory generation remain limited: Traditional approaches rely on geometric optimization or handcrafted procedural systems, while recent learning-based methods often inherit structural biases or lack textual alignment, constraining creative synthesis. In this work, we introduce an auto-regressive model inspired by the expertise of Directors of Photography to generate artistic and expressive camera trajectories. We first introduce DataDoP, a large-scale multi-modal dataset containing 29K real-world shots with free-moving camera trajectories, depth maps, and detailed captions in specific movements, interaction with the scene, and directorial intent. Thanks to the comprehensive and diverse database, we further train an auto-regressive, decoder-only Transformer for high-quality, context-aware camera movement generation based on text guidance and RGBD inputs, named GenDoP. Extensive experiments demonstrate that compared to existing methods, GenDoP offers better controllability, finer-grained trajectory adjustments, and higher motion stability. We believe our approach establishes a new standard for learning-based cinematography, paving the way for future advancements in camera control and filmmaking. Our project website: https://kszpxxzmc.github.io/GenDoP/."
      },
      {
        "id": "oai:arXiv.org:2210.13300v3",
        "title": "Designing Universal Causal Deep Learning Models: The Case of Infinite-Dimensional Dynamical Systems from Stochastic Analysis",
        "link": "https://arxiv.org/abs/2210.13300",
        "author": "Luca Galimberti, Anastasis Kratsios, Giulia Livieri",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2210.13300v3 Announce Type: replace-cross \nAbstract: Several non-linear operators in stochastic analysis, such as solution maps to stochastic differential equations, depend on a temporal structure which is not leveraged by contemporary neural operators designed to approximate general maps between Banach space. This paper therefore proposes an operator learning solution to this open problem by introducing a deep learning model-design framework that takes suitable infinite-dimensional linear metric spaces, e.g. Banach spaces, as inputs and returns a universal \\textit{sequential} deep learning model adapted to these linear geometries specialized for the approximation of operators encoding a temporal structure. We call these models \\textit{Causal Neural Operators}. Our main result states that the models produced by our framework can uniformly approximate on compact sets and across arbitrarily finite-time horizons H\\\"older or smooth trace class operators, which causally map sequences between given linear metric spaces. Our analysis uncovers new quantitative relationships on the latent state-space dimension of Causal Neural Operators, which even have new implications for (classical) finite-dimensional Recurrent Neural Networks. In addition, our guarantees for recurrent neural networks are tighter than the available results inherited from feedforward neural networks when approximating dynamical systems between finite-dimensional spaces."
      },
      {
        "id": "oai:arXiv.org:2302.07425v5",
        "title": "Bandit Social Learning: Exploration under Myopic Behavior",
        "link": "https://arxiv.org/abs/2302.07425",
        "author": "Kiarash Banihashem, MohammadTaghi Hajiaghayi, Suho Shin, Aleksandrs Slivkins",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2302.07425v5 Announce Type: replace-cross \nAbstract: We study social learning dynamics motivated by reviews on online platforms. The agents collectively follow a simple multi-armed bandit protocol, but each agent acts myopically, without regards to exploration. We allow the greedy (exploitation-only) algorithm, as well as a wide range of behavioral biases. Specifically, we allow myopic behaviors that are consistent with (parameterized) confidence intervals for the arms' expected rewards. We derive stark learning failures for any such behavior, and provide matching positive results. The learning-failure results extend to Bayesian agents and Bayesian bandit environments.\n  In particular, we obtain general, quantitatively strong results on failure of the greedy bandit algorithm, both for ``frequentist\" and ``Bayesian\" versions. Failure results known previously are quantitatively weak, and either trivial or very specialized. Thus, we provide a theoretical foundation for designing non-trivial bandit algorithms, \\ie algorithms that intentionally explore, which has been missing from the literature.\n  Our general behavioral model can be interpreted as agents' optimism or pessimism. The matching positive results entail a maximal allowed amount of optimism. Moreover, we find that no amount of pessimism helps against the learning failures, whereas even a small-but-constant fraction of extreme optimists avoids the failures and leads to near-optimal regret rates."
      },
      {
        "id": "oai:arXiv.org:2307.02284v3",
        "title": "Universal Scaling Laws of Absorbing Phase Transitions in Artificial Deep Neural Networks",
        "link": "https://arxiv.org/abs/2307.02284",
        "author": "Keiichi Tamai, Tsuyoshi Okubo, Truong Vinh Truong Duy, Naotake Natori, Synge Todo",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2307.02284v3 Announce Type: replace-cross \nAbstract: We demonstrate that conventional artificial deep neural networks operating near the phase boundary of the signal propagation dynamics, also known as the edge of chaos, exhibit universal scaling laws of absorbing phase transitions in non-equilibrium statistical mechanics. We exploit the fully deterministic nature of the propagation dynamics to elucidate an analogy between a signal collapse in the neural networks and an absorbing state (a state that the system can enter but cannot escape from). Our numerical results indicate that the multilayer perceptrons and the convolutional neural networks belong to the mean-field and the directed percolation universality classes, respectively. Also, the finite-size scaling is successfully applied, suggesting a potential connection to the depth-width trade-off in deep learning. Furthermore, our analysis of the training dynamics under the gradient descent reveals that hyperparameter tuning to the phase boundary is necessary but insufficient for achieving optimal generalization in deep networks. Remarkably, nonuniversal metric factors associated with the scaling laws are shown to play a significant role in concretizing the above observations. These findings highlight the usefulness of the notion of criticality for analyzing the behavior of artificial deep neural networks and offer new insights toward a unified understanding of the essential relationship between criticality and intelligence."
      },
      {
        "id": "oai:arXiv.org:2310.05273v3",
        "title": "Physics-tailored machine learning reveals unexpected physics in dusty plasmas",
        "link": "https://arxiv.org/abs/2310.05273",
        "author": "Wentao Yu, Eslam Abdelaleem, Ilya Nemenman, Justin C. Burton",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2310.05273v3 Announce Type: replace-cross \nAbstract: Dusty plasma is a mixture of ions, electrons, and macroscopic charged particles that is commonly found in space and planetary environments. The particles interact through Coulomb forces mediated by the surrounding plasma, and as a result, the effective forces between particles can be non-conservative and non-reciprocal. Machine learning (ML) models are a promising route to learn these complex forces, yet their structure should match the underlying physical constraints to provide useful insight. Here we demonstrate and experimentally validate an ML approach that incorporates physical intuition to infer force laws in a laboratory dusty plasma. Trained on 3D particle trajectories, the model accounts for inherent symmetries, non-identical particles, and learns the effective non-reciprocal forces between particles with exquisite accuracy (R^2>0.99). We validate the model by inferring particle masses in two independent yet consistent ways. The model's accuracy enables precise measurements of particle charge and screening length, discovering large deviations from common theoretical assumptions. Our ability to identify new physics from experimental data demonstrates how ML-powered approaches can guide new routes of scientific discovery in many-body systems. Furthermore, we anticipate our ML approach to be a starting point for inferring laws from dynamics in a wide range of many-body systems, from colloids to living organisms."
      },
      {
        "id": "oai:arXiv.org:2310.07891v4",
        "title": "A Theory of Non-Linear Feature Learning with One Gradient Step in Two-Layer Neural Networks",
        "link": "https://arxiv.org/abs/2310.07891",
        "author": "Behrad Moniri, Donghwan Lee, Hamed Hassani, Edgar Dobriban",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2310.07891v4 Announce Type: replace-cross \nAbstract: Feature learning is thought to be one of the fundamental reasons for the success of deep neural networks. It is rigorously known that in two-layer fully-connected neural networks under certain conditions, one step of gradient descent on the first layer can lead to feature learning; characterized by the appearance of a separated rank-one component -- spike -- in the spectrum of the feature matrix. However, with a constant gradient descent step size, this spike only carries information from the linear component of the target function and therefore learning non-linear components is impossible. We show that with a learning rate that grows with the sample size, such training in fact introduces multiple rank-one components, each corresponding to a specific polynomial feature. We further prove that the limiting large-dimensional and large sample training and test errors of the updated neural networks are fully characterized by these spikes. By precisely analyzing the improvement in the training and test errors, we demonstrate that these non-linear features can enhance learning."
      },
      {
        "id": "oai:arXiv.org:2310.12806v4",
        "title": "DCSI -- An improved measure of cluster separability based on separation and connectedness",
        "link": "https://arxiv.org/abs/2310.12806",
        "author": "Jana Gauss, Fabian Scheipl, Moritz Herrmann",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2310.12806v4 Announce Type: replace-cross \nAbstract: Whether class labels in a given data set correspond to meaningful clusters is crucial for the evaluation of clustering algorithms using real-world data sets. This property can be quantified by separability measures. The central aspects of separability for density-based clustering are between-class separation and within-class connectedness, and neither classification-based complexity measures nor cluster validity indices (CVIs) adequately incorporate them. A newly developed measure (density cluster separability index, DCSI) aims to quantify these two characteristics and can also be used as a CVI. Extensive experiments on synthetic data indicate that DCSI correlates strongly with the performance of DBSCAN measured via the adjusted Rand index (ARI) but lacks robustness when it comes to multi-class data sets with overlapping classes that are ill-suited for density-based hard clustering. Detailed evaluation on frequently used real-world data sets shows that DCSI can correctly identify touching or overlapping classes that do not correspond to meaningful density-based clusters."
      },
      {
        "id": "oai:arXiv.org:2311.13706v3",
        "title": "Multi-view Hybrid Graph Convolutional Network for Volume-to-mesh Reconstruction in Cardiovascular MRI",
        "link": "https://arxiv.org/abs/2311.13706",
        "author": "Nicol\\'as Gaggion, Benjamin A. Matheson, Yan Xia, Rodrigo Bonazzola, Nishant Ravikumar, Zeike A. Taylor, Diego H. Milone, Alejandro F. Frangi, Enzo Ferrante",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2311.13706v3 Announce Type: replace-cross \nAbstract: Cardiovascular magnetic resonance imaging is emerging as a crucial tool to examine cardiac morphology and function. Essential to this endeavour are anatomical 3D surface and volumetric meshes derived from CMR images, which facilitate computational anatomy studies, biomarker discovery, and in-silico simulations. Traditional approaches typically follow complex multi-step pipelines, first segmenting images and then reconstructing meshes, making them time-consuming and prone to error propagation. In response, we introduce HybridVNet, a novel architecture for direct image-to-mesh extraction seamlessly integrating standard convolutional neural networks with graph convolutions, which we prove can efficiently handle surface and volumetric meshes by encoding them as graph structures. To further enhance accuracy, we propose a multi-view HybridVNet architecture which processes both long axis and short axis CMR, showing that it can increase the performance of cardiac MR mesh generation. Our model combines traditional convolutional networks with variational graph generative models, deep supervision and mesh-specific regularisation. Experiments on a comprehensive dataset from the UK Biobank confirm the potential of HybridVNet to significantly advance cardiac imaging and computational cardiology by efficiently generating high-fidelity meshes from CMR images. Multi-view HybridVNet outperforms the state-of-the-art, achieving improvements of up to $\\sim$27\\% reduction in Mean Contour Distance (from 1.86 mm to 1.35 mm for the LV Myocardium), up to $\\sim$18\\% improvement in Hausdorff distance (from 4.74 mm to 3.89mm, for the LV Endocardium), and up to $\\sim$8\\% in Dice Coefficient (from 0.78 to 0.84, for the LV Myocardium), highlighting its superior accuracy."
      },
      {
        "id": "oai:arXiv.org:2406.07409v2",
        "title": "Accelerating Ill-conditioned Hankel Matrix Recovery via Structured Newton-like Descent",
        "link": "https://arxiv.org/abs/2406.07409",
        "author": "HanQin Cai, Longxiu Huang, Xiliang Lu, Juntao You",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2406.07409v2 Announce Type: replace-cross \nAbstract: This paper studies the robust Hankel recovery problem, which simultaneously removes the sparse outliers and fulfills missing entries from the partial observation. We propose a novel non-convex algorithm, coined Hankel Structured Newton-Like Descent (HSNLD), to tackle the robust Hankel recovery problem. HSNLD is highly efficient with linear convergence, and its convergence rate is independent of the condition number of the underlying Hankel matrix. The recovery guarantee has been established under some mild conditions. Numerical experiments on both synthetic and real datasets show the superior performance of HSNLD against state-of-the-art algorithms."
      },
      {
        "id": "oai:arXiv.org:2406.12123v3",
        "title": "ChatEMG: Synthetic Data Generation to Control a Robotic Hand Orthosis for Stroke",
        "link": "https://arxiv.org/abs/2406.12123",
        "author": "Jingxi Xu, Runsheng Wang, Siqi Shang, Ava Chen, Lauren Winterbottom, To-Liang Hsu, Wenxi Chen, Khondoker Ahmed, Pedro Leandro La Rotta, Xinyue Zhu, Dawn M. Nilsen, Joel Stein, Matei Ciocarlie",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2406.12123v3 Announce Type: replace-cross \nAbstract: Intent inferral on a hand orthosis for stroke patients is challenging due to the difficulty of data collection. Additionally, EMG signals exhibit significant variations across different conditions, sessions, and subjects, making it hard for classifiers to generalize. Traditional approaches require a large labeled dataset from the new condition, session, or subject to train intent classifiers; however, this data collection process is burdensome and time-consuming. In this paper, we propose ChatEMG, an autoregressive generative model that can generate synthetic EMG signals conditioned on prompts (i.e., a given sequence of EMG signals). ChatEMG enables us to collect only a small dataset from the new condition, session, or subject and expand it with synthetic samples conditioned on prompts from this new context. ChatEMG leverages a vast repository of previous data via generative training while still remaining context-specific via prompting. Our experiments show that these synthetic samples are classifier-agnostic and can improve intent inferral accuracy for different types of classifiers. We demonstrate that our complete approach can be integrated into a single patient session, including the use of the classifier for functional orthosis-assisted tasks. To the best of our knowledge, this is the first time an intent classifier trained partially on synthetic data has been deployed for functional control of an orthosis by a stroke survivor. Videos, source code, and additional information can be found at https://jxu.ai/chatemg."
      },
      {
        "id": "oai:arXiv.org:2406.19388v3",
        "title": "Taming Data and Transformers for Scalable Audio Generation",
        "link": "https://arxiv.org/abs/2406.19388",
        "author": "Moayed Haji-Ali, Willi Menapace, Aliaksandr Siarohin, Guha Balakrishnan, Sergey Tulyakov, Vicente Ordonez",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2406.19388v3 Announce Type: replace-cross \nAbstract: The scalability of ambient sound generators is hindered by data scarcity, insufficient caption quality, and limited scalability in model architecture. This work addresses these challenges by advancing both data and model scaling. First, we propose an efficient and scalable dataset collection pipeline tailored for ambient audio generation, resulting in AutoReCap-XL, the largest ambient audio-text dataset with over 47 million clips. To provide high-quality textual annotations, we propose AutoCap, a high-quality automatic audio captioning model. By adopting a Q-Former module and leveraging audio metadata, AutoCap substantially enhances caption quality, reaching a CIDEr score of $83.2$, a $3.2\\%$ improvement over previous captioning models. Finally, we propose GenAu, a scalable transformer-based audio generation architecture that we scale up to 1.25B parameters. We demonstrate its benefits from data scaling with synthetic captions as well as model size scaling. When compared to baseline audio generators trained at similar size and data scale, GenAu obtains significant improvements of $4.7\\%$ in FAD score, $11.1\\%$ in IS, and $13.5\\%$ in CLAP score. Our code, model checkpoints, and dataset are publicly available."
      },
      {
        "id": "oai:arXiv.org:2407.00085v2",
        "title": "Compressing Search with Language Models",
        "link": "https://arxiv.org/abs/2407.00085",
        "author": "Thomas Mulc, Jennifer L. Steele",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2407.00085v2 Announce Type: replace-cross \nAbstract: Millions of people turn to Google Search each day for information on things as diverse as new cars or flu symptoms. The terms that they enter contain valuable information on their daily intent and activities, but the information in these search terms has been difficult to fully leverage. User-defined categorical filters have been the most common way to shrink the dimensionality of search data to a tractable size for analysis and modeling. In this paper we present a new approach to reducing the dimensionality of search data while retaining much of the information in the individual terms without user-defined rules. Our contributions are two-fold: 1) we introduce SLaM Compression, a way to quantify search terms using pre-trained language models and create a representation of search data that has low dimensionality, is memory efficient, and effectively acts as a summary of search, and 2) we present CoSMo, a Constrained Search Model for estimating real world events using only search data. We demonstrate the efficacy of our contributions by estimating with high accuracy U.S. automobile sales and U.S. flu rates using only Google Search data."
      },
      {
        "id": "oai:arXiv.org:2407.15817v2",
        "title": "Enhancing Cell Instance Segmentation in Scanning Electron Microscopy Images via a Deep Contour Closing Operator",
        "link": "https://arxiv.org/abs/2407.15817",
        "author": "Florian Robert, Alexia Calovoulos, Laurent Facq, Fanny Decoeur, Etienne Gontier, Christophe F. Grosset, Baudouin Denis de Senneville",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2407.15817v2 Announce Type: replace-cross \nAbstract: Accurately segmenting and individualizing cells in SEM images is a highly promising technique for elucidating tissue architecture in oncology. While current AI-based methods are effective, errors persist, necessitating time-consuming manual corrections, particularly in areas where the quality of cell contours in the image is poor and requires gap filling. This study presents a novel AI-driven approach for refining cell boundary delineation to improve instance-based cell segmentation in SEM images, also reducing the necessity for residual manual correction. A CNN COp-Net is introduced to address gaps in cell contours, effectively filling in regions with deficient or absent information. The network takes as input cell contour probability maps with potentially inadequate or missing information and outputs corrected cell contour delineations. The lack of training data was addressed by generating low integrity probability maps using a tailored PDE. We showcase the efficacy of our approach in augmenting cell boundary precision using both private SEM images from PDX hepatoblastoma tissues and publicly accessible images datasets. The proposed cell contour closing operator exhibits a notable improvement in tested datasets, achieving respectively close to 50% (private data) and 10% (public data) increase in the accurately-delineated cell proportion compared to state-of-the-art methods. Additionally, the need for manual corrections was significantly reduced, therefore facilitating the overall digitalization process. Our results demonstrate a notable enhancement in the accuracy of cell instance segmentation, particularly in highly challenging regions where image quality compromises the integrity of cell boundaries, necessitating gap filling. Therefore, our work should ultimately facilitate the study of tumour tissue bioarchitecture in onconanotomy field."
      },
      {
        "id": "oai:arXiv.org:2408.07644v2",
        "title": "SigmaRL: A Sample-Efficient and Generalizable Multi-Agent Reinforcement Learning Framework for Motion Planning",
        "link": "https://arxiv.org/abs/2408.07644",
        "author": "Jianye Xu, Pan Hu, Bassam Alrifaee",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2408.07644v2 Announce Type: replace-cross \nAbstract: This paper introduces an open-source, decentralized framework named SigmaRL, designed to enhance both sample efficiency and generalization of multi-agent Reinforcement Learning (RL) for motion planning of connected and automated vehicles. Most RL agents exhibit a limited capacity to generalize, often focusing narrowly on specific scenarios, and are usually evaluated in similar or even the same scenarios seen during training. Various methods have been proposed to address these challenges, including experience replay and regularization. However, how observation design in RL affects sample efficiency and generalization remains an under-explored area. We address this gap by proposing five strategies to design information-dense observations, focusing on general features that are applicable to most traffic scenarios. We train our RL agents using these strategies on an intersection and evaluate their generalization through numerical experiments across completely unseen traffic scenarios, including a new intersection, an on-ramp, and a roundabout. Incorporating these information-dense observations reduces training times to under one hour on a single CPU, and the evaluation results reveal that our RL agents can effectively zero-shot generalize. Code: github.com/bassamlab/SigmaRL"
      },
      {
        "id": "oai:arXiv.org:2410.03041v3",
        "title": "Minmax Trend Filtering: Generalizations of Total Variation Denoising via a Local Minmax/Maxmin Formula",
        "link": "https://arxiv.org/abs/2410.03041",
        "author": "Sabyasachi Chatterjee",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.03041v3 Announce Type: replace-cross \nAbstract: Total Variation Denoising (TVD) is a fundamental denoising and smoothing method. In this article, we identify a new local minmax/maxmin formula producing two estimators which sandwich the univariate TVD estimator at every point. Operationally, this formula gives a local definition of TVD as a minmax/maxmin of a simple function of local averages. Moreover we find that this minmax/maxmin formula is generalizeable and can be used to define other TVD like estimators. In this article we propose and study higher order polynomial versions of TVD which are defined pointwise lying between minmax and maxmin optimizations of penalized local polynomial regressions over intervals of different scales. These appear to be new nonparametric regression methods, different from usual Trend Filtering and any other existing method in the nonparametric regression toolbox. We call these estimators Minmax Trend Filtering (MTF). We show how the proposed local definition of TVD/MTF estimator makes it tractable to bound pointwise estimation errors in terms of a local bias variance like trade-off. This type of local analysis of TVD/MTF is new and arguably simpler than existing analyses of TVD/Trend Filtering. In particular, apart from minimax rate optimality over bounded variation and piecewise polynomial classes, our pointwise estimation error bounds also enable us to derive local rates of convergence for (locally) Holder Smooth signals. These local rates offer a new pointwise explanation of local adaptivity of TVD/MTF instead of global (MSE) based justifications."
      },
      {
        "id": "oai:arXiv.org:2410.23912v2",
        "title": "RL-STaR: Theoretical Analysis of Reinforcement Learning Frameworks for Self-Taught Reasoner",
        "link": "https://arxiv.org/abs/2410.23912",
        "author": "Fu-Chieh Chang, Yu-Ting Lee, Hui-Ying Shih, Yi Hsuan Tseng, Pei-Yuan Wu",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.23912v2 Announce Type: replace-cross \nAbstract: The reasoning abilities of large language models (LLMs) have improved with chain-of-thought (CoT) prompting, allowing models to solve complex tasks stepwise. However, training CoT capabilities requires detailed reasoning data, which is often scarce. The self-taught reasoner (STaR) framework addresses this by using reinforcement learning to automatically generate reasoning steps, reducing reliance on human-labeled data. Although STaR and its variants have demonstrated empirical success, a theoretical foundation explaining these improvements is lacking. This work provides a theoretical framework for understanding the effectiveness of reinforcement learning on CoT reasoning and STaR. Our contributions are: (1) criteria for the quality of pre-trained models necessary to initiate effective reasoning improvement; (2) an analysis of policy improvement, showing why LLM reasoning improves iteratively with STaR; (3) conditions for convergence to an optimal reasoning policy; and (4) an examination of STaR's robustness, explaining how it can improve reasoning even when incorporating occasional incorrect steps; This framework aims to bridge empirical findings with theoretical insights, advancing reinforcement learning approaches for reasoning in LLMs."
      },
      {
        "id": "oai:arXiv.org:2411.10034v2",
        "title": "EveGuard: Defeating Vibration-based Side-Channel Eavesdropping with Audio Adversarial Perturbations",
        "link": "https://arxiv.org/abs/2411.10034",
        "author": "Jung-Woo Chang, Ke Sun, David Xia, Xinyu Zhang, Farinaz Koushanfar",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.10034v2 Announce Type: replace-cross \nAbstract: Vibrometry-based side channels pose a significant privacy risk, exploiting sensors like mmWave radars, light sensors, and accelerometers to detect vibrations from sound sources or proximate objects, enabling speech eavesdropping. Despite various proposed defenses, these involve costly hardware solutions with inherent physical limitations. This paper presents EveGuard, a software-driven defense framework that creates adversarial audio, protecting voice privacy from side channels without compromising human perception. We leverage the distinct sensing capabilities of side channels and traditional microphones, where side channels capture vibrations and microphones record changes in air pressure, resulting in different frequency responses. EveGuard first proposes a perturbation generator model (PGM) that effectively suppresses sensor-based eavesdropping while maintaining high audio quality. Second, to enable end-to-end training of PGM, we introduce a new domain translation task called Eve-GAN for inferring an eavesdropped signal from a given audio. We further apply few-shot learning to mitigate the data collection overhead for Eve-GAN training. Our extensive experiments show that EveGuard achieves a protection rate of more than 97 percent from audio classifiers and significantly hinders eavesdropped audio reconstruction. We further validate the performance of EveGuard across three adaptive attack mechanisms. We have conducted a user study to verify the perceptual quality of our perturbed audio."
      },
      {
        "id": "oai:arXiv.org:2411.16693v2",
        "title": "UQ of 2D Slab Burner DNS: Surrogates, Uncertainty Propagation, and Parameter Calibration",
        "link": "https://arxiv.org/abs/2411.16693",
        "author": "Georgios Georgalis, Alejandro Becerra, Kenneth Budzinski, Matthew McGurn, Danial Faghihi, Paul E. DesJardin, Abani Patra",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.16693v2 Announce Type: replace-cross \nAbstract: The goal of this paper is to demonstrate and address challenges related to all aspects of performing a complete uncertainty quantification analysis of a complicated physics-based simulation like a 2D slab burner direct numerical simulation (DNS). The UQ framework includes the development of data-driven surrogate models, propagation of parametric uncertainties to the fuel regression rate--the primary quantity of interest--and Bayesian calibration of the latent heat of sublimation and a chemical reaction temperature exponent using experimental data. Two surrogate models, a Gaussian Process (GP) and a Hierarchical Multiscale Surrogate (HMS) were constructed using an ensemble of 64 simulations generated via Latin Hypercube sampling. HMS is superior for prediction demonstrated by cross-validation and able to achieve an error < 15% when predicting multiscale boundary quantities just from a few far field inputs. Subsequent Bayesian calibration of chemical kinetics and fuel response parameters against experimental observations showed that the default values used in the DNS should be higher to better match measurements. This study highlights the importance of surrogate model selection and parameter calibration in quantifying uncertainty in predictions of fuel regression rates in complex combustion systems."
      },
      {
        "id": "oai:arXiv.org:2411.19379v3",
        "title": "Marconi: Prefix Caching for the Era of Hybrid LLMs",
        "link": "https://arxiv.org/abs/2411.19379",
        "author": "Rui Pan, Zhuang Wang, Zhen Jia, Can Karakus, Luca Zancato, Tri Dao, Yida Wang, Ravi Netravali",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.19379v3 Announce Type: replace-cross \nAbstract: Hybrid models that combine the language modeling capabilities of Attention layers with the efficiency of Recurrent layers (e.g., State Space Models) have gained traction in practically supporting long contexts in Large Language Model serving. Yet, the unique properties of these models complicate the usage of complementary efficiency optimizations such as prefix caching that skip redundant computations across requests. Most notably, their use of in-place state updates for recurrent layers precludes rolling back cache entries for partial sequence overlaps, and instead mandates only exact-match cache hits; the effect is a deluge of (large) cache entries per sequence, most of which yield minimal reuse opportunities. We present Marconi, the first system that supports efficient prefix caching with Hybrid LLMs. Key to Marconi are its novel admission and eviction policies that more judiciously assess potential cache entries based not only on recency, but also on (1) forecasts of their reuse likelihood across a taxonomy of different hit scenarios, and (2) the compute savings that hits deliver relative to memory footprints. Across diverse workloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token hit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix caching systems."
      },
      {
        "id": "oai:arXiv.org:2412.11943v2",
        "title": "autrainer: A Modular and Extensible Deep Learning Toolkit for Computer Audition Tasks",
        "link": "https://arxiv.org/abs/2412.11943",
        "author": "Simon Rampp, Andreas Triantafyllopoulos, Manuel Milling, Bj\\\"orn W. Schuller",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.11943v2 Announce Type: replace-cross \nAbstract: This work introduces the key operating principles for autrainer, our new deep learning training framework for computer audition tasks. autrainer is a PyTorch-based toolkit that allows for rapid, reproducible, and easily extensible training on a variety of different computer audition tasks. Concretely, autrainer offers low-code training and supports a wide range of neural networks as well as preprocessing routines. In this work, we present an overview of its inner workings and key capabilities."
      },
      {
        "id": "oai:arXiv.org:2412.17534v2",
        "title": "CiteBART: Learning to Generate Citations for Local Citation Recommendation",
        "link": "https://arxiv.org/abs/2412.17534",
        "author": "Ege Yi\\u{g}it \\c{C}elik, Selma Tekir",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.17534v2 Announce Type: replace-cross \nAbstract: Local citation recommendation (LCR) suggests a set of papers for a citation placeholder within a given context. The task has evolved as generative approaches have become more promising than the traditional pre-fetch and re-rank-based state-of-the-art approaches. This paper introduces citation-specific pre-training within an encoder-decoder architecture, where author-date citation tokens are masked to learn to reconstruct them to fulfill LCR. There are two variants for this pre-training. In the local context-only base scheme (CiteBART-Base), the citation token in a local context is masked to learn to predict the citation. The global version (CiteBART-Global) extends the local context with the citing paper's title and abstract to enrich the learning signal. CiteBART-Global achieves state-of-the-art performance on LCR benchmarks except for the FullTextPeerRead dataset, which is quite small to see the advantage of generative pre-training. The effect is significant in the larger benchmarks, e.g., Refseer and ArXiv., with the Refseer benchmark-trained model emerging as the best-performing model. We perform comprehensive experiments, including an ablation study, a qualitative analysis, and a taxonomy of hallucinations with detailed statistics. Our analyses confirm that CiteBART-Global has a cross-dataset generalization capability; the macro hallucination rate (MaHR) at the top-3 predictions is 4\\%, and when the ground-truth is in the top-k prediction list, the hallucination tendency in the other predictions drops significantly."
      },
      {
        "id": "oai:arXiv.org:2412.21037v2",
        "title": "TangoFlux: Super Fast and Faithful Text to Audio Generation with Flow Matching and Clap-Ranked Preference Optimization",
        "link": "https://arxiv.org/abs/2412.21037",
        "author": "Chia-Yu Hung, Navonil Majumder, Zhifeng Kong, Ambuj Mehrish, Amir Ali Bagherzadeh, Chuan Li, Rafael Valle, Bryan Catanzaro, Soujanya Poria",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.21037v2 Announce Type: replace-cross \nAbstract: We introduce TangoFlux, an efficient Text-to-Audio (TTA) generative model with 515M parameters, capable of generating up to 30 seconds of 44.1kHz audio in just 3.7 seconds on a single A40 GPU. A key challenge in aligning TTA models lies in the difficulty of creating preference pairs, as TTA lacks structured mechanisms like verifiable rewards or gold-standard answers available for Large Language Models (LLMs). To address this, we propose CLAP-Ranked Preference Optimization (CRPO), a novel framework that iteratively generates and optimizes preference data to enhance TTA alignment. We demonstrate that the audio preference dataset generated using CRPO outperforms existing alternatives. With this framework, TangoFlux achieves state-of-the-art performance across both objective and subjective benchmarks. We open source all code and models to support further research in TTA generation."
      },
      {
        "id": "oai:arXiv.org:2503.17845v2",
        "title": "Graphical Transformation Models",
        "link": "https://arxiv.org/abs/2503.17845",
        "author": "Matthias Herp, Johannes Brachem, Michael Altenbuchinger, Thomas Kneib",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.17845v2 Announce Type: replace-cross \nAbstract: Graphical Transformation Models (GTMs) are introduced as a novel approach to effectively model multivariate data with intricate marginals and complex dependency structures non-parametrically, while maintaining interpretability through the identification of varying conditional independencies. GTMs extend multivariate transformation models by replacing the Gaussian copula with a custom-designed multivariate transformation, offering two major advantages. Firstly, GTMs can capture more complex interdependencies using penalized splines, which also provide an efficient regularization scheme. Secondly, we demonstrate how to approximately regularize GTMs using a lasso penalty towards pairwise conditional independencies, akin to Gaussian graphical models. The model's robustness and effectiveness are validated through simulations, showcasing its ability to accurately learn parametric vine copulas and identify conditional independencies. Additionally, the model is applied to a benchmark astrophysics dataset, where the GTM demonstrates favorable performance compared to non-parametric vine copulas in learning complex multivariate distributions."
      },
      {
        "id": "oai:arXiv.org:2503.19190v2",
        "title": "Universal Architectures for the Learning of Polyhedral Norms and Convex Regularizers",
        "link": "https://arxiv.org/abs/2503.19190",
        "author": "Michael Unser, Stanislas Ducotterd",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.19190v2 Announce Type: replace-cross \nAbstract: This paper addresses the task of learning convex regularizers to guide the reconstruction of images from limited data. By imposing that the reconstruction be amplitude-equivariant, we narrow down the class of admissible functionals to those that can be expressed as a power of a seminorm. We then show that such functionals can be approximated to arbitrary precision with the help of polyhedral norms. In particular, we identify two dual parameterizations of such systems: (i) a synthesis form with an $\\ell_1$-penalty that involves some learnable dictionary; and (ii) an analysis form with an $\\ell_\\infty$-penalty that involves a trainable regularization operator. After having provided geometric insights and proved that the two forms are universal, we propose an implementation that relies on a specific architecture (tight frame with a weighted $\\ell_1$ penalty) that is easy to train. We illustrate its use for denoising and the reconstruction of biomedical images. We find that the proposed framework outperforms the sparsity-based methods of compressed sensing, while it offers essentially the same convergence and robustness guarantees."
      },
      {
        "id": "oai:arXiv.org:2503.19949v3",
        "title": "Automated Video-EEG Analysis in Epilepsy Studies: Advances and Challenges",
        "link": "https://arxiv.org/abs/2503.19949",
        "author": "Valerii A. Zuev, Elena G. Salmagambetova, Stepan N. Djakov, Lev V. Utkin",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.19949v3 Announce Type: replace-cross \nAbstract: Epilepsy is typically diagnosed through electroencephalography (EEG) and long-term video-EEG (vEEG) monitoring. The manual analysis of vEEG recordings is time-consuming, necessitating automated tools for seizure detection. Recent advancements in machine learning have shown promise in real-time seizure detection and prediction using EEG and video data. However, diversity of seizure symptoms, markup ambiguities, and limited availability of multimodal datasets hinder progress. This paper reviews the latest developments in automated video-EEG analysis and discusses the integration of multimodal data. We also propose a novel pipeline for treatment effect estimation from vEEG data using concept-based learning, offering a pathway for future research in this domain."
      },
      {
        "id": "oai:arXiv.org:2504.02670v2",
        "title": "Affordable AI Assistants with Knowledge Graph of Thoughts",
        "link": "https://arxiv.org/abs/2504.02670",
        "author": "Maciej Besta, Lorenzo Paleari, Jia Hao Andrea Jiang, Robert Gerstenberger, You Wu, Patrick Iff, Ales Kubicek, Piotr Nyczyk, Diana Khimey, J\\'on Gunnar Hannesson, Grzegorz Kwa\\'sniewski, Marcin Copik, Hubert Niewiadomski, Torsten Hoefler",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02670v2 Announce Type: replace-cross \nAbstract: Large Language Models (LLMs) are revolutionizing the development of AI assistants capable of performing diverse tasks across domains. However, current state-of-the-art LLM-driven agents face significant challenges, including high operational costs and limited success rates on complex benchmarks like GAIA. To address these issues, we propose the Knowledge Graph of Thoughts (KGoT), an innovative AI assistant architecture that integrates LLM reasoning with dynamically constructed knowledge graphs (KGs). KGoT extracts and structures task-relevant knowledge into a dynamic KG representation, iteratively enhanced through external tools such as math solvers, web crawlers, and Python scripts. Such structured representation of task-relevant knowledge enables low-cost models to solve complex tasks effectively. For example, KGoT achieves a 29% improvement in task success rates on the GAIA benchmark compared to Hugging Face Agents with GPT-4o mini, while reducing costs by over 36x compared to GPT-4o. Improvements for recent reasoning models are similar, e.g., 36% and 37.5% for Qwen2.5-32B and Deepseek-R1-70B, respectively. KGoT offers a scalable, affordable, and high-performing solution for AI assistants."
      },
      {
        "id": "oai:arXiv.org:2504.03699v2",
        "title": "Reinforcing Clinical Decision Support through Multi-Agent Systems and Ethical AI Governance",
        "link": "https://arxiv.org/abs/2504.03699",
        "author": "Ying-Jung Chen, Chi-Sheng Chen, Ahmad Albarqawi",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03699v2 Announce Type: replace-cross \nAbstract: In the age of data-driven medicine, it is paramount to include explainable and ethically managed artificial intelligence in explaining clinical decision support systems to achieve trustworthy and effective patient care. The focus of this paper is on a new architecture of a multi-agent system for clinical decision support that uses modular agents to analyze laboratory results, vital signs, and the clinical context and then integrates these results to drive predictions and validate outcomes. We describe our implementation with the eICU database to run lab-analysis-specific agents, vitals-only interpreters, and contextual reasoners and then run the prediction module and a validation agent. Everything is a transparent implementation of business logic, influenced by the principles of ethical AI governance such as Autonomy, Fairness, and Accountability. It provides visible results that this agent-based framework not only improves on interpretability and accuracy but also on reinforcing trust in AI-assisted decisions in an intensive care setting."
      },
      {
        "id": "oai:arXiv.org:2504.04372v2",
        "title": "How Accurately Do Large Language Models Understand Code?",
        "link": "https://arxiv.org/abs/2504.04372",
        "author": "Sabaat Haroon, Ahmad Faraz Khan, Ahmad Humayun, Waris Gill, Abdul Haddi Amjad, Ali R. Butt, Mohammad Taha Khan, Muhammad Ali Gulzar",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.04372v2 Announce Type: replace-cross \nAbstract: Large Language Models (LLMs) are increasingly used in post-development tasks such as code repair and testing. A key factor in these tasks' success is the model's deep understanding of code. However, the extent to which LLMs truly understand code remains largely unevaluated. Quantifying code comprehension is challenging due to its abstract nature and the lack of a standardized metric. Previously, this was assessed through developer surveys, which are not feasible for evaluating LLMs. Existing LLM benchmarks focus primarily on code generation, fundamentally different from code comprehension. Additionally, fixed benchmarks quickly become obsolete as they become part of the training data. This paper presents the first large-scale empirical investigation into LLMs' ability to understand code. Inspired by mutation testing, we use an LLM's fault-finding ability as a proxy for its deep code understanding. This approach is based on the insight that a model capable of identifying subtle functional discrepancies must understand the code well. We inject faults in real-world programs and ask the LLM to localize them, ensuring the specifications suffice for fault localization. Next, we apply semantic-preserving code mutations (SPMs) to the faulty programs and test whether the LLMs still locate the faults, verifying their confidence in code understanding. We evaluate nine popular LLMs on 600,010 debugging tasks from 670 Java and 637 Python programs. We find that LLMs lose the ability to debug the same bug in 78% of faulty programs when SPMs are applied, indicating a shallow understanding of code and reliance on features irrelevant to semantics. We also find that LLMs understand code earlier in the program better than later. This suggests that LLMs' code comprehension remains tied to lexical and syntactic features due to tokenization designed for natural languages, which overlooks code semantics."
      },
      {
        "id": "oai:arXiv.org:2504.05500v2",
        "title": "Prism: Dynamic and Flexible Benchmarking of LLMs Code Generation with Monte Carlo Tree Search",
        "link": "https://arxiv.org/abs/2504.05500",
        "author": "Vahid Majdinasab, Amin Nikanjam, Foutse Khomh",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.05500v2 Announce Type: replace-cross \nAbstract: The rapid advancement of Large Language Models (LLMs) has outpaced traditional evaluation methods. Static benchmarks fail to capture the depth and breadth of LLM capabilities and eventually become obsolete, while most dynamic approaches either rely too heavily on LLM-based evaluation or remain constrained by predefined test sets. We introduce Prism, a flexible, dynamic benchmarking framework designed for comprehensive LLM assessment. Prism builds on three key components: (1) a tree-based state representation that models evaluation as a Markov Decision Process, (2) a Monte Carlo Tree Search algorithm adapted to uncover challenging evaluation scenarios, and (3) a multi-agent evaluation pipeline that enables simultaneous assessment of diverse capabilities. To ensure robust evaluation, Prism integrates structural measurements of tree exploration patterns with performance metrics across difficulty levels, providing detailed diagnostics of error patterns, test coverage, and solution approaches. Through extensive experiments on five state-of-the-art LLMs, we analyze how model architecture and scale influence code generation performance across varying task difficulties. Our results demonstrate Prism's effectiveness as a dynamic benchmark that evolves with model advancements while offering deeper insights into their limitations."
      },
      {
        "id": "oai:arXiv.org:2504.06293v2",
        "title": "Generative AI Enhanced Financial Risk Management Information Retrieval",
        "link": "https://arxiv.org/abs/2504.06293",
        "author": "Amin Haeri, Jonathan Vitrano, Mahdi Ghelichi",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.06293v2 Announce Type: replace-cross \nAbstract: Risk management in finance involves recognizing, evaluating, and addressing financial risks to maintain stability and ensure regulatory compliance. Extracting relevant insights from extensive regulatory documents is a complex challenge requiring advanced retrieval and language models. This paper introduces RiskData, a dataset specifically curated for finetuning embedding models in risk management, and RiskEmbed, a finetuned embedding model designed to improve retrieval accuracy in financial question-answering systems. The dataset is derived from 94 regulatory guidelines published by the Office of the Superintendent of Financial Institutions (OSFI) from 1991 to 2024. We finetune a state-of-the-art sentence BERT embedding model to enhance domain-specific retrieval performance typically for Retrieval-Augmented Generation (RAG) systems. Experimental results demonstrate that RiskEmbed significantly outperforms general-purpose and financial embedding models, achieving substantial improvements in ranking metrics. By open-sourcing both the dataset and the model, we provide a valuable resource for financial institutions and researchers aiming to develop more accurate and efficient risk management AI solutions."
      },
      {
        "id": "oai:arXiv.org:2504.06301v2",
        "title": "Subjective Visual Quality Assessment for High-Fidelity Learning-Based Image Compression",
        "link": "https://arxiv.org/abs/2504.06301",
        "author": "Mohsen Jenadeleh, Jon Sneyers, Panqi Jia, Shima Mohammadi, Joao Ascenso, Dietmar Saupe",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.06301v2 Announce Type: replace-cross \nAbstract: Learning-based image compression methods have recently emerged as promising alternatives to traditional codecs, offering improved rate-distortion performance and perceptual quality. JPEG AI represents the latest standardized framework in this domain, leveraging deep neural networks for high-fidelity image reconstruction. In this study, we present a comprehensive subjective visual quality assessment of JPEG AI-compressed images using the JPEG AIC-3 methodology, which quantifies perceptual differences in terms of Just Noticeable Difference (JND) units. We generated a dataset of 50 compressed images with fine-grained distortion levels from five diverse sources. A large-scale crowdsourced experiment collected 96,200 triplet responses from 459 participants. We reconstructed JND-based quality scales using a unified model based on boosted and plain triplet comparisons. Additionally, we evaluated the alignment of multiple objective image quality metrics with human perception in the high-fidelity range. The CVVDP metric achieved the overall highest performance; however, most metrics including CVVDP were overly optimistic in predicting the quality of JPEG AI-compressed images. These findings emphasize the necessity for rigorous subjective evaluations in the development and benchmarking of modern image codecs, particularly in the high-fidelity range. Another technical contribution is the introduction of the well-known Meng-Rosenthal-Rubin statistical test to the field of Quality of Experience research. This test can reliably assess the significance of difference in performance of quality metrics in terms of correlation between metrics and ground truth. The complete dataset, including all subjective scores, is publicly available at https://github.com/jpeg-aic/dataset-JPEG-AI-SDR25."
      },
      {
        "id": "oai:arXiv.org:2504.06385v2",
        "title": "Fast Globally Optimal and Geometrically Consistent 3D Shape Matching",
        "link": "https://arxiv.org/abs/2504.06385",
        "author": "Paul Roetzer, Florian Bernard",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.06385v2 Announce Type: replace-cross \nAbstract: Geometric consistency, i.e. the preservation of neighbourhoods, is a natural and strong prior in 3D shape matching. Geometrically consistent matchings are crucial for many downstream applications, such as texture transfer or statistical shape modelling. Yet, in practice, geometric consistency is often overlooked, or only achieved under severely limiting assumptions (e.g. a good initialisation). In this work, we propose a novel formalism for computing globally optimal and geometrically consistent matchings between 3D shapes which is scalable in practice. Our key idea is to represent the surface of the source shape as a collection of cyclic paths, which are then consistently matched to the target shape. Mathematically, we construct a hyper product graph (between source and target shape), and then cast 3D shape matching as a minimum-cost circulation flow problem in this hyper graph, which yields global geometrically consistent matchings between both shapes. We empirically show that our formalism is efficiently solvable and that it leads to high-quality results."
      },
      {
        "id": "oai:arXiv.org:2504.06553v2",
        "title": "ASHiTA: Automatic Scene-grounded HIerarchical Task Analysis",
        "link": "https://arxiv.org/abs/2504.06553",
        "author": "Yun Chang, Leonor Fermoselle, Duy Ta, Bernadette Bucher, Luca Carlone, Jiuguang Wang",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.06553v2 Announce Type: replace-cross \nAbstract: While recent work in scene reconstruction and understanding has made strides in grounding natural language to physical 3D environments, it is still challenging to ground abstract, high-level instructions to a 3D scene. High-level instructions might not explicitly invoke semantic elements in the scene, and even the process of breaking a high-level task into a set of more concrete subtasks, a process called hierarchical task analysis, is environment-dependent. In this work, we propose ASHiTA, the first framework that generates a task hierarchy grounded to a 3D scene graph by breaking down high-level tasks into grounded subtasks. ASHiTA alternates LLM-assisted hierarchical task analysis, to generate the task breakdown, with task-driven 3D scene graph construction to generate a suitable representation of the environment. Our experiments show that ASHiTA performs significantly better than LLM baselines in breaking down high-level tasks into environment-dependent subtasks and is additionally able to achieve grounding performance comparable to state-of-the-art methods."
      },
      {
        "id": "oai:arXiv.org:2504.06575v2",
        "title": "Defending LLM Watermarking Against Spoofing Attacks with Contrastive Representation Learning",
        "link": "https://arxiv.org/abs/2504.06575",
        "author": "Li An, Yujian Liu, Yepeng Liu, Yang Zhang, Yuheng Bu, Shiyu Chang",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.06575v2 Announce Type: replace-cross \nAbstract: Watermarking has emerged as a promising technique for detecting texts generated by LLMs. Current research has primarily focused on three design criteria: high quality of the watermarked text, high detectability, and robustness against removal attack. However, the security against spoofing attacks remains relatively understudied. For example, a piggyback attack can maliciously alter the meaning of watermarked text-transforming it into hate speech-while preserving the original watermark, thereby damaging the reputation of the LLM provider. We identify two core challenges that make defending against spoofing difficult: (1) the need for watermarks to be both sensitive to semantic-distorting changes and insensitive to semantic-preserving edits, and (2) the contradiction between the need to detect global semantic shifts and the local, auto-regressive nature of most watermarking schemes. To address these challenges, we propose a semantic-aware watermarking algorithm that post-hoc embeds watermarks into a given target text while preserving its original meaning. Our method introduces a semantic mapping model, which guides the generation of a green-red token list, contrastively trained to be sensitive to semantic-distorting changes and insensitive to semantic-preserving changes. Experiments on two standard benchmarks demonstrate strong robustness against removal attacks and security against spoofing attacks, including sentiment reversal and toxic content insertion, while maintaining high watermark detectability. Our approach offers a significant step toward more secure and semantically aware watermarking for LLMs. Our code is available at https://github.com/UCSB-NLP-Chang/contrastive-watermark."
      }
    ]
  },
  "https://rss.arxiv.org/rss/cs.SD+eess.AS": {
    "feed": {
      "title": "cs.SD, eess.AS updates on arXiv.org",
      "link": "http://rss.arxiv.org/rss/cs.SD+eess.AS",
      "updated": "Fri, 11 Apr 2025 04:02:02 +0000",
      "published": "Fri, 11 Apr 2025 00:00:00 -0400"
    },
    "entries": [
      {
        "id": "oai:arXiv.org:2504.07153v1",
        "title": "Artificial intelligence in creating, representing or expressing an immersive soundscape",
        "link": "https://arxiv.org/abs/2504.07153",
        "author": "Rima Ayoubi (CRENAU, AAU), Laurent Lescop (CRENAU, AAU), Sang Bum Park",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07153v1 Announce Type: new \nAbstract: In today's tech-driven world, significant advancements in artificial intelligence and virtual reality have emerged. These developments drive research into exploring their intersection in the realm of soundscape. Not only do these technologies raise questions about how they will revolutionize the way we design and create soundscapes, but they also draw significant inquiries into their impact on human perception, understanding, and expression of auditory environments. This paper aims to review and discuss the latest applications of artificial intelligence in this domain. It explores how artificial intelligence can be utilized to create a virtual reality immersive soundscape, exploiting its ability to recognize complex patterns in various forms of data. This includes translating between different modalities such as text, sounds, and animations as well as predicting and generating data across these domains. It addresses questions surrounding artificial intelligence's capacity to predict, detect, and comprehend soundscape data, ultimately aiming to bridge the gap between sound and other forms of human-readable data.  1."
      },
      {
        "id": "oai:arXiv.org:2504.07345v1",
        "title": "Quantum-Inspired Genetic Algorithm for Robust Source Separation in Smart City Acoustics",
        "link": "https://arxiv.org/abs/2504.07345",
        "author": "Minh K. Quan, Mayuri Wijayasundara, Sujeeva Setunge, Pubudu N. Pathirana",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07345v1 Announce Type: new \nAbstract: The cacophony of urban sounds presents a significant challenge for smart city applications that rely on accurate acoustic scene analysis. Effectively analyzing these complex soundscapes, often characterized by overlapping sound sources, diverse acoustic events, and unpredictable noise levels, requires precise source separation. This task becomes more complicated when only limited training data is available. This paper introduces a novel Quantum-Inspired Genetic Algorithm (p-QIGA) for source separation, drawing inspiration from quantum information theory to enhance acoustic scene analysis in smart cities. By leveraging quantum superposition for efficient solution space exploration and entanglement to handle correlated sources, p-QIGA achieves robust separation even with limited data. These quantum-inspired concepts are integrated into a genetic algorithm framework to optimize source separation parameters. The effectiveness of our approach is demonstrated on two datasets: the TAU Urban Acoustic Scenes 2020 Mobile dataset, representing typical urban soundscapes, and the Silent Cities dataset, capturing quieter urban environments during the COVID-19 pandemic. Experimental results show that the p-QIGA achieves accuracy comparable to state-of-the-art methods while exhibiting superior resilience to noise and limited training data, achieving up to 8.2 dB signal-to-distortion ratio (SDR) in noisy environments and outperforming baseline methods by up to 2 dB with only 10% of the training data. This research highlights the potential of p-QIGA to advance acoustic signal processing in smart cities, particularly for noise pollution monitoring and acoustic surveillance."
      },
      {
        "id": "oai:arXiv.org:2504.07406v1",
        "title": "Towards Generalizability to Tone and Content Variations in the Transcription of Amplifier Rendered Electric Guitar Audio",
        "link": "https://arxiv.org/abs/2504.07406",
        "author": "Yu-Hua Chen, Yuan-Chiao Cheng, Yen-Tung Yeh, Jui-Te Wu, Jyh-Shing Roger Jang, Yi-Hsuan Yang",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07406v1 Announce Type: new \nAbstract: Transcribing electric guitar recordings is challenging due to the scarcity of diverse datasets and the complex tone-related variations introduced by amplifiers, cabinets, and effect pedals. To address these issues, we introduce EGDB-PG, a novel dataset designed to capture a wide range of tone-related characteristics across various amplifier-cabinet configurations. In addition, we propose the Tone-informed Transformer (TIT), a Transformer-based transcription model enhanced with a tone embedding mechanism that leverages learned representations to improve the model's adaptability to tone-related nuances. Experiments demonstrate that TIT, trained on EGDB-PG, outperforms existing baselines across diverse amplifier types, with transcription accuracy improvements driven by the dataset's diversity and the tone embedding technique. Through detailed benchmarking and ablation studies, we evaluate the impact of tone augmentation, content augmentation, audio normalization, and tone embedding on transcription performance. This work advances electric guitar transcription by overcoming limitations in dataset diversity and tone modeling, providing a robust foundation for future research."
      },
      {
        "id": "oai:arXiv.org:2504.07652v1",
        "title": "Categorical Unsupervised Variational Acoustic Clustering",
        "link": "https://arxiv.org/abs/2504.07652",
        "author": "Luan Vin\\'icius Fiorio, Ivana Nikoloska, Ronald M. Aarts",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07652v1 Announce Type: new \nAbstract: We propose a categorical approach for unsupervised variational acoustic clustering of audio data in the time-frequency domain. The consideration of a categorical distribution enforces sharper clustering even when data points strongly overlap in time and frequency, which is the case for most datasets of urban acoustic scenes. To this end, we use a Gumbel-Softmax distribution as a soft approximation to the categorical distribution, allowing for training via backpropagation. In this settings, the softmax temperature serves as the main mechanism to tune clustering performance. The results show that the proposed model can obtain impressive clustering performance for all considered datasets, even when data points strongly overlap in time and frequency."
      },
      {
        "id": "oai:arXiv.org:2504.07776v1",
        "title": "SlimSpeech: Lightweight and Efficient Text-to-Speech with Slim Rectified Flow",
        "link": "https://arxiv.org/abs/2504.07776",
        "author": "Kaidi Wang, Wenhao Guan, Shenghui Lu, Jianglong Yao, Lin Li, Qingyang Hong",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07776v1 Announce Type: new \nAbstract: Recently, flow matching based speech synthesis has significantly enhanced the quality of synthesized speech while reducing the number of inference steps. In this paper, we introduce SlimSpeech, a lightweight and efficient speech synthesis system based on rectified flow. We have built upon the existing speech synthesis method utilizing the rectified flow model, modifying its structure to reduce parameters and serve as a teacher model. By refining the reflow operation, we directly derive a smaller model with a more straight sampling trajectory from the larger model, while utilizing distillation techniques to further enhance the model performance. Experimental results demonstrate that our proposed method, with significantly reduced model parameters, achieves comparable performance to larger models through one-step sampling."
      },
      {
        "id": "oai:arXiv.org:2504.07858v1",
        "title": "Empowering Global Voices: A Data-Efficient, Phoneme-Tone Adaptive Approach to High-Fidelity Speech Synthesis",
        "link": "https://arxiv.org/abs/2504.07858",
        "author": "Yizhong Geng, Jizhuo Xu, Zeyu Liang, Jinghan Yang, Xiaoyi Shi, Xiaoyu Shen",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07858v1 Announce Type: new \nAbstract: Text-to-speech (TTS) technology has achieved impressive results for widely spoken languages, yet many under-resourced languages remain challenged by limited data and linguistic complexities. In this paper, we present a novel methodology that integrates a data-optimized framework with an advanced acoustic model to build high-quality TTS systems for low-resource scenarios. We demonstrate the effectiveness of our approach using Thai as an illustrative case, where intricate phonetic rules and sparse resources are effectively addressed. Our method enables zero-shot voice cloning and improved performance across diverse client applications, ranging from finance to healthcare, education, and law. Extensive evaluations - both subjective and objective - confirm that our model meets state-of-the-art standards, offering a scalable solution for TTS production in data-limited settings, with significant implications for broader industry adoption and multilingual accessibility."
      },
      {
        "id": "oai:arXiv.org:2504.07229v1",
        "title": "Visual-Aware Speech Recognition for Noisy Scenarios",
        "link": "https://arxiv.org/abs/2504.07229",
        "author": "Lakshmipathi Balaji, Karan Singla",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07229v1 Announce Type: cross \nAbstract: Humans have the ability to utilize visual cues, such as lip movements and visual scenes, to enhance auditory perception, particularly in noisy environments. However, current Automatic Speech Recognition (ASR) or Audio-Visual Speech Recognition (AVSR) models often struggle in noisy scenarios. To solve this task, we propose a model that improves transcription by correlating noise sources to visual cues. Unlike works that rely on lip motion and require the speaker's visibility, we exploit broader visual information from the environment. This allows our model to naturally filter speech from noise and improve transcription, much like humans do in noisy scenarios. Our method re-purposes pretrained speech and visual encoders, linking them with multi-headed attention. This approach enables the transcription of speech and the prediction of noise labels in video inputs. We introduce a scalable pipeline to develop audio-visual datasets, where visual cues correlate to noise in the audio. We show significant improvements over existing audio-only models in noisy scenarios. Results also highlight that visual cues play a vital role in improved transcription accuracy."
      },
      {
        "id": "oai:arXiv.org:2406.19388v3",
        "title": "Taming Data and Transformers for Scalable Audio Generation",
        "link": "https://arxiv.org/abs/2406.19388",
        "author": "Moayed Haji-Ali, Willi Menapace, Aliaksandr Siarohin, Guha Balakrishnan, Sergey Tulyakov, Vicente Ordonez",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2406.19388v3 Announce Type: replace \nAbstract: The scalability of ambient sound generators is hindered by data scarcity, insufficient caption quality, and limited scalability in model architecture. This work addresses these challenges by advancing both data and model scaling. First, we propose an efficient and scalable dataset collection pipeline tailored for ambient audio generation, resulting in AutoReCap-XL, the largest ambient audio-text dataset with over 47 million clips. To provide high-quality textual annotations, we propose AutoCap, a high-quality automatic audio captioning model. By adopting a Q-Former module and leveraging audio metadata, AutoCap substantially enhances caption quality, reaching a CIDEr score of $83.2$, a $3.2\\%$ improvement over previous captioning models. Finally, we propose GenAu, a scalable transformer-based audio generation architecture that we scale up to 1.25B parameters. We demonstrate its benefits from data scaling with synthetic captions as well as model size scaling. When compared to baseline audio generators trained at similar size and data scale, GenAu obtains significant improvements of $4.7\\%$ in FAD score, $11.1\\%$ in IS, and $13.5\\%$ in CLAP score. Our code, model checkpoints, and dataset are publicly available."
      },
      {
        "id": "oai:arXiv.org:2412.11943v2",
        "title": "autrainer: A Modular and Extensible Deep Learning Toolkit for Computer Audition Tasks",
        "link": "https://arxiv.org/abs/2412.11943",
        "author": "Simon Rampp, Andreas Triantafyllopoulos, Manuel Milling, Bj\\\"orn W. Schuller",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.11943v2 Announce Type: replace \nAbstract: This work introduces the key operating principles for autrainer, our new deep learning training framework for computer audition tasks. autrainer is a PyTorch-based toolkit that allows for rapid, reproducible, and easily extensible training on a variety of different computer audition tasks. Concretely, autrainer offers low-code training and supports a wide range of neural networks as well as preprocessing routines. In this work, we present an overview of its inner workings and key capabilities."
      },
      {
        "id": "oai:arXiv.org:2412.21037v2",
        "title": "TangoFlux: Super Fast and Faithful Text to Audio Generation with Flow Matching and Clap-Ranked Preference Optimization",
        "link": "https://arxiv.org/abs/2412.21037",
        "author": "Chia-Yu Hung, Navonil Majumder, Zhifeng Kong, Ambuj Mehrish, Amir Ali Bagherzadeh, Chuan Li, Rafael Valle, Bryan Catanzaro, Soujanya Poria",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.21037v2 Announce Type: replace \nAbstract: We introduce TangoFlux, an efficient Text-to-Audio (TTA) generative model with 515M parameters, capable of generating up to 30 seconds of 44.1kHz audio in just 3.7 seconds on a single A40 GPU. A key challenge in aligning TTA models lies in the difficulty of creating preference pairs, as TTA lacks structured mechanisms like verifiable rewards or gold-standard answers available for Large Language Models (LLMs). To address this, we propose CLAP-Ranked Preference Optimization (CRPO), a novel framework that iteratively generates and optimizes preference data to enhance TTA alignment. We demonstrate that the audio preference dataset generated using CRPO outperforms existing alternatives. With this framework, TangoFlux achieves state-of-the-art performance across both objective and subjective benchmarks. We open source all code and models to support further research in TTA generation."
      },
      {
        "id": "oai:arXiv.org:2310.10803v3",
        "title": "SD-HuBERT: Sentence-Level Self-Distillation Induces Syllabic Organization in HuBERT",
        "link": "https://arxiv.org/abs/2310.10803",
        "author": "Cheol Jun Cho, Abdelrahman Mohamed, Shang-Wen Li, Alan W Black, Gopala K. Anumanchipalli",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2310.10803v3 Announce Type: replace-cross \nAbstract: Data-driven unit discovery in self-supervised learning (SSL) of speech has embarked on a new era of spoken language processing. Yet, the discovered units often remain in phonetic space and the units beyond phonemes are largely underexplored. Here, we demonstrate that a syllabic organization emerges in learning sentence-level representation of speech. In particular, we adopt \"self-distillation\" objective to fine-tune the pretrained HuBERT with an aggregator token that summarizes the entire sentence. Without any supervision, the resulting model draws definite boundaries in speech, and the representations across frames exhibit salient syllabic structures. We demonstrate that this emergent structure largely corresponds to the ground truth syllables. Furthermore, we propose a new benchmark task, Spoken Speech ABX, for evaluating sentence-level representation of speech. When compared to previous models, our model outperforms in both unsupervised syllable discovery and learning sentence-level representation. Together, we demonstrate that the self-distillation of HuBERT gives rise to syllabic organization without relying on external labels or modalities, and potentially provides novel data-driven units for spoken language modeling."
      },
      {
        "id": "oai:arXiv.org:2411.10034v2",
        "title": "EveGuard: Defeating Vibration-based Side-Channel Eavesdropping with Audio Adversarial Perturbations",
        "link": "https://arxiv.org/abs/2411.10034",
        "author": "Jung-Woo Chang, Ke Sun, David Xia, Xinyu Zhang, Farinaz Koushanfar",
        "published": "Fri, 11 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.10034v2 Announce Type: replace-cross \nAbstract: Vibrometry-based side channels pose a significant privacy risk, exploiting sensors like mmWave radars, light sensors, and accelerometers to detect vibrations from sound sources or proximate objects, enabling speech eavesdropping. Despite various proposed defenses, these involve costly hardware solutions with inherent physical limitations. This paper presents EveGuard, a software-driven defense framework that creates adversarial audio, protecting voice privacy from side channels without compromising human perception. We leverage the distinct sensing capabilities of side channels and traditional microphones, where side channels capture vibrations and microphones record changes in air pressure, resulting in different frequency responses. EveGuard first proposes a perturbation generator model (PGM) that effectively suppresses sensor-based eavesdropping while maintaining high audio quality. Second, to enable end-to-end training of PGM, we introduce a new domain translation task called Eve-GAN for inferring an eavesdropped signal from a given audio. We further apply few-shot learning to mitigate the data collection overhead for Eve-GAN training. Our extensive experiments show that EveGuard achieves a protection rate of more than 97 percent from audio classifiers and significantly hinders eavesdropped audio reconstruction. We further validate the performance of EveGuard across three adaptive attack mechanisms. We have conducted a user study to verify the perceptual quality of our perturbed audio."
      }
    ]
  }
}